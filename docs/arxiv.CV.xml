<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>

<item>
<title>V-Agent: An Interactive Video Search System Using Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16925</link>
<guid>https://arxiv.org/abs/2512.16925</guid>
<content:encoded><![CDATA[
<div> Keywords: V-Agent, vision-language model, video search, multimodal retrieval, MultiVENT 2.0  

<br><br>Summary:  
This paper presents V-Agent, a novel multi-agent system designed to enhance advanced video search and interactive user conversations. The core innovation lies in fine-tuning a vision-language model (VLM) using a small video preference dataset, which, combined with a retrieval vector from an image-text retrieval model, surpasses traditional text-based retrieval methods in handling multimodal data. V-Agent’s retrieval model independently encodes video frames and audio transcriptions generated by an automatic speech recognition (ASR) module into a unified multimodal representation space, enabling the system to understand both visual and spoken content for more context-aware video searching. The framework is composed of three collaborative agents: a routing agent that directs user intents, a search agent leveraging the enhanced VLM-based retrieval along with a re-ranking module to improve the search output quality, and a chat agent that interacts with users to refine results and address queries. Experimental results demonstrate that V-Agent achieves state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, showcasing its effectiveness and potential applicability in both research and practical multimedia retrieval scenarios. <div>
arXiv:2512.16925v1 Announce Type: new 
Abstract: We introduce V-Agent, a novel multi-agent platform designed for advanced video search and interactive user-system conversations. By fine-tuning a vision-language model (VLM) with a small video preference dataset and enhancing it with a retrieval vector from an image-text retrieval model, we overcome the limitations of traditional text-based retrieval systems in multimodal scenarios. The VLM-based retrieval model independently embeds video frames and audio transcriptions from an automatic speech recognition (ASR) module into a shared multimodal representation space, enabling V-Agent to interpret both visual and spoken content for context-aware video search. This system consists of three agents-a routing agent, a search agent, and a chat agent-that work collaboratively to address user intents by refining search outputs and communicating with users. The search agent utilizes the VLM-based retrieval model together with an additional re-ranking module to further enhance video retrieval quality. Our proposed framework demonstrates state-of-the-art zero-shot performance on the MultiVENT 2.0 benchmark, highlighting its potential for both academic research and real-world applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of deep learning models: CNN and VGG-16 in identifying pornographic content</title>
<link>https://arxiv.org/abs/2512.16947</link>
<guid>https://arxiv.org/abs/2512.16947</guid>
<content:encoded><![CDATA[
<div> Keywords: pornographic content detection, CNN, VGG-16, deep learning, Indonesia website blocking<br><br>Summary:<br><br>1. In 2020, the Indonesian government blocked 59,741 websites for containing negative content, including 14,266 identified as pornographic, but these sites remained accessible via VPNs.<br><br>2. This accessibility issue motivated the development of a fast and accurate system to identify pornographic website content automatically.<br><br>3. The study employed deep learning techniques, specifically comparing a custom convolutional neural network (CNN) model with the Visual Geometry Group 16 (VGG-16) pre-trained model, to detect pornographic images.<br><br>4. Both models were comprehensively evaluated to determine which was more effective for rapid and accurate content identification.<br><br>5. Experimental results demonstrated that the CNN model outperformed VGG-16, achieving the highest accuracy of 94.87% at 50 epochs and a learning rate of 0.001, indicating CNN’s superior capability for quick and precise pornographic content detection.<br><br>6. The findings suggest that implementing CNN-based detectors could significantly aid in controlling access to prohibited content on the internet, enhancing content filtering efforts in Indonesia and similar contexts. <div>
arXiv:2512.16947v1 Announce Type: new 
Abstract: In 2020, a total of 59,741 websites were blocked by the Indonesian government due to containing negative content, including pornography, with 14,266 websites falling into this category. However, these blocked websites could still be accessed by the public using virtual private networks (VPNs). This prompted the research idea to quickly identify pornographic content. This study aims to develop a system capable of identifying websites suspected of containing pornographic image content, using a deep learning approach with convolutional neural network (CNN) and visual geometry group 16 (VGG-16) model. The two models were then explored comprehensively and holistically to determine which model was most effective in detecting pornographic content quickly. Based on the findings of the comparison between testing the CNN and VGG-16 models, research results showed that the best test results were obtained in the eighth experiment using the CNN model at an epoch value level of 50 and a learning rate of 0.001 of 0.9487 or 94.87%. This can be interpreted that the CNN model is more effective in detecting pornographic content quickly and accurately compared to using the VGG-16 model.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVM: Towards Structure-Preserving Neural Response Modeling in the Visual Cortex Across Stimuli and Individuals</title>
<link>https://arxiv.org/abs/2512.16948</link>
<guid>https://arxiv.org/abs/2512.16948</guid>
<content:encoded><![CDATA[
<div> Keywords: Adaptive Visual Model, Vision Transformer, neural response, cross-dataset adaptation, cortical modeling<br><br>Summary:<br><br>1. Deep learning models for neural response simulation often struggle to differentiate between stable visual encoding and condition-specific adaptation, limiting their generalization across different stimuli and subjects.<br><br>2. The Adaptive Visual Model (AVM) is introduced as a framework that preserves core visual representations while enabling condition-aware adaptation through modular subnetworks, maintaining a frozen Vision Transformer encoder.<br><br>3. AVM's modular design includes independently trained modulation paths that capture neural response variability due to stimulus content and subject identity without altering the primary encoder.<br><br>4. The model was evaluated under three experimental paradigms: stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all reflecting structured changes in inputs and individuals.<br><br>5. Tested on two large-scale mouse V1 datasets, AVM outperformed the state-of-the-art V1T model by roughly 2% in predictive correlation, achieved a 9.1% gain in explained variance (FEVE) during cross-dataset adaptation, demonstrating robust generalization, interpretable condition-specific modulation, and architectural efficiency.<br><br>6. The AVM framework offers a unified and scalable approach for adaptive neural modeling across biological and experimental conditions, potentially guiding future cortical modeling in neuroscience and biologically inspired AI systems. <div>
arXiv:2512.16948v1 Announce Type: new 
Abstract: While deep learning models have shown strong performance in simulating neural responses, they often fail to clearly separate stable visual encoding from condition-specific adaptation, which limits their ability to generalize across stimuli and individuals. We introduce the Adaptive Visual Model (AVM), a structure-preserving framework that enables condition-aware adaptation through modular subnetworks, without modifying the core representation. AVM keeps a Vision Transformer-based encoder frozen to capture consistent visual features, while independently trained modulation paths account for neural response variations driven by stimulus content and subject identity. We evaluate AVM in three experimental settings, including stimulus-level variation, cross-subject generalization, and cross-dataset adaptation, all of which involve structured changes in inputs and individuals. Across two large-scale mouse V1 datasets, AVM outperforms the state-of-the-art V1T model by approximately 2% in predictive correlation, demonstrating robust generalization, interpretable condition-wise modulation, and high architectural efficiency. Specifically, AVM achieves a 9.1% improvement in explained variance (FEVE) under the cross-dataset adaptation setting. These results suggest that AVM provides a unified framework for adaptive neural modeling across biological and experimental conditions, offering a scalable solution under structural constraints. Its design may inform future approaches to cortical modeling in both neuroscience and biologically inspired AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Tree Species Classification: Insights from YOLOv8 and Explainable AI Applied to TLS Point Cloud Projections</title>
<link>https://arxiv.org/abs/2512.16950</link>
<guid>https://arxiv.org/abs/2512.16950</guid>
<content:encoded><![CDATA[
<div> Tree species classification, Terrestrial Laser Scanning (TLS), YOLOv8, Finer-CAM, deep learning<br><br>Summary:<br><br>1. The study addresses the challenge of classifying tree species using Terrestrial Laser Scanning (TLS) data combined with deep learning techniques, specifically focusing on the interpretability of classification models. <br><br>2. Researchers trained five YOLOv8 models on TLS data from 2,445 trees representing seven European species, achieving a high mean classification accuracy of 96% with low variance. <br><br>3. They proposed a novel method linking Finer-CAM (Class Activation Mapping) explanations to TLS projection segments, allowing systematic identification of which tree structural features drive species discrimination. <br><br>4. Analysis of 630 saliency maps revealed that model decisions predominantly rely on crown features for most species such as Silver Birch, European Beech, English Oak, and Norway Spruce, while stem and finer branch features contribute more to classification of European Ash, Scots Pine, and Douglas Fir. <br><br>5. The model’s notion of species similarity aligned well with expert human classification, highlighting the importance of model interpretability to better understand data biases, model limitations, and to increase confidence in automated tree species identification. <div>
arXiv:2512.16950v1 Announce Type: new 
Abstract: Classifying tree species has been a core research area in forest remote sensing for decades. New sensors and classification approaches like TLS and deep learning achieve state-of-the art accuracy but their decision processes remain unclear. Methods such as Finer-CAM (Class Activation Mapping) can highlight features in TLS projections that contribute to the classification of a target species, yet are uncommon in similar looking contrastive tree species. We propose a novel method linking Finer-CAM explanations to segments of TLS projections representing structural tree features to systemically evaluate which features drive species discrimination. Using TLS data from 2,445 trees across seven European tree species, we trained and validated five YOLOv8 models with cross-validation, reaching a mean accuracy of 96% (SD = 0.24%). Analysis of 630 saliency maps shows the models primarily rely on crown features in TLS projections for species classification. While this result is pronounced in Silver Birch, European Beech, English oak, and Norway spruce, stem features contribute more frequently to the differentiation of European ash, Scots pine, and Douglas fir. Particularly representations of finer branches contribute to the decisions of the models. The models consider those tree species similar to each other which a human expert would also regard as similar. Furthermore, our results highlight the need for an improved understanding of the decision processes of tree species classification models to help reveal data set and model limitations, biases, and to build confidence in model predictions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lights, Camera, Consistency: A Multistage Pipeline for Character-Stable AI Video Stories</title>
<link>https://arxiv.org/abs/2512.16954</link>
<guid>https://arxiv.org/abs/2512.16954</guid>
<content:encoded><![CDATA[
<div> Keywords: video generation, character consistency, large language model, text-to-image, cultural bias

<br><br>Summary: This paper addresses the challenge of generating long and cohesive video stories with consistent characters using text-to-video AI. The authors propose a novel multi-stage pipeline that mimics the filmmaking process. First, a large language model generates a detailed production script that guides the overall narrative. Next, a text-to-image model uses this script to create consistent visual representations of each character, which act as visual anchors. Finally, these visuals guide a video generation model to synthesize individual scenes, enhancing character identity preservation. Comparative experiments demonstrate the critical importance of the visual anchoring mechanism; eliminating this step causes a drastic drop in character consistency scores, confirming visual priors are essential. Additionally, the study investigates cultural differences in current video generation models, identifying biases related to subject consistency and dynamic expression when comparing Indian-themed versus Western-themed video outputs. The findings highlight the need for culturally aware and multi-step approaches to improve identity consistency and reliability in long video story generation. <div>
arXiv:2512.16954v1 Announce Type: new 
Abstract: Generating long, cohesive video stories with consistent characters is a significant challenge for current text-to-video AI. We introduce a method that approaches video generation in a filmmaker-like manner. Instead of creating a video in one step, our proposed pipeline first uses a large language model to generate a detailed production script. This script guides a text-to-image model in creating consistent visuals for each character, which then serve as anchors for a video generation model to synthesize each scene individually. Our baseline comparisons validate the necessity of this multi-stage decomposition; specifically, we observe that removing the visual anchoring mechanism results in a catastrophic drop in character consistency scores (from 7.99 to 0.55), confirming that visual priors are essential for identity preservation. Furthermore, we analyze cultural disparities in current models, revealing distinct biases in subject consistency and dynamic degree between Indian vs Western-themed generations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfoTok: Adaptive Discrete Video Tokenizer via Information-Theoretic Compression</title>
<link>https://arxiv.org/abs/2512.16975</link>
<guid>https://arxiv.org/abs/2512.16975</guid>
<content:encoded><![CDATA[
<div> Keywords: video tokenization, adaptive compression, information theory, transformer, ELBO algorithm<br><br>Summary:<br><br>1. The paper addresses the challenge of discrete video tokenization for long video sequences, emphasizing the need for accurate and efficient compression methods due to varying information density in video content. 2. Current video tokenizers compress all video content at a fixed rate, which causes redundancy or loss of important information. 3. Inspired by Shannon's information theory, the authors introduce InfoTok, a new framework for adaptive video tokenization that dynamically adjusts token allocation based on informational richness. 4. The paper proves that existing data-agnostic training methods result in suboptimal representation lengths and proposes an evidence lower bound (ELBO)-based algorithm that approaches optimal compression. 5. Leveraging this theoretical framework, the authors develop a transformer-based adaptive compressor that significantly improves compression efficiency. Empirical results show InfoTok saves 20% of tokens without degrading performance and achieves a 2.3x compression rate while outperforming heuristic adaptive approaches. 6. InfoTok’s approach optimally allocates tokens according to the information content of video segments, enabling more compact yet accurate video representations and offering valuable insights for future video understanding and compression research. <div>
arXiv:2512.16975v1 Announce Type: new 
Abstract: Accurate and efficient discrete video tokenization is essential for long video sequences processing. Yet, the inherent complexity and variable information density of videos present a significant bottleneck for current tokenizers, which rigidly compress all content at a fixed rate, leading to redundancy or information loss. Drawing inspiration from Shannon's information theory, this paper introduces InfoTok, a principled framework for adaptive video tokenization. We rigorously prove that existing data-agnostic training methods are suboptimal in representation length, and present a novel evidence lower bound (ELBO)-based algorithm that approaches theoretical optimality. Leveraging this framework, we develop a transformer-based adaptive compressor that enables adaptive tokenization. Empirical results demonstrate state-of-the-art compression performance, saving 20% tokens without influence on performance, and achieving 2.3x compression rates while still outperforming prior heuristic adaptive approaches. By allocating tokens according to informational richness, InfoTok enables a more compressed yet accurate tokenization for video representation, offering valuable insights for future research.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endo-SemiS: Towards Robust Semi-Supervised Image Segmentation for Endoscopic Video</title>
<link>https://arxiv.org/abs/2512.16977</link>
<guid>https://arxiv.org/abs/2512.16977</guid>
<content:encoded><![CDATA[
<div> Endoscopic video segmentation, Semi-supervised learning, Pseudo-labeling, Mutual learning, Spatiotemporal correction<br><br>Summary:<br><br>This paper introduces Endo-SemiS, a semi-supervised framework designed for accurate segmentation of endoscopic video frames using limited annotated data. The method incorporates four core strategies to leverage both labeled and unlabeled data effectively: (1) Cross-supervision between two individual networks, where each network supervises the other to enhance learning; (2) Generation of uncertainty-guided pseudo-labels by selecting high-confidence regions in unlabeled data to ensure label quality; (3) Joint pseudo-label supervision, which combines reliable pixel information from both networks' pseudo-labels, providing precise supervision for unlabeled samples; and (4) Mutual learning, enabling both networks to learn collaboratively at feature and image levels, minimizing variance and encouraging consistency. Furthermore, a separate corrective network utilizes spatiotemporal information from the video context to further refine segmentation results. The approach is validated on two clinical tasks: kidney stone laser lithotomy segmentation via ureteroscopy and polyp detection through colonoscopy. Comparative experiments demonstrate that Endo-SemiS outperforms current state-of-the-art segmentation methods, particularly when labeled data are scarce. The paper also provides publicly accessible code, promoting transparency and facilitating future research. <div>
arXiv:2512.16977v1 Announce Type: new 
Abstract: In this paper, we present Endo-SemiS, a semi-supervised segmentation framework for providing reliable segmentation of endoscopic video frames with limited annotation. EndoSemiS uses 4 strategies to improve performance by effectively utilizing all available data, particularly unlabeled data: (1) Cross-supervision between two individual networks that supervise each other; (2) Uncertainty-guided pseudo-labels from unlabeled data, which are generated by selecting high-confidence regions to improve their quality; (3) Joint pseudolabel supervision, which aggregates reliable pixels from the pseudo-labels of both networks to provide accurate supervision for unlabeled data; and (4) Mutual learning, where both networks learn from each other at the feature and image levels, reducing variance and guiding them toward a consistent solution. Additionally, a separate corrective network that utilizes spatiotemporal information from endoscopy video to improve segmentation performance. Endo-SemiS is evaluated on two clinical applications: kidney stone laser lithotomy from ureteroscopy and polyp screening from colonoscopy. Compared to state-of-the-art segmentation methods, Endo-SemiS substantially achieves superior results on both datasets with limited labeled data. The code is publicly available at https://github.com/MedICL-VU/Endo-SemiS
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark and Agentic Framework for Omni-Modal Reasoning and Tool Use in Long Videos</title>
<link>https://arxiv.org/abs/2512.16978</link>
<guid>https://arxiv.org/abs/2512.16978</guid>
<content:encoded><![CDATA[
<div> Long-form video understanding, Multimodal reasoning, Benchmark, Agentic system, Evaluation rubric<br><br>Summary:<br><br>1. The paper introduces LongShOTBench, a new diagnostic benchmark designed for long-form multimodal video understanding that integrates vision, speech, and ambient audio while requiring coherent long-range reasoning. <br><br>2. LongShOTBench addresses limitations of existing benchmarks by combining temporal length and multimodal richness with open-ended, intent-driven questions, single- and multi-turn dialogues, and tasks demanding multimodal reasoning and agentic tool use across video, audio, and speech. <br><br>3. Every sample in LongShOTBench includes a reference answer and a graded rubric for interpretable and traceable evaluation, ensuring coverage and reproducibility through a scalable, human-validated pipeline with human verification and corrections. <br><br>4. The authors also present LongShOTAgent, an agentic system designed to analyze long videos using preprocessing, search, and iterative refinement methods. <br><br>5. Evaluation results on LongShOTBench reveal significant gaps in the performance of state-of-the-art multimodal large language models (MLLMs): Gemini-2.5-Flash achieves 52.95% accuracy, open-source models fall below 30%, and LongShOTAgent attains 44.66%, highlighting the challenge of real-world long-form video understanding. <br><br>6. LongShOTBench offers a practical and reproducible foundation for the evaluation and improvement of MLLMs, with all related resources made publicly available on GitHub. <div>
arXiv:2512.16978v1 Announce Type: new 
Abstract: Long-form multimodal video understanding requires integrating vision, speech, and ambient audio with coherent long-range reasoning. Existing benchmarks emphasize either temporal length or multimodal richness, but rarely both and while some incorporate open-ended questions and advanced metrics, they mostly rely on single-score accuracy, obscuring failure modes. We introduce LongShOTBench, a diagnostic benchmark with open-ended, intent-driven questions; single- and multi-turn dialogues; and tasks requiring multimodal reasoning and agentic tool use across video, audio, and speech. Each item includes a reference answer and graded rubric for interpretable, and traceable evaluation. LongShOTBench is produced via a scalable, human-validated pipeline to ensure coverage and reproducibility. All samples in our LongShOTBench are human-verified and corrected. Furthermore, we present LongShOTAgent, an agentic system that analyzes long videos via preprocessing, search, and iterative refinement. On LongShOTBench, state-of-the-art MLLMs show large gaps: Gemini-2.5-Flash achieves 52.95%, open-source models remain below 30%, and LongShOTAgent attains 44.66%. These results underscore the difficulty of real-world long-form video understanding. LongShOTBench provides a practical, reproducible foundation for evaluating and improving MLLMs. All resources are available on GitHub: https://github.com/mbzuai-oryx/longshot.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D-RGPT: Toward Region-level 4D Understanding via Perceptual Distillation</title>
<link>https://arxiv.org/abs/2512.17012</link>
<guid>https://arxiv.org/abs/2512.17012</guid>
<content:encoded><![CDATA[
<div> 4D-RGPT, Multimodal LLM, 4D perception, Video Question Answering, region-level prompting<br><br>Summary:<br><br>This paper addresses limitations in current Multimodal Large Language Models (MLLMs) regarding their reasoning over 3D structures and temporal dynamics, citing weak 4D perception and temporal understanding as key challenges. To overcome these issues, the authors introduce 4D-RGPT, a specialized MLLM architecture designed to capture and process 4D (spatial plus temporal) representations from video inputs, thereby enhancing temporal perception capabilities. Alongside this model, they propose Perceptual 4D Distillation (P4D), a novel training framework that transfers rich 4D representations from a frozen expert model into 4D-RGPT, aimed at fostering comprehensive 4D perceptual learning. Additionally, to evaluate and benchmark performance in this domain more effectively, the authors develop R4D-Bench, a new dataset featuring depth-aware dynamic scenes with region-level prompting. This benchmark is constructed through a hybrid automated and human-verified process, addressing the shortcomings of previous VQA datasets that focused largely on static scenes and lacked fine-grained regional queries. Experimental results demonstrate that 4D-RGPT significantly outperforms existing methods on both established 4D VQA benchmarks and the newly introduced R4D-Bench, validating the benefits of their approach in advancing 4D video understanding and question answering. <div>
arXiv:2512.17012v1 Announce Type: new 
Abstract: Despite advances in Multimodal LLMs (MLLMs), their ability to reason over 3D structures and temporal dynamics remains limited, constrained by weak 4D perception and temporal understanding. Existing 3D and 4D Video Question Answering (VQA) benchmarks also emphasize static scenes and lack region-level prompting. We tackle these issues by introducing: (a) 4D-RGPT, a specialized MLLM designed to capture 4D representations from video inputs with enhanced temporal perception; (b) Perceptual 4D Distillation (P4D), a training framework that transfers 4D representations from a frozen expert model into 4D-RGPT for comprehensive 4D perception; and (c) R4D-Bench, a benchmark for depth-aware dynamic scenes with region-level prompting, built via a hybrid automated and human-verified pipeline. Our 4D-RGPT achieves notable improvements on both existing 4D VQA benchmarks and the proposed R4D-Bench benchmark.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FORMSpoT: A Decade of Tree-Level, Country-Scale Forest Monitoring</title>
<link>https://arxiv.org/abs/2512.17021</link>
<guid>https://arxiv.org/abs/2512.17021</guid>
<content:encoded><![CDATA[
<div> Keywords: forest monitoring, canopy height, SPOT satellites, disturbance detection, climate change  

<br><br>Summary:  
1. The study addresses the decline of the European forest carbon sink by developing advanced forest monitoring tools with high spatial resolution.  
2. Existing satellite disturbance products have limitations in detecting changes below 100 m², typically missing individual tree-level disturbances.  
3. FORMSpoT, introduced in this work, is a nationwide forest canopy height mapping over France from 2014 to 2024, using 1.5 m resolution data from SPOT-6/7 satellite composites.  
4. Canopy heights were estimated via a hierarchical transformer model (PVTv2) trained on airborne laser scanning (ALS) data, ensuring high accuracy.  
5. The authors developed a post-processing pipeline incorporating co-registration and spatio-temporal total variation denoising to robustly detect changes across heterogeneous satellite acquisitions.  
6. Validation against ALS revisits and National Forest Inventory plots demonstrates that FORMSpoT-Δ outperforms existing disturbance products, achieving notably higher F1-scores, especially in fragmented mountainous forests.  
7. This tool enables tree-level monitoring across a national scale, allowing analysis of forest management, early detection of decline, and improved carbon loss quantification from subtle disturbances like thinning or selective logging.  
8. The study highlights the importance of sustaining very high-resolution satellite missions such as SPOT and open-data initiatives like DINAMIS for effective forest monitoring under climate change pressures. <div>
arXiv:2512.17021v1 Announce Type: new 
Abstract: The recent decline of the European forest carbon sink highlights the need for spatially explicit and frequently updated forest monitoring tools. Yet, existing satellite-based disturbance products remain too coarse to detect changes at the scale of individual trees, typically below 100 m$^{2}$. Here, we introduce FORMSpoT (Forest Mapping with SPOT Time series), a decade-long (2014-2024) nationwide mapping of forest canopy height at 1.5 m resolution, together with annual disturbance polygons (FORMSpoT-$\Delta$) covering mainland France. Canopy heights were derived from annual SPOT-6/7 composites using a hierarchical transformer model (PVTv2) trained on high-resolution airborne laser scanning (ALS) data. To enable robust change detection across heterogeneous acquisitions, we developed a dedicated post-processing pipeline combining co-registration and spatio-temporal total variation denoising. Validation against ALS revisits across 19 sites and 5,087 National Forest Inventory plots shows that FORMSpoT-$\Delta$ substantially outperforms existing disturbance products. In mountainous forests, where disturbances are small and spatially fragmented, FORMSpoT-$\Delta$ achieves an F1-score of 0.44, representing an order of magnitude higher than existing benchmarks. By enabling tree-level monitoring of forest dynamics at national scale, FORMSpoT-$\Delta$ provides a unique tool to analyze management practices, detect early signals of forest decline, and better quantify carbon losses from subtle disturbances such as thinning or selective logging. These results underscore the critical importance of sustaining very high-resolution satellite missions like SPOT and open-data initiatives such as DINAMIS for monitoring forests under climate change.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infinite-Homography as Robust Conditioning for Camera-Controlled Video Generation</title>
<link>https://arxiv.org/abs/2512.17040</link>
<guid>https://arxiv.org/abs/2512.17040</guid>
<content:encoded><![CDATA[
<div> Keywords: video diffusion models, camera-controlled video generation, pose fidelity, infinite homography warping, data augmentation<br><br>Summary:<br><br>This paper addresses the challenge of generating camera-controlled novel-view videos of dynamic scenes that maintain high fidelity to specified camera poses and ensure view consistency despite limited input observations. Existing approaches either rely on trajectory-conditioned video generation models trained on limited trajectory-video datasets or use depth estimation to reproject video frames along new trajectories. However, depth-based reprojection is prone to errors due to inaccurate depth maps, and existing datasets lack sufficient diversity in camera trajectories, limiting model generalization. To overcome these issues, the authors propose InfCam, a depth-free video-to-video generation framework that achieves high camera-pose accuracy. InfCam introduces infinite homography warping, which encodes 3D camera rotations directly into the 2D latent space of a video diffusion model, enabling noise-free rotational conditioning. The residual parallax is predicted via end-to-end training, enhancing pose fidelity without requiring explicit depth information. Furthermore, a novel data augmentation pipeline expands synthetic multiview datasets by simulating diverse trajectories and focal lengths, improving model robustness. Experimental evaluations show that InfCam surpasses baseline methods in both camera-pose accuracy and visual quality, and generalizes effectively from synthetic data to real-world videos, advancing the state-of-the-art in camera-controlled novel-view video generation. <div>
arXiv:2512.17040v1 Announce Type: new 
Abstract: Recent progress in video diffusion models has spurred growing interest in camera-controlled novel-view video generation for dynamic scenes, aiming to provide creators with cinematic camera control capabilities in post-production. A key challenge in camera-controlled video generation is ensuring fidelity to the specified camera pose, while maintaining view consistency and reasoning about occluded geometry from limited observations. To address this, existing methods either train trajectory-conditioned video generation model on trajectory-video pair dataset, or estimate depth from the input video to reproject it along a target trajectory and generate the unprojected regions. Nevertheless, existing methods struggle to generate camera-pose-faithful, high-quality videos for two main reasons: (1) reprojection-based approaches are highly susceptible to errors caused by inaccurate depth estimation; and (2) the limited diversity of camera trajectories in existing datasets restricts learned models. To address these limitations, we present InfCam, a depth-free, camera-controlled video-to-video generation framework with high pose fidelity. The framework integrates two key components: (1) infinite homography warping, which encodes 3D camera rotations directly within the 2D latent space of a video diffusion model. Conditioning on this noise-free rotational information, the residual parallax term is predicted through end-to-end training to achieve high camera-pose fidelity; and (2) a data augmentation pipeline that transforms existing synthetic multiview datasets into sequences with diverse trajectories and focal lengths. Experimental results demonstrate that InfCam outperforms baseline methods in camera-pose accuracy and visual fidelity, generalizing well from synthetic to real-world data. Link to our project page:https://emjay73.github.io/InfCam/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Similarity of Synthetic Image Utility</title>
<link>https://arxiv.org/abs/2512.17080</link>
<guid>https://arxiv.org/abs/2512.17080</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic medical images, deep learning, clinical decision support, interpretable utility similarity, image dataset evaluation<br><br>Summary: This paper addresses the challenge of quantitatively assessing the similarity between synthetic and real medical image datasets in the context of deep learning-based clinical decision support (CDS) systems. The authors propose a new metric called Interpretable Utility Similarity (IUS), which is inspired by generalized neural additive models and designed to be interpretable, unlike common inception-based measures. IUS evaluates how useful a synthetic dataset is compared to real data for training CDS systems, focusing on clinically relevant image features. Experiments were conducted on public datasets spanning various color medical imaging modalities, including endoscopic, dermoscopic, and fundus images, demonstrating that selecting synthetic images with high IUS scores can lead to classification performance improvements of up to 54.6%. Furthermore, the authors show that IUS generalizes well to grayscale imaging modalities such as X-rays and ultrasound, supporting its broad applicability. The study thereby provides a novel, interpretable, and effective approach to benchmark synthetic medical image datasets for training DL models in clinical applications. The implementation of IUS is made publicly available on GitHub, encouraging further research and practical use. <div>
arXiv:2512.17080v1 Announce Type: new 
Abstract: Synthetic medical image data can unlock the potential of deep learning (DL)-based clinical decision support (CDS) systems through the creation of large scale, privacy-preserving, training sets. Despite the significant progress in this field, there is still a largely unanswered research question: "How can we quantitatively assess the similarity of a synthetically generated set of images with a set of real images in a given application domain?". Today, answers to this question are mainly provided via user evaluation studies, inception-based measures, and the classification performance achieved on synthetic images. This paper proposes a novel measure to assess the similarity between synthetically generated and real sets of images, in terms of their utility for the development of DL-based CDS systems. Inspired by generalized neural additive models, and unlike inception-based measures, the proposed measure is interpretable (Interpretable Utility Similarity, IUS), explaining why a synthetic dataset could be more useful than another one in the context of a CDS system based on clinically relevant image features. The experimental results on publicly available datasets from various color medical imaging modalities including endoscopic, dermoscopic and fundus imaging, indicate that selecting synthetic images of high utility similarity using IUS can result in relative improvements of up to 54.6% in terms of classification performance. The generality of IUS for synthetic data assessment is demonstrated also for greyscale X-ray and ultrasound imaging modalities. IUS implementation is available at https://github.com/innoisys/ius
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGH: Dynamic Gaussian Hair</title>
<link>https://arxiv.org/abs/2512.17094</link>
<guid>https://arxiv.org/abs/2512.17094</guid>
<content:encoded><![CDATA[
<div> Keywords: Dynamic Hair, Gaussian Representation, Hair Dynamics, Differentiable Rendering, 3D Avatar  

<br><br>Summary:  
This paper introduces Dynamic Gaussian Hair (DGH), a novel approach for modeling photorealistic dynamic hair that addresses challenges such as complex motions, occlusions, and light scattering. (1) DGH employs a coarse-to-fine model designed to learn temporally coherent hair motion dynamics, effectively accommodating a wide variety of hairstyles. (2) The framework includes a strand-guided optimization module that learns a dynamic 3D Gaussian representation for hair appearance, supporting differentiable rendering which facilitates gradient-based learning for consistent view-dependent appearance during motion. Unlike traditional physics-based simulations that require extensive manual tuning and heavy computation, DGH is fully data-driven, scalable with training data, and generalizes well across diverse hair styles and head motions. Furthermore, DGH integrates seamlessly into existing 3D Gaussian avatar frameworks, enabling realistic and animatable hair for high-fidelity avatar representations. Experimental results show that DGH achieves competitive geometry and appearance quality, presenting an efficient and scalable alternative to simulation-based hair modeling and rendering techniques. This makes DGH a promising solution for the creation of dynamic, photorealistic hair in digital human modeling and virtual avatar applications. <div>
arXiv:2512.17094v1 Announce Type: new 
Abstract: The creation of photorealistic dynamic hair remains a major challenge in digital human modeling because of the complex motions, occlusions, and light scattering. Existing methods often resort to static capture and physics-based models that do not scale as they require manual parameter fine-tuning to handle the diversity of hairstyles and motions, and heavy computation to obtain high-quality appearance. In this paper, we present Dynamic Gaussian Hair (DGH), a novel framework that efficiently learns hair dynamics and appearance. We propose: (1) a coarse-to-fine model that learns temporally coherent hair motion dynamics across diverse hairstyles; (2) a strand-guided optimization module that learns a dynamic 3D Gaussian representation for hair appearance with support for differentiable rendering, enabling gradient-based learning of view-consistent appearance under motion. Unlike prior simulation-based pipelines, our approach is fully data-driven, scales with training data, and generalizes across various hairstyles and head motion sequences. Additionally, DGH can be seamlessly integrated into a 3D Gaussian avatar framework, enabling realistic, animatable hair for high-fidelity avatar representation. DGH achieves promising geometry and appearance results, providing a scalable, data-driven alternative to physics-based simulation and rendering.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of Maritime Radar Data Using Transformer Architecture</title>
<link>https://arxiv.org/abs/2512.17098</link>
<guid>https://arxiv.org/abs/2512.17098</guid>
<content:encoded><![CDATA[
<div> Keywords: maritime autonomous systems, transformer architectures, radar frame prediction, spatiotemporal forecasting, trajectory prediction  

<br><br>Summary:  
1. Maritime autonomous systems need robust predictive models to forecast vessel movements and environmental dynamics for safer and more efficient navigation.  
2. Transformer architectures have made significant advancements in AIS-based trajectory prediction and sonar frame forecasting, showing great potential in maritime applications.  
3. Despite these successes, transformer-based prediction models have not yet been applied to maritime radar frame forecasting, which is crucial due to radar's reliability under varying weather conditions.  
4. This survey presents a systematic review of existing predictive modeling techniques applied to maritime radar data, focusing especially on transformer architectures used for spatiotemporal sequence forecasting.  
5. The analysis categorizes existing methods based on the type of data, model architecture, and prediction horizons, highlighting that no current studies address transformer-based radar frame prediction.  
6. The identified research gap underlines the urgent need to explore transformer models for maritime radar data, suggesting a promising future direction for improving autonomous maritime navigation. <div>
arXiv:2512.17098v1 Announce Type: new 
Abstract: Maritime autonomous systems require robust predictive capabilities to anticipate vessel motion and environmental dynamics. While transformer architectures have revolutionized AIS-based trajectory prediction and demonstrated feasibility for sonar frame forecasting, their application to maritime radar frame prediction remains unexplored, creating a critical gap given radar's all-weather reliability for navigation. This survey systematically reviews predictive modeling approaches relevant to maritime radar, with emphasis on transformer architectures for spatiotemporal sequence forecasting, where existing representative methods are analyzed according to data type, architecture, and prediction horizon. Our review shows that, while the literature has demonstrated transformer-based frame prediction for sonar sensing, no prior work addresses transformer-based maritime radar frame prediction, thereby defining a clear research gap and motivating a concrete research direction for future work in this area.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDUM: A Scalable Deep Unrolled Model for Universal MRI Reconstruction</title>
<link>https://arxiv.org/abs/2512.17137</link>
<guid>https://arxiv.org/abs/2512.17137</guid>
<content:encoded><![CDATA[
<div> Clinical MRI, deep learning reconstruction, scalable model, coil sensitivity estimation, data consistency<br><br>Summary: This work introduces the Scalable Deep Unrolled Model (SDUM), a universal deep learning framework designed to address the diversity of clinical MRI protocols, including various anatomical targets, contrast types, sampling patterns, and acceleration factors. Unlike existing methods that are protocol-specific, SDUM integrates several key components: a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME) applied per cascade, sampling-aware weighted data consistency (SWDC), universal conditioning (UC) using cascade index and protocol metadata, and a progressive cascade expansion training approach. The model shows foundation-model-like scaling behavior where reconstruction quality measured by PSNR improves predictably logarithmically with the number of parameters and cascades, achieving high correlation (r=0.986) up to 18 cascades. A single SDUM model trained on heterogeneous datasets achieves state-of-the-art results across multiple challenging MRI reconstruction benchmarks, including the 2025 multi-center, multi-disease, 5T, and pediatric tracks, outperforming task-specific baselines by up to +1.0 dB. It also surpasses top entries from other major challenges like CMRxRecon2024 and fastMRI brain by substantial margins. Ablation studies confirm the performance gains contributed by each component: SWDC (+0.43 dB), per-cascade CSME (+0.51 dB), and UC (+0.38 dB). Overall, SDUM paves the way for a versatile, scalable, and practical universal MRI reconstruction solution. <div>
arXiv:2512.17137v1 Announce Type: new 
Abstract: Clinical MRI encompasses diverse imaging protocols--spanning anatomical targets (cardiac, brain, knee), contrasts (T1, T2, mapping), sampling patterns (Cartesian, radial, spiral, kt-space), and acceleration factors--yet current deep learning reconstructions are typically protocol-specific, hindering generalization and deployment. We introduce Scalable Deep Unrolled Model (SDUM), a universal framework combining a Restormer-based reconstructor, a learned coil sensitivity map estimator (CSME), sampling-aware weighted data consistency (SWDC), universal conditioning (UC) on cascade index and protocol metadata, and progressive cascade expansion training. SDUM exhibits foundation-model-like scaling behavior: reconstruction quality follows PSNR ${\sim}$ log(parameters) with correlation $r{=}0.986$ ($R^2{=}0.973$) up to 18 cascades, demonstrating predictable performance gains with model depth. A single SDUM trained on heterogeneous data achieves state-of-the-art results across all four CMRxRecon2025 challenge tracks--multi-center, multi-disease, 5T, and pediatric--without task-specific fine-tuning, surpassing specialized baselines by up to ${+}1.0$~dB. On CMRxRecon2024, SDUM outperforms the winning method PromptMR+ by ${+}0.55$~dB; on fastMRI brain, it exceeds PC-RNN by ${+}1.8$~dB. Ablations validate each component: SWDC ${+}0.43$~dB over standard DC, per-cascade CSME ${+}0.51$~dB, UC ${+}0.38$~dB. These results establish SDUM as a practical path toward universal, scalable MRI reconstruction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pro-Pose: Unpaired Full-Body Portrait Synthesis via Canonical UV Maps</title>
<link>https://arxiv.org/abs/2512.17143</link>
<guid>https://arxiv.org/abs/2512.17143</guid>
<content:encoded><![CDATA[
<div> Keywords: professional photography, canonical UV space, reposing, multi image finetuning, novel view synthesis<br><br>Summary:  
1) This paper addresses the challenge of transforming everyday photos of people ("in the wild") into professional-quality portraits characterized by good lighting, chosen poses, and standard clothing, while preserving the unique identity, face, and body features of the person.  
2) Since large paired datasets of the same individuals photographed under casual and professional conditions do not exist, the authors propose novel techniques that do not rely on such data.  
3) The first key insight is to transform the input photo and the person’s face into a canonical UV space, which facilitates modeling occlusions and enables novel view synthesis through reposing methodologies, leveraging existing unpaired datasets effectively.  
4) The second insight involves personalizing the generated output by performing multi-image finetuning, allowing the model to better capture individual-specific details for higher fidelity in the reposed portraits.  
5) Experimental results demonstrate that this approach generates high-quality, reposed professional-style portraits and achieves strong qualitative and quantitative performance on real-world imagery, outperforming previous methods. <div>
arXiv:2512.17143v1 Announce Type: new 
Abstract: Photographs of people taken by professional photographers typically present the person in beautiful lighting, with an interesting pose, and flattering quality. This is unlike common photos people can take of themselves. In this paper, we explore how to create a ``professional'' version of a person's photograph, i.e., in a chosen pose, in a simple environment, with good lighting, and standard black top/bottom clothing. A key challenge is to preserve the person's unique identity, face and body features while transforming the photo. If there would exist a large paired dataset of the same person photographed both ``in the wild'' and by a professional photographer, the problem would potentially be easier to solve. However, such data does not exist, especially for a large variety of identities. To that end, we propose two key insights: 1) Our method transforms the input photo and person's face to a canonical UV space, which is further coupled with reposing methodology to model occlusions and novel view synthesis. Operating in UV space allows us to leverage existing unpaired datasets. 2) We personalize the output photo via multi image finetuning. Our approach yields high-quality, reposed portraits and achieves strong qualitative and quantitative performance on real-world imagery.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Conditioned Background Generation for Editable Multi-Layer Documents</title>
<link>https://arxiv.org/abs/2512.17151</link>
<guid>https://arxiv.org/abs/2512.17151</guid>
<content:encoded><![CDATA[
<div> Keywords: document-centric background generation, latent masking, Automated Readability Optimization (ARO), multi-page consistency, multi-layer editing<br><br>Summary:  
The paper introduces a novel framework focused on document-centric background generation that supports multi-page editing with thematic continuity. It employs a latent masking technique in the diffusion space inspired by smooth barrier functions to softly limit updates, ensuring text regions remain readable. Automated Readability Optimization (ARO) is presented as a method to place semi-transparent, rounded shapes behind text, automatically adjusting opacity to meet WCAG 2.2 perceptual contrast standards for optimal readability without manual effort. To maintain multi-page consistency, the framework summarizes each page into compact representations that guide the generation of subsequent pages, mimicking human context retention for coherent visual motif evolution. The framework treats documents as structured compositions with separate layers for text, figures, and backgrounds, enabling targeted background editing without degrading text readability. Additionally, user prompts facilitate stylistic customization in color and texture, balancing automated consistency with flexibility. Notably, the entire system is training-free, producing visually coherent, text-preserving, and thematically aligned documents that integrate generative modeling with practical design workflows. <div>
arXiv:2512.17151v1 Announce Type: new 
Abstract: We present a framework for document-centric background generation with multi-page editing and thematic continuity. To ensure text regions remain readable, we employ a \emph{latent masking} formulation that softly attenuates updates in the diffusion space, inspired by smooth barrier functions in physics and numerical optimization. In addition, we introduce \emph{Automated Readability Optimization (ARO)}, which automatically places semi-transparent, rounded backing shapes behind text regions. ARO determines the minimal opacity needed to satisfy perceptual contrast standards (WCAG 2.2) relative to the underlying background, ensuring readability while maintaining aesthetic harmony without human intervention. Multi-page consistency is maintained through a summarization-and-instruction process, where each page is distilled into a compact representation that recursively guides subsequent generations. This design reflects how humans build continuity by retaining prior context, ensuring that visual motifs evolve coherently across an entire document. Our method further treats a document as a structured composition in which text, figures, and backgrounds are preserved or regenerated as separate layers, allowing targeted background editing without compromising readability. Finally, user-provided prompts allow stylistic adjustments in color and texture, balancing automated consistency with flexible customization. Our training-free framework produces visually coherent, text-preserving, and thematically aligned documents, bridging generative modeling with natural design workflows.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysFire-WM: A Physics-Informed World Model for Emulating Fire Spread Dynamics</title>
<link>https://arxiv.org/abs/2512.17152</link>
<guid>https://arxiv.org/abs/2512.17152</guid>
<content:encoded><![CDATA[
<div> Keywords: fire prediction, physics-informed model, world model, cross-task training, fire spread dynamics<br><br>Summary:<br><br>1. Fine-grained fire prediction is critical for effective emergency response, but current methods mainly rely on binary fire masks that lack sufficient detail to capture complex fire dynamics. 2. Infrared images and fire masks provide complementary thermal and spatial boundary information, but existing approaches fail to fully utilize this multi-modal data. 3. PhysFire-WM is introduced as a novel physics-informed world model designed to emulate fire spread by integrating combustion dynamics through structured physical priors derived from a physical simulator, addressing inconsistencies in purely data-driven video generation models. 4. The approach employs a Cross-task Collaborative Training (CC-Train) strategy that shares parameters and coordinates gradients between tasks of thermal radiation prediction and boundary delineation, overcoming the limitations of sparse mask information and enhancing both physical realism and geometric accuracy. 5. Extensive experiments on a fine-grained multimodal fire dataset demonstrate that PhysFire-WM achieves superior accuracy in predicting fire spread, validating the importance of incorporating physical priors and collaborative training, and offering valuable insights for advancing physics-informed modeling in disaster prediction applications. <div>
arXiv:2512.17152v1 Announce Type: new 
Abstract: Fine-grained fire prediction plays a crucial role in emergency response. Infrared images and fire masks provide complementary thermal and boundary information, yet current methods are predominantly limited to binary mask modeling with inherent signal sparsity, failing to capture the complex dynamics of fire. While world models show promise in video generation, their physical inconsistencies pose significant challenges for fire forecasting. This paper introduces PhysFire-WM, a Physics-informed World Model for emulating Fire spread dynamics. Our approach internalizes combustion dynamics by encoding structured priors from a Physical Simulator to rectify physical discrepancies, coupled with a Cross-task Collaborative Training strategy (CC-Train) that alleviates the issue of limited information in mask-based modeling. Through parameter sharing and gradient coordination, CC-Train effectively integrates thermal radiation dynamics and spatial boundary delineation, enhancing both physical realism and geometric accuracy. Extensive experiments on a fine-grained multimodal fire dataset demonstrate the superior accuracy of PhysFire-WM in fire spread prediction. Validation underscores the importance of physical priors and cross-task collaboration, providing new insights for applying physics-informed world models to disaster prediction.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</title>
<link>https://arxiv.org/abs/2512.17160</link>
<guid>https://arxiv.org/abs/2512.17160</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Large Language Model, Diffusion Model, Zero-shot Classification, Prompt Generation  

<br><br>Summary:  
The paper addresses limitations in current Vision-Language Models (VLMs) like CLIP, which depend heavily on annotated text-image pairs and require dual-tower encoders, increasing cost and complexity. To overcome these challenges, the authors propose LGCLIP, a novel framework that integrates a Large Language Model (LLM) to generate class-specific prompts. These prompts guide a diffusion model to synthesize reference images serving as visual prototypes. Real images are then compared with these generated prototypes via extracted visual features to perform classification. The LGCLIP framework simplifies the architecture by using only a visual encoder, making it lightweight and efficient. Importantly, it eliminates the requirement for manually annotated text-image pairs during training, requiring only class labels as input. Experimental results demonstrate that LGCLIP achieves strong zero-shot classification performance while reducing dataset preparation costs and computational complexity. This approach establishes a new paradigm for classification by combining prompt generation, image synthesis, and contrastive learning without the need for extensive paired data or dual encoders. <div>
arXiv:2512.17160v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)" framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABE-CLIP: Training-Free Attribute Binding Enhancement for Compositional Image-Text Matching</title>
<link>https://arxiv.org/abs/2512.17178</link>
<guid>https://arxiv.org/abs/2512.17178</guid>
<content:encoded><![CDATA[
<div> Keywords: CLIP, attribute binding, compositional image-text matching, semantic refinement, local token-patch alignment  

<br><br>Summary:  
This paper addresses the challenge of compositional image-text matching in CLIP-like models, focusing on the difficulty of correctly associating attributes with their corresponding objects. Traditional CLIP models rely on global representations that tend to overlook fine-grained semantic details, leading to attribute confusion. Existing solutions often require additional training or hard negative sampling but struggle to generalize well to novel compositional concepts. To overcome these issues, the authors introduce ABE-CLIP, a training-free Attribute Binding Enhancement method that strengthens the link between objects and attributes without extra training overhead. ABE-CLIP incorporates a Semantic Refinement Mechanism that refines token embeddings for both objects and attributes in the text, which enhances semantic precision and reduces confusion. Furthermore, the method employs a Local Token-Patch Alignment strategy, calculating localized similarity scores between refined textual tokens and their corresponding image patches rather than relying on global features. By aggregating these local similarities, ABE-CLIP produces a more accurate image-text similarity score. Experimental results on several datasets demonstrate that ABE-CLIP significantly improves the ability to bind attributes to objects, outperforming prior methods that depend on extensive training, thus offering an efficient and effective solution to compositional image-text understanding. <div>
arXiv:2512.17178v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining (CLIP) has achieved remarkable performance in various multimodal tasks. However, it still struggles with compositional image-text matching, particularly in accurately associating objects with their corresponding attributes, because its inherent global representation often overlooks fine-grained semantics for attribute binding. Existing methods often require additional training or extensive hard negative sampling, yet they frequently show limited generalization to novel compositional concepts and fail to fundamentally address the drawbacks of global representations. In this paper, we propose ABE-CLIP, a novel training-free Attribute Binding Enhancement method designed to strengthen attribute-object binding in CLIP-like models. Specifically, we employ a Semantic Refinement Mechanism to refine token embeddings for both object and attribute phrases in the text, thereby mitigating attribute confusion and improving semantic precision. We further introduce a Local Token-Patch Alignment strategy that computes similarity scores between refined textual tokens and their most relevant image patches. By aggregating localized similarity scores, ABE-CLIP computes the final image-text similarity. Experiments on multiple datasets demonstrate that ABE-CLIP significantly improves attribute-object binding performance, even surpassing methods that require extensive training.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It is not always greener on the other side: Greenery perception across demographics and personalities in multiple cities</title>
<link>https://arxiv.org/abs/2512.17186</link>
<guid>https://arxiv.org/abs/2512.17186</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban greenery, perception, Green View Index, spatial variation, cultural influence  

<br><br>Summary:  
This study focuses on quantifying and assessing urban greenery by comparing objective measures such as the Green View Index (GVI) with subjective perceptions derived from surveys. Researchers collected data through street view imagery and an extensive urban visual perception survey from 1,000 participants across five countries, including detailed demographic and personality profiles. The investigation highlights discrepancies between objective greenery measurements and how green spaces are perceived, revealing that these differences are consistent worldwide. Notably, demographics and personality traits have limited influence on how greenery is perceived. The study finds a strong correlation between perceived and measured greenery influenced by geographic factors, particularly where participants live. Location emerged as one of the most significant factors affecting perception, suggesting cultural, environmental, and experiential backgrounds play crucial roles in shaping individual views of urban greenery. The findings underscore the importance of considering spatial and human factors when planning green spaces to better align objective greenery with community perceptions and well-being outcomes. This work advances the understanding of urban greenery perception by integrating human, geographic, and spatial dimensions across diverse global contexts. <div>
arXiv:2512.17186v1 Announce Type: new 
Abstract: Quantifying and assessing urban greenery is consequential for planning and development, reflecting the everlasting importance of green spaces for multiple climate and well-being dimensions of cities. Evaluation can be broadly grouped into objective (e.g., measuring the amount of greenery) and subjective (e.g., polling the perception of people) approaches, which may differ -- what people see and feel about how green a place is might not match the measurements of the actual amount of vegetation. In this work, we advance the state of the art by measuring such differences and explaining them through human, geographic, and spatial dimensions. The experiments rely on contextual information extracted from street view imagery and a comprehensive urban visual perception survey collected from 1,000 people across five countries with their extensive demographic and personality information. We analyze the discrepancies between objective measures (e.g., Green View Index (GVI)) and subjective scores (e.g., pairwise ratings), examining whether they can be explained by a variety of human and visual factors such as age group and spatial variation of greenery in the scene. The findings reveal that such discrepancies are comparable around the world and that demographics and personality do not play a significant role in perception. Further, while perceived and measured greenery correlate consistently across geographies (both where people and where imagery are from), where people live plays a significant role in explaining perceptual differences, with these two, as the top among seven, features that influences perceived greenery the most. This location influence suggests that cultural, environmental, and experiential factors substantially shape how individuals observe greenery in cities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Globally Optimal Solution to the Generalized Relative Pose Estimation Problem using Affine Correspondences</title>
<link>https://arxiv.org/abs/2512.17188</link>
<guid>https://arxiv.org/abs/2512.17188</guid>
<content:encoded><![CDATA[
<div> affine correspondences, multi-camera systems, relative pose estimation, global optimization, inertial measurement unit (IMU)  

<br><br>Summary:  
This paper addresses the problem of relative pose estimation in multi-camera systems aided by inertial measurement units (IMUs), which is crucial for applications like self-driving cars. The authors propose a globally optimal solver that utilizes affine correspondences and assumes a known vertical direction to estimate the generalized relative pose. Their method first decouples the rotation matrix and translation vector, establishing a cost function based on the relative rotation angle that minimizes algebraic errors resulting from geometric constraints given by affine correspondences. They then convert the global optimization problem into solving two polynomials with two unknowns, derived from the characteristic equation and its first derivative set to zero. The relative rotation angle is computed using a polynomial eigenvalue solver, and the translation vector is subsequently obtained from the corresponding eigenvector. Additionally, the paper introduces a novel linear solution tailored for scenarios where the relative rotation is small. The effectiveness of the proposed solver is demonstrated through extensive experiments on both synthetic and real-world datasets, showing superior accuracy compared to existing state-of-the-art methods in relative pose estimation for multi-camera setups. <div>
arXiv:2512.17188v1 Announce Type: new 
Abstract: Mobile devices equipped with a multi-camera system and an inertial measurement unit (IMU) are widely used nowadays, such as self-driving cars. The task of relative pose estimation using visual and inertial information has important applications in various fields. To improve the accuracy of relative pose estimation of multi-camera systems, we propose a globally optimal solver using affine correspondences to estimate the generalized relative pose with a known vertical direction. First, a cost function about the relative rotation angle is established after decoupling the rotation matrix and translation vector, which minimizes the algebraic error of geometric constraints from affine correspondences. Then, the global optimization problem is converted into two polynomials with two unknowns based on the characteristic equation and its first derivative is zero. Finally, the relative rotation angle can be solved using the polynomial eigenvalue solver, and the translation vector can be obtained from the eigenvector. Besides, a new linear solution is proposed when the relative rotation is small. The proposed solver is evaluated on synthetic data and real-world datasets. The experiment results demonstrate that our method outperforms comparable state-of-the-art methods in accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs</title>
<link>https://arxiv.org/abs/2512.17189</link>
<guid>https://arxiv.org/abs/2512.17189</guid>
<content:encoded><![CDATA[
<div> Medical Vision-Language Models, hallucinations, contrastive decoding, anatomical mask, diagnostic accuracy

<br><br>Summary: Medical Vision-Language Models (MedVLMs) have significant potential for use in clinical settings but are often plagued by hallucinations, where the models incorrectly generate answers not based on visual evidence but on textual biases. Current solutions face challenges: training-based methods require expensive expert annotations, limiting their scalability, whereas training-free approaches like contrastive decoding apply broad, untargeted corrections that may be unreliable in complex medical contexts. To overcome these limitations, the authors propose Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play technique that specifically targets hallucinations by using anatomical masks to guide the model's focus. ARCD performs a novel three-tier contrastive decoding process that dynamically adjusts weights at the token, attention, and logits levels, enhancing the model’s attention on relevant anatomical regions while suppressing unsupported outputs. Extensive experiments were conducted on diverse medical imaging datasets, including chest X-rays, CT scans, brain MRIs, and ocular ultrasounds. Results show that ARCD effectively improves regional anatomical understanding, significantly reduces hallucinations, and boosts overall diagnostic accuracy, demonstrating its promise as a reliable strategy for enhancing MedVLMs in real-world clinical applications. <div>
arXiv:2512.17189v1 Announce Type: new 
Abstract: Medical Vision-Language Models (MedVLMs) show immense promise in clinical applicability. However, their reliability is hindered by hallucinations, where models often fail to derive answers from visual evidence, instead relying on learned textual priors. Existing mitigation strategies for MedVLMs have distinct limitations: training-based methods rely on costly expert annotations, limiting scalability, while training-free interventions like contrastive decoding, though data-efficient, apply a global, untargeted correction whose effects in complex real-world clinical settings can be unreliable. To address these challenges, we introduce Anatomical Region-Guided Contrastive Decoding (ARCD), a plug-and-play strategy that mitigates hallucinations by providing targeted, region-specific guidance. Our module leverages an anatomical mask to direct a three-tiered contrastive decoding process. By dynamically re-weighting at the token, attention, and logits levels, it verifiably steers the model's focus onto specified regions, reinforcing anatomical understanding and suppressing factually incorrect outputs. Extensive experiments across diverse datasets, including chest X-ray, CT, brain MRI, and ocular ultrasound, demonstrate our method's effectiveness in improving regional understanding, reducing hallucinations, and enhancing overall diagnostic accuracy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fose: Fusion of One-Step Diffusion and End-to-End Network for Pansharpening</title>
<link>https://arxiv.org/abs/2512.17202</link>
<guid>https://arxiv.org/abs/2512.17202</guid>
<content:encoded><![CDATA[
<div> Fose, pansharpening, diffusion model, end-to-end model, image fusion<br><br>Summary:  
Pansharpening is an important image fusion technique that merges low-resolution multispectral images (LRMSI) with high-resolution panchromatic images (PAN) to produce high-resolution multispectral images (HRMSI). Recent advancements in diffusion models (DM) and end-to-end (E2E) models have advanced the state-of-the-art in pansharpening. Diffusion models rely on a multi-step process to accurately estimate the residual between LRMSI and HRMSI, but this approach demands substantial computational resources and is time-intensive. Conversely, E2E models offer simplicity but are constrained by limited prior information and structural complexity, which limits their performance. To address these challenges, the paper proposes a novel four-stage training strategy that combines a one-step diffusion model with an end-to-end model into a lightweight network called Fose. This strategy involves performing one-step distillation on an enhanced state-of-the-art diffusion model, effectively reducing the inference steps from 50 to just 1. Subsequently, the one-step diffusion model and the E2E model are fused using lightweight ensemble blocks. Extensive experiments on three widely-used benchmarks demonstrate that Fose significantly outperforms existing methods. Additionally, it achieves a 7.42 times speedup in inference compared to the baseline diffusion model while delivering superior pansharpening results. The authors have released the code and model publicly for community use. <div>
arXiv:2512.17202v1 Announce Type: new 
Abstract: Pansharpening is a significant image fusion task that fuses low-resolution multispectral images (LRMSI) and high-resolution panchromatic images (PAN) to obtain high-resolution multispectral images (HRMSI). The development of the diffusion models (DM) and the end-to-end models (E2E model) has greatly improved the frontier of pansharping. DM takes the multi-step diffusion to obtain an accurate estimation of the residual between LRMSI and HRMSI. However, the multi-step process takes large computational power and is time-consuming. As for E2E models, their performance is still limited by the lack of prior and simple structure. In this paper, we propose a novel four-stage training strategy to obtain a lightweight network Fose, which fuses one-step DM and an E2E model. We perform one-step distillation on an enhanced SOTA DM for pansharping to compress the inference process from 50 steps to only 1 step. Then we fuse the E2E model with one-step DM with lightweight ensemble blocks. Comprehensive experiments are conducted to demonstrate the significant improvement of the proposed Fose on three commonly used benchmarks. Moreover, we achieve a 7.42 speedup ratio compared to the baseline DM while achieving much better performance. The code and model are released at https://github.com/Kai-Liu001/Fose.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs</title>
<link>https://arxiv.org/abs/2512.17206</link>
<guid>https://arxiv.org/abs/2512.17206</guid>
<content:encoded><![CDATA[
<div> Exploration capacity, latent modulation, variational autoencoder, reinforcement learning, reasoning strategies<br><br>Summary:<br><br>This paper introduces Reasoning Palette, a novel approach to improve exploration and strategic reasoning in large (vision-) language models. First, the method addresses the issue of redundant reasoning paths produced by stochastic sampling during inference, which limits diversity and exploration efficiency. Second, Reasoning Palette employs a stochastic latent variable inferred from the mean-pooled embedding of question-answer pairs via a variational autoencoder (VAE). Third, during inference, this latent variable is decoded into learnable token prefixes, which are prepended to the model input to modulate its internal reasoning trajectory, thus shaping the style and structure of the model’s generated responses. Fourth, a brief supervised fine-tuning phase helps the model adapt to this latent conditioning, enabling more effective integration of diverse reasoning contexts. Finally, integrating this framework within reinforcement learning leads to enhanced structured exploration by injecting diverse reasoning modes on demand, which improves sustained learning ability and exploration efficiency. Experimental results on multiple reasoning benchmarks demonstrate that Reasoning Palette provides interpretable and controllable strategic behavior in language models and achieves consistent performance gains compared to standard reinforcement learning methods. <div>
arXiv:2512.17206v1 Announce Type: new 
Abstract: Exploration capacity shapes both inference-time performance and reinforcement learning (RL) training for large (vision-) language models, as stochastic sampling often yields redundant reasoning paths with little high-level diversity. This paper proposes Reasoning Palette, a novel latent-modulation framework that endows the model with a stochastic latent variable for strategic contextualization, guiding its internal planning prior to token generation. This latent context is inferred from the mean-pooled embedding of a question-answer pair via a variational autoencoder (VAE), where each sampled latent potentially encodes a distinct reasoning context. During inference, a sampled latent is decoded into learnable token prefixes and prepended to the input prompt, modulating the model's internal reasoning trajectory. In this way, the model performs internal sampling over reasoning strategies prior to output generation, which shapes the style and structure of the entire response sequence. A brief supervised fine-tuning (SFT) warm-up phase allows the model to adapt to this latent conditioning. Within RL optimization, Reasoning Palette facilitates structured exploration by enabling on-demand injection for diverse reasoning modes, significantly enhancing exploration efficiency and sustained learning capability. Experiments across multiple reasoning benchmarks demonstrate that our method enables interpretable and controllable control over the (vision-) language model's strategic behavior, thereby achieving consistent performance gains over standard RL methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency</title>
<link>https://arxiv.org/abs/2512.17213</link>
<guid>https://arxiv.org/abs/2512.17213</guid>
<content:encoded><![CDATA[
<div> Medical Vision-Language Models, hallucinations, Knowledge Graph Consistency Reward, process supervision, MIMIC-CXR-VQA<br><br>Summary:<br><br>1. Medical Vision-Language Models (VLMs) often suffer from hallucinations, which undermine their clinical reliability by producing inaccurate or unverifiable outputs.  
2. Existing reinforcement learning approaches, such as Group Relative Policy Optimization (GRPO), use sparse, outcome-based rewards that lead models to "overthink" — generating verbose and convoluted Chain-of-Thought reasoning that hides factual errors and poses safety risks.  
3. To tackle these issues, the authors propose CheXPO-v2, an alignment framework that replaces outcome supervision with process supervision to better guide model reasoning.  
4. The core innovation is the Knowledge Graph Consistency Reward mechanism, which uses Entity-Relation Matching to parse reasoning into structured triplets ("Disease, Relation, Anatomy") allowing fine-grained supervision that penalizes incoherent logic and hallucinations at an atomic level.  
5. Integrating this reward with a hard-example mining strategy, CheXPO-v2 significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA, achieving a new state-of-the-art accuracy with only 5,000 samples, showcasing exceptional data efficiency and clinically sound, verifiable reasoning.  
6. The authors have made the source code publicly available to foster further development and transparency. <div>
arXiv:2512.17213v1 Announce Type: new 
Abstract: Medical Vision-Language Models (VLMs) are prone to hallucinations, compromising clinical reliability. While reinforcement learning methods like Group Relative Policy Optimization (GRPO) offer a low-cost alignment solution, their reliance on sparse, outcome-based rewards inadvertently encourages models to "overthink" -- generating verbose, convoluted, and unverifiable Chain-of-Thought reasoning to justify answers. This focus on outcomes obscures factual errors and poses significant safety risks. To address this, we propose CheXPO-v2, a novel alignment framework that shifts from outcome to process supervision. Our core innovation is a Knowledge Graph Consistency Reward mechanism driven by Entity-Relation Matching. By explicitly parsing reasoning steps into structured "Disease, Relation, Anatomy" triplets, we provide fine-grained supervision that penalizes incoherent logic and hallucinations at the atomic level. Integrating this with a hard-example mining strategy, our approach significantly outperforms GRPO and state-of-the-art models on benchmarks like MIMIC-CXR-VQA. Crucially, CheXPO-v2 achieves new state-of-the-art accuracy using only 5k samples, demonstrating exceptional data efficiency while producing clinically sound and verifiable reasoning. The project source code is publicly available at: https://github.com/ecoxial2007/CheX-Phi4MM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAVE: A VLM Vision Encoder for Document Understanding and Web Agents</title>
<link>https://arxiv.org/abs/2512.17221</link>
<guid>https://arxiv.org/abs/2512.17221</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, DAVE, document understanding, web agents, self-supervised pretraining  

<br><br>Summary: This paper addresses a critical limitation in vision-language models (VLMs), specifically their vision encoders, which often lack robust structural and spatial features necessary for document understanding and web-based agent tasks. To overcome this, the authors introduce DAVE, a specialized vision encoder designed to enhance VLM performance on these domains. The training process of DAVE begins with self-supervised pretraining on abundant unlabeled images, reducing reliance on expensive large-scale annotated datasets. Following this, a supervised autoregressive pretraining phase enables the model to learn parsing and localization tasks using a smaller set of high-quality labeled data. During supervised training, the authors propose two innovations: (i) a novel model-merging scheme that combines encoders paired with different text decoders, promoting compatibility with multiple web agent architectures, and (ii) ensemble training that integrates features from both pretrained generalist encoders like SigLIP2 and the specialized document/web representations developed by DAVE. The effectiveness of DAVE is demonstrated through extensive experiments spanning document understanding tasks, visual question answering (VQA), web localization, and agent-based benchmarks. Results confirm that DAVE significantly improves the vision encoding capabilities of VLMs in document and web contexts, establishing it as a powerful and versatile vision encoder for these applications. <div>
arXiv:2512.17221v1 Announce Type: new 
Abstract: While Vision-language models (VLMs) have demonstrated remarkable performance across multi-modal tasks, their choice of vision encoders presents a fundamental weakness: their low-level features lack the robust structural and spatial information essential for document understanding and web agents. To bridge this gap, we introduce DAVE, a vision encoder purpose-built for VLMs and tailored for these tasks. Our training pipeline is designed to leverage abundant unlabeled data to bypass the need for costly large-scale annotations for document and web images. We begin with a self-supervised pretraining stage on unlabeled images, followed by a supervised autoregressive pretraining stage, where the model learns tasks like parsing and localization from limited, high-quality data. Within the supervised stage, we adopt two strategies to improve our encoder's alignment with both general visual knowledge and diverse document and web agentic tasks: (i) We introduce a novel model-merging scheme, combining encoders trained with different text decoders to ensure broad compatibility with different web agentic architectures. (ii) We use ensemble training to fuse features from pretrained generalist encoders (e.g., SigLIP2) with our own document and web-specific representations. Extensive experiments on classic document tasks, VQAs, web localization, and agent-based benchmarks validate the effectiveness of our approach, establishing DAVE as a strong vision encoder for document and web applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Any-Optical-Model: A Universal Foundation Model for Optical Remote Sensing</title>
<link>https://arxiv.org/abs/2512.17224</link>
<guid>https://arxiv.org/abs/2512.17224</guid>
<content:encoded><![CDATA[
<div> Keywords: Remote Sensing Foundation Models, optical satellites, multi-scale adaptive patch embedding, spectral-spatial relationships, cross sensor fusion<br><br>Summary:  
This paper addresses the challenges posed by varying band compositions and spatial resolutions in optical satellite imagery, which hinder the generalization of existing Remote Sensing Foundation Models (RSFMs). Traditional RSFMs are limited due to their pretraining on fixed band layouts and resolutions, making them less effective in real-world scenarios involving missing bands, cross-sensor fusion, and unseen spatial scales. To overcome these issues, the authors propose the Any Optical Model (AOM), a universal RSFM capable of handling arbitrary band configurations, sensor types, and resolution scales. AOM introduces a spectrum-independent tokenizer that assigns a dedicated band embedding to each channel to explicitly encode spectral identity, preserving distinctive spectral characteristics even when bands are missing or newly introduced. The model incorporates a multi-scale adaptive patch embedding mechanism to dynamically adjust the receptive field, enabling effective capture of texture and contextual patterns from sub-meter to hundred-meter scale imagery. Furthermore, a multi-scale semantic alignment mechanism ensures global semantic consistency across varying resolutions. The pretraining strategy includes a channel-wise self-supervised masking and reconstruction approach that jointly models spectral-spatial relationships. Extensive evaluation on more than 10 public datasets from sensors like Sentinel-2, Landsat, and HLS confirms that AOM achieves state-of-the-art performance under challenging conditions such as missing bands, cross sensor usage, and cross resolution settings. <div>
arXiv:2512.17224v1 Announce Type: new 
Abstract: Optical satellites, with their diverse band layouts and ground sampling distances, supply indispensable evidence for tasks ranging from ecosystem surveillance to emergency response. However, significant discrepancies in band composition and spatial resolution across different optical sensors present major challenges for existing Remote Sensing Foundation Models (RSFMs). These models are typically pretrained on fixed band configurations and resolutions, making them vulnerable to real world scenarios involving missing bands, cross sensor fusion, and unseen spatial scales, thereby limiting their generalization and practical deployment. To address these limitations, we propose Any Optical Model (AOM), a universal RSFM explicitly designed to accommodate arbitrary band compositions, sensor types, and resolution scales. To preserve distinctive spectral characteristics even when bands are missing or newly introduced, AOM introduces a spectrum-independent tokenizer that assigns each channel a dedicated band embedding, enabling explicit encoding of spectral identity. To effectively capture texture and contextual patterns from sub-meter to hundred-meter imagery, we design a multi-scale adaptive patch embedding mechanism that dynamically modulates the receptive field. Furthermore, to maintain global semantic consistency across varying resolutions, AOM incorporates a multi-scale semantic alignment mechanism alongside a channel-wise self-supervised masking and reconstruction pretraining strategy that jointly models spectral-spatial relationships. Extensive experiments on over 10 public datasets, including those from Sentinel-2, Landsat, and HLS, demonstrate that AOM consistently achieves state-of-the-art (SOTA) performance under challenging conditions such as band missing, cross sensor, and cross resolution settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors</title>
<link>https://arxiv.org/abs/2512.17226</link>
<guid>https://arxiv.org/abs/2512.17226</guid>
<content:encoded><![CDATA[
<div> Keywords: visual localization, global descriptors, geometric cues, contrastive loss, learning-based  

<br><br>Summary:  
Recent learning-based visual localization techniques commonly rely on global descriptors derived primarily from geometric information such as covisibility graphs. However, this approach limits descriptor discriminative power and robustness, especially when geometric constraints are noisy or unreliable. To address this, the authors propose a novel aggregator module that learns global descriptors consistent with both the underlying geometric structure and visual similarity. This ensures that only images that are visually similar and spatially connected are close in descriptor space, thereby correcting erroneous associations stemming from unreliable overlap scores. The training method employs a batch-mining strategy based solely on overlap scores combined with a modified contrastive loss, allowing the model to train without requiring manual place labels. This strategy enhances generalization across diverse and large-scale environments. Experimental validation on challenging benchmarks demonstrates that the proposed method substantially improves localization accuracy while maintaining computational and memory efficiency. The approach integrates visual and geometric cues robustly, overcoming limitations of previous methods that depended exclusively on geometric constraints. The authors also provide code to facilitate reproducibility and further research. <div>
arXiv:2512.17226v1 Announce Type: new 
Abstract: Recent learning-based visual localization methods use global descriptors to disambiguate visually similar places, but existing approaches often derive these descriptors from geometric cues alone (e.g., covisibility graphs), limiting their discriminative power and reducing robustness in the presence of noisy geometric constraints. We propose an aggregator module that learns global descriptors consistent with both geometrical structure and visual similarity, ensuring that images are close in descriptor space only when they are visually similar and spatially connected. This corrects erroneous associations caused by unreliable overlap scores. Using a batch-mining strategy based solely on the overlap scores and a modified contrastive loss, our method trains without manual place labels and generalizes across diverse environments. Experiments on challenging benchmarks show substantial localization gains in large-scale environments while preserving computational and memory efficiency. Code is available at \href{https://github.com/sontung/robust\_scr}{github.com/sontung/robust\_scr}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Look: A Disentangled Curriculum for Strategic Perception in Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2512.17227</link>
<guid>https://arxiv.org/abs/2512.17227</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, visual forgetting, disentangled reasoning, Perception-Grounded Chain-of-Thought, reinforcement learning  

<br><br>Summary: Multimodal Large Language Models (MLLMs) face significant challenges in complex visual reasoning tasks due to a phenomenon termed "visual forgetting," where the model progressively loses visual grounding during extended reasoning. This issue arises because current training paradigms prematurely combine two distinct cognitive skills: abstract logical reasoning ("how-to-think") and strategic visual perception ("when-to-look"). This premature entanglement leads to a foundational cold-start deficiency that weakens abstract reasoning and a strategic perception deficit as the model lacks a policy for deciding when to attend to visual inputs. To address these deficiencies, the paper proposes a novel two-stage curriculum-based framework. First, a disentangled Supervised Fine-Tuning (SFT) curriculum is introduced to build a robust abstract reasoning backbone using text-only data. This is then connected to visual inputs using a novel Perception-Grounded Chain-of-Thought (PG-CoT) method. Second, the strategic perception deficit is tackled by formulating the timing of visual attention as a reinforcement learning problem. The authors design a Pivotal Perception Reward that teaches the model when to look by linking perceptual actions with linguistic indicators of cognitive uncertainty such as "wait" or "verify," allowing the model to autonomously learn a grounded perception policy. This approach transforms the MLLM from a heuristic-driven observer into a strategic, grounded reasoner. Code is publicly available. <div>
arXiv:2512.17227v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate significant potential but remain brittle in complex, long-chain visual reasoning tasks. A critical failure mode is "visual forgetting", where models progressively lose visual grounding as reasoning extends, a phenomenon aptly described as "think longer, see less". We posit this failure stems from current training paradigms prematurely entangling two distinct cognitive skills: (1) abstract logical reasoning "how-to-think") and (2) strategic visual perception ("when-to-look"). This creates a foundational cold-start deficiency -- weakening abstract reasoning -- and a strategic perception deficit, as models lack a policy for when to perceive. In this paper, we propose a novel curriculum-based framework to disentangle these skills. First, we introduce a disentangled Supervised Fine-Tuning (SFT) curriculum that builds a robust abstract reasoning backbone on text-only data before anchoring it to vision with a novel Perception-Grounded Chain-of-Thought (PG-CoT) paradigm. Second, we resolve the strategic perception deficit by formulating timing as a reinforcement learning problem. We design a Pivotal Perception Reward that teaches the model when to look by coupling perceptual actions to linguistic markers of cognitive uncertainty (e.g., "wait", "verify"), thereby learning an autonomous grounding policy. Our contributions include the formalization of these two deficiencies and the development of a principled, two-stage framework to address them, transforming the model from a heuristic-driven observer to a strategic, grounded reasoner. \textbf{Code}: \url{https://github.com/gaozilve-max/learning-when-to-look}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Detective: Seek Critical Clues Recurrently to Answer Question from Long Videos</title>
<link>https://arxiv.org/abs/2512.17229</link>
<guid>https://arxiv.org/abs/2512.17229</guid>
<content:encoded><![CDATA[
<div> Keywords: Long Video Question-Answering, Multi-modal Large Language Models, Question-aware Memory, VideoDetective, GLVC Dataset<br><br>Summary:<br><br>Long Video Question-Answering (LVQA) poses challenges for Multi-modal Large Language Models (MLLMs) due to the large amount of contextual and visual information, often leading to high memory consumption and computational cost. Existing approaches attempt to mitigate this by either reducing the number of visual tokens or extending the model’s context length, but these methods risk losing important information or require extensive computation. The proposed method, VideoDetective, introduces an efficient question-aware memory mechanism that enables MLLMs to focus on critical video clues through iterative processing of video sub-segments. For each sub-segment, a question-aware compression approach utilizes special memory tokens to purposefully compress information, allowing the model to maintain essential clues while minimizing token usage. These memory tokens are recurrently aggregated to update the historical context, which influences the understanding of subsequent sub-segments. To better evaluate long video understanding, the authors introduce GLVC (Grounding Long Video Clues), a dataset designed with critical clues scattered across entire videos. Experiments show that VideoDetective can effectively process extremely long videos (up to 100K tokens, equivalent to an hour at 1fps) within practical resource limits (2 minutes runtime and 37GB GPU memory). The method outperforms other baselines in seeking relevant clues across multiple long video QA benchmarks. <div>
arXiv:2512.17229v1 Announce Type: new 
Abstract: Long Video Question-Answering (LVQA) presents a significant challenge for Multi-modal Large Language Models (MLLMs) due to immense context and overloaded information, which could also lead to prohibitive memory consumption. While existing methods attempt to address these issues by reducing visual tokens or extending model's context length, they may miss useful information or take considerable computation. In fact, when answering given questions, only a small amount of crucial information is required. Therefore, we propose an efficient question-aware memory mechanism, enabling MLLMs to recurrently seek these critical clues. Our approach, named VideoDetective, simplifies this task by iteratively processing video sub-segments. For each sub-segment, a question-aware compression strategy is employed by introducing a few special memory tokens to achieve purposefully compression. This allows models to effectively seek critical clues while reducing visual tokens. Then, due to history context could have a significant impact, we recurrently aggregate and store these memory tokens to update history context, which would be reused for subsequent sub-segments. Furthermore, to more effectively measure model's long video understanding ability, we introduce GLVC (Grounding Long Video Clues), a long video question-answering dataset, which features grounding critical and concrete clues scattered throughout entire videos. Experimental results demonstrate our method enables MLLMs with limited context length of 32K to efficiently process 100K tokens (3600 frames, an hour-long video sampled at 1fps), requiring only 2 minutes and 37GB GPU memory usage. Evaluation results across multiple long video benchmarks illustrate our method can more effectively seek critical clues from massive information.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitty: Diffusion-based Human-to-Robot Video Generation</title>
<link>https://arxiv.org/abs/2512.17253</link>
<guid>https://arxiv.org/abs/2512.17253</guid>
<content:encoded><![CDATA[
<div> Keywords: human demonstration videos, diffusion transformer, video In-Context Learning, robot-execution videos, paired-data synthesis  

<br><br>Summary: Learning from human demonstration videos is critical for advancing scalable and generalizable robot learning, yet existing approaches suffer from reliance on intermediate representations like keypoints or trajectories, causing information loss and inconsistencies. The paper introduces Mitty, a Diffusion Transformer designed for video In-Context Learning that generates robot-execution videos directly from human demonstrations in an end-to-end manner without needing action labels or intermediate abstractions. Mitty utilizes a pretrained video diffusion model to leverage strong visual-temporal priors, compressing human demonstration videos into condition tokens and integrating these with robot denoising tokens via bidirectional attention during diffusion. To address the challenge of limited paired data, the authors develop an automatic synthesis pipeline that creates high-quality human-robot video pairs derived from large egocentric video datasets. Experiments on Human2Robot and EPIC-Kitchens datasets demonstrate that Mitty achieves state-of-the-art performance, exhibits strong generalization capabilities to previously unseen environments, and provides valuable insights for scalable robot learning directly from human visual observations. This work marks a significant step towards more practical and efficient robot learning frameworks grounded in raw human demonstration videos. <div>
arXiv:2512.17253v1 Announce Type: new 
Abstract: Learning directly from human demonstration videos is a key milestone toward scalable and generalizable robot learning. Yet existing methods rely on intermediate representations such as keypoints or trajectories, introducing information loss and cumulative errors that harm temporal and visual consistency. We present Mitty, a Diffusion Transformer that enables video In-Context Learning for end-to-end Human2Robot video generation. Built on a pretrained video diffusion model, Mitty leverages strong visual-temporal priors to translate human demonstrations into robot-execution videos without action labels or intermediate abstractions. Demonstration videos are compressed into condition tokens and fused with robot denoising tokens through bidirectional attention during diffusion. To mitigate paired-data scarcity, we also develop an automatic synthesis pipeline that produces high-quality human-robot pairs from large egocentric datasets. Experiments on Human2Robot and EPIC-Kitchens show that Mitty delivers state-of-the-art results, strong generalization to unseen environments, and new insights for scalable robot learning from human observations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyCXR: Human Anatomy Segmentation of Chest X-ray at Any Acquisition Position using Multi-stage Domain Randomized Synthetic Data with Imperfect Annotations and Conditional Joint Annotation Regularization Learning</title>
<link>https://arxiv.org/abs/2512.17263</link>
<guid>https://arxiv.org/abs/2512.17263</guid>
<content:encoded><![CDATA[
<div> Chest X-ray, Anatomical segmentation, Synthetic supervision, Multi-organ segmentation, Domain randomization  

<br><br>Summary:  
1. The paper addresses the challenge of robust anatomical segmentation in chest X-rays (CXRs), focusing on the limitations posed by scarce annotations and variability in real-world imaging conditions.  
2. The authors introduce AnyCXR, a unified framework designed for generalizable multi-organ segmentation across varied CXR projection angles using entirely synthetic supervision without relying on real annotated data.  
3. AnyCXR integrates a Multi-stage Domain Randomization (MSDR) engine that synthesizes over 100,000 anatomically accurate and diverse radiographs from 3D CT volumes to simulate a wide range of imaging scenarios.  
4. A Conditional Joint Annotation Regularization (CAR) learning method is employed to handle partial and imperfect labels by enforcing anatomical consistency in a latent space, improving model robustness.  
5. Trained solely on synthetic data, AnyCXR demonstrates strong zero-shot generalization to multiple real-world datasets, accurately segmenting 54 anatomical structures in posterior-anterior (PA), lateral, and oblique views.  
6. The produced segmentation maps facilitate downstream clinical applications, including automated cardiothoracic ratio measurement, spine curvature analysis, and disease classification, with enhanced performance through anatomical prior integration.  
7. Overall, AnyCXR offers a scalable, practical solution that reduces annotation burden while enhancing robustness in diverse CXR imaging conditions, laying a reliable foundation for anatomy-aware CXR analysis. <div>
arXiv:2512.17263v1 Announce Type: new 
Abstract: Robust anatomical segmentation of chest X-rays (CXRs) remains challenging due to the scarcity of comprehensive annotations and the substantial variability of real-world acquisition conditions. We propose AnyCXR, a unified framework that enables generalizable multi-organ segmentation across arbitrary CXR projection angles using only synthetic supervision. The method combines a Multi-stage Domain Randomization (MSDR) engine, which generates over 100,000 anatomically faithful and highly diverse synthetic radiographs from 3D CT volumes, with a Conditional Joint Annotation Regularization (CAR) learning strategy that leverages partial and imperfect labels by enforcing anatomical consistency in a latent space. Trained entirely on synthetic data, AnyCXR achieves strong zero-shot generalization on multiple real-world datasets, providing accurate delineation of 54 anatomical structures in PA, lateral, and oblique views. The resulting segmentation maps support downstream clinical tasks, including automated cardiothoracic ratio estimation, spine curvature assessment, and disease classification, where the incorporation of anatomical priors improves diagnostic performance. These results demonstrate that AnyCXR establishes a scalable and reliable foundation for anatomy-aware CXR analysis and offers a practical pathway toward reducing annotation burdens while improving robustness across diverse imaging conditions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WDFFU-Mamba: A Wavelet-guided Dual-attention Feature Fusion Mamba for Breast Tumor Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.17278</link>
<guid>https://arxiv.org/abs/2512.17278</guid>
<content:encoded><![CDATA[
<div> Breast ultrasound segmentation, wavelet enhancement, dual-attention fusion, tumor detection, WDFFU-Mamba  

<br><br>Summary:  
This work addresses the challenges in segmenting breast tumors in ultrasound (BUS) images, such as speckle noise, imaging artifacts, irregular lesion shapes, and blurred boundaries that hinder accurate detection. To overcome these issues, the authors propose WDFFU-Mamba, a novel U-shaped segmentation network incorporating wavelet-guided enhancement and dual-attention feature fusion. The network includes a Wavelet-denoised High-Frequency-guided Feature (WHF) module that enhances low-level features by suppressing noise and emphasizing high-frequency information. In addition, a Dual Attention Feature Fusion (DAFF) module merges skip-connected and semantic features to improve contextual consistency across the image. Extensive experiments conducted on two public BUS datasets demonstrate that WDFFU-Mamba achieves significantly higher segmentation accuracy compared to existing methods, measured by Dice coefficient and 95th percentile Hausdorff Distance (HD95). The integration of wavelet domain enhancement and attention mechanisms allows the model to be both accurate and robust while maintaining computational efficiency. Furthermore, the model shows strong generalization ability across different datasets, indicating its potential applicability in real-world clinical settings for automated breast tumor ultrasound analysis. Overall, WDFFU-Mamba provides a promising solution for improving BUS image segmentation performance in clinical diagnosis and early tumor screening. <div>
arXiv:2512.17278v1 Announce Type: new 
Abstract: Breast ultrasound (BUS) image segmentation plays a vital role in assisting clinical diagnosis and early tumor screening. However, challenges such as speckle noise, imaging artifacts, irregular lesion morphology, and blurred boundaries severely hinder accurate segmentation. To address these challenges, this work aims to design a robust and efficient model capable of automatically segmenting breast tumors in BUS images.We propose a novel segmentation network named WDFFU-Mamba, which integrates wavelet-guided enhancement and dual-attention feature fusion within a U-shaped Mamba architecture. A Wavelet-denoised High-Frequency-guided Feature (WHF) module is employed to enhance low-level representations through noise-suppressed high-frequency cues. A Dual Attention Feature Fusion (DAFF) module is also introduced to effectively merge skip-connected and semantic features, improving contextual consistency.Extensive experiments on two public BUS datasets demonstrate that WDFFU-Mamba achieves superior segmentation accuracy, significantly outperforming existing methods in terms of Dice coefficient and 95th percentile Hausdorff Distance (HD95).The combination of wavelet-domain enhancement and attention-based fusion greatly improves both the accuracy and robustness of BUS image segmentation, while maintaining computational efficiency.The proposed WDFFU-Mamba model not only delivers strong segmentation performance but also exhibits desirable generalization ability across datasets, making it a promising solution for real-world clinical applications in breast tumor ultrasound analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnostic Performance of Universal-Learning Ultrasound AI Across Multiple Organs and Tasks: the UUSIC25 Challenge</title>
<link>https://arxiv.org/abs/2512.17279</link>
<guid>https://arxiv.org/abs/2512.17279</guid>
<content:encoded><![CDATA[
<div> ultrasound AI, deep learning, multi-organ segmentation, diagnostic accuracy, domain generalization  

<br><br>Summary:  
This study addresses the limitations of current ultrasound AI tools, which are typically designed for single tasks and lack versatility compared to modern ultrasound systems. The objective was to evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models capable of performing multi-organ classification and segmentation. The Universal UltraSound Image Challenge 2025 (UUSIC25) provided a large dataset of 11,644 images, supplemented with an independent multi-center test set of 2,479 images including data from an entirely unseen center to test the models’ generalization abilities. Key outcome measures were diagnostic performance using Dice Similarity Coefficient (DSC) and Area Under the ROC Curve (AUC), as well as computational efficiency measured by inference time and GPU memory usage. Among 15 submitted algorithms, the top model named SMART achieved a macro-averaged DSC of 0.854 across five segmentation tasks and an AUC of 0.766 for binary classification. While segmentation tasks, such as fetal head identification, showed high accuracy (DSC of 0.942), performance varied on more complex tasks with domain shifts, as evident in breast cancer molecular subtyping where AUC dropped from 0.571 internally to 0.508 on the unseen external dataset. The study concludes that general-purpose AI models can efficiently handle multiple ultrasound tasks with high accuracy, but domain generalization remains a key challenge for clinical application. <div>
arXiv:2512.17279v1 Announce Type: new 
Abstract: IMPORTANCE: Current ultrasound AI remains fragmented into single-task tools, limiting clinical utility compared to versatile modern ultrasound systems.
  OBJECTIVE: To evaluate the diagnostic accuracy and efficiency of single general-purpose deep learning models for multi-organ classification and segmentation.
  DESIGN: The Universal UltraSound Image Challenge 2025 (UUSIC25) involved developing algorithms on 11,644 images (public/private). Evaluation used an independent, multi-center test set of 2,479 images, including data from a center completely unseen during training to assess generalization.
  OUTCOMES: Diagnostic performance (Dice Similarity Coefficient [DSC]; Area Under the Receiver Operating Characteristic Curve [AUC]) and computational efficiency (inference time, GPU memory).
  RESULTS: Of 15 valid algorithms, the top model (SMART) achieved a macro-averaged DSC of 0.854 across 5 segmentation tasks and AUC of 0.766 for binary classification. Models showed high capability in segmentation (e.g., fetal head DSC: 0.942) but variability in complex tasks subject to domain shift. Notably, in breast cancer molecular subtyping, the top model's performance dropped from AUC 0.571 (internal) to 0.508 (unseen external center), highlighting generalization challenges.
  CONCLUSIONS: General-purpose AI models achieve high accuracy and efficiency across multiple tasks using a single architecture. However, performance degradation on unseen data suggests domain generalization is critical for future clinical deployment.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language Model Guided Image Restoration</title>
<link>https://arxiv.org/abs/2512.17292</link>
<guid>https://arxiv.org/abs/2512.17292</guid>
<content:encoded><![CDATA[
<div> Vision-language models, image restoration, diffusion model, semantic coherence, CLIP<br><br>Summary:<br><br> This paper addresses the challenge of combining pixel-level fidelity and high-level semantic understanding in image restoration (IR) tasks. Traditional IR approaches have difficulties in leveraging both visual and linguistic knowledge effectively. To overcome this, the authors propose the Vision-Language Model Guided Image Restoration (VLMIR) framework, which integrates vision-language priors from models like CLIP to enhance IR quality. The framework operates in two key stages: first, it performs feature extraction by capturing complementary visual and linguistic embeddings from input images, using techniques such as LoRA fine-tuning and cosine similarity loss to align low-quality and high-quality image captions. Additionally, a degradation predictor is used to separate degradations from clean image content in the embedding space. Second, these rich embeddings are incorporated into a diffusion-based restoration model through cross-attention, enabling improved visual perception and semantic coherence during restoration. Extensive experiments and ablation studies validate VLMIR’s superior performance in both universal and degradation-specific IR tasks, highlighting how integrating visual and linguistic knowledge from vision-language models advances the state-of-the-art in image restoration. <div>
arXiv:2512.17292v1 Announce Type: new 
Abstract: Many image restoration (IR) tasks require both pixel-level fidelity and high-level semantic understanding to recover realistic photos with fine-grained details. However, previous approaches often struggle to effectively leverage both the visual and linguistic knowledge. Recent efforts have attempted to incorporate Vision-language models (VLMs), which excel at aligning visual and textual features, into universal IR. Nevertheless, these methods fail to utilize the linguistic priors to ensure semantic coherence during the restoration process. To address this issue, in this paper, we propose the Vision-Language Model Guided Image Restoration (VLMIR) framework, which leverages the rich vision-language priors of VLMs, such as CLIP, to enhance IR performance through improved visual perception and semantic understanding. Our approach consists of two stages: VLM-based feature extraction and diffusion-based image restoration. In the first stage, we extract complementary visual and linguistic representations of input images by condensing the visual perception and high-level semantic priors through VLMs. Specifically, we align the embeddings of captions from low-quality and high-quality images using a cosine similarity loss with LoRA fine-tuning, and employ a degradation predictor to decompose degradation and clean image content embeddings. These complementary visual and textual embeddings are then integrated into a diffusion-based model via cross-attention mechanisms for enhanced restoration. Extensive experiments and ablation studies demonstrate that VLMIR achieves superior performance across both universal and degradation-specific IR tasks, underscoring the critical role of integrated visual and linguistic knowledge from VLMs in advancing image restoration capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Pixel-Wise Anomaly Location for High-Resolution PCBA \\ via Self-Supervised Image Reconstruction</title>
<link>https://arxiv.org/abs/2512.17296</link>
<guid>https://arxiv.org/abs/2512.17296</guid>
<content:encoded><![CDATA[
<div> Keywords: PCBA defect inspection, self-supervised learning, high-resolution images, anomaly localization, SIPCBA-500 dataset<br><br>Summary:<br><br>1. The paper addresses the challenge of automated defect inspection on Printed Circuit Board Assemblies (PCBA), focusing on difficulties posed by limited labeled data and micro-defects in high-resolution, visually complex images.  
2. The authors propose HiSIR-Net, a High-resolution, Self-supervised Reconstruction framework designed for pixel-wise localization of defects in PCBAs.  
3. HiSIR-Net incorporates two key modules: (i) the Selective Input-Reconstruction Gate (SIR-Gate), which dynamically determines whether to rely on the reconstructed image or the original input to reduce false positive anomalies and artifacts, and (ii) the Region-level Optimized Patch Selection (ROPS) scheme, which uses positional cues to coherently stitch overlapping patch reconstructions at arbitrary image resolutions.  
4. This design yields clean, high-resolution anomaly maps with a low false positive rate, making defect detection more precise and reliable in practical scenarios, including 4K resolution images.  
5. To support research in high-resolution PCBA defect localization, the authors contribute a new dataset called SIPCBA-500, consisting of 500 self-collected images, and showcase the superior localization performance and practical speed of HiSIR-Net on both the new dataset and existing public benchmarks. Full code and data will be released after acceptance. <div>
arXiv:2512.17296v1 Announce Type: new 
Abstract: Automated defect inspection of assembled Printed Circuit Board Assemblies (PCBA) is quite challenging due to the insufficient labeled data, micro-defects with just a few pixels in visually-complex and high-resolution images. To address these challenges, we present HiSIR-Net, a High resolution, Self-supervised Reconstruction framework for pixel-wise PCBA localization. Our design combines two lightweight modules that make this practical on real 4K-resolution boards: (i) a Selective Input-Reconstruction Gate (SIR-Gate) that lets the model decide where to trust reconstruction versus the original input, thereby reducing irrelevant reconstruction artifacts and false alarms; and (ii) a Region-level Optimized Patch Selection (ROPS) scheme with positional cues to select overlapping patch reconstructions coherently across arbitrary resolutions. Organically integrating these mechanisms yields clean, high-resolution anomaly maps with low false positive (FP) rate. To bridge the gap in high-resolution PCBA datasets, we further contribute a self-collected dataset named SIPCBA-500 of 500 images. We conduct extensive experiments on our SIPCBA-500 as well as public benchmarks, demonstrating the superior localization performance of our method while running at practical speed. Full code and dataset will be made available upon acceptance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration</title>
<link>https://arxiv.org/abs/2512.17298</link>
<guid>https://arxiv.org/abs/2512.17298</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformers, feature caching, ProCache, dynamic caching, generative modeling<br><br>Summary:<br><br>1. Diffusion Transformers (DiTs) excel in generative modeling but are computationally expensive, limiting their real-time use.<br><br>2. Existing feature caching techniques aim to accelerate DiTs by reusing features across time steps but suffer from uniform caching intervals that do not match the non-uniform feature evolution and lead to error accumulation over large intervals.<br><br>3. This paper studies how DiT features change during denoising and finds that both feature dynamics and error propagation vary significantly depending on the time step and network depth.<br><br>4. To address these issues, the authors propose ProCache, a training-free dynamic caching framework with two main components: (i) a constraint-aware caching pattern search module that creates non-uniform caching schedules aligned with the model's temporal feature changes through offline constrained sampling, and (ii) a selective computation module that selectively recomputes key activations within deep blocks and for high-importance tokens to reduce errors while keeping overhead low.<br><br>5. Experiments on PixArt-alpha and DiT models show that ProCache achieves up to 1.96x and 2.90x speedup respectively, with negligible impact on output quality, outperforming existing caching methods significantly. <div>
arXiv:2512.17298v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatLat: Material Latent Space for PBR Texture Generation</title>
<link>https://arxiv.org/abs/2512.17302</link>
<guid>https://arxiv.org/abs/2512.17302</guid>
<content:encoded><![CDATA[
<div> PBR textures, generative framework, latent space, VAE fine-tuning, cross-view consistency  

<br><br>Summary:  
This paper proposes a novel generative framework designed to produce high-quality Physically Based Rendering (PBR) textures for 3D meshes, addressing the challenge of scarce large-scale PBR texture datasets. The method leverages pretrained latent image generative models by learning a specialized material latent space (MatLat) through targeted fine-tuning. Unlike prior approaches that freeze the embedding network—leading to distribution shifts when encoding additional PBR channels and complicating diffusion training—this work fine-tunes the pretrained Variational Autoencoder (VAE) to incorporate new material channels with minimal latent distribution deviation. Furthermore, the authors identify that correspondence-aware attention alone does not ensure cross-view consistency unless the latent-to-image mapping preserves spatial locality. To tackle this, they introduce a regularization technique during VAE fine-tuning that crops latent patches, decodes them, and aligns the corresponding image regions, thereby maintaining strong pixel-latent spatial correspondence. Ablation studies and comparisons with previous methods show that each component of the proposed framework is critical and collectively contributes to achieving state-of-the-art results in PBR texture fidelity. This work advances the generation of realistic and consistent materials for 3D assets by improving both texture quality and multi-view coherence. <div>
arXiv:2512.17302v1 Announce Type: new 
Abstract: We propose a generative framework for producing high-quality PBR textures on a given 3D mesh. As large-scale PBR texture datasets are scarce, our approach focuses on effectively leveraging the embedding space and diffusion priors of pretrained latent image generative models while learning a material latent space, MatLat, through targeted fine-tuning. Unlike prior methods that freeze the embedding network and thus lead to distribution shifts when encoding additional PBR channels and hinder subsequent diffusion training, we fine-tune the pretrained VAE so that new material channels can be incorporated with minimal latent distribution deviation. We further show that correspondence-aware attention alone is insufficient for cross-view consistency unless the latent-to-image mapping preserves locality. To enforce this locality, we introduce a regularization in the VAE fine-tuning that crops latent patches, decodes them, and aligns the corresponding image regions to maintain strong pixel-latent spatial correspondence. Ablation studies and comparison with previous baselines demonstrate that our framework improves PBR texture fidelity and that each component is critical for achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMAG: Self-Rectifying Diffusion Sampling with Exponential Moving Average Guidance</title>
<link>https://arxiv.org/abs/2512.17303</link>
<guid>https://arxiv.org/abs/2512.17303</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, classifier-free guidance, attention modification, generative quality, adaptive layer selection<br><br>Summary:  
The paper addresses challenges in diffusion and flow-matching generative models by focusing on guidance techniques that enhance sample quality and consistency. It discusses classifier-free guidance (CFG) as the current standard, which improves performance by contrasting conditional and unconditional samples. However, recent advancements rely on contrasting negative samples at inference time using weaker auxiliary models or attention perturbations but fall short in controlling the granularity or difficulty of these negative samples, with fixed target-layer selection limiting flexibility. To overcome these issues, the authors introduce Exponential Moving Average Guidance (EMAG), a novel, training-free approach that adaptively modifies attention mechanisms within diffusion transformers during inference. This method employs a statistics-based adaptive layer selection that yields more challenging, semantically faithful negative samples, revealing subtle failure modes. EMAG enables the denoiser to correct fine-grained artifacts, resulting in a significant quality boost and an improvement of +0.46 in the human preference score (HPS) compared to CFG. Furthermore, EMAG is compatible with other advanced guidance methods like APG and CADS, allowing further enhancement in sample quality and preference metrics. This work demonstrates a practical and effective strategy for improving generative model outputs without additional training overhead. <div>
arXiv:2512.17303v1 Announce Type: new 
Abstract: In diffusion and flow-matching generative models, guidance techniques are widely used to improve sample quality and consistency. Classifier-free guidance (CFG) is the de facto choice in modern systems and achieves this by contrasting conditional and unconditional samples. Recent work explores contrasting negative samples at inference using a weaker model, via strong/weak model pairs, attention-based masking, stochastic block dropping, or perturbations to the self-attention energy landscape. While these strategies refine the generation quality, they still lack reliable control over the granularity or difficulty of the negative samples, and target-layer selection is often fixed. We propose Exponential Moving Average Guidance (EMAG), a training-free mechanism that modifies attention at inference time in diffusion transformers, with a statistics-based, adaptive layer-selection rule. Unlike prior methods, EMAG produces harder, semantically faithful negatives (fine-grained degradations), surfacing difficult failure modes, enabling the denoiser to refine subtle artifacts, boosting the quality and human preference score (HPS) by +0.46 over CFG. We further demonstrate that EMAG naturally composes with advanced guidance techniques, such as APG and CADS, further improving HPS.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images</title>
<link>https://arxiv.org/abs/2512.17306</link>
<guid>https://arxiv.org/abs/2512.17306</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Chain-of-Thought, multi-turn reasoning, self-reflection, reinforcement learning<br><br>Summary: Recent developments in large Vision-Language Models (VLMs) show strong reasoning abilities on complex visual tasks by utilizing Chain-of-Thought (CoT) processes that involve active tool use to analyze images rather than passive perception. However, current models lack effective self-reflection and often fail to correct incorrect reasoning paths. To overcome this, the paper introduces DRIM, a model designed to enable deep, reliable multi-turn reasoning within multimodal CoT frameworks. The proposed approach involves a three-stage pipeline: data construction, cold-start supervised fine-tuning (SFT), and reinforcement learning (RL). Initially, a high-resolution image dataset with challenging, verifiable visual question-answer pairs requiring multiple tool calls is created. In the SFT phase, tool operation trajectories are gathered to guide the model towards structured multi-turn reasoning. The RL phase employs redundancy-penalized policy optimization, encouraging self-reflective reasoning by penalizing insufficient exploration and incorrect answer generation. Experiments demonstrate that DRIM significantly improves performance on visual understanding benchmarks by fostering multi-scale exploration and self-correction, addressing key limitations in prior VLM reasoning approaches. <div>
arXiv:2512.17306v1 Announce Type: new 
Abstract: Recent advances in large Vision-Language Models (VLMs) have exhibited strong reasoning capabilities on complex visual tasks by thinking with images in their Chain-of-Thought (CoT), which is achieved by actively invoking tools to analyze visual inputs rather than merely perceiving them. However, existing models often struggle to reflect on and correct themselves when attempting incorrect reasoning trajectories. To address this limitation, we propose DRIM, a model that enables deep but reliable multi-turn reasoning when thinking with images in its multimodal CoT. Our pipeline comprises three stages: data construction, cold-start SFT and RL. Based on a high-resolution image dataset, we construct high-difficulty and verifiable visual question-answer pairs, where solving each task requires multi-turn tool calls to reach the correct answer. In the SFT stage, we collect tool trajectories as cold-start data, guiding a multi-turn reasoning pattern. In the RL stage, we introduce redundancy-penalized policy optimization, which incentivizes the model to develop a self-reflective reasoning pattern. The basic idea is to impose judgment on reasoning trajectories and penalize those that produce incorrect answers without sufficient multi-scale exploration. Extensive experiments demonstrate that DRIM achieves superior performance on visual understanding benchmarks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeDance: A Dynamic Tool-integrated MLLM for Executable Visual Reasoning</title>
<link>https://arxiv.org/abs/2512.17312</link>
<guid>https://arxiv.org/abs/2512.17312</guid>
<content:encoded><![CDATA[
<div> Keywords: CodeDance, visual reasoning, executable code, tool orchestration, reinforcement learning  

<br><br>Summary: This paper presents CodeDance, an innovative approach for visual reasoning that leverages executable code to orchestrate multiple tools, allowing stepwise and transparent reasoning through the creation and execution of code snippets. Unlike traditional methods relying on fixed visual schemas or text-only reasoning chains, CodeDance integrates flexible tool calls to compute intermediate results and generate visual artifacts such as bounding boxes, lines, and plots to facilitate interpretability and self-verifiability. A key contribution is the introduction of a reward mechanism balancing exploration and efficiency during reinforcement learning, preventing overuse of any single tool. Remarkably, CodeDance exhibits emergent behaviors beyond its initial training, including novel tool invocations, unseen compositions of tools, and cross-task transfer capabilities, all without task-specific fine-tuning. This suggests the approach has strong generalization potential for scalable executable visual reasoning. Extensive evaluation on various benchmarks—such as visual search, mathematical problem-solving, and chart question answering—demonstrates that CodeDance consistently outperforms schema-driven and text-only baselines. It also surpasses performance of advanced closed models, including GPT-4o, and larger open-source models, highlighting its effectiveness and versatility in complex reasoning tasks that require integrated visual and computational understanding. <div>
arXiv:2512.17312v1 Announce Type: new 
Abstract: Recent releases such as o3 highlight human-like "thinking with images" reasoning that combines structured tool use with stepwise verification, yet most open-source approaches still rely on text-only chains, rigid visual schemas, or single-step pipelines, limiting flexibility, interpretability, and transferability on complex tasks. We introduce CodeDance, which explores executable code as a general solver for visual reasoning. Unlike fixed-schema calls (e.g., only predicting bounding-box coordinates), CodeDance defines, composes, and executes code to orchestrate multiple tools, compute intermediate results, and render visual artifacts (e.g., boxes, lines, plots) that support transparent, self-checkable reasoning. To guide this process, we introduce a reward for balanced and adaptive tool-call, which balances exploration with efficiency and mitigates tool overuse. Interestingly, beyond the expected capabilities taught by atomic supervision, we empirically observe novel emergent behaviors during RL training: CodeDance demonstrates novel tool invocations, unseen compositions, and cross-task transfer. These behaviors arise without task-specific fine-tuning, suggesting a general and scalable mechanism of executable visual reasoning. Extensive experiments across reasoning benchmarks (e.g., visual search, math, chart QA) show that CodeDance not only consistently outperforms schema-driven and text-only baselines, but also surpasses advanced closed models such as GPT-4o and larger open-source models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.17313</link>
<guid>https://arxiv.org/abs/2512.17313</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Few-Shot Adaptation, Parameter-Efficient Fine-Tuning, Auxiliary Descriptive Knowledge, Large Language Model<br><br>Summary: The paper addresses the challenge that Vision-Language Models (VLMs) face in downstream tasks when there is a distribution shift from their pre-training data, particularly focusing on Few-Shot Adaptation (FSA) using Parameter-Efficient Fine-Tuning (PEFT). Existing PEFT approaches rely heavily on fixed, handcrafted prompts, which limit their semantic understanding of classes and reduce effectiveness. To overcome this, the authors introduce Auxiliary Descriptive Knowledge (ADK), a novel framework designed to enrich text representations efficiently without increasing inference costs. ADK utilizes a Large Language Model offline to generate a comprehensive set of descriptive prompts for each class. These descriptions are used in two forms: Compositional Knowledge, which averages prompts to offer rich semantics useful for ambiguous or unfamiliar class names, and Instance-Specific Knowledge, which applies a lightweight, non-parametric attention mechanism to dynamically select relevant descriptions for each image. This method supplements the existing handcrafted prompts, improving category discrimination across domains. ADK is a parameter-free, plug-and-play module that enhances various PEFT methods. Extensive experiments demonstrate that ADK consistently improves multiple PEFT baselines and establishes a new state-of-the-art performance in diverse scenarios. <div>
arXiv:2512.17313v1 Announce Type: new 
Abstract: Despite the impressive zero-shot capabilities of Vision-Language Models (VLMs), they often struggle in downstream tasks with distribution shifts from the pre-training data. Few-Shot Adaptation (FSA-VLM) has emerged as a key solution, typically using Parameter-Efficient Fine-Tuning (PEFT) to adapt models with minimal data. However, these PEFT methods are constrained by their reliance on fixed, handcrafted prompts, which are often insufficient to understand the semantics of classes. While some studies have proposed leveraging image-induced prompts to provide additional clues for classification, they introduce prohibitive computational overhead at inference. Therefore, we introduce Auxiliary Descriptive Knowledge (ADK), a novel framework that efficiently enriches text representations without compromising efficiency. ADK first leverages a Large Language Model to generate a rich set of descriptive prompts for each class offline. These pre-computed features are then deployed in two ways: (1) as Compositional Knowledge, an averaged representation that provides rich semantics, especially beneficial when class names are ambiguous or unfamiliar to the VLM; and (2) as Instance-Specific Knowledge, where a lightweight, non-parametric attention mechanism dynamically selects the most relevant descriptions for a given image. This approach provides two additional types of knowledge alongside the handcrafted prompt, thereby facilitating category distinction across various domains. Also, ADK acts as a parameter-free, plug-and-play component that enhances existing PEFT methods. Extensive experiments demonstrate that ADK consistently boosts the performance of multiple PEFT baselines, setting a new state-of-the-art across various scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs</title>
<link>https://arxiv.org/abs/2512.17319</link>
<guid>https://arxiv.org/abs/2512.17319</guid>
<content:encoded><![CDATA[
<div> Multimodal Large Language Models, Remote Sensing, Super-High-Resolution Benchmark, Visual Question Answering, Adversarial Filtering<br><br>Summary: The paper addresses limitations in current remote sensing (RS) benchmarks, which mostly use low-resolution images or have poorly designed reasoning tasks. It is observed that text-only large language models (LLMs) can perform comparably to multimodal vision-language models (VLMs) on RS reasoning tasks without visual input, exposing a mismatch between current benchmarks and evaluation goals focused on visual understanding. To tackle this, the authors propose RSHR-Bench, a novel super-high-resolution benchmark comprising 5,329 full-scene images with a minimum dimension of 4,000 pixels and up to approximately 300 million pixels per image, sourced from popular RS and UAV datasets. The benchmark features four main task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks encompass nine perception categories and four reasoning types, supporting multi-turn and multi-image dialogue scenarios. To minimize reliance on language biases, they introduce adversarial filtering using strong LLMs, followed by rigorous human verification. The dataset includes 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation pairs. Evaluation of various open-source, closed-source, and RS-specific VLMs reveals significant performance gaps in handling super-high-resolution data, highlighting challenges for future research. <div>
arXiv:2512.17319v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA: Concept Erasure Benchmark with Comprehensive Semantic Metrics and Diverse Categories</title>
<link>https://arxiv.org/abs/2512.17320</link>
<guid>https://arxiv.org/abs/2512.17320</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image generation, concept erasure, EMMA benchmark, bias evaluation, model robustness<br><br>Summary:<br>The paper addresses key challenges in text-to-image (T2I) generation, particularly privacy, bias, and copyright issues arising from widespread adoption. It focuses on concept erasure techniques that selectively remove unwanted concepts from pre-trained T2I models without full retraining. Existing concept erasure methods face limitations as they are often evaluated on a narrow set of concepts using simplistic prompts. To overcome this, the authors introduce EMMA, a comprehensive benchmark designed to evaluate concept erasure methods across five critical dimensions using 12 diverse metrics. EMMA extends evaluation beyond traditional criteria like image quality and computational efficiency by incorporating tests under challenging conditions such as indirect concept references, visually similar but non-target concepts, and potential amplification of gender and ethnicity biases. Using EMMA, the study systematically analyzes five concept erasure methods across five domains: objects, celebrities, art styles, NSFW (not safe for work), and copyright-sensitive content. Results reveal that current methods struggle to fully erase concepts when indirectly prompted and frequently fail to avoid generating visually similar unintended concepts. Additionally, some methods exacerbate gender and ethnicity biases present in the original models. The findings highlight substantial gaps and call for improved, socially aware concept erasure strategies in T2I generation. <div>
arXiv:2512.17320v1 Announce Type: new 
Abstract: The widespread adoption of text-to-image (T2I) generation has raised concerns about privacy, bias, and copyright violations. Concept erasure techniques offer a promising solution by selectively removing undesired concepts from pre-trained models without requiring full retraining. However, these methods are often evaluated on a limited set of concepts, relying on overly simplistic and direct prompts. To test the boundaries of concept erasure techniques, and assess whether they truly remove targeted concepts from model representations, we introduce EMMA, a benchmark that evaluates five key dimensions of concept erasure over 12 metrics. EMMA goes beyond standard metrics like image quality and time efficiency, testing robustness under challenging conditions, including indirect descriptions, visually similar non-target concepts, and potential gender and ethnicity bias, providing a socially aware analysis of method behavior. Using EMMA, we analyze five concept erasure methods across five domains (objects, celebrities, art styles, NSFW, and copyright). Our results show that existing methods struggle with implicit prompts (i.e., generating the erased concept when it is indirectly referenced) and visually similar non-target concepts (i.e., failing to generate non-targeted concepts resembling the erased one), while some amplify gender and ethnicity bias compared to the original model.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rotterdam artery-vein segmentation (RAV) dataset</title>
<link>https://arxiv.org/abs/2512.17322</link>
<guid>https://arxiv.org/abs/2512.17322</guid>
<content:encoded><![CDATA[
<div> artery-vein segmentation, fundus images, retinal vascular analysis, machine learning, image quality<br><br>Summary:<br><br>Purpose: The article presents a diverse and high-quality dataset of color fundus images (CFIs) annotated with detailed artery-vein (A/V) segmentation to advance machine learning techniques in ophthalmic vascular analysis.<br><br>Methods: Images were obtained from the Rotterdam Study, covering a broad spectrum of ages, imaging devices, and conditions. Annotation was performed via a custom interface enabling graders to label arteries, veins, and unknown vessels on separate layers, starting from initial vessel segmentations. The connectivity of vessel annotations was explicitly checked and corrected through connected component visualization.<br><br>Results: The dataset comprises 1024x1024 pixel PNG images available in three forms: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. It contains a wide range of image qualities, including challenging samples typically excluded in automated quality assessments but valuable for vascular information.<br><br>Conclusion: This dataset provides an extensive and heterogeneous resource of CFIs with validated segmentations, supporting robust training and benchmarking of machine learning models capable of handling real-world imaging variability.<br><br>Translational Relevance: By offering connectivity-validated A/V masks alongside diverse imaging conditions, the dataset supports the creation of clinically relevant and generalizable machine learning tools, potentially enhancing automated screening and diagnosis of ocular and systemic diseases. <div>
arXiv:2512.17322v1 Announce Type: new 
Abstract: Purpose: To provide a diverse, high-quality dataset of color fundus images (CFIs) with detailed artery-vein (A/V) segmentation annotations, supporting the development and evaluation of machine learning algorithms for vascular analysis in ophthalmology.
  Methods: CFIs were sampled from the longitudinal Rotterdam Study (RS), encompassing a wide range of ages, devices, and capture conditions. Images were annotated using a custom interface that allowed graders to label arteries, veins, and unknown vessels on separate layers, starting from an initial vessel segmentation mask. Connectivity was explicitly verified and corrected using connected component visualization tools.
  Results: The dataset includes 1024x1024-pixel PNG images in three modalities: original RGB fundus images, contrast-enhanced versions, and RGB-encoded A/V masks. Image quality varied widely, including challenging samples typically excluded by automated quality assessment systems, but judged to contain valuable vascular information.
  Conclusion: This dataset offers a rich and heterogeneous source of CFIs with high-quality segmentations. It supports robust benchmarking and training of machine learning models under real-world variability in image quality and acquisition settings.
  Translational Relevance: By including connectivity-validated A/V masks and diverse image conditions, this dataset enables the development of clinically applicable, generalizable machine learning tools for retinal vascular analysis, potentially improving automated screening and diagnosis of systemic and ocular diseases.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DESSERT: Diffusion-based Event-driven Single-frame Synthesis via Residual Training</title>
<link>https://arxiv.org/abs/2512.17323</link>
<guid>https://arxiv.org/abs/2512.17323</guid>
<content:encoded><![CDATA[
<div> Keywords: Video frame prediction, event cameras, diffusion model, residual training, temporal consistency<br><br>Summary:<br>1. This paper addresses the challenge of video frame prediction in dynamic scenes, where traditional methods struggle due to a lack of future frame information and suffer from errors like holes and blurring when using optical flow-based reconstruction.<br>2. The authors leverage event cameras, which capture asynchronous per-pixel brightness changes with high temporal resolution, providing richer motion information for better prediction.<br>3. They propose DESSERT, a novel framework combining diffusion models with residual training to synthesize video frames from event data, improving temporal consistency and sharpness.<br>4. The training occurs in two stages: first, an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) aligns event frames with residual differences between anchor and target frames; second, a diffusion model denoises residual latents conditioned on event inputs.<br>5. Additionally, a Diverse-Length Temporal (DLT) augmentation strategy is introduced to enhance robustness by training on frame segments with varying temporal lengths.<br>6. Experimental results demonstrate that DESSERT outperforms prior event-based reconstruction, image-based video prediction, event-based video prediction, and event-based interpolation methods, delivering superior frame quality and temporal coherence. <div>
arXiv:2512.17323v1 Announce Type: new 
Abstract: Video frame prediction extrapolates future frames from previous frames, but suffers from prediction errors in dynamic scenes due to the lack of information about the next frame. Event cameras address this limitation by capturing per-pixel brightness changes asynchronously with high temporal resolution. Prior research on event-based video frame prediction has leveraged motion information from event data, often by predicting event-based optical flow and reconstructing frames via pixel warping. However, such approaches introduce holes and blurring when pixel displacement is inaccurate. To overcome this limitation, we propose DESSERT, a diffusion-based event-driven single-frame synthesis framework via residual training. Leveraging a pre-trained Stable Diffusion model, our method is trained on inter-frame residuals to ensure temporal consistency. The training pipeline consists of two stages: (1) an Event-to-Residual Alignment Variational Autoencoder (ER-VAE) that aligns the event frame between anchor and target frames with the corresponding residual, and (2) a diffusion model that denoises the residual latent conditioned on event data. Furthermore, we introduce Diverse-Length Temporal (DLT) augmentation, which improves robustness by training on frame segments of varying temporal lengths. Experimental results demonstrate that our method outperforms existing event-based reconstruction, image-based video frame prediction, event-based video frame prediction, and one-sided event-based video frame interpolation methods, producing sharper and more temporally consistent frame synthesis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling</title>
<link>https://arxiv.org/abs/2512.17326</link>
<guid>https://arxiv.org/abs/2512.17326</guid>
<content:encoded><![CDATA[
<div> Polysome, HISTAI, Vision-Language Models, Whole-Slide Images, Visual-Question Answering<br><br>Summary:<br><br>1. The paper addresses limitations in current vision-language models (VLMs) for pathology, which often focus only on small regions within whole-slide images (WSIs), provide static outputs, or depend on non-public data, thus hindering reproducibility and generalizability.<br><br>2. To overcome these challenges, the authors introduce Polysome, a standardized tool designed for synthetic instruction generation that can create detailed instructional data for model training.<br><br>3. Using Polysome, they generate HISTAI-Instruct from the publicly available HISTAI dataset, resulting in a large instruction tuning dataset comprising 24,259 WSIs and over 1.1 million instruction-response pairs.<br><br>4. The authors train a new vision-language model named ANTONI-α on HISTAI-Instruct, equipping it with the capability to perform visual-question answering (VQA) tasks at the whole-slide image level.<br><br>5. Experimental results demonstrate that ANTONI-α surpasses the existing MedGemma model in critical pathology-related VQA tasks such as tissue identification, neoplasm detection, and differential diagnosis.<br><br>6. Furthermore, the study explores how varying training data amounts impact ANTONI-α’s performance, offering insights into model scalability.<br><br>7. Importantly, all developed methods, data, and code are made publicly available, promoting transparency and reproducibility in future research. <div>
arXiv:2512.17326v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have the potential to become co-pilots for pathologists. However, most VLMs either focus on small regions of interest within whole-slide images, provide only static slide-level outputs, or rely on data that is not publicly available, limiting reproducibility. Furthermore, training data containing WSIs paired with detailed clinical reports is scarce, restricting progress toward transparent and generalisable VLMs. We address these limitations with three main contributions. First, we introduce Polysome, a standardised tool for synthetic instruction generation. Second, we apply Polysome to the public HISTAI dataset, generating HISTAI-Instruct, a large whole-slide instruction tuning dataset spanning 24,259 slides and over 1.1 million instruction-response pairs. Finally, we use HISTAI-Instruct to train ANTONI-{\alpha}, a VLM capable of visual-question answering (VQA). We show that ANTONI-{\alpha} outperforms MedGemma on WSI-level VQA tasks of tissue identification, neoplasm detection, and differential diagnosis. We also compare the performance of multiple incarnations of ANTONI-{\alpha} trained with different amounts of data. All methods, data, and code are publicly available.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynergyWarpNet: Attention-Guided Cooperative Warping for Neural Portrait Animation</title>
<link>https://arxiv.org/abs/2512.17331</link>
<guid>https://arxiv.org/abs/2512.17331</guid>
<content:encoded><![CDATA[
<div> Keywords: neural portrait animation, warping framework, 3D dense optical flow, cross-attention, confidence-guided fusion<br><br>Summary:<br><br>This paper introduces SynergyWarpNet, a novel attention-guided cooperative warping framework aimed at enhancing high-fidelity talking head synthesis. The authors address limitations of previous methods such as explicit warping's difficulty with accurate motion transfer and missing regions, and attention-based warping's high complexity and poor geometric grounding. SynergyWarpNet operates in three progressive stages: First, it employs an explicit warping module that uses 3D dense optical flow to achieve coarse spatial alignment between a source portrait and a driving image. Second, a reference-augmented correction module utilizes cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions, improving detail restoration. Third, a confidence-guided fusion module integrates the warped outputs using spatially-adaptive fusion governed by a learned confidence map, balancing structural alignment and visual consistency in the final output. Extensive experiments on benchmark datasets demonstrate that SynergyWarpNet achieves state-of-the-art results in neural portrait animation, making it a promising solution for applications such as virtual avatars, telepresence, and digital content creation. <div>
arXiv:2512.17331v1 Announce Type: new 
Abstract: Recent advances in neural portrait animation have demonstrated remarked potential for applications in virtual avatars, telepresence, and digital content creation. However, traditional explicit warping approaches often struggle with accurate motion transfer or recovering missing regions, while recent attention-based warping methods, though effective, frequently suffer from high complexity and weak geometric grounding. To address these issues, we propose SynergyWarpNet, an attention-guided cooperative warping framework designed for high-fidelity talking head synthesis. Given a source portrait, a driving image, and a set of reference images, our model progressively refines the animation in three stages. First, an explicit warping module performs coarse spatial alignment between the source and driving image using 3D dense optical flow. Next, a reference-augmented correction module leverages cross-attention across 3D keypoints and texture features from multiple reference images to semantically complete occluded or distorted regions. Finally, a confidence-guided fusion module integrates the warped outputs with spatially-adaptive fusing, using a learned confidence map to balance structural alignment and visual consistency. Comprehensive evaluations on benchmark datasets demonstrate state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-level distortion-aware deformable network for omnidirectional image super-resolution</title>
<link>https://arxiv.org/abs/2512.17343</link>
<guid>https://arxiv.org/abs/2512.17343</guid>
<content:encoded><![CDATA[
<div> OmniDirectional Image Super-Resolution, EquiRectangular Projection, geometric distortion, deformable convolutions, multi-level feature fusion<br><br>Summary:<br><br>This paper addresses the challenge of enhancing visual quality in OmniDirectional Images (ODIs) through super-resolution techniques, focusing on the geometric distortions introduced by EquiRectangular Projection (ERP). ERP projection causes latitude-dependent distortions, with minimal distortion near the equator but significant stretching near the poles, complicating feature extraction in ODIs. Existing super-resolution methods struggle with limited sampling ranges and insufficient capacity to capture these wide-ranging distortions. To overcome these limitations, the authors propose a novel Multi-level Distortion-aware Deformable Network (MDDN) tailored for OmniDirectional Image Super-Resolution (ODISR). MDDN’s feature extractor includes three parallel branches: a deformable attention mechanism with dilation rate 1 and two dilated deformable convolutions with dilation rates 2 and 3, which together broaden the receptive field and sampling range to better capture distorted patterns. The network integrates a multi-level feature fusion module to adaptively combine extracted features for richer representation of geometric distortions. Additionally, a low-rank decomposition strategy is employed on the dilated deformable convolutions to reduce computational costs. Extensive experiments on public datasets demonstrate that MDDN outperforms current state-of-the-art methods, validating its effectiveness and superiority in handling distortion-aware ODISR tasks. <div>
arXiv:2512.17343v1 Announce Type: new 
Abstract: As augmented reality and virtual reality applications gain popularity, image processing for OmniDirectional Images (ODIs) has attracted increasing attention. OmniDirectional Image Super-Resolution (ODISR) is a promising technique for enhancing the visual quality of ODIs. Before performing super-resolution, ODIs are typically projected from a spherical surface onto a plane using EquiRectangular Projection (ERP). This projection introduces latitude-dependent geometric distortion in ERP images: distortion is minimal near the equator but becomes severe toward the poles, where image content is stretched across a wider area. However, existing ODISR methods have limited sampling ranges and feature extraction capabilities, which hinder their ability to capture distorted patterns over large areas. To address this issue, we propose a novel Multi-level Distortion-aware Deformable Network (MDDN) for ODISR, designed to expand the sampling range and receptive field. Specifically, the feature extractor in MDDN comprises three parallel branches: a deformable attention mechanism (serving as the dilation=1 path) and two dilated deformable convolutions with dilation rates of 2 and 3. This architecture expands the sampling range to include more distorted patterns across wider areas, generating dense and comprehensive features that effectively capture geometric distortions in ERP images. The representations extracted from these deformable feature extractors are adaptively fused in a multi-level feature fusion module. Furthermore, to reduce computational cost, a low-rank decomposition strategy is applied to dilated deformable convolutions. Extensive experiments on publicly available datasets demonstrate that MDDN outperforms state-of-the-art methods, underscoring its effectiveness and superiority in ODISR.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Semantic Features: Pixel-level Mapping for Generalized AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.17350</link>
<guid>https://arxiv.org/abs/2512.17350</guid>
<content:encoded><![CDATA[

arXiv:2512.17350v1 Announce Type: new 
Abstract: The rapid evolution of generative technologies necessitates reliable methods for detecting AI-generated images. A critical limitation of current detectors is their failure to generalize to images from unseen generative models, as they often overfit to source-specific semantic cues rather than learning universal generative artifacts. To overcome this, we introduce a simple yet remarkably effective pixel-level mapping pre-processing step to disrupt the pixel value distribution of images and break the fragile, non-essential semantic patterns that detectors commonly exploit as shortcuts. This forces the detector to focus on more fundamental and generalizable high-frequency traces inherent to the image generation process. Through comprehensive experiments on GAN and diffusion-based generators, we show that our approach significantly boosts the cross-generator performance of state-of-the-art detectors. Extensive analysis further verifies our hypothesis that the disruption of semantic cues is the key to generalization.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Deeper Emotional Reflection: Crafting Affective Image Filters with Generative Priors</title>
<link>https://arxiv.org/abs/2512.17376</link>
<guid>https://arxiv.org/abs/2512.17376</guid>
<content:encoded><![CDATA[

arXiv:2512.17376v1 Announce Type: new 
Abstract: Social media platforms enable users to express emotions by posting text with accompanying images. In this paper, we propose the Affective Image Filter (AIF) task, which aims to reflect visually-abstract emotions from text into visually-concrete images, thereby creating emotionally compelling results. We first introduce the AIF dataset and the formulation of the AIF models. Then, we present AIF-B as an initial attempt based on a multi-modal transformer architecture. After that, we propose AIF-D as an extension of AIF-B towards deeper emotional reflection, effectively leveraging generative priors from pre-trained large-scale diffusion models. Quantitative and qualitative experiments demonstrate that AIF models achieve superior performance for both content consistency and emotional fidelity compared to state-of-the-art methods. Extensive user study experiments demonstrate that AIF models are significantly more effective at evoking specific emotions. Based on the presented results, we comprehensively discuss the value and potential of AIF models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.17396</link>
<guid>https://arxiv.org/abs/2512.17396</guid>
<content:encoded><![CDATA[

arXiv:2512.17396v1 Announce Type: new 
Abstract: In this work, we introduce RadImageNet-VQA, a large-scale dataset designed to advance radiologic visual question answering (VQA) on CT and MRI exams. Existing medical VQA datasets are limited in scale, dominated by X-ray imaging or biomedical illustrations, and often prone to text-based shortcuts. RadImageNet-VQA is built from expert-curated annotations and provides 750K images paired with 7.5M question-answer samples. It covers three key tasks - abnormality detection, anatomy recognition, and pathology identification - spanning eight anatomical regions and 97 pathology categories, and supports open-ended, closed-ended, and multiple-choice questions. Extensive experiments show that state-of-the-art vision-language models still struggle with fine-grained pathology identification, particularly in open-ended settings and even after fine-tuning. Text-only analysis further reveals that model performance collapses to near-random without image inputs, confirming that RadImageNet-VQA is free from linguistic shortcuts. The full dataset and benchmark are publicly available at https://huggingface.co/datasets/raidium/RadImageNet-VQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Occlusion: In Search for Near Real-Time Explainability of CNN-Based Prostate Cancer Classification</title>
<link>https://arxiv.org/abs/2512.17416</link>
<guid>https://arxiv.org/abs/2512.17416</guid>
<content:encoded><![CDATA[

arXiv:2512.17416v1 Announce Type: new 
Abstract: Deep neural networks are starting to show their worth in critical applications such as assisted cancer diagnosis. However, for their outputs to get accepted in practice, the results they provide should be explainable in a way easily understood by pathologists. A well-known and widely used explanation technique is occlusion, which, however, can take a long time to compute, thus slowing the development and interaction with pathologists. In this work, we set out to find a faster replacement for occlusion in a successful system for detecting prostate cancer. Since there is no established framework for comparing the performance of various explanation methods, we first identified suitable comparison criteria and selected corresponding metrics. Based on the results, we were able to choose a different explanation method, which cut the previously required explanation time at least by a factor of 10, without any negative impact on the quality of outputs. This speedup enables rapid iteration in model development and debugging and brings us closer to adopting AI-assisted prostate cancer detection in clinical settings. We propose that our approach to finding the replacement for occlusion can be used to evaluate candidate methods in other related applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIFloodSense: A Global Aerial Imagery Dataset for Semantic Segmentation and Understanding of Flooded Environments</title>
<link>https://arxiv.org/abs/2512.17432</link>
<guid>https://arxiv.org/abs/2512.17432</guid>
<content:encoded><![CDATA[

arXiv:2512.17432v1 Announce Type: new 
Abstract: Accurate flood detection from visual data is a critical step toward improving disaster response and risk assessment, yet datasets for flood segmentation remain scarce due to the challenges of collecting and annotating large-scale imagery. Existing resources are often limited in geographic scope and annotation detail, hindering the development of robust, generalized computer vision methods. To bridge this gap, we introduce AIFloodSense, a comprehensive, publicly available aerial imagery dataset comprising 470 high-resolution images from 230 distinct flood events across 64 countries and six continents. Unlike prior benchmarks, AIFloodSense ensures global diversity and temporal relevance (2022-2024), supporting three complementary tasks: (i) Image Classification with novel sub-tasks for environment type, camera angle, and continent recognition; (ii) Semantic Segmentation providing precise pixel-level masks for flood, sky, and buildings; and (iii) Visual Question Answering (VQA) to enable natural language reasoning for disaster assessment. We establish baseline benchmarks for all tasks using state-of-the-art architectures, demonstrating the dataset's complexity and its value in advancing domain-generalized AI tools for climate resilience.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Xiaomi MiMo-VL-Miloco Technical Report</title>
<link>https://arxiv.org/abs/2512.17436</link>
<guid>https://arxiv.org/abs/2512.17436</guid>
<content:encoded><![CDATA[

arXiv:2512.17436v1 Announce Type: new 
Abstract: We open-source \textbf{MiMo-VL-Miloco-7B} and its quantized variant \textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LangDriveCTRL: Natural Language Controllable Driving Scene Editing with Multi-modal Agents</title>
<link>https://arxiv.org/abs/2512.17445</link>
<guid>https://arxiv.org/abs/2512.17445</guid>
<content:encoded><![CDATA[

arXiv:2512.17445v1 Announce Type: new 
Abstract: LangDriveCTRL is a natural-language-controllable framework for editing real-world driving videos to synthesize diverse traffic scenarios. It leverages explicit 3D scene decomposition to represent driving videos as a scene graph, containing static background and dynamic objects. To enable fine-grained editing and realism, it incorporates an agentic pipeline in which an Orchestrator transforms user instructions into execution graphs that coordinate specialized agents and tools. Specifically, an Object Grounding Agent establishes correspondence between free-form text descriptions and target object nodes in the scene graph; a Behavior Editing Agent generates multi-object trajectories from language instructions; and a Behavior Reviewer Agent iteratively reviews and refines the generated trajectories. The edited scene graph is rendered and then refined using a video diffusion tool to address artifacts introduced by object insertion and significant view changes. LangDriveCTRL supports both object node editing (removal, insertion and replacement) and multi-object behavior editing from a single natural-language instruction. Quantitatively, it achieves nearly $2\times$ higher instruction alignment than the previous SoTA, with superior structural preservation, photorealism, and traffic realism. Project page is available at: https://yunhe24.github.io/langdrivectrl/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTIAQUA: A multimodal maritime dataset and robust training strategies for multimodal semantic segmentation</title>
<link>https://arxiv.org/abs/2512.17450</link>
<guid>https://arxiv.org/abs/2512.17450</guid>
<content:encoded><![CDATA[

arXiv:2512.17450v1 Announce Type: new 
Abstract: Unmanned surface vehicles can encounter a number of varied visual circumstances during operation, some of which can be very difficult to interpret. While most cases can be solved only using color camera images, some weather and lighting conditions require additional information. To expand the available maritime data, we present a novel multimodal maritime dataset MULTIAQUA (Multimodal Aquatic Dataset). Our dataset contains synchronized, calibrated and annotated data captured by sensors of different modalities, such as RGB, thermal, IR, LIDAR, etc. The dataset is aimed at developing supervised methods that can extract useful information from these modalities in order to provide a high quality of scene interpretation regardless of potentially poor visibility conditions. To illustrate the benefits of the proposed dataset, we evaluate several multimodal methods on our difficult nighttime test set. We present training approaches that enable multimodal methods to be trained in a more robust way, thus enabling them to retain reliable performance even in near-complete darkness. Our approach allows for training a robust deep neural network only using daytime images, thus significantly simplifying data acquisition, annotation, and the training process.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework</title>
<link>https://arxiv.org/abs/2512.17459</link>
<guid>https://arxiv.org/abs/2512.17459</guid>
<content:encoded><![CDATA[

arXiv:2512.17459v1 Announce Type: new 
Abstract: Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.
  Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TwinSegNet: A Digital Twin-Enabled Federated Learning Framework for Brain Tumor Analysis</title>
<link>https://arxiv.org/abs/2512.17488</link>
<guid>https://arxiv.org/abs/2512.17488</guid>
<content:encoded><![CDATA[

arXiv:2512.17488v1 Announce Type: new 
Abstract: Brain tumor segmentation is critical in diagnosis and treatment planning for the disease. Yet, current deep learning methods rely on centralized data collection, which raises privacy concerns and limits generalization across diverse institutions. In this paper, we propose TwinSegNet, which is a privacy-preserving federated learning framework that integrates a hybrid ViT-UNet model with personalized digital twins for accurate and real-time brain tumor segmentation. Our architecture combines convolutional encoders with Vision Transformer bottlenecks to capture local and global context. Each institution fine-tunes the global model of private data to form its digital twin. Evaluated on nine heterogeneous MRI datasets, including BraTS 2019-2021 and custom tumor collections, TwinSegNet achieves high Dice scores (up to 0.90%) and sensitivity/specificity exceeding 90%, demonstrating robustness across non-independent and identically distributed (IID) client distributions. Comparative results against centralized models such as TumorVisNet highlight TwinSegNet's effectiveness in preserving privacy without sacrificing performance. Our approach enables scalable, personalized segmentation for multi-institutional clinical settings while adhering to strict data confidentiality requirements.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LumiCtrl : Learning Illuminant Prompts for Lighting Control in Personalized Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.17489</link>
<guid>https://arxiv.org/abs/2512.17489</guid>
<content:encoded><![CDATA[

arXiv:2512.17489v1 Announce Type: new 
Abstract: Current text-to-image (T2I) models have demonstrated remarkable progress in creative image generation, yet they still lack precise control over scene illuminants, which is a crucial factor for content designers aiming to manipulate the mood, atmosphere, and visual aesthetics of generated images. In this paper, we present an illuminant personalization method named LumiCtrl that learns an illuminant prompt given a single image of an object. LumiCtrl consists of three basic components: given an image of the object, our method applies (a) physics-based illuminant augmentation along the Planckian locus to create fine-tuning variants under standard illuminants; (b) edge-guided prompt disentanglement using a frozen ControlNet to ensure prompts focus on illumination rather than structure; and (c) a masked reconstruction loss that focuses learning on the foreground object while allowing the background to adapt contextually, enabling what we call contextual light adaptation. We qualitatively and quantitatively compare LumiCtrl against other T2I customization methods. The results show that our method achieves significantly better illuminant fidelity, aesthetic quality, and scene coherence compared to existing personalization baselines. A human preference study further confirms strong user preference for LumiCtrl outputs. The code and data will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMLANDMARKS: a Cross-View Instance-Level Benchmark for Geo-Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.17492</link>
<guid>https://arxiv.org/abs/2512.17492</guid>
<content:encoded><![CDATA[

arXiv:2512.17492v1 Announce Type: new 
Abstract: Geo-spatial analysis of our world benefits from a multimodal approach, as every single geographic location can be described in numerous ways (images from various viewpoints, textual descriptions, and geographic coordinates). Current geo-spatial benchmarks have limited coverage across modalities, considerably restricting progress in the field, as current approaches cannot integrate all relevant modalities within a unified framework. We introduce the Multi-Modal Landmark dataset (MMLANDMARKS), a benchmark composed of four modalities: 197k highresolution aerial images, 329k ground-view images, textual information, and geographic coordinates for 18,557 distinct landmarks in the United States. The MMLANDMARKS dataset has a one-to-one correspondence across every modality, which enables training and benchmarking models for various geo-spatial tasks, including cross-view Ground-to-Satellite retrieval, ground and satellite geolocalization, Text-to-Image, and Text-to-GPS retrieval. We demonstrate broad generalization and competitive performance against off-the-shelf foundational models and specialized state-of-the-art models across different tasks by employing a simple CLIP-inspired baseline, illustrating the necessity for multimodal datasets to achieve broad geo-spatial understanding.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation</title>
<link>https://arxiv.org/abs/2512.17495</link>
<guid>https://arxiv.org/abs/2512.17495</guid>
<content:encoded><![CDATA[

arXiv:2512.17495v1 Announce Type: new 
Abstract: Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Validation of Diagnostic Artificial Intelligence Models for Prostate Pathology in a Middle Eastern Cohort</title>
<link>https://arxiv.org/abs/2512.17499</link>
<guid>https://arxiv.org/abs/2512.17499</guid>
<content:encoded><![CDATA[

arXiv:2512.17499v1 Announce Type: new 
Abstract: Background: Artificial intelligence (AI) is improving the efficiency and accuracy of cancer diagnostics. The performance of pathology AI systems has been almost exclusively evaluated on European and US cohorts from large centers. For global AI adoption in pathology, validation studies on currently under-represented populations - where the potential gains from AI support may also be greatest - are needed. We present the first study with an external validation cohort from the Middle East, focusing on AI-based diagnosis and Gleason grading of prostate cancer.
  Methods: We collected and digitised 339 prostate biopsy specimens from the Kurdistan region, Iraq, representing a consecutive series of 185 patients spanning the period 2013-2024. We evaluated a task-specific end-to-end AI model and two foundation models in terms of their concordance with pathologists and consistency across samples digitised on three scanner models (Hamamatsu, Leica, and Grundium).
  Findings: Grading concordance between AI and pathologists was similar to pathologist-pathologist concordance with Cohen's quadratically weighted kappa 0.801 vs. 0.799 (p=0.9824). Cross-scanner concordance was high (quadratically weighted kappa > 0.90) for all AI models and scanner pairs, including low-cost compact scanner.
  Interpretation: AI models demonstrated pathologist-level performance in prostate histopathology assessment. Compact scanners can provide a route for validation studies in non-digitalised settings and enable cost-effective adoption of AI in laboratories with limited sample volumes. This first openly available digital pathology dataset from the Middle East supports further research into globally equitable AI pathology.
  Funding: SciLifeLab and Wallenberg Data Driven Life Science Program, Instrumentarium Science Foundation, Karolinska Institutet Research Foundation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</title>
<link>https://arxiv.org/abs/2512.17504</link>
<guid>https://arxiv.org/abs/2512.17504</guid>
<content:encoded><![CDATA[

arXiv:2512.17504v1 Announce Type: new 
Abstract: Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection</title>
<link>https://arxiv.org/abs/2512.17514</link>
<guid>https://arxiv.org/abs/2512.17514</guid>
<content:encoded><![CDATA[

arXiv:2512.17514v1 Announce Type: new 
Abstract: Current state-of-the-art approaches in Source-Free Object Detection (SFOD) typically rely on Mean-Teacher self-labeling. However, domain shift often reduces the detector's ability to maintain strong object-focused representations, causing high-confidence activations over background clutter. This weak object focus results in unreliable pseudo-labels from the detection head. While prior works mainly refine these pseudo-labels, they overlook the underlying need to strengthen the feature space itself. We propose FALCON-SFOD (Foundation-Aligned Learning with Clutter suppression and Noise robustness), a framework designed to enhance object-focused adaptation under domain shift. It consists of two complementary components. SPAR (Spatial Prior-Aware Regularization) leverages the generalization strength of vision foundation models to regularize the detector's feature space. Using class-agnostic binary masks derived from OV-SAM, SPAR promotes structured and foreground-focused activations by guiding the network toward object regions. IRPL (Imbalance-aware Noise Robust Pseudo-Labeling) complements SPAR by promoting balanced and noise-tolerant learning under severe foreground-background imbalance. Guided by a theoretical analysis that connects these designs to tighter localization and classification error bounds, FALCON-SFOD achieves competitive performance across SFOD benchmarks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathBench-MIL: A Comprehensive AutoML and Benchmarking Framework for Multiple Instance Learning in Histopathology</title>
<link>https://arxiv.org/abs/2512.17517</link>
<guid>https://arxiv.org/abs/2512.17517</guid>
<content:encoded><![CDATA[

arXiv:2512.17517v1 Announce Type: new 
Abstract: We introduce PathBench-MIL, an open-source AutoML and benchmarking framework for multiple instance learning (MIL) in histopathology. The system automates end-to-end MIL pipeline construction, including preprocessing, feature extraction, and MIL-aggregation, and provides reproducible benchmarking of dozens of MIL models and feature extractors. PathBench-MIL integrates visualization tooling, a unified configuration system, and modular extensibility, enabling rapid experimentation and standardization across datasets and tasks. PathBench-MIL is publicly available at https://github.com/Sbrussee/PathBench-MIL
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding</title>
<link>https://arxiv.org/abs/2512.17532</link>
<guid>https://arxiv.org/abs/2512.17532</guid>
<content:encoded><![CDATA[

arXiv:2512.17532v1 Announce Type: new 
Abstract: Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLEG: Feed-Forward Language Embedded Gaussian Splatting from Any Views</title>
<link>https://arxiv.org/abs/2512.17541</link>
<guid>https://arxiv.org/abs/2512.17541</guid>
<content:encoded><![CDATA[

arXiv:2512.17541v1 Announce Type: new 
Abstract: We present FLEG, a feed-forward network that reconstructs language-embedded 3D Gaussians from any views. Previous straightforward solutions combine feed-forward reconstruction with Gaussian heads but suffer from fixed input views and insufficient 3D training data. In contrast, we propose a 3D-annotation-free training framework for 2D-to-3D lifting from arbitrary uncalibrated and unposed multi-view images. Since the framework does not require 3D annotations, we can leverage large-scale video data with easily obtained 2D instance information to enrich semantic embedding. We also propose an instance-guided contrastive learning to align 2D semantics with the 3D representations. In addition, to mitigate the high memory and computational cost of dense views, we further propose a geometry-semantic hierarchical sparsification strategy. Our FLEG efficiently reconstructs language-embedded 3D Gaussian representation in a feed-forward manner from arbitrary sparse or dense views, jointly producing accurate geometry, high-fidelity appearance, and language-aligned semantics. Extensive experiments show that it outperforms existing methods on various related tasks. Project page: https://fangzhou2000.github.io/projects/fleg.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image</title>
<link>https://arxiv.org/abs/2512.17545</link>
<guid>https://arxiv.org/abs/2512.17545</guid>
<content:encoded><![CDATA[

arXiv:2512.17545v1 Announce Type: new 
Abstract: With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G3Splat: Geometrically Consistent Generalizable Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.17547</link>
<guid>https://arxiv.org/abs/2512.17547</guid>
<content:encoded><![CDATA[

arXiv:2512.17547v1 Announce Type: new 
Abstract: 3D Gaussians have recently emerged as an effective scene representation for real-time splatting and accurate novel-view synthesis, motivating several works to adapt multi-view structure prediction networks to regress per-pixel 3D Gaussians from images. However, most prior work extends these networks to predict additional Gaussian parameters -- orientation, scale, opacity, and appearance -- while relying almost exclusively on view-synthesis supervision. We show that a view-synthesis loss alone is insufficient to recover geometrically meaningful splats in this setting. We analyze and address the ambiguities of learning 3D Gaussian splats under self-supervision for pose-free generalizable splatting, and introduce G3Splat, which enforces geometric priors to obtain geometrically consistent 3D scene representations. Trained on RE10K, our approach achieves state-of-the-art performance in (i) geometrically consistent reconstruction, (ii) relative pose estimation, and (iii) novel-view synthesis. We further demonstrate strong zero-shot generalization on ScanNet, substantially outperforming prior work in both geometry recovery and relative pose estimation. Code and pretrained models are released on our project page (https://m80hz.github.io/g3splat/).
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points</title>
<link>https://arxiv.org/abs/2512.17566</link>
<guid>https://arxiv.org/abs/2512.17566</guid>
<content:encoded><![CDATA[

arXiv:2512.17566v1 Announce Type: new 
Abstract: T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoomEditor++: A Parameter-Sharing Diffusion Architecture for High-Fidelity Furniture Synthesis</title>
<link>https://arxiv.org/abs/2512.17573</link>
<guid>https://arxiv.org/abs/2512.17573</guid>
<content:encoded><![CDATA[

arXiv:2512.17573v1 Announce Type: new 
Abstract: Virtual furniture synthesis, which seamlessly integrates reference objects into indoor scenes while maintaining geometric coherence and visual realism, holds substantial promise for home design and e-commerce applications. However, this field remains underexplored due to the scarcity of reproducible benchmarks and the limitations of existing image composition methods in achieving high-fidelity furniture synthesis while preserving background integrity. To overcome these challenges, we first present RoomBench++, a comprehensive and publicly available benchmark dataset tailored for this task. It consists of 112,851 training pairs and 1,832 testing pairs drawn from both real-world indoor videos and realistic home design renderings, thereby supporting robust training and evaluation under practical conditions. Then, we propose RoomEditor++, a versatile diffusion-based architecture featuring a parameter-sharing dual diffusion backbone, which is compatible with both U-Net and DiT architectures. This design unifies the feature extraction and inpainting processes for reference and background images. Our in-depth analysis reveals that the parameter-sharing mechanism enforces aligned feature representations, facilitating precise geometric transformations, texture preservation, and seamless integration. Extensive experiments validate that RoomEditor++ is superior over state-of-the-art approaches in terms of quantitative metrics, qualitative assessments, and human preference studies, while highlighting its strong generalization to unseen indoor scenes and general scenes without task-specific fine-tuning. The dataset and source code are available at \url{https://github.com/stonecutter-21/roomeditor}.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3One2: One-step Regression Plus One-step Diffusion for One-hot Modulation in Dual-path Video Snapshot Compressive Imaging</title>
<link>https://arxiv.org/abs/2512.17578</link>
<guid>https://arxiv.org/abs/2512.17578</guid>
<content:encoded><![CDATA[

arXiv:2512.17578v1 Announce Type: new 
Abstract: Video snapshot compressive imaging (SCI) captures dynamic scene sequences through a two-dimensional (2D) snapshot, fundamentally relying on optical modulation for hardware compression and the corresponding software reconstruction. While mainstream video SCI using random binary modulation has demonstrated success, it inevitably results in temporal aliasing during compression. One-hot modulation, activating only one sub-frame per pixel, provides a promising solution for achieving perfect temporal decoupling, thereby alleviating issues associated with aliasing. However, no algorithms currently exist to fully exploit this potential. To bridge this gap, we propose an algorithm specifically designed for one-hot masks. First, leveraging the decoupling properties of one-hot modulation, we transform the reconstruction task into a generative video inpainting problem and introduce a stochastic differential equation (SDE) of the forward process that aligns with the hardware compression process. Next, we identify limitations of the pure diffusion method for video SCI and propose a novel framework that combines one-step regression initialization with one-step diffusion refinement. Furthermore, to mitigate the spatial degradation caused by one-hot modulation, we implement a dual optical path at the hardware level, utilizing complementary information from another path to enhance the inpainted video. To our knowledge, this is the first work integrating diffusion into video SCI reconstruction. Experiments conducted on synthetic datasets and real scenes demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medical Imaging AI Competitions Lack Fairness</title>
<link>https://arxiv.org/abs/2512.17581</link>
<guid>https://arxiv.org/abs/2512.17581</guid>
<content:encoded><![CDATA[

arXiv:2512.17581v1 Announce Type: new 
Abstract: Benchmarking competitions are central to the development of artificial intelligence (AI) in medical imaging, defining performance standards and shaping methodological progress. However, it remains unclear whether these benchmarks provide data that are sufficiently representative, accessible, and reusable to support clinically meaningful AI. In this work, we assess fairness along two complementary dimensions: (1) whether challenge datasets are representative of real-world clinical diversity, and (2) whether they are accessible and legally reusable in line with the FAIR principles. To address this question, we conducted a large-scale systematic study of 241 biomedical image analysis challenges comprising 458 tasks across 19 imaging modalities. Our findings show substantial biases in dataset composition, including geographic location, modality-, and problem type-related biases, indicating that current benchmarks do not adequately reflect real-world clinical diversity. Despite their widespread influence, challenge datasets were frequently constrained by restrictive or ambiguous access conditions, inconsistent or non-compliant licensing practices, and incomplete documentation, limiting reproducibility and long-term reuse. Together, these shortcomings expose foundational fairness limitations in our benchmarking ecosystem and highlight a disconnect between leaderboard success and clinical relevance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeadHunt-VAD: Hunting Robust Anomaly-Sensitive Heads in MLLM for Tuning-Free Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.17601</link>
<guid>https://arxiv.org/abs/2512.17601</guid>
<content:encoded><![CDATA[

arXiv:2512.17601v1 Announce Type: new 
Abstract: Video Anomaly Detection (VAD) aims to locate events that deviate from normal patterns in videos. Traditional approaches often rely on extensive labeled data and incur high computational costs. Recent tuning-free methods based on Multimodal Large Language Models (MLLMs) offer a promising alternative by leveraging their rich world knowledge. However, these methods typically rely on textual outputs, which introduces information loss, exhibits normalcy bias, and suffers from prompt sensitivity, making them insufficient for capturing subtle anomalous cues. To address these constraints, we propose HeadHunt-VAD, a novel tuning-free VAD paradigm that bypasses textual generation by directly hunting robust anomaly-sensitive internal attention heads within the frozen MLLM. Central to our method is a Robust Head Identification module that systematically evaluates all attention heads using a multi-criteria analysis of saliency and stability, identifying a sparse subset of heads that are consistently discriminative across diverse prompts. Features from these expert heads are then fed into a lightweight anomaly scorer and a temporal locator, enabling efficient and accurate anomaly detection with interpretable outputs. Extensive experiments show that HeadHunt-VAD achieves state-of-the-art performance among tuning-free methods on two major VAD benchmarks while maintaining high efficiency, validating head-level probing in MLLMs as a powerful and practical solution for real-world anomaly detection.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGRegBench: A Novel Benchmark Dataset with Anatomical Landmarks for Mammography Image Registration</title>
<link>https://arxiv.org/abs/2512.17605</link>
<guid>https://arxiv.org/abs/2512.17605</guid>
<content:encoded><![CDATA[

arXiv:2512.17605v1 Announce Type: new 
Abstract: Robust mammography registration is essential for clinical applications like tracking disease progression and monitoring longitudinal changes in breast tissue. However, progress has been limited by the absence of public datasets and standardized benchmarks. Existing studies are often not directly comparable, as they use private data and inconsistent evaluation frameworks. To address this, we present MGRegBench, a public benchmark dataset for mammogram registration. It comprises over 5,000 image pairs, with 100 containing manual anatomical landmarks and segmentation masks for rigorous evaluation. This makes MGRegBench one of the largest public 2D registration datasets with manual annotations. Using this resource, we benchmarked diverse registration methods including classical (ANTs), learning-based (VoxelMorph, TransMorph), implicit neural representation (IDIR), a classic mammography-specific approach, and a recent state-of-the-art deep learning method MammoRegNet. The implementations were adapted to this modality from the authors' implementations or re-implemented from scratch. Our contributions are: (1) the first public dataset of this scale with manual landmarks and masks for mammography registration; (2) the first like-for-like comparison of diverse methods on this modality; and (3) an extensive analysis of deep learning-based registration. We publicly release our code and data to establish a foundational resource for fair comparisons and catalyze future research. The source code and data are at https://github.com/KourtKardash/MGRegBench.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised 3D Segmentation for Type-B Aortic Dissection with Slim UNETR</title>
<link>https://arxiv.org/abs/2512.17610</link>
<guid>https://arxiv.org/abs/2512.17610</guid>
<content:encoded><![CDATA[

arXiv:2512.17610v1 Announce Type: new 
Abstract: Convolutional neural networks (CNN) for multi-class segmentation of medical images are widely used today. Especially models with multiple outputs that can separately predict segmentation classes (regions) without relying on a probabilistic formulation of the segmentation of regions. These models allow for more precise segmentation by tailoring the network's components to each class (region). They have a common encoder part of the architecture but branch out at the output layers, leading to improved accuracy.
  These methods are used to diagnose type B aortic dissection (TBAD), which requires accurate segmentation of aortic structures based on the ImageTBDA dataset, which contains 100 3D computed tomography angiography (CTA) images. These images identify three key classes: true lumen (TL), false lumen (FL), and false lumen thrombus (FLT) of the aorta, which is critical for diagnosis and treatment decisions. In the dataset, 68 examples have a false lumen, while the remaining 32 do not, creating additional complexity for pathology detection.
  However, implementing these CNN methods requires a large amount of high-quality labeled data. Obtaining accurate labels for the regions of interest can be an expensive and time-consuming process, particularly for 3D data. Semi-supervised learning methods allow models to be trained by using both labeled and unlabeled data, which is a promising approach for overcoming the challenge of obtaining accurate labels. However, these learning methods are not well understood for models with multiple outputs.
  This paper presents a semi-supervised learning method for models with multiple outputs. The method is based on the additional rotations and flipping, and does not assume the probabilistic nature of the model's responses. This makes it a universal approach, which is especially important for architectures that involve separate segmentation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution</title>
<link>https://arxiv.org/abs/2512.17612</link>
<guid>https://arxiv.org/abs/2512.17612</guid>
<content:encoded><![CDATA[

arXiv:2512.17612v1 Announce Type: new 
Abstract: High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions. This physics-informed objective allows the models to learn from clinical wMRI without HR qMRI supervision. To validate the concept, we generate training data by synthesizing wMRI guides from HR qMRI using signal equations, then degrading qMRI resolution via k-space truncation. A deep neural network learns the super-resolution mapping. Ablation experiments demonstrate that T1-weighted images primarily enhance T1 maps, T2-weighted images improve T2 maps, and combined guidance optimally enhances all parameters simultaneously. Validation on independently acquired in-vivo data from a different qMRI sequence confirms cross-qMRI sequence generalizability. Models trained on synthetic data can produce super-resolved maps from a 1-minute acquisition with quality comparable to a 5-minute reference scan, leveraging the scanner-independent nature of relaxometry parameters. By decoupling training from HR qMRI requirement, our framework enables fast qMRI acquisitions enhanced via routine clinical images, offering a practical pathway for integrating quantitative relaxometry into clinical workflows with acceptable additional scan time.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoMV2D: A Sparse Temporal Stereo-Enhanced Framework for Robust Multi-View 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.17620</link>
<guid>https://arxiv.org/abs/2512.17620</guid>
<content:encoded><![CDATA[

arXiv:2512.17620v1 Announce Type: new 
Abstract: Multi-view 3D object detection is a fundamental task in autonomous driving perception, where achieving a balance between detection accuracy and computational efficiency remains crucial. Sparse query-based 3D detectors efficiently aggregate object-relevant features from multi-view images through a set of learnable queries, offering a concise and end-to-end detection paradigm. Building on this foundation, MV2D leverages 2D detection results to provide high-quality object priors for query initialization, enabling higher precision and recall. However, the inherent depth ambiguity in single-frame 2D detections still limits the accuracy of 3D query generation. To address this issue, we propose StereoMV2D, a unified framework that integrates temporal stereo modeling into the 2D detection-guided multi-view 3D detector. By exploiting cross-temporal disparities of the same object across adjacent frames, StereoMV2D enhances depth perception and refines the query priors, while performing all computations efficiently within 2D regions of interest (RoIs). Furthermore, a dynamic confidence gating mechanism adaptively evaluates the reliability of temporal stereo cues through learning statistical patterns derived from the inter-frame matching matrix together with appearance consistency, ensuring robust detection under object appearance and occlusion. Extensive experiments on the nuScenes and Argoverse 2 datasets demonstrate that StereoMV2D achieves superior detection performance without incurring significant computational overhead. Code will be available at https://github.com/Uddd821/StereoMV2D.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology</title>
<link>https://arxiv.org/abs/2512.17621</link>
<guid>https://arxiv.org/abs/2512.17621</guid>
<content:encoded><![CDATA[

arXiv:2512.17621v1 Announce Type: new 
Abstract: While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs</title>
<link>https://arxiv.org/abs/2512.17640</link>
<guid>https://arxiv.org/abs/2512.17640</guid>
<content:encoded><![CDATA[

arXiv:2512.17640v1 Announce Type: new 
Abstract: Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Region-Constraint In-Context Generation for Instructional Video Editing</title>
<link>https://arxiv.org/abs/2512.17650</link>
<guid>https://arxiv.org/abs/2512.17650</guid>
<content:encoded><![CDATA[

arXiv:2512.17650v1 Announce Type: new 
Abstract: The In-context generation paradigm recently has demonstrated strong power in instructional image editing with both data efficiency and synthesis quality. Nevertheless, shaping such in-context learning for instruction-based video editing is not trivial. Without specifying editing regions, the results can suffer from the problem of inaccurate editing regions and the token interference between editing and non-editing areas during denoising. To address these, we present ReCo, a new instructional video editing paradigm that novelly delves into constraint modeling between editing and non-editing regions during in-context generation. Technically, ReCo width-wise concatenates source and target video for joint denoising. To calibrate video diffusion learning, ReCo capitalizes on two regularization terms, i.e., latent and attention regularization, conducting on one-step backward denoised latents and attention maps, respectively. The former increases the latent discrepancy of the editing region between source and target videos while reducing that of non-editing areas, emphasizing the modification on editing area and alleviating outside unexpected content generation. The latter suppresses the attention of tokens in the editing region to the tokens in counterpart of the source video, thereby mitigating their interference during novel object generation in target video. Furthermore, we propose a large-scale, high-quality video editing dataset, i.e., ReCo-Data, comprising 500K instruction-video pairs to benefit model training. Extensive experiments conducted on four major instruction-based video editing tasks demonstrate the superiority of our proposal.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bitbox: Behavioral Imaging Toolbox for Computational Analysis of Behavior from Videos</title>
<link>https://arxiv.org/abs/2512.17655</link>
<guid>https://arxiv.org/abs/2512.17655</guid>
<content:encoded><![CDATA[

arXiv:2512.17655v1 Announce Type: new 
Abstract: Computational measurement of human behavior from video has recently become feasible due to major advances in AI. These advances now enable granular and precise quantification of facial expression, head movement, body action, and other behavioral modalities and are increasingly used in psychology, psychiatry, neuroscience, and mental health research. However, mainstream adoption remains slow. Most existing methods and software are developed for engineering audiences, require specialized software stacks, and fail to provide behavioral measurements at a level directly useful for hypothesis-driven research. As a result, there is a large barrier to entry for researchers who wish to use modern, AI-based tools in their work. We introduce Bitbox, an open-source toolkit designed to remove this barrier and make advanced computational analysis directly usable by behavioral scientists and clinical researchers. Bitbox is guided by principles of reproducibility, modularity, and interpretability. It provides a standardized interface for extracting high-level behavioral measurements from video, leveraging multiple face, head, and body processors. The core modules have been tested and validated on clinical samples and are designed so that new measures can be added with minimal effort. Bitbox is intended to serve both sides of the translational gap. It gives behavioral researchers access to robust, high-level behavioral metrics without requiring engineering expertise, and it provides computer scientists a practical mechanism for disseminating methods to domains where their impact is most needed. We expect that Bitbox will accelerate integration of computational behavioral measurement into behavioral, clinical, and mental health research. Bitbox has been designed from the beginning as a community-driven effort that will evolve through contributions from both method developers and domain scientists.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Spatio-Temporal Feature Representations for Video-Based Gaze Estimation</title>
<link>https://arxiv.org/abs/2512.17673</link>
<guid>https://arxiv.org/abs/2512.17673</guid>
<content:encoded><![CDATA[

arXiv:2512.17673v1 Announce Type: new 
Abstract: Video-based gaze estimation methods aim to capture the inherently temporal dynamics of human eye gaze from multiple image frames. However, since models must capture both spatial and temporal relationships, performance is limited by the feature representations within a frame but also between multiple frames. We propose the Spatio-Temporal Gaze Network (ST-Gaze), a model that combines a CNN backbone with dedicated channel attention and self-attention modules to fuse eye and face features optimally. The fused features are then treated as a spatial sequence, allowing for the capture of an intra-frame context, which is then propagated through time to model inter-frame dynamics. We evaluated our method on the EVE dataset and show that ST-Gaze achieves state-of-the-art performance both with and without person-specific adaptation. Additionally, our ablation study provides further insights into the model performance, showing that preserving and modelling intra-frame spatial context with our spatio-temporal recurrence is fundamentally superior to premature spatial pooling. As such, our results pave the way towards more robust video-based gaze estimation using commonly available cameras.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Study of Sampling Hyperparameters in Diffusion-Based Super-Resolution</title>
<link>https://arxiv.org/abs/2512.17675</link>
<guid>https://arxiv.org/abs/2512.17675</guid>
<content:encoded><![CDATA[

arXiv:2512.17675v1 Announce Type: new 
Abstract: Diffusion models have shown strong potential for solving inverse problems such as single-image super-resolution, where a high-resolution image is recovered from a low-resolution observation using a pretrained unconditional prior. Conditioning methods, including Diffusion Posterior Sampling (DPS) and Manifold Constrained Gradient (MCG), can substantially improve reconstruction quality, but they introduce additional hyperparameters that require careful tuning. In this work, we conduct an empirical ablation study on FFHQ super-resolution to identify the dominant factors affecting performance when applying conditioning to pretrained diffusion models, and show that the conditioning step size has a significantly greater impact than the diffusion step count, with step sizes in the range of [2.0, 3.0] yielding the best overall performance in our experiments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlexAvatar: Flexible Large Reconstruction Model for Animatable Gaussian Head Avatars with Detailed Deformation</title>
<link>https://arxiv.org/abs/2512.17717</link>
<guid>https://arxiv.org/abs/2512.17717</guid>
<content:encoded><![CDATA[

arXiv:2512.17717v1 Announce Type: new 
Abstract: We present FlexAvatar, a flexible large reconstruction model for high-fidelity 3D head avatars with detailed dynamic deformation from single or sparse images, without requiring camera poses or expression labels. It leverages a transformer-based reconstruction model with structured head query tokens as canonical anchor to aggregate flexible input-number-agnostic, camera-pose-free and expression-free inputs into a robust canonical 3D representation. For detailed dynamic deformation, we introduce a lightweight UNet decoder conditioned on UV-space position maps, which can produce detailed expression-dependent deformations in real time. To better capture rare but critical expressions like wrinkles and bared teeth, we also adopt a data distribution adjustment strategy during training to balance the distribution of these expressions in the training set. Moreover, a lightweight 10-second refinement can further enhances identity-specific details in extreme identities without affecting deformation quality. Extensive experiments demonstrate that our FlexAvatar achieves superior 3D consistency, detailed dynamic realism compared with previous methods, providing a practical solution for animatable 3D avatar creation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAVeD: A First-Person Social Media Video Dataset for ADAS-equipped vehicle Near-Miss and Crash Event Analyses</title>
<link>https://arxiv.org/abs/2512.17724</link>
<guid>https://arxiv.org/abs/2512.17724</guid>
<content:encoded><![CDATA[

arXiv:2512.17724v1 Announce Type: new 
Abstract: The advancement of safety-critical research in driving behavior in ADAS-equipped vehicles require real-world datasets that not only include diverse traffic scenarios but also capture high-risk edge cases such as near-miss events and system failures. However, existing datasets are largely limited to either simulated environments or human-driven vehicle data, lacking authentic ADAS (Advanced Driver Assistance System) vehicle behavior under risk conditions. To address this gap, this paper introduces SAVeD, a large-scale video dataset curated from publicly available social media content, explicitly focused on ADAS vehicle-related crashes, near-miss incidents, and disengagements. SAVeD features 2,119 first-person videos, capturing ADAS vehicle operations in diverse locations, lighting conditions, and weather scenarios. The dataset includes video frame-level annotations for collisions, evasive maneuvers, and disengagements, enabling analysis of both perception and decision-making failures. We demonstrate SAVeD's utility through multiple analyses and contributions: (1) We propose a novel framework integrating semantic segmentation and monocular depth estimation to compute real-time Time-to-Collision (TTC) for dynamic objects. (2) We utilize the Generalized Extreme Value (GEV) distribution to model and quantify the extreme risk in crash and near-miss events across different roadway types. (3) We establish benchmarks for state-of-the-art VLLMs (VideoLLaMA2 and InternVL2.5 HiCo R16), showing that SAVeD's detailed annotations significantly enhance model performance through domain adaptation in complex near-miss scenarios.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaMIL+: Modeling Long-Term Contextual Patterns for Gigapixel Whole Slide Image</title>
<link>https://arxiv.org/abs/2512.17726</link>
<guid>https://arxiv.org/abs/2512.17726</guid>
<content:encoded><![CDATA[

arXiv:2512.17726v1 Announce Type: new 
Abstract: Whole-slide images (WSIs) are an important data modality in computational pathology, yet their gigapixel resolution and lack of fine-grained annotations challenge conventional deep learning models. Multiple instance learning (MIL) offers a solution by treating each WSI as a bag of patch-level instances, but effectively modeling ultra-long sequences with rich spatial context remains difficult. Recently, Mamba has emerged as a promising alternative for long sequence learning, scaling linearly to thousands of tokens. However, despite its efficiency, it still suffers from limited spatial context modeling and memory decay, constraining its effectiveness to WSI analysis. To address these limitations, we propose MambaMIL+, a new MIL framework that explicitly integrates spatial context while maintaining long-range dependency modeling without memory forgetting. Specifically, MambaMIL+ introduces 1) overlapping scanning, which restructures the patch sequence to embed spatial continuity and instance correlations; 2) a selective stripe position encoder (S2PE) that encodes positional information while mitigating the biases of fixed scanning orders; and 3) a contextual token selection (CTS) mechanism, which leverages supervisory knowledge to dynamically enlarge the contextual memory for stable long-range modeling. Extensive experiments on 20 benchmarks across diagnostic classification, molecular prediction, and survival analysis demonstrate that MambaMIL+ consistently achieves state-of-the-art performance under three feature extractors (ResNet-50, PLIP, and CONCH), highlighting its effectiveness and robustness for large-scale computational pathology
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection</title>
<link>https://arxiv.org/abs/2512.17730</link>
<guid>https://arxiv.org/abs/2512.17730</guid>
<content:encoded><![CDATA[

arXiv:2512.17730v1 Announce Type: new 
Abstract: Recent advances in image generation have led to the widespread availability of highly realistic synthetic media, increasing the difficulty of reliable deepfake detection. A key challenge is generalization, as detectors trained on a narrow class of generators often fail when confronted with unseen models. In this work, we address the pressing need for generalizable detection by leveraging large vision-language models, specifically CLIP, to identify synthetic content across diverse generative techniques. First, we introduce Diff-Gen, a large-scale benchmark dataset comprising 100k diffusion-generated fakes that capture broad spectral artifacts unlike traditional GAN datasets. Models trained on Diff-Gen demonstrate stronger cross-domain generalization, particularly on previously unseen image generators. Second, we propose AdaptPrompt, a parameter-efficient transfer learning framework that jointly learns task-specific textual prompts and visual adapters while keeping the CLIP backbone frozen. We further show via layer ablation that pruning the final transformer block of the vision encoder enhances the retention of high-frequency generative artifacts, significantly boosting detection accuracy. Our evaluation spans 25 challenging test sets, covering synthetic content generated by GANs, diffusion models, and commercial tools, establishing a new state-of-the-art in both standard and cross-domain scenarios. We further demonstrate the framework's versatility through few-shot generalization (using as few as 320 images) and source attribution, enabling the precise identification of generator architectures in closed-set settings.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image</title>
<link>https://arxiv.org/abs/2512.17773</link>
<guid>https://arxiv.org/abs/2512.17773</guid>
<content:encoded><![CDATA[

arXiv:2512.17773v1 Announce Type: new 
Abstract: Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence</title>
<link>https://arxiv.org/abs/2512.17781</link>
<guid>https://arxiv.org/abs/2512.17781</guid>
<content:encoded><![CDATA[

arXiv:2512.17781v1 Announce Type: new 
Abstract: Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover</title>
<link>https://arxiv.org/abs/2512.17782</link>
<guid>https://arxiv.org/abs/2512.17782</guid>
<content:encoded><![CDATA[

arXiv:2512.17782v1 Announce Type: new 
Abstract: Satellite-derived Land Surface Temperature (LST) products are central to surface urban heat island (SUHI) monitoring due to their consistent grid-based coverage over large metropolitan regions. However, cloud contamination frequently obscures LST observations, limiting their usability for continuous SUHI analysis. Most existing LST reconstruction methods rely on multitemporal information or multisensor data fusion, requiring auxiliary observations that may be unavailable or unreliable under persistent cloud cover. Purely spatial gap-filling approaches offer an alternative, but traditional statistical methods degrade under large or spatially contiguous gaps, while many deep learning based spatial models deteriorate rapidly with increasing missingness.
  Recent advances in denoising diffusion based image inpainting models have demonstrated improved robustness under high missingness, motivating their adoption for spatial LST reconstruction. In this work, we introduce UrbanDIFF, a purely spatial denoising diffusion model for reconstructing cloud contaminated urban LST imagery. The model is conditioned on static urban structure information, including built-up surface data and a digital elevation model, and enforces strict consistency with revealed cloud free pixels through a supervised pixel guided refinement step during inference.
  UrbanDIFF is trained and evaluated using NASA MODIS Terra LST data from seven major United States metropolitan areas spanning 2002 to 2025. Experiments using synthetic cloud masks with 20 to 85 percent coverage show that UrbanDIFF consistently outperforms an interpolation baseline, particularly under dense cloud occlusion, achieving SSIM of 0.89, RMSE of 1.2 K, and R2 of 0.84 at 85 percent cloud coverage, while exhibiting slower performance degradation as cloud density increases.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Range depth estimation using learning based Hybrid Distortion Model for CCTV cameras</title>
<link>https://arxiv.org/abs/2512.17784</link>
<guid>https://arxiv.org/abs/2512.17784</guid>
<content:encoded><![CDATA[

arXiv:2512.17784v1 Announce Type: new 
Abstract: Accurate camera models are essential for photogrammetry applications such as 3D mapping and object localization, particularly for long distances. Various stereo-camera based 3D localization methods are available but are limited to few hundreds of meters' range. This is majorly due to the limitation of the distortion models assumed for the non-linearities present in the camera lens. This paper presents a framework for modeling a suitable distortion model that can be used for localizing the objects at longer distances. It is well known that neural networks can be a better alternative to model a highly complex non-linear lens distortion function; on contrary, it is observed that a direct application of neural networks to distortion models fails to converge to estimate the camera parameters. To resolve this, a hybrid approach is presented in this paper where the conventional distortion models are initially extended to incorporate higher-order terms and then enhanced using neural network based residual correction model. This hybrid approach has substantially improved long-range localization performance and is capable of estimating the 3D position of objects at distances up to 5 kilometres. The estimated 3D coordinates are transformed to GIS coordinates and are plotted on a GIS map for visualization. Experimental validation demonstrates the robustness and effectiveness of proposed framework, offering a practical solution to calibrate CCTV cameras for long-range photogrammetry applications.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animate Any Character in Any World</title>
<link>https://arxiv.org/abs/2512.17796</link>
<guid>https://arxiv.org/abs/2512.17796</guid>
<content:encoded><![CDATA[

arXiv:2512.17796v1 Announce Type: new 
Abstract: Recent advances in world models have greatly enhanced interactive environment simulation. Existing methods mainly fall into two categories: (1) static world generation models, which construct 3D environments without active agents, and (2) controllable-entity models, which allow a single entity to perform limited actions in an otherwise uncontrollable environment. In this work, we introduce AniX, leveraging the realism and structural grounding of static world generation while extending controllable-entity models to support user-specified characters capable of performing open-ended actions. Users can provide a 3DGS scene and a character, then direct the character through natural language to perform diverse behaviors from basic locomotion to object-centric interactions while freely exploring the environment. AniX synthesizes temporally coherent video clips that preserve visual fidelity with the provided scene and character, formulated as a conditional autoregressive video generation problem. Built upon a pre-trained video generator, our training strategy significantly enhances motion dynamics while maintaining generalization across actions and characters. Our evaluation covers a broad range of aspects, including visual quality, character consistency, action controllability, and long-horizon coherence.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chorus: Multi-Teacher Pretraining for Holistic 3D Gaussian Scene Encoding</title>
<link>https://arxiv.org/abs/2512.17817</link>
<guid>https://arxiv.org/abs/2512.17817</guid>
<content:encoded><![CDATA[

arXiv:2512.17817v1 Announce Type: new 
Abstract: While 3DGS has emerged as a high-fidelity scene representation, encoding rich, general-purpose features directly from its primitives remains under-explored. We address this gap by introducing Chorus, a multi-teacher pretraining framework that learns a holistic feed-forward 3D Gaussian Splatting (3DGS) scene encoder by distilling complementary signals from 2D foundation models. Chorus employs a shared 3D encoder and teacher-specific projectors to learn from language-aligned, generalist, and object-aware teachers, encouraging a shared embedding space that captures signals from high-level semantics to fine-grained structure.
  We evaluate Chorus on a wide range of tasks: open-vocabulary semantic and instance segmentation, linear and decoder probing, as well as data-efficient supervision. Besides 3DGS, we also test Chorus on several benchmarks that only support point clouds by pretraining a variant using only Gaussians' centers, colors, estimated normals as inputs. Interestingly, this encoder shows strong transfer and outperforms the point clouds baseline while using 39.9 times fewer training scenes. Finally, we propose a render-and-distill adaptation that facilitates out-of-domain finetuning. Our code and model will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges</title>
<link>https://arxiv.org/abs/2512.17838</link>
<guid>https://arxiv.org/abs/2512.17838</guid>
<content:encoded><![CDATA[

arXiv:2512.17838v1 Announce Type: new 
Abstract: Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&amp;D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.17851</link>
<guid>https://arxiv.org/abs/2512.17851</guid>
<content:encoded><![CDATA[

arXiv:2512.17851v1 Announce Type: new 
Abstract: Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-Driven Deep Learning Framework for Raman Spectral Denoising Under Fluorescence-Dominant Conditions</title>
<link>https://arxiv.org/abs/2512.17852</link>
<guid>https://arxiv.org/abs/2512.17852</guid>
<content:encoded><![CDATA[

arXiv:2512.17852v1 Announce Type: new 
Abstract: Raman spectroscopy enables non-destructive, label-free molecular analysis with high specificity, making it a powerful tool for biomedical diagnostics. However, its application to biological tissues is challenged by inherently weak Raman scattering and strong fluorescence background, which significantly degrade signal quality. In this study, we present a simulation-driven denoising framework that combines a statistically grounded noise model with deep learning to enhance Raman spectra acquired under fluorescence-dominated conditions. We comprehensively modeled major noise sources. Based on this model, we generated biologically realistic Raman spectra and used them to train a cascaded deep neural network designed to jointly suppress stochastic detector noise and fluorescence baseline interference. To evaluate the performance of our approach, we simulated human skin spectra derived from real experimental data as a validation case study. Our results demonstrate the potential of physics-informed learning to improve spectral quality and enable faster, more accurate Raman-based tissue analysis.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</title>
<link>https://arxiv.org/abs/2512.17864</link>
<guid>https://arxiv.org/abs/2512.17864</guid>
<content:encoded><![CDATA[

arXiv:2512.17864v1 Announce Type: new 
Abstract: Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InSPECT: Invariant Spectral Features Preservation of Diffusion Models</title>
<link>https://arxiv.org/abs/2512.17873</link>
<guid>https://arxiv.org/abs/2512.17873</guid>
<content:encoded><![CDATA[

arXiv:2512.17873v1 Announce Type: new 
Abstract: Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visually Prompted Benchmarks Are Surprisingly Fragile</title>
<link>https://arxiv.org/abs/2512.17875</link>
<guid>https://arxiv.org/abs/2512.17875</guid>
<content:encoded><![CDATA[

arXiv:2512.17875v1 Announce Type: new 
Abstract: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</title>
<link>https://arxiv.org/abs/2512.17891</link>
<guid>https://arxiv.org/abs/2512.17891</guid>
<content:encoded><![CDATA[

arXiv:2512.17891v1 Announce Type: new 
Abstract: Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadarGen: Automotive Radar Point Cloud Generation from Cameras</title>
<link>https://arxiv.org/abs/2512.17897</link>
<guid>https://arxiv.org/abs/2512.17897</guid>
<content:encoded><![CDATA[

arXiv:2512.17897v1 Announce Type: new 
Abstract: We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Forcing for Multi-Agent Interaction Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.17900</link>
<guid>https://arxiv.org/abs/2512.17900</guid>
<content:encoded><![CDATA[

arXiv:2512.17900v1 Announce Type: new 
Abstract: Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness of Vision in Open Foundation Models</title>
<link>https://arxiv.org/abs/2512.17902</link>
<guid>https://arxiv.org/abs/2512.17902</guid>
<content:encoded><![CDATA[

arXiv:2512.17902v1 Announce Type: new 
Abstract: With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta's Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta's Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dexterous World Models</title>
<link>https://arxiv.org/abs/2512.17907</link>
<guid>https://arxiv.org/abs/2512.17907</guid>
<content:encoded><![CDATA[

arXiv:2512.17907v1 Announce Type: new 
Abstract: Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.
  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.
  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</title>
<link>https://arxiv.org/abs/2512.17908</link>
<guid>https://arxiv.org/abs/2512.17908</guid>
<content:encoded><![CDATA[

arXiv:2512.17908v1 Announce Type: new 
Abstract: Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</title>
<link>https://arxiv.org/abs/2512.17909</link>
<guid>https://arxiv.org/abs/2512.17909</guid>
<content:encoded><![CDATA[

arXiv:2512.17909v1 Announce Type: new 
Abstract: Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder's inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colormap-Enhanced Vision Transformers for MRI-Based Multiclass (4-Class) Alzheimer's Disease Classification</title>
<link>https://arxiv.org/abs/2512.16964</link>
<guid>https://arxiv.org/abs/2512.16964</guid>
<content:encoded><![CDATA[

arXiv:2512.16964v1 Announce Type: cross 
Abstract: Magnetic Resonance Imaging (MRI) plays a pivotal role in the early diagnosis and monitoring of Alzheimer's disease (AD). However, the subtle structural variations in brain MRI scans often pose challenges for conventional deep learning models to extract discriminative features effectively. In this work, we propose PseudoColorViT-Alz, a colormap-enhanced Vision Transformer framework designed to leverage pseudo-color representations of MRI images for improved Alzheimer's disease classification. By combining colormap transformations with the global feature learning capabilities of Vision Transformers, our method amplifies anatomical texture and contrast cues that are otherwise subdued in standard grayscale MRI scans.
  We evaluate PseudoColorViT-Alz on the OASIS-1 dataset using a four-class classification setup (non-demented, moderate dementia, mild dementia, and very mild dementia). Our model achieves a state-of-the-art accuracy of 99.79% with an AUC of 100%, surpassing the performance of recent 2024--2025 methods, including CNN-based and Siamese-network approaches, which reported accuracies ranging from 96.1% to 99.68%. These results demonstrate that pseudo-color augmentation combined with Vision Transformers can significantly enhance MRI-based Alzheimer's disease classification. PseudoColorViT-Alz offers a robust and interpretable framework that outperforms current methods, providing a promising tool to support clinical decision-making and early detection of Alzheimer's disease.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?</title>
<link>https://arxiv.org/abs/2512.17394</link>
<guid>https://arxiv.org/abs/2512.17394</guid>
<content:encoded><![CDATA[

arXiv:2512.17394v1 Announce Type: cross 
Abstract: Theory of Mind (ToM) -- the ability to attribute beliefs, desires, and emotions to others -- is fundamental for human social intelligence, yet remains a major challenge for artificial agents. Existing Vision-Language Models (VLMs) are increasingly applied in socially grounded tasks, but their capacity for cross-cultural ToM reasoning is largely unexplored. In this work, we introduce CulturalToM-VQA, a new evaluation benchmark containing 5095 questions designed to probe ToM reasoning across diverse cultural contexts through visual question answering. The dataset captures culturally grounded cues such as rituals, attire, gestures, and interpersonal dynamics, enabling systematic evaluation of ToM reasoning beyond Western-centric benchmarks. Our dataset is built through a VLM-assisted human-in-the-loop pipeline, where human experts first curate culturally rich images across traditions, rituals, and social interactions; a VLM then assist in generating structured ToM-focused scene descriptions, which are refined into question-answer pairs spanning a taxonomy of six ToM tasks and four graded complexity levels. The resulting dataset covers diverse theory of mind facets such as mental state attribution, false belief reasoning, non-literal communication, social norm violations, perspective coordination, and multi-agent reasoning.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry</title>
<link>https://arxiv.org/abs/2512.17505</link>
<guid>https://arxiv.org/abs/2512.17505</guid>
<content:encoded><![CDATA[

arXiv:2512.17505v1 Announce Type: cross 
Abstract: This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkinGenBench: Generative Model and Preprocessing Effects for Synthetic Dermoscopic Augmentation in Melanoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.17585</link>
<guid>https://arxiv.org/abs/2512.17585</guid>
<content:encoded><![CDATA[

arXiv:2512.17585v1 Announce Type: cross 
Abstract: This work introduces SkinGenBench, a systematic biomedical imaging benchmark that investigates how preprocessing complexity interacts with generative model choice for synthetic dermoscopic image augmentation and downstream melanoma diagnosis. Using a curated dataset of 14,116 dermoscopic images from HAM10000 and MILK10K across five lesion classes, we evaluate the two representative generative paradigms: StyleGAN2-ADA and Denoising Diffusion Probabilistic Models (DDPMs) under basic geometric augmentation and advanced artifact removal pipelines. Synthetic melanoma images are assessed using established perceptual and distributional metrics (FID, KID, IS), feature space analysis, and their impact on diagnostic performance across five downstream classifiers. Experimental results demonstrate that generative architecture choice has a stronger influence on both image fidelity and diagnostic utility than preprocessing complexity. StyleGAN2-ADA consistently produced synthetic images more closely aligned with real data distributions, achieving the lowest FID (~65.5) and KID (~0.05), while diffusion models generated higher variance samples at the cost of reduces perceptual fidelity and class anchoring. Advanced artifact removal yielded only marginal improvements in generative metrics and provided limited downstream diagnostic gains, suggesting possible suppression of clinically relevant texture cues. In contrast, synthetic data augmentation substantially improved melanoma detection with 8-15% absolute gains in melanoma F1-score, and ViT-B/16 achieving F1~0.88 and ROC-AUC~0.98, representing an improvement of approximately 14% over non-augmented baselines. Our code can be found at https://github.com/adarsh-crafts/SkinGenBench
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAD-OOD: A Deep Learning Cluster-Driven Framework for an Out-of-Distribution Malware Detection and Classification</title>
<link>https://arxiv.org/abs/2512.17594</link>
<guid>https://arxiv.org/abs/2512.17594</guid>
<content:encoded><![CDATA[

arXiv:2512.17594v1 Announce Type: cross 
Abstract: Out of distribution (OOD) detection remains a critical challenge in malware classification due to the substantial intra family variability introduced by polymorphic and metamorphic malware variants. Most existing deep learning based malware detectors rely on closed world assumptions and fail to adequately model this intra class variation, resulting in degraded performance when confronted with previously unseen malware families. This paper presents MADOOD, a novel two stage, cluster driven deep learning framework for robust OOD malware detection and classification. In the first stage, malware family embeddings are modeled using class conditional spherical decision boundaries derived from Gaussian Discriminant Analysis (GDA), enabling statistically grounded separation of indistribution and OOD samples without requiring OOD data during training. Z score based distance analysis across multiple class centroids is employed to reliably identify anomalous samples in the latent space. In the second stage, a deep neural network integrates cluster based predictions, refined embeddings, and supervised classifier outputs to enhance final classification accuracy. Extensive evaluations on benchmark malware datasets comprising 25 known families and multiple novel OOD variants demonstrate that MADOOD significantly outperforms state of the art OOD detection methods, achieving an AUC of up to 0.911 on unseen malware families. The proposed framework provides a scalable, interpretable, and statistically principled solution for real world malware detection and anomaly identification in evolving cybersecurity environments.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breast Cancer Neoadjuvant Chemotherapy Treatment Response Prediction Using Aligned Longitudinal MRI and Clinical Data</title>
<link>https://arxiv.org/abs/2512.17759</link>
<guid>https://arxiv.org/abs/2512.17759</guid>
<content:encoded><![CDATA[

arXiv:2512.17759v1 Announce Type: cross 
Abstract: Aim: This study investigates treatment response prediction to neoadjuvant chemotherapy (NACT) in breast cancer patients, using longitudinal contrast-enhanced magnetic resonance images (CE-MRI) and clinical data. The goal is to develop machine learning (ML) models to predict pathologic complete response (PCR binary classification) and 5-year relapse-free survival status (RFS binary classification). Method: The proposed framework includes tumour segmentation, image registration, feature extraction, and predictive modelling. Using the image registration method, MRI image features can be extracted and compared from the original tumour site at different time points, therefore monitoring the intratumor changes during NACT process. Four feature extractors, including one radiomics and three deep learning-based (MedicalNet, Segformer3D, SAM-Med3D) were implemented and compared. In combination with three feature selection methods and four ML models, predictive models are built and compared. Results: The proposed image registration-based feature extraction consistently improves the predictive models. In the PCR and RFS classification tasks logistic regression model trained on radiomic features performed the best with an AUC of 0.88 and classification accuracy of 0.85 for PCR classification, and AUC of 0.78 and classification accuracy of 0.72 for RFS classification. Conclusions: It is evidenced that the image registration method has significantly improved performance in longitudinal feature learning in predicting PCR and RFS. The radiomics feature extractor is more effective than the pre-trained deep learning feature extractors, with higher performance and better interpretability.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedNeXt-v2: Scaling 3D ConvNeXts for Large-Scale Supervised Representation Learning in Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.17774</link>
<guid>https://arxiv.org/abs/2512.17774</guid>
<content:encoded><![CDATA[

arXiv:2512.17774v1 Announce Type: cross 
Abstract: Large-scale supervised pretraining is rapidly reshaping 3D medical image segmentation. However, existing efforts focus primarily on increasing dataset size and overlook the question of whether the backbone network is an effective representation learner at scale. In this work, we address this gap by revisiting ConvNeXt-based architectures for volumetric segmentation and introducing MedNeXt-v2, a compound-scaled 3D ConvNeXt that leverages improved micro-architecture and data scaling to deliver state-of-the-art performance. First, we show that routinely used backbones in large-scale pretraining pipelines are often suboptimal. Subsequently, we use comprehensive backbone benchmarking prior to scaling and demonstrate that stronger from scratch performance reliably predicts stronger downstream performance after pretraining. Guided by these findings, we incorporate a 3D Global Response Normalization module and use depth, width, and context scaling to improve our architecture for effective representation learning. We pretrain MedNeXt-v2 on 18k CT volumes and demonstrate state-of-the-art performance when fine-tuning across six challenging CT and MR benchmarks (144 structures), showing consistent gains over seven publicly released pretrained models. Beyond improvements, our benchmarking of these models also reveals that stronger backbones yield better results on similar data, representation scaling disproportionately benefits pathological segmentation, and that modality-specific pretraining offers negligible benefit once full finetuning is applied. In conclusion, our results establish MedNeXt-v2 as a strong backbone for large-scale supervised representation learning in 3D Medical Image Segmentation. Our code and pretrained models are made available with the official nnUNet repository at: https://www.github.com/MIC-DKFZ/nnUNet
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LN3DIFF++: Scalable Latent Neural Fields Diffusion for Speedy 3D Generation</title>
<link>https://arxiv.org/abs/2403.12019</link>
<guid>https://arxiv.org/abs/2403.12019</guid>
<content:encoded><![CDATA[

arXiv:2403.12019v3 Announce Type: replace 
Abstract: The field of neural rendering has witnessed significant progress with advancements in generative models and differentiable rendering techniques. Though 2D diffusion has achieved success, a unified 3D diffusion pipeline remains unsettled. This paper introduces a novel framework called LN3Diff++ to address this gap and enable fast, high-quality, and generic conditional 3D generation. Our approach harnesses a 3D-aware architecture and variational autoencoder (VAE) to encode the input image into a structured, compact, and 3D latent space. The latent is decoded by a transformer-based decoder into a high-capacity 3D neural field. Through training a diffusion model on this 3D-aware latent space, our method achieves state-of-the-art performance on ShapeNet for 3D generation and demonstrates superior performance in monocular 3D reconstruction and conditional 3D generation across various datasets. Moreover, it surpasses existing 3D diffusion methods in terms of inference speed, requiring no per-instance optimization. Our proposed LN3Diff presents a significant advancement in 3D generative modeling and holds promise for various applications in 3D vision and graphics tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution</title>
<link>https://arxiv.org/abs/2411.07449</link>
<guid>https://arxiv.org/abs/2411.07449</guid>
<content:encoded><![CDATA[

arXiv:2411.07449v3 Announce Type: replace 
Abstract: Diffusion models have transformed image synthesis through iterative denoising, by defining trajectories from noise to coherent data. While their capabilities are widely celebrated, a critical challenge remains unaddressed: ensuring responsible use by verifying whether an image originates from a model's training set, its novel generations or external sources. We introduce a framework that analyzes diffusion trajectories for this purpose. Specifically, we demonstrate that temporal dynamics across the entire trajectory allow for more robust classification and challenge the widely-adopted "Goldilocks zone" conjecture, which posits that membership inference is effective only within narrow denoising stages. More fundamentally, we expose critical flaws in current membership inference practices by showing that representative methods fail under distribution shifts or when model-generated data is present. For model attribution, we demonstrate a first white-box approach directly applicable to diffusion. Ultimately, we propose the unification of data provenance into a single, cohesive framework tailored to modern generative systems.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations</title>
<link>https://arxiv.org/abs/2411.15355</link>
<guid>https://arxiv.org/abs/2411.15355</guid>
<content:encoded><![CDATA[

arXiv:2411.15355v3 Announce Type: replace 
Abstract: Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous driving. Our contributions are two-fold. First, we propose a new differentiable rendering method that distorts 3D Gaussians using a series of affine transformations tailored to fisheye camera models. This addresses the compatibility issue of 3D Gaussian splatting with fisheye cameras, which is hindered by light ray distortion caused by lenses or mirrors. Besides, our method maintains real-time rendering while ensuring differentiability. Second, built on the differentiable rendering method, we design a new framework that learns a unified Gaussian representation from multiple camera models. By applying affine transformations to adapt different camera models and regularizing the shared Gaussians with supervision from different modalities, our framework learns a unified 3D Gaussian representation with input data from multiple sources and achieves holistic driving scene understanding. As a result, our approach models multiple sensors (pinhole and fisheye cameras) and modalities (depth, semantic, normal and LiDAR point clouds). Our experiments show that our method achieves superior rendering quality and fast rendering speed for driving scene simulation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding-Driven Data Distillation for 360-Degree IQA With Residual-Aware Refinement</title>
<link>https://arxiv.org/abs/2412.12667</link>
<guid>https://arxiv.org/abs/2412.12667</guid>
<content:encoded><![CDATA[

arXiv:2412.12667v2 Announce Type: replace 
Abstract: This article identifies and addresses a fundamental bottleneck in data-driven 360-degree image quality assessment (IQA): the lack of intelligent, sample-level data selection. Hence, we propose a novel framework that introduces a critical refinement step between patches sampling and model training. The core of our contribution is an embedding similarity-based selection algorithm that distills an initial, potentially redundant set of patches into a compact, maximally informative subset. This is formulated as a regularized optimization problem that preserves intrinsic perceptual relationships in a low-dimensional space, using residual analysis to explicitly filter out irrelevant or redundant samples. Extensive experiments on three benchmark datasets (CVIQ, OIQA, MVAQD) demonstrate that our selection enables a baseline model to match or exceed the performance of using all sampled data while keeping only 40-50% of patches. Particularly, we demonstrate the universal applicability of our approach by integrating it with several state-of-the-art IQA models, incleasy to deploy. Most significantly, its value as a generic,uding CNN- and transformer-based architectures, consistently enabling them to maintain or improve performance with 20-40\% reduced computational load. This work establishes that adaptive, post-sampling data refinement is a powerful and widely applicable strategy for achieving efficient and robust 360-degree IQA.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2501.15151</link>
<guid>https://arxiv.org/abs/2501.15151</guid>
<content:encoded><![CDATA[

arXiv:2501.15151v4 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) are the third generation of neural networks. They have gained widespread attention in object detection due to their low power consumption and biological interpretability. However, existing SNN-based object detection methods suffer from local firing saturation, where adjacent neurons concurrently reach maximum firing rates, especially in object-centric regions. This abnormal neuron firing pattern reduces the feature discrimination capability and detection accuracy, while also increasing the firing rates that prevent SNNs from achieving their potential energy efficiency. To address this problem, we propose SpikeDet, a novel spiking object detector that optimizes firing patterns for accurate and energy-efficient detection. Specifically, we design a spiking backbone network, MDSNet, which effectively adjusts the membrane synaptic input distribution at each layer, achieving better neuron firing patterns during spiking feature extraction. For the neck, to better utilize and preserve these high-quality backbone features, we introduce the Spiking Multi-direction Fusion Module (SMFM), which realizes multi-direction fusion of spiking features, enhancing the multi-scale detection capability of the model. Furthermore, we propose the Local Firing Saturation Index (LFSI) to quantitatively measure local firing saturation. Experimental results validate the effectiveness of our method, with SpikeDet achieving superior performance. On the COCO 2017 dataset, it achieves 52.2% AP, outperforming previous SNN-based methods by 3.3% AP while requiring only half the power consumption. On object detection sub-tasks, including event-based GEN1, underwater URPC 2019, low-light ExDARK, and dense scene CrowdHuman datasets, SpikeDet also achieves the best performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Cell Oversegmentation Correction via Geo-Wasserstein Divergence</title>
<link>https://arxiv.org/abs/2502.01890</link>
<guid>https://arxiv.org/abs/2502.01890</guid>
<content:encoded><![CDATA[

arXiv:2502.01890v4 Announce Type: replace 
Abstract: 3D cell segmentation methods are often hindered by \emph{oversegmentation}, where a single cell is incorrectly split into multiple fragments. This degrades the final segmentation quality and is notoriously difficult to resolve, as oversegmentation errors often resemble natural gaps between adjacent cells. Our work makes two key contributions. First, for 3D cell segmentation, we are the first work to formulate oversegmentation as a concrete problem and propose a geometric framework to identify and correct these errors. Our approach builds a pre-trained classifier using both 2D geometric and 3D topological features extracted from flawed 3D segmentation results. Second, we introduce a novel metric, Geo-Wasserstein divergence, to quantify changes in 2D geometries. This captures the evolving trends of cell mask shape in a geometry-aware manner. We validate our method through extensive experiments on in-domain plant datasets, including both synthesized and real oversegmented cases, as well as on out-of-domain animal datasets to demonstrate transfer learning performance. An ablation study further highlights the contribution of the Geo-Wasserstein divergence. A clear pipeline is provided for end-users to build pre-trained models to any labeled dataset.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphCompNet: A Position-Aware Model for Predicting and Compensating Shape Deviations in 3D Printing</title>
<link>https://arxiv.org/abs/2502.09652</link>
<guid>https://arxiv.org/abs/2502.09652</guid>
<content:encoded><![CDATA[

arXiv:2502.09652v3 Announce Type: replace 
Abstract: Shape deviation modeling and compensation in additive manufacturing are pivotal for achieving high geometric accuracy and enabling industrial-scale production. Critical challenges persist, including generalizability across complex geometries and adaptability to position-dependent variations in batch production. Traditional methods of controlling geometric deviations often rely on complex parameterized models and repetitive metrology, which can be time-consuming yet not applicable for batch production. In this paper, we present a novel, process-agnostic approach to address the challenge of ensuring geometric precision and accuracy in position-dependent AM production. The proposed GraphCompNet presents a novel computational framework integrating graph-based neural networks with a GAN inspired training paradigm. The framework leverages point cloud representations and dynamic graph convolutional neural networks (DGCNNs) to model intricate geometries while incorporating position-specific thermal and mechanical variations. A two-stage adversarial training process iteratively refines compensated designs using a compensator-predictor architecture, enabling real-time feedback and optimization. Experimental validation across various shapes and positions demonstrates the framework's ability to predict deviations in freeform geometries and adapt to position-dependent batch production conditions, significantly improving compensation accuracy (35 to 65 percent) across the entire printing space, addressing position-dependent variabilities within the print chamber. The proposed method advances the development of a Digital Twin for AM, offering scalable, real-time monitoring and compensation capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoGameQA-Bench: Evaluating Vision-Language Models for Video Game Quality Assurance</title>
<link>https://arxiv.org/abs/2505.15952</link>
<guid>https://arxiv.org/abs/2505.15952</guid>
<content:encoded><![CDATA[

arXiv:2505.15952v2 Announce Type: replace 
Abstract: With video games now generating the highest revenues in the entertainment industry, optimizing game development workflows has become essential for the sector's sustained growth. Recent advancements in Vision-Language Models (VLMs) offer considerable potential to automate and enhance various aspects of game development, particularly Quality Assurance (QA), which remains one of the industry's most labor-intensive processes with limited automation options. To accurately evaluate the performance of VLMs in video game QA tasks and determine their effectiveness in handling real-world scenarios, there is a clear need for standardized benchmarks, as existing benchmarks are insufficient to address the specific requirements of this domain. To bridge this gap, we introduce VideoGameQA-Bench, a comprehensive benchmark that covers a wide array of game QA activities, including visual unit testing, visual regression testing, needle-in-a-haystack tasks, glitch detection, and bug report generation for both images and videos of various games. Code and data are available at: https://asgaardlab.github.io/videogameqa-bench/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors</title>
<link>https://arxiv.org/abs/2505.20680</link>
<guid>https://arxiv.org/abs/2505.20680</guid>
<content:encoded><![CDATA[

arXiv:2505.20680v3 Announce Type: replace 
Abstract: Continual learning (CL) enables deep networks to acquire new knowledge while avoiding catastrophic forgetting. The powerful generalization ability of pre-trained models (PTMs), such as the Contrastive Language-Image Pre-training (CLIP) model, has inspired a range of CL methods targeting new and specialized tasks, providing rich multi-modal embeddings that support lightweight, incremental prompt tuning. Existing methods often rely on complex designs built upon specific assumptions, such as intricate regularization schemes for prompt pools, specialized routing mechanisms, or multi-stage incrementations, that introduce additional-and possibly unnecessary-complexity, underutilizing CLIP's intrinsic capabilities. In this paper, we propose a concise CL approach for CLIP based on incremental prompt tuning that fully exploits its multi-modal structure and the stability of textual representations. Our method, Textual Prototype-guided Prompt Tuning (TPPT), introduces textual prototypes not merely as static classifiers, as in existing methods, but as stable anchors to guide the learning of visual prompts, thereby shaping the embedding space (i.e., TPPT-V). We show that our bidirectional supervision strategy enables more effective learning of new knowledge while reducing forgetting. To further close the vision-language gap during CL, we jointly optimizes visual and textual prompts (i.e., TPPT-VT). We also introduce a relational diversity regularization on the textual anchors to prevent embedding space collapse and mitigate correlated forgetting. Extensive experiments and analyses demonstrate the effectiveness of our proposed approach, highlighting the benefits of leveraging CLIP's intrinsic guidance for continual adaptation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features</title>
<link>https://arxiv.org/abs/2507.00724</link>
<guid>https://arxiv.org/abs/2507.00724</guid>
<content:encoded><![CDATA[

arXiv:2507.00724v2 Announce Type: replace 
Abstract: Large vision models (LVMs) achieve remarkable performance in various downstream tasks, primarily by personalizing pre-trained models through fine-tuning with private and valuable local data, which makes the personalized model a valuable intellectual property. Similar to the era of traditional DNNs, model stealing attacks also pose significant risks to LVMs. However, this paper reveals that most existing defense methods (developed for traditional DNNs), typically designed for models trained from scratch, either introduce additional security risks, are prone to misjudgment, or are even ineffective for fine-tuned models. To alleviate these problems, this paper proposes a harmless model ownership verification method for personalized LVMs by decoupling similar common features. In general, our method consists of three main stages. In the first stage, we create shadow models that retain common features of the victim model while disrupting dataset-specific features. We represent the dataset-specific features of the victim model by computing the output differences between the shadow and victim models, without altering the victim model or its training process. After that, a meta-classifier is trained to identify stolen models by determining whether suspicious models contain the dataset-specific features of the victim. In the third stage, we conduct model ownership verification by hypothesis test to mitigate randomness and enhance robustness. Extensive experiments on benchmark datasets verify the effectiveness of the proposed method in detecting different types of model stealing simultaneously. Our codes are available at https://github.com/zlh-thu/Holmes.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis</title>
<link>https://arxiv.org/abs/2507.17860</link>
<guid>https://arxiv.org/abs/2507.17860</guid>
<content:encoded><![CDATA[

arXiv:2507.17860v3 Announce Type: replace 
Abstract: Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing</title>
<link>https://arxiv.org/abs/2508.05899</link>
<guid>https://arxiv.org/abs/2508.05899</guid>
<content:encoded><![CDATA[

arXiv:2508.05899v2 Announce Type: replace 
Abstract: 3D scene generation plays a crucial role in gaming, artistic creation, virtual reality, and many other domains. However, current 3D scene design still relies heavily on extensive manual effort from creators, and existing automated methods struggle to generate open-domain scenes or support flexible editing. To address those challenges, we introduce HOLODECK 2.0, an advanced vision-language-guided framework for 3D world generation with support for interactive scene editing based on human feedback. HOLODECK 2.0 can generate diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and cyberpunk styles) that exhibit high semantic fidelity to fine-grained input descriptions, suitable for both indoor and open-domain environments. HOLODECK 2.0 leverages vision-language models (VLMs) to identify and parse the objects required in a scene and generates corresponding high-quality assets via state-of-the-art 3D generative models. Then, HOLODECK 2.0 iteratively applies spatial constraints derived from the VLMs to achieve semantically coherent and physically plausible layouts. Both human and model evaluations demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely aligned with detailed textual descriptions, consistently outperforming baselines across indoor and open-domain scenarios. Additionally, HOLODECK 2.0 provides editing capabilities that flexibly adapt to human feedback, supporting layout refinement and style-consistent object edits. Finally, we present a practical application of HOLODECK 2.0 in procedural game modeling to generate visually rich and immersive environments that can boost efficiency in game design.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding</title>
<link>https://arxiv.org/abs/2508.07313</link>
<guid>https://arxiv.org/abs/2508.07313</guid>
<content:encoded><![CDATA[

arXiv:2508.07313v3 Announce Type: replace 
Abstract: Understanding multi-page documents poses a significant challenge for multimodal large language models (MLLMs), as it requires fine-grained visual comprehension and multi-hop reasoning across pages. While prior work has explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs, its application to multi-page document understanding remains underexplored. In this paper, we introduce DocR1, an MLLM trained with a novel RL framework, Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the model to first retrieve relevant pages before generating answers. This training paradigm enables us to build high-quality models with limited supervision. To support this, we design a two-stage annotation pipeline and a curriculum learning strategy, based on which we construct two datasets: EviBench, a high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments across a wide range of benchmarks demonstrate that DocR1 achieves state-of-the-art performance on multi-page tasks, while consistently maintaining strong results on single-page benchmarks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the dynamic evolution of CLIP texture-shape bias and its relationship to human alignment and model robustness</title>
<link>https://arxiv.org/abs/2508.09814</link>
<guid>https://arxiv.org/abs/2508.09814</guid>
<content:encoded><![CDATA[

arXiv:2508.09814v2 Announce Type: replace 
Abstract: Contrastive language-image models such as CLIP have demonstrated remarkable generalization capabilities. However, how their internal visual representations evolve during training and how this evolution relates to human perception remains poorly understood. Most existing analysis characterize fully trained models, leaving the dynamics of representational biases and perceptual alignment largely unexplored. In this work, we present an epoch-by-epoch analysis of CLIP models throughout training, focusing on the evolution of texture-shape bias, alignment with human perceptual judgements, and sensitivity to image noise. Using multiple perceptual benchmarks spanning low-level image quality assessment, mid-level perceptual similarity, saliency correspondence, and noisy robustness, we identify a consistent, training-stage-dependent representational transition. Early training stages exhibit strong texture bias, elevated alignment with low-level human perceptual measures, and increased sensitivity to Gaussian noise perturbations. As training progresses, this texture bias gradually diminishes in favor of more shape-based representations, coinciding with improved robustness to noise and a decline in low-level perceptual alignment. Importantly, these dynamics are consistently observed across multiple CLIP model scales, indicating that the phenomenon is not specific to a particular architecture size. Our findings provide an empirical characterization of how perceptual alignment, feature bias, and robustness co-evolve during multimodal model training. This work reveals a systematic trade-off between early low-level perceptual alignment and later robustness, offering new insights into the representational dynamics of vision-language models and their relationship to human visual processing.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeSenCLIP: A Time Series Vision-Language Model for Remote Sensing Using Single-Pixel</title>
<link>https://arxiv.org/abs/2508.11919</link>
<guid>https://arxiv.org/abs/2508.11919</guid>
<content:encoded><![CDATA[

arXiv:2508.11919v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) have shown significant promise in remote sensing applications, particularly for land-use and land-cover (LULC) mapping via zero-shot classification and retrieval. However, current approaches face several key challenges, such as the dependence on caption-based supervision, which is often not available or very limited in terms of the covered semantics, and the fact of being adapted from generic VLM architectures that are suitable for very high resolution images. Consequently, these models tend to prioritize spatial context over spectral and temporal information, limiting their effectiveness for medium-resolution remote sensing imagery. In this work, we present TimeSenCLIP, a lightweight VLM for remote sensing time series, using a cross-view temporal contrastive framework to align multispectral Sentinel-2 time series with geo-tagged ground-level imagery, without requiring textual annotations. Unlike prior VLMs, TimeSenCLIP emphasizes temporal and spectral signals over spatial context, investigating whether single-pixel time series contain sufficient information for solving a variety of tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysGM: Large Physical Gaussian Model for Feed-Forward 4D Synthesis</title>
<link>https://arxiv.org/abs/2508.13911</link>
<guid>https://arxiv.org/abs/2508.13911</guid>
<content:encoded><![CDATA[

arXiv:2508.13911v2 Announce Type: replace 
Abstract: Despite advances in physics-based 3D motion synthesis, current methods face key limitations: reliance on pre-reconstructed 3D Gaussian Splatting (3DGS) built from dense multi-view images with time-consuming per-scene optimization; physics integration via either inflexible, hand-specified attributes or unstable, optimization-heavy guidance from video models using Score Distillation Sampling (SDS); and naive concatenation of prebuilt 3DGS with physics modules, which ignores physical information embedded in appearance and yields suboptimal performance. To address these issues, we propose PhysGM, a feed-forward framework that jointly predicts 3D Gaussian representation and physical properties from a single image, enabling immediate simulation and high-fidelity 4D rendering. Unlike slow appearance-agnostic optimization methods, we first pre-train a physics-aware reconstruction model that directly infers both Gaussian and physical parameters. We further refine the model with Direct Preference Optimization (DPO), aligning simulations with the physically plausible reference videos and avoiding the high-cost SDS optimization. To address the absence of a supporting dataset for this task, we propose PhysAssets, a dataset of 50K+ 3D assets annotated with physical properties and corresponding reference videos. Experiments show that PhysGM produces high-fidelity 4D simulations from a single image in one minute, achieving a significant speedup over prior work while delivering realistic renderings.Our project page is at:https://hihixiaolv.github.io/PhysGM.github.io/
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework</title>
<link>https://arxiv.org/abs/2508.16295</link>
<guid>https://arxiv.org/abs/2508.16295</guid>
<content:encoded><![CDATA[

arXiv:2508.16295v2 Announce Type: replace 
Abstract: The digitization of structured handwritten documents, such as academic marksheets, remains a significant challenge due to the dual complexity of irregular table structures and diverse handwriting styles. While recent Transformer-based approaches like TableNet and TrOCR achieve state-of-the-art accuracy, their high computational cost renders them unsuitable for resource-constrained edge deployments. This paper introduces a resource-efficient hybrid framework that integrates a heuristic OpenCV-based pipeline for rapid table structure detection with a modified lightweight YOLOv8 architecture for handwritten character recognition. By strategically removing the SPPF and deep C2f layers from the standard YOLOv8 backbone, we reduce computational overhead while maintaining high recognition fidelity. Experimental results on the EMNIST digit benchmark demonstrate that our Modified YOLOv8 model achieves 97.5% accuracy. Furthermore, we provide a comprehensive efficiency analysis showing that our framework offers a 95 times inference speedup over standard OCR pipelines and massive efficiency gains over emerging Large Multimodal Models (LMMs) like Qwen2.5-VL, achieving real-time performance 29 FPS on standard CPU hardware. A qualitative and quantitative evaluation on the AMES dataset, a challenging subset of real-world marksheets, confirms the system's robustness in handling mixed alphanumeric content, bridging the gap between high-performance deep learning and practical, scalable document automation.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-like Content Analysis for Generative AI with Language-Grounded Sparse Encoders</title>
<link>https://arxiv.org/abs/2508.18236</link>
<guid>https://arxiv.org/abs/2508.18236</guid>
<content:encoded><![CDATA[

arXiv:2508.18236v3 Announce Type: replace 
Abstract: The rapid development of generative AI has transformed content creation, communication, and human development. However, this technology raises profound concerns in high-stakes domains, demanding rigorous methods to analyze and evaluate AI-generated content. While existing analytic methods often treat images as indivisible wholes, real-world AI failures generally manifest as specific visual patterns that can evade holistic detection and suit more granular and decomposed analysis. Here we introduce a content analysis tool, Language-Grounded Sparse Encoders (LanSE), which decompose images into interpretable visual patterns with natural language descriptions. Utilizing interpretability modules and large multimodal models, LanSE can automatically identify visual patterns within data modalities. Our method discovers more than 5,000 visual patterns with 93\% human agreement, provides decomposed evaluation outperforming existing methods, establishes the first systematic evaluation of physical plausibility, and extends to medical imaging settings. Our method's capability to extract language-grounded patterns can be naturally adapted to numerous fields, including biology and geography, as well as other data modalities such as protein structures and time series, thereby advancing content analysis for generative AI.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeParts: a New Family of AI-Generated DeepFakes</title>
<link>https://arxiv.org/abs/2508.21052</link>
<guid>https://arxiv.org/abs/2508.21052</guid>
<content:encoded><![CDATA[

arXiv:2508.21052v2 Announce Type: replace 
Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle, localized manipulations to specific spatial regions or temporal segments of otherwise authentic videos. Unlike fully synthetic content, these partial manipulations - ranging from altered facial expressions to object substitutions and background modifications - blend seamlessly with real elements, making them particularly deceptive and difficult to detect. To address the critical gap in detection, we present FakePartsBench, the first large-scale benchmark specifically designed to capture the full spectrum of partial deepfakes. Comprising over 81K (including 44K FakeParts) videos with pixel- and frame-level manipulation annotations, our dataset enables comprehensive evaluation of detection methods. Our user studies demonstrate that FakeParts reduces human detection accuracy by up to 26% compared to traditional deepfakes, with similar performance degradation observed in state-of-the-art detection models. This work identifies an urgent vulnerability in current detectors and provides the necessary resources to develop methods robust to partial manipulations.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</title>
<link>https://arxiv.org/abs/2509.05144</link>
<guid>https://arxiv.org/abs/2509.05144</guid>
<content:encoded><![CDATA[

arXiv:2509.05144v2 Announce Type: replace 
Abstract: Accurate 3D instance segmentation is crucial for high-quality scene understanding in the 3D vision domain. However, 3D instance segmentation based on 2D-to-3D lifting approaches struggle to produce precise instance-level segmentation, due to accumulated errors introduced during the lifting process from ambiguous semantic guidance and insufficient depth constraints. To tackle these challenges, we propose splitting and growing reliable semantic mask for high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow" framework that first purifies and splits ambiguous lifted masks using geometric primitives, and then grows them into complete instances within the scene. Unlike existing approaches that directly rely on raw lifted masks and sacrifice segmentation accuracy, SGS-3D serves as a training-free refinement method that jointly fuses semantic and geometric information, enabling effective cooperation between the two levels of representation. Specifically, for semantic guidance, we introduce a mask filtering strategy that leverages the co-occurrence of 3D geometry primitives to identify and remove ambiguous masks, thereby ensuring more reliable semantic consistency with the 3D object instances. For the geometric refinement, we construct fine-grained object instances by exploiting both spatial continuity and high-level features, particularly in the case of semantic ambiguity between distinct objects. Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that SGS-3D substantially improves segmentation accuracy and robustness against inaccurate masks from pre-trained models, yielding high-fidelity object instances while maintaining strong generalization across diverse indoor and outdoor environments. Code is available at https://github.com/wangchaolei7/SGS-3D.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention</title>
<link>https://arxiv.org/abs/2509.09116</link>
<guid>https://arxiv.org/abs/2509.09116</guid>
<content:encoded><![CDATA[

arXiv:2509.09116v3 Announce Type: replace 
Abstract: Foundation segmentation models achieve reasonable leaf instance extraction from top-view crop images without training (i.e., zero-shot). However, segmenting entire plant individuals with each consisting of multiple overlapping leaves remains challenging. This problem is referred to as a hierarchical segmentation task, typically requiring annotated training datasets, which are often species-specific and require notable human labor. To address this, we introduce ZeroPlantSeg, a zero-shot segmentation for rosette-shaped plant individuals from top-view images. We integrate a foundation segmentation model, extracting leaf instances, and a vision-language model, reasoning about plants' structures to extract plant individuals without additional training. Evaluations on datasets with multiple plant species, growth stages, and shooting environments demonstrate that our method surpasses existing zero-shot methods and achieves better cross-domain performance than supervised methods. Implementations are available at https://github.com/JunhaoXing/ZeroPlantSeg.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi Anatomy X-Ray Foundation Model</title>
<link>https://arxiv.org/abs/2509.12146</link>
<guid>https://arxiv.org/abs/2509.12146</guid>
<content:encoded><![CDATA[

arXiv:2509.12146v2 Announce Type: replace 
Abstract: X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Blind Face Restoration through Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.23339</link>
<guid>https://arxiv.org/abs/2509.23339</guid>
<content:encoded><![CDATA[

arXiv:2509.23339v2 Announce Type: replace 
Abstract: Blind Face Restoration (BFR) encounters inherent challenges in exploring its large solution space, leading to common artifacts like missing details and identity ambiguity in the restored images. To tackle these challenges, we propose a Likelihood-Regularized Policy Optimization (LRPO) framework, the first to apply online reinforcement learning (RL) to the BFR task. LRPO leverages rewards from sampled candidates to refine the policy network, increasing the likelihood of high-quality outputs while improving restoration performance on low-quality inputs. However, directly applying RL to BFR creates incompatibility issues, producing restoration results that deviate significantly from the ground truth. To balance perceptual quality and fidelity, we propose three key strategies: 1) a composite reward function tailored for face restoration assessment, 2) ground-truth guided likelihood regularization, and 3) noise-level advantage assignment. Extensive experiments demonstrate that our proposed LRPO significantly improves the face restoration quality over baseline methods and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation</title>
<link>https://arxiv.org/abs/2510.11176</link>
<guid>https://arxiv.org/abs/2510.11176</guid>
<content:encoded><![CDATA[

arXiv:2510.11176v2 Announce Type: replace 
Abstract: Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2510.16442</link>
<guid>https://arxiv.org/abs/2510.16442</guid>
<content:encoded><![CDATA[

arXiv:2510.16442v2 Announce Type: replace 
Abstract: The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The project page is available at: https://11ouo1.github.io/edvd-llama/.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CharDiff-LP: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration</title>
<link>https://arxiv.org/abs/2510.17330</link>
<guid>https://arxiv.org/abs/2510.17330</guid>
<content:encoded><![CDATA[

arXiv:2510.17330v2 Announce Type: replace 
Abstract: License plate image restoration is important not only as a preprocessing step for license plate recognition but also for enhancing evidential value, improving visual clarity, and enabling broader reuse of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff-LP, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff-LP leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff-LP incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character's guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff-LP significantly outperformed baseline restoration models in both restoration quality and recognition accuracy, achieving a 28.3% relative reduction in character error rate (CER) on the Roboflow-LP dataset compared with the best-performing baseline.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation</title>
<link>https://arxiv.org/abs/2510.22107</link>
<guid>https://arxiv.org/abs/2510.22107</guid>
<content:encoded><![CDATA[

arXiv:2510.22107v2 Announce Type: replace 
Abstract: Capturing diversity is crucial in conditional and prompt-based image generation, particularly when conditions contain uncertainty that can lead to multiple plausible outputs. To generate diverse images reflecting this diversity, traditional methods often modify random seeds, making it difficult to discern meaningful differences between samples, or diversify the input prompt, which is limited in verbally interpretable diversity. We propose Rainbow, a novel conditional image generation framework, applicable to any pretrained conditional generative model, that addresses inherent condition/prompt uncertainty and generates diverse plausible images. Rainbow is based on a simple yet effective idea: decomposing the input condition into diverse latent representations, each capturing an aspect of the uncertainty and generating a distinct image. First, we integrate a latent graph, parameterized by Generative Flow Networks (GFlowNets), into the prompt representation computation. Second, leveraging GFlowNets' advanced graph sampling capabilities to capture uncertainty and output diverse trajectories over the graph, we produce multiple trajectories that collectively represent the input condition, leading to diverse condition representations and corresponding output images. Evaluations on natural image and medical image datasets demonstrate Rainbow's improvement in both diversity and fidelity across image synthesis, image generation, and counterfactual generation tasks.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Generation Phases of Flow Matching: a Denoising Perspective</title>
<link>https://arxiv.org/abs/2510.24830</link>
<guid>https://arxiv.org/abs/2510.24830</guid>
<content:encoded><![CDATA[

arXiv:2510.24830v2 Announce Type: replace 
Abstract: Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human Mesh Modeling for Anny Body</title>
<link>https://arxiv.org/abs/2511.03589</link>
<guid>https://arxiv.org/abs/2511.03589</guid>
<content:encoded><![CDATA[

arXiv:2511.03589v2 Announce Type: replace 
Abstract: Parametric body models provide the structural basis for many human-centric tasks, yet existing models often rely on costly 3D scans and learned shape spaces that are proprietary and demographically narrow. We introduce Anny, a simple, fully differentiable, and scan-free human body model grounded in anthropometric knowledge from the MakeHuman community. Anny defines a continuous, interpretable shape space, where phenotype parameters (e.g. gender, age, height, weight) control blendshapes spanning a wide range of human forms--across ages (from infants to elders), body types, and proportions. Calibrated using WHO population statistics, it provides realistic and demographically grounded human shape variation within a single unified model. Thanks to its openness and semantic control, Anny serves as a versatile foundation for 3D human modeling--supporting millimeter-accurate scan fitting, controlled synthetic data generation, and Human Mesh Recovery (HMR). We further introduce Anny-One, a collection of 800k photorealistic images generated with Anny, showing that despite its simplicity, HMR models trained with Anny can match the performance of those trained with scan-based body models. The Anny body model and its code are released under the Apache 2.0 license, making Anny an accessible foundation for human-centric 3D modeling.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification</title>
<link>https://arxiv.org/abs/2511.12346</link>
<guid>https://arxiv.org/abs/2511.12346</guid>
<content:encoded><![CDATA[

arXiv:2511.12346v2 Announce Type: replace 
Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under severe class imbalance.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CD-DPE: Dual-Prompt Expert Network Based on Convolutional Dictionary Feature Decoupling for Multi-Contrast MRI Super-Resolution</title>
<link>https://arxiv.org/abs/2511.14014</link>
<guid>https://arxiv.org/abs/2511.14014</guid>
<content:encoded><![CDATA[

arXiv:2511.14014v3 Announce Type: replace 
Abstract: Multi-contrast magnetic resonance imaging (MRI) super-resolution intends to reconstruct high-resolution (HR) images from low-resolution (LR) scans by leveraging structural information present in HR reference images acquired with different contrasts. This technique enhances anatomical detail and soft tissue differentiation, which is vital for early diagnosis and clinical decision-making. However, inherent contrasts disparities between modalities pose fundamental challenges in effectively utilizing reference image textures to guide target image reconstruction, often resulting in suboptimal feature integration. To address this issue, we propose a dual-prompt expert network based on a convolutional dictionary feature decoupling (CD-DPE) strategy for multi-contrast MRI super-resolution. Specifically, we introduce an iterative convolutional dictionary feature decoupling module (CD-FDM) to separate features into cross-contrast and intra-contrast components, thereby reducing redundancy and interference. To fully integrate these features, a novel dual-prompt feature fusion expert module (DP-FFEM) is proposed. This module uses a frequency prompt to guide the selection of relevant reference features for incorporation into the target image, while an adaptive routing prompt determines the optimal method for fusing reference and target features to enhance reconstruction quality. Extensive experiments on public multi-contrast MRI datasets demonstrate that CD-DPE outperforms state-of-the-art methods in reconstructing fine details. Additionally, experiments on unseen datasets demonstrated that CD-DPE exhibits strong generalization capabilities.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation</title>
<link>https://arxiv.org/abs/2511.21029</link>
<guid>https://arxiv.org/abs/2511.21029</guid>
<content:encoded><![CDATA[

arXiv:2511.21029v2 Announce Type: replace 
Abstract: Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization. Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance. Project page: https://flowerdance25.github.io/ .
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant symmetry-aware head pose estimation for fetal MRI</title>
<link>https://arxiv.org/abs/2512.04890</link>
<guid>https://arxiv.org/abs/2512.04890</guid>
<content:encoded><![CDATA[

arXiv:2512.04890v4 Announce Type: replace 
Abstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLUENet: Cluster Attention Makes Neural Networks Have Eyes</title>
<link>https://arxiv.org/abs/2512.06345</link>
<guid>https://arxiv.org/abs/2512.06345</guid>
<content:encoded><![CDATA[

arXiv:2512.06345v2 Announce Type: replace 
Abstract: Despite the success of convolution- and attention-based models in vision tasks, their rigid receptive fields and complex architectures limit their ability to model irregular spatial patterns and hinder interpretability, therefore posing challenges for tasks requiring high model transparency. Clustering paradigms offer promising interpretability and flexible semantic modeling, but suffer from limited accuracy, low efficiency, and gradient vanishing during training. To address these issues, we propose CLUster attEntion Network (CLUENet), an transparent deep architecture for visual semantic understanding. We propose three key innovations include (i) a Global Soft Aggregation and Hard Assignment with a Temperature-Scaled Cosin Attention and gated residual connections for enhanced local modeling, (ii) inter-block Hard and Shared Feature Dispatching, and (iii) an improved cluster pooling strategy. These enhancements significantly improve both classification performance and visual interpretability. Experiments on CIFAR-100 and Mini-ImageNet demonstrate that CLUENet outperforms existing clustering methods and mainstream visual models, offering a compelling balance of accuracy, efficiency, and transparency.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection</title>
<link>https://arxiv.org/abs/2512.07984</link>
<guid>https://arxiv.org/abs/2512.07984</guid>
<content:encoded><![CDATA[

arXiv:2512.07984v2 Announce Type: replace 
Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2512.08557</link>
<guid>https://arxiv.org/abs/2512.08557</guid>
<content:encoded><![CDATA[

arXiv:2512.08557v2 Announce Type: replace 
Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI-driven Assessment of Bone Density as a Biomarker Leading to the Aging Law</title>
<link>https://arxiv.org/abs/2308.02815</link>
<guid>https://arxiv.org/abs/2308.02815</guid>
<content:encoded><![CDATA[

arXiv:2308.02815v2 Announce Type: replace-cross 
Abstract: As global population aging intensifies, there is growing interest in the study of biological age. Bones have long been used to evaluate biological age, and the decline in bone density with age is a well-recognized phenomenon in adults. However, the pattern of this decline remains controversial, making it difficult to serve as a reliable indicator of the aging process. Here we present a novel AI-driven statistical method to assess the bone density, and a discovery that the bone mass distribution in trabecular bone of vertebrae follows a non-Gaussian, unimodal, and skewed distribution in CT images. The statistical mode of the distribution is defined as the measure of bone mass, which is a groundbreaking assessment of bone density, named Trabecular Bone Density (TBD). The dataset of CT images are collected from 1,719 patients who underwent PET/CT scans in three hospitals, in which a subset of the dataset is used for AI model training and generalization. Based upon the cases, we demonstrate that the pattern of bone density declining with aging exhibits a consistent trend of exponential decline across sexes and age groups using TBD assessment. The developed AI-driven statistical method blazes a trail in the field of AI for reliable quantitative computation and AI for medicine. The findings suggest that human aging is a gradual process, with the rate of decline slowing progressively over time, which will provide a valuable basis for scientific prediction of life expectancy.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSDiff: Multi-Scale Diffusion Model for Ultra-Sparse View CT Reconstruction</title>
<link>https://arxiv.org/abs/2405.05814</link>
<guid>https://arxiv.org/abs/2405.05814</guid>
<content:encoded><![CDATA[

arXiv:2405.05814v2 Announce Type: replace-cross 
Abstract: Computed Tomography (CT) technology reduces radiation haz-ards to the human body through sparse sampling, but fewer sampling angles pose challenges for image reconstruction. Score-based generative models are widely used in sparse-view CT re-construction, performance diminishes significantly with a sharp reduction in projection angles. Therefore, we propose an ultra-sparse view CT reconstruction method utilizing multi-scale dif-fusion models (MSDiff), designed to concentrate on the global distribution of information and facilitate the reconstruction of sparse views with local image characteristics. Specifically, the proposed model ingeniously integrates information from both comprehensive sampling and selectively sparse sampling tech-niques. Through precise adjustments in diffusion model, it is capable of extracting diverse noise distribution, furthering the understanding of the overall structure of images, and aiding the fully sampled model in recovering image information more effec-tively. By leveraging the inherent correlations within the projec-tion data, we have designed an equidistant mask, enabling the model to focus its attention more effectively. Experimental re-sults demonstrated that the multi-scale model approach signifi-cantly improved the quality of image reconstruction under ultra-sparse angles, with good generalization across various datasets.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sell It Before You Make It: Revolutionizing E-Commerce with Personalized AI-Generated Items</title>
<link>https://arxiv.org/abs/2503.22182</link>
<guid>https://arxiv.org/abs/2503.22182</guid>
<content:encoded><![CDATA[

arXiv:2503.22182v2 Announce Type: replace-cross 
Abstract: E-commerce has revolutionized retail, yet its traditional workflows remain inefficient, with significant resource costs tied to product design and inventory. This paper introduces a novel system deployed at Alibaba that uses AI-generated items (AIGI) to address these challenges with personalized text-to-image generation for e-commerce product design. AIGI enables an innovative business mode called "sell it before you make it", where merchants can design fashion items and generate photorealistic images with digital models based on textual descriptions. Only when the items have received a certain number of orders, do the merchants start to produce them, which largely reduces reliance on physical prototypes and thus accelerates time to market. For such a promising application, we identify the underlying key scientific challenge, i.e., capturing users' group-level personalized preferences towards multiple generated images. To this end, we propose a Personalized Group-Level Preference Alignment Framework for Diffusion Models (PerFusion). We first design PerFusion Reward Model for user preference estimation with a feature-crossing-based personalized plug-in. Then we develop PerFusion with a personalized adaptive network to model diverse preferences across users, and meanwhile derive the group-level preference optimization objective to model comparative behaviors among multiple images. Both offline and online experiments demonstrate the effectiveness of our proposed algorithm. The AI-generated items achieve over 13% relative improvements for both click-through rate and conversion rate, as well as 7.9% decrease in return rate, compared to their human-designed counterparts, validating the transformative potential of AIGI for e-commerce platforms.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics</title>
<link>https://arxiv.org/abs/2505.04006</link>
<guid>https://arxiv.org/abs/2505.04006</guid>
<content:encoded><![CDATA[

arXiv:2505.04006v2 Announce Type: replace-cross 
Abstract: The unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. The retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. The advancement in imaging technology leveraging Artificial Intelligence has seized this opportunity to bridge the gap between the eye and human health. This track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. The new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. In this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of AI-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. We also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-guided multi-stage cross-perception network for medical image segmentation</title>
<link>https://arxiv.org/abs/2506.07475</link>
<guid>https://arxiv.org/abs/2506.07475</guid>
<content:encoded><![CDATA[

arXiv:2506.07475v3 Announce Type: replace-cross 
Abstract: Medical image segmentation plays a crucial role in clinical medicine, serving as a key tool for auxiliary diagnosis, treatment planning, and disease monitoring. However, traditional segmentation methods such as U-Net are often limited by weak semantic expression of target regions, which stems from insufficient generalization and a lack of interactivity. Incorporating text prompts offers a promising avenue to more accurately pinpoint lesion locations, yet existing text-guided methods are still hindered by insufficient cross-modal interaction and inadequate cross-modal feature representation. To address these challenges, we propose the Text-guided Multi-stage Cross-perception network (TMC). TMC incorporates a Multi-stage Cross-attention Module (MCM) to enhance the model's understanding of fine-grained semantic details and a Multi-stage Alignment Loss (MA Loss) to improve the consistency of cross-modal semantics across different feature levels. Experimental results on three public datasets (QaTa-COV19, MosMedData, and Duke-Breast-Cancer-MRI) demonstrate the superior performance of TMC, achieving Dice scores of 84.65\%, 78.39\%, and 88.09\%, respectively, and consistently outperforming both U-Net-based networks and existing text-guided methods.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction</title>
<link>https://arxiv.org/abs/2510.23117</link>
<guid>https://arxiv.org/abs/2510.23117</guid>
<content:encoded><![CDATA[

arXiv:2510.23117v3 Announce Type: replace-cross 
Abstract: Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.
  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.00324</link>
<guid>https://arxiv.org/abs/2512.00324</guid>
<content:encoded><![CDATA[

arXiv:2512.00324v2 Announce Type: replace-cross 
Abstract: Imitation learning provides a promising approach to dexterous hand manipulation, but its effectiveness is limited by the lack of large-scale, high-fidelity data. Existing data-collection pipelines suffer from inaccurate motion retargeting, low data-collection efficiency, and missing high-resolution fingertip tactile sensing. We address this gap with MILE, a mechanically isomorphic teleoperation and data-collection system co-designed from human hand to exoskeleton to robotic hand. The exoskeleton is anthropometrically derived from the human hand, and the robotic hand preserves one-to-one joint-position isomorphism, eliminating nonlinear retargeting and enabling precise, natural control. The exoskeleton achieves a multi-joint mean absolute angular error below one degree, while the robotic hand integrates compact fingertip visuotactile modules that provide high-resolution tactile observations. Built on this retargeting-free interface, we teleoperate complex, contact-rich in-hand manipulation and efficiently collect a multimodal dataset comprising high-resolution fingertip visuotactile signals, RGB-D images, and joint positions. The teleoperation pipeline achieves a mean success rate improvement of 64%. Incorporating fingertip tactile observations further increases the success rate by an average of 25% over the vision-only baseline, validating the fidelity and utility of the dataset. Further details are available at: https://sites.google.com/view/mile-system.
]]></content:encoded>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real</title>
<link>https://arxiv.org/abs/2512.15774</link>
<guid>https://arxiv.org/abs/2512.15774</guid>
<content:encoded><![CDATA[
<div> Keywords: masked face detection, data augmentation, GANs, mask warping, face recognition<br /><br />Summary:<br /><br />1. The paper addresses challenges in masked face detection and recognition caused by data scarcity and distribution shifts.<br /><br />2. It proposes a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using Generative Adversarial Networks (GANs). This approach generates more realistic masked-face samples beyond what purely synthetic transformations can achieve.<br /><br />3. The method shows consistent qualitative improvements compared to using rule-based warping alone and serves as a complement to existing GAN-based masked face generation techniques such as IAMGAN.<br /><br />4. Key contributions include the introduction of a non-mask preservation loss and stochastic noise injection, both designed to stabilize the GAN training process and increase the diversity of generated samples.<br /><br />5. Experimental results validate the effectiveness of these components and provide insights for future research directions focused on data-centric augmentation strategies for improving face recognition under masked conditions. <div>
arXiv:2512.15774v1 Announce Type: new 
Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.15885</link>
<guid>https://arxiv.org/abs/2512.15885</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, visual reasoning, self-supervised learning, I-JEPA, vision-language alignment<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) have shown strong abilities in linking vision and language but still struggle with foundational visual reasoning tasks.  
2. The main challenge is that MLLMs primarily learn visual understanding from textual descriptions, which provide subjective and incomplete supervision, limiting the model's ability to fully grasp visual details.  
3. Additionally, multimodal instruction tuning datasets are smaller compared to massive text-only pre-training, causing MLLMs to overfit language priors while neglecting detailed visual information.  
4. To overcome these limitations, the authors present JARVIS, a framework inspired by JEPA, that integrates self-supervised visual enhancement into the MLLMs training process.  
5. JARVIS incorporates the I-JEPA learning paradigm with frozen vision foundation models as context and target encoders, training the predictor (early LLM layers) to capture structural and semantic image regularities without relying solely on language-based supervision.  
6. Experiments demonstrate that JARVIS consistently boosts performance on vision-centric benchmarks across various large language model families, without compromising multimodal reasoning capabilities.  
7. The source code for JARVIS is made publicly accessible to facilitate further research and development in this area. <div>
arXiv:2512.15885v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs</title>
<link>https://arxiv.org/abs/2512.15933</link>
<guid>https://arxiv.org/abs/2512.15933</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, embodied agents, visual navigation, CityNav benchmark, Verbalization of Path (VoP)  

<br /><br />Summary:  
1. This paper introduces Sparsely Grounded Visual Navigation, a novel task aimed at evaluating the sequential decision-making capabilities of multimodal large language models (MLLMs) in complex, real-world environments.  
2. The authors develop CityNav, a benchmark consisting of navigation challenges across four diverse global cities, where agents must navigate through 50+ decision points relying solely on visual inputs and internal multimodal reasoning, without extra environment annotations or specialized model changes.  
3. Key requirements for agents include autonomous localization by interpreting city-specific cues and landmarks, spatial reasoning, and strategic route planning to reach their destinations effectively.  
4. Extensive experiments reveal that current state-of-the-art MLLMs and reasoning strategies like Chain-of-Thought and Reflection perform poorly under these demanding conditions.  
5. To improve navigation success, the paper proposes Verbalization of Path (VoP), a method that grounds the agent's internal reasoning explicitly by extracting a cognitive map, including key landmarks and directional information from the MLLMs, significantly enhancing performance. <div>
arXiv:2512.15933v1 Announce Type: new 
Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space</title>
<link>https://arxiv.org/abs/2512.15940</link>
<guid>https://arxiv.org/abs/2512.15940</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D spatio-temporal reasoning, retrieval-augmented framework, vision-language models, lifelong memory, embodied AI<br /><br />Summary:  
Humans naturally perceive and reason about the world in four dimensions, using structured internal representations that encode semantics, spatial layout, and temporal dynamics. Inspired by this, the paper introduces R4, a training-free framework designed to augment vision-language models (VLMs) with structured lifelong memory in 4D spatio-temporal space. R4 continuously builds a knowledge database by anchoring object-level semantic information within metric space and time, creating a persistent world model shareable across multiple agents. During inference, natural language queries are broken down into semantic, spatial, and temporal components, which serve as keys to retrieve relevant past observations from this 4D database. These retrieved observations are then integrated into the reasoning process of VLMs. Unlike traditional retrieval-augmented generation methods, R4 conducts retrieval directly within 4D space, facilitating episodic and collaborative reasoning without the need for additional training. Experiments conducted on embodied question answering and navigation tasks demonstrate that R4 significantly enhances retrieval and reasoning involving spatio-temporal information when compared to baseline methods. Overall, this work establishes a new paradigm for embodied 4D reasoning in dynamic environments, enabling more persistent, contextual, and collaborative AI agents. <div>
arXiv:2512.15940v1 Announce Type: new 
Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs</title>
<link>https://arxiv.org/abs/2512.15949</link>
<guid>https://arxiv.org/abs/2512.15949</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Perceptual Capacities, Visual Grounding, Robustness Evaluation, The Perceptual Observatory<br /><br />Summary: Recent advances in multimodal large language models (MLLMs) have led to increasingly powerful systems; however, their true perceptual capabilities remain inadequately understood. Most current models scale their language components while reusing similar vision encoders, raising concerns about whether improvements stem from genuine visual understanding or merely extensive textual knowledge. Existing evaluations focus mainly on end-task accuracy, ignoring factors like robustness, attribution fidelity, and reasoning under controlled perturbations. To address this, the authors propose The Perceptual Observatory, a comprehensive framework designed to characterize MLLMs across multiple verticals. These include simple vision tasks (such as face matching and text-in-vision comprehension), as well as local-to-global understanding tasks (like image matching, grid pointing games, and attribute localization) to test general visual grounding. The framework uses meticulously curated ground-truth datasets consisting of faces and words, which are systematically perturbed using pixel-based augmentations and diffusion-based stylized illusions to test robustness. The Perceptual Observatory moves beyond typical leaderboard accuracy metrics to provide insights into how MLLMs maintain perceptual grounding and relational structure under various perturbations, thereby establishing a principled foundation for evaluating the strengths and weaknesses of contemporary and future models. <div>
arXiv:2512.15949v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models</title>
<link>https://arxiv.org/abs/2512.15957</link>
<guid>https://arxiv.org/abs/2512.15957</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-human behavior prediction, vision language model, context-aware, scene graphs, synthetic data<br /><br />Summary:<br /><br />1. This paper addresses the challenge of accurately predicting human behaviors in environments with multiple people, which is critical for mobile robots operating in human-populated spaces. <br /><br />2. Unlike prior work focused on egocentric views and single-human behavior, the authors focus on a third-person observer perspective for multi-human behavior prediction. <br /><br />3. The proposed method, CAMP-VLM (Context-Aware Multi-human behavior Prediction), utilizes a Vision Language Model (VLM) framework that integrates contextual features from visual inputs and spatial awareness derived from scene graphs to capture humans-scene interactions effectively. <br /><br />4. Due to the lack of real-world datasets suitable for multi-human behavior prediction from an observer view, the model was fine-tuned using synthetic human behavior data generated via a photorealistic simulation environment. <br /><br />5. The model was evaluated on both synthetic and real-world sequences, leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), and achieved up to 66.9% improvement in prediction accuracy compared to the best existing baseline methods, demonstrating strong generalization and improved predictive performance. <div>
arXiv:2512.15957v1 Announce Type: new 
Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection</title>
<link>https://arxiv.org/abs/2512.15971</link>
<guid>https://arxiv.org/abs/2512.15971</guid>
<content:encoded><![CDATA[
<div> Multispectral object detection, Vision-Language Models, few-shot learning, multispectral datasets, semantic supervision<br /><br />Summary:<br /><br />Multispectral object detection plays a crucial role in applications such as autonomous driving and surveillance, where perception must remain robust under diverse illumination conditions. A major challenge in this area is the scarcity of annotated multispectral data, which limits the effective training of deep learning detectors. To address this, the authors explore the use of textual class information as a form of semantic supervision, leveraging the recent advancements in Vision-Language Models (VLMs). They adapt two prominent VLM-based detectors, Grounding DINO and YOLO-World, to work with multispectral inputs by integrating text, visual, and thermal modalities through a newly proposed mechanism. Experiments conducted on FLIR and M3FD benchmarks reveal that these VLM-based detectors significantly outperform specialized multispectral models in few-shot learning scenarios. Furthermore, the VLM-based methods achieve competitive or even superior results compared to state-of-the-art models in fully supervised settings. The study demonstrates that semantic priors learned from large-scale VLMs transfer effectively to previously unseen spectral modalities, offering a promising and data-efficient approach to multispectral object detection and perception. <div>
arXiv:2512.15971v1 Announce Type: new 
Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are vision-language models ready to zero-shot replace supervised classification models in agriculture?</title>
<link>https://arxiv.org/abs/2512.15977</link>
<guid>https://arxiv.org/abs/2512.15977</guid>
<content:encoded><![CDATA[
<div> Vision-language models, agricultural classification, zero-shot learning, model benchmarking, semantic judging<br /><br />Summary:<br /><br />1. This study evaluates the reliability of vision-language models (VLMs) for agricultural decision support by benchmarking various open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, covering 162 classes including plant disease, pest and damage identification, and plant and weed species classification.<br />2. Results reveal that zero-shot VLMs significantly underperform compared to a supervised baseline model, YOLO11, which consistently achieves much higher accuracy across all tasks.<br />3. Under multiple-choice prompting, the best VLM, Gemini-3 Pro, attains around 62% average accuracy, whereas open-ended prompting yields substantially lower performance, typically below 25% raw accuracy.<br />4. Incorporating large language model (LLM)-based semantic judging improves open-ended accuracy (e.g., from 21% to 30% for top models) and changes the relative rankings of models, indicating that the chosen evaluation methods strongly influence reported outcomes.<br />5. Among open-source options, Qwen-VL-72B shows the best results, nearing closed-source model performance under limited prompting scenarios but still trailing leading proprietary models.<br />6. Task-level analysis finds that plant and weed species classification is generally easier for VLMs than pest and damage identification, which remains the most difficult category.<br />7. The overall conclusion is that current off-the-shelf VLMs are not yet adequate as standalone systems for agricultural diagnostics but may be useful as assistive tools when combined with constrained interfaces, explicit label ontologies, and domain-aware evaluations. <div>
arXiv:2512.15977v1 Announce Type: new 
Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings</title>
<link>https://arxiv.org/abs/2512.15993</link>
<guid>https://arxiv.org/abs/2512.15993</guid>
<content:encoded><![CDATA[
<div> Keywords: robotic mowing, biodiversity, visual perception, deep learning, adaptive decision-making<br /><br />Summary:  
This paper introduces a novel robotic mowing framework designed to enhance garden biodiversity actively by leveraging visual perception combined with adaptive decision-making. Unlike traditional passive rewilding methods, the system employs deep feature-space analysis to detect and preserve visually diverse vegetation patches captured in camera images by selectively deactivating the mower blades over these areas. The core technology utilizes a ResNet50 neural network pretrained on the PlantNet300K dataset, generating ecologically meaningful embeddings that represent plant diversity without needing species-level identification. From these embeddings, a global deviation metric is derived to estimate biodiversity, which then informs a selective mowing algorithm. This algorithm dynamically switches between mowing and conservation behavior based on real-time visual biodiversity cues. The framework was successfully implemented on a commercially available robotic mower modified for the experiment and validated both in controlled mock-up lawns and real garden environments. Experimental results show a strong correlation between the dispersion of embeddings in feature space and expert biodiversity assessments, validating the use of deep visual diversity as a proxy for ecological richness. Overall, the proposed system has the potential to transform monocultural lawns into ecologically valuable, biodiverse urban green spaces through intelligent, biodiversity-aware mowing. <div>
arXiv:2512.15993v1 Announce Type: new 
Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion</title>
<link>https://arxiv.org/abs/2512.16023</link>
<guid>https://arxiv.org/abs/2512.16023</guid>
<content:encoded><![CDATA[
<div> video diffusion, action generation, robotic policy learning, cross-modal interaction, action refinement  

<br /><br />Summary:  
This paper introduces a novel method to generate video-action pairs guided by text instructions, starting from initial image observations and robot joint states. The approach addresses the common challenge of lacking action annotations for video diffusion models, enabling their comprehensive application in robotic policy learning. Unlike existing approaches limited by two-stage pipelines or single-modal diffusion adaptation, the authors propose a threefold solution: (1) extending a pretrained video diffusion model with a dedicated parallel action diffusion model, which retains pretrained video knowledge; (2) incorporating a Bridge Attention mechanism to facilitate effective cross-modal interaction between video and action modalities; and (3) designing an action refinement module that transforms coarse actions into precise control signals, particularly useful when working with low-resolution datasets. Through extensive evaluations on multiple public benchmarks and real-world datasets, the method demonstrates superior performance by generating higher-quality videos and more accurate actions compared to existing baselines. The overall framework provides a scalable way to leverage large-scale video data for advancing robotic learning tasks. <div>
arXiv:2512.16023v1 Announce Type: new 
Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.16055</link>
<guid>https://arxiv.org/abs/2512.16055</guid>
<content:encoded><![CDATA[
<div> Keywords: adversarial evaluation, end-to-end autonomous driving, real-world image generation, corner cases, traffic policy<br /><br />Summary: This paper addresses the challenge of evaluating end-to-end autonomous driving systems in real-world, safety-critical corner cases that are difficult to collect naturally. The authors introduce a closed-loop evaluation platform capable of generating adversarial interactions within real-world driving scenes to test these autonomous models. Central to the platform is a real-world image generator based on flow matching, which efficiently and stably produces realistic driving images conditioned on dynamic traffic environment information. Alongside, an adversarial traffic policy simulates challenging interactions by controlling surrounding vehicles to create corner cases that expose weaknesses in current autonomous driving systems. The platform evaluates multiple end-to-end driving models trained on real data, such as UniAD and VAD, showing that these models exhibit performance degradation in the generated adversarial scenarios. Experimental results confirm the realism of the images and the effectiveness of the adversarial policy in provoking difficult driving situations. This approach allows for systematic detection of potential model issues under realistic and complex conditions, thus providing a valuable tool to improve the safety and robustness of end-to-end autonomous driving technologies. <div>
arXiv:2512.16055v1 Announce Type: new 
Abstract: Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution</title>
<link>https://arxiv.org/abs/2512.16075</link>
<guid>https://arxiv.org/abs/2512.16075</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion MRI, fiber orientation distribution, high angular resolution, spherical harmonics, patch diffusion model<br /><br />Summary: This paper addresses the challenge of accurately estimating high angular resolution fiber orientation distribution (HAR-FOD) from diffusion MRI (dMRI) data. Traditional single-shell low angular resolution dMRI (LAR-FOD) methods are limited in accuracy, while multi-shell high angular resolution dMRI (HAR-FOD) provides better results but requires long scanning times. The authors propose a novel 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD data, aiming to combine accuracy with efficiency. To enhance learning, they introduce a FOD-patch adapter that incorporates prior brain anatomy, enabling more effective patch-based training. A voxel-level conditional coordinating module is designed to improve the model’s global context understanding, addressing spatial dependencies. They also develop a spherical harmonic (SH) attention module to capture complex correlations among the numerous SH coefficients representing FOD, which is critical due to the high dimensionality of the data. Experimental results demonstrate their approach achieves superior performance in HAR-FOD prediction compared to state-of-the-art techniques, highlighting its potential for improving white matter characterization while reducing scanning times. <div>
arXiv:2512.16075v1 Announce Type: new 
Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Vocabulary 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.16077</link>
<guid>https://arxiv.org/abs/2512.16077</guid>
<content:encoded><![CDATA[
<div> Open-vocabulary 3D object detection, auto-generated classes, Semantic Score, 2D vision-language models, ScanNetV2 and SUNRGB-D datasets  

<br /><br />Summary:  
This paper introduces a new paradigm called Auto-Vocabulary 3D Object Detection (AV3DOD), which aims to automatically generate class names for detected 3D objects without any user input, differing from existing open-vocabulary methods that still rely on user-specified classes. To assess the quality of the generated class names, the authors propose a novel metric called Semantic Score (SS). The AV3DOD framework leverages 2D vision-language models (VLMs), incorporating techniques such as image captioning, pseudo 3D box generation, and feature-space semantics expansion to generate rich semantic candidates for detected objects. The method is evaluated on two prominent 3D detection benchmarks, ScanNetV2 and SUNRGB-D, where it demonstrates state-of-the-art (SOTA) performance in both object localization, measured by mean Average Precision (mAP), and semantic quality, assessed by SS. Notably, AV3DOD surpasses the previous SOTA method, CoDA, by 3.48 in overall mAP and achieves a 24.5% relative improvement in semantic score on the ScanNetV2 dataset. This work represents a significant step toward fully autonomous 3D object detection systems that do not require predefined vocabulary, enhancing both detection accuracy and semantic understanding. <div>
arXiv:2512.16077v1 Announce Type: new 
Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPX: Lightweight Hourglass Network with Global Context</title>
<link>https://arxiv.org/abs/2512.16089</link>
<guid>https://arxiv.org/abs/2512.16089</guid>
<content:encoded><![CDATA[
<div> Keywords: Human pose estimation, lightweight model, self-attention, Hourglass network, edge devices<br /><br />Summary: Human pose estimation is vital in computer vision, yet methods with state-of-the-art accuracy often require large models and high computational costs. To address this, many lightweight variants have been developed to reduce both model size and computation. However, these lighter models frequently incorporate components unsuited for efficient deployment on edge devices, or they focus too heavily on inference speed, resulting in compromised accuracy. The proposed model, LAPX, is an Hourglass network enhanced with a self-attention mechanism to better capture global contextual information, building upon the prior LAP model. LAPX not only integrates self-attention but also advances stage design and refines lightweight attention modules to balance efficiency and performance. It achieves competitive accuracy on two major benchmarks, MPII and COCO, while maintaining a small parameter count of only 2.3 million. Moreover, LAPX demonstrates real-time inference capability, confirming its suitability for use on resource-constrained edge devices. Overall, LAPX effectively addresses the trade-off between accuracy, model complexity, and inference speed in human pose estimation for edge deployment. <div>
arXiv:2512.16089v1 Announce Type: new 
Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collimator-assisted high-precision calibration method for event cameras</title>
<link>https://arxiv.org/abs/2512.16092</link>
<guid>https://arxiv.org/abs/2512.16092</guid>
<content:encoded><![CDATA[
<div> Event cameras, calibration, collimator, flickering star patterns, geometric parameters  

<br /><br />Summary:  
Event cameras offer advantages like high dynamic range and temporal resolution but calibrating them—specifically determining intrinsic and extrinsic parameters—remains challenging, especially for long-range measurements. To meet the needs of long-distance and high-precision calibration, the authors propose a novel method that uses a collimator displaying flickering star-based patterns as a calibration target. The approach begins with a linear solution for the camera parameters, utilizing the sphere motion model derived from the collimator’s movement. This initial solution is then refined through nonlinear optimization to enhance accuracy. The method was validated via extensive real-world experiments conducted under various environmental conditions. Results demonstrate that this calibration technique consistently outperforms current event camera calibration methods in both accuracy and reliability, showing strong potential for improving long-range event camera applications. <div>
arXiv:2512.16092v1 Announce Type: new 
Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</title>
<link>https://arxiv.org/abs/2512.16093</link>
<guid>https://arxiv.org/abs/2512.16093</guid>
<content:encoded><![CDATA[
<div> TurboDiffusion, video generation acceleration, attention acceleration, step distillation, quantization<br /><br />Summary:<br /><br />TurboDiffusion is a framework designed to significantly accelerate end-to-end video diffusion model generation by 100-200 times while maintaining video quality. The acceleration is achieved through multiple key components: (1) Attention acceleration, utilizing low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computations crucial for diffusion models. (2) Step distillation, where TurboDiffusion adopts rCM (likely refined Classifier-Free Model) for efficient distillation of inference steps, reducing the number of required steps in the generation process. (3) W8A8 quantization, quantizing both model parameters and activations to 8-bit precision to speed up linear layer operations and compress the model for a smaller footprint. Additionally, the framework incorporates other engineering optimizations to further enhance speed. Experiments conducted on multiple models, including Wan2.2-I2V-14B-720P and Wan2.1-T2V variants at different resolutions and sizes, demonstrate that TurboDiffusion achieves 100-200x speedups even on a single RTX 5090 GPU. Importantly, it manages to maintain video quality comparable to slower baseline methods. The project is openly available with model checkpoints and user-friendly code at their GitHub repository: https://github.com/thu-ml/TurboDiffusion. <div>
arXiv:2512.16093v1 Announce Type: new 
Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flexible Camera Calibration using a Collimator System</title>
<link>https://arxiv.org/abs/2512.16113</link>
<guid>https://arxiv.org/abs/2512.16113</guid>
<content:encoded><![CDATA[
<div> Camera calibration, collimator system, spherical motion model, angle invariance, minimal solver<br /><br />Summary:<br /><br />This paper presents a novel camera calibration method utilizing a specially designed collimator system, which offers a reliable and controllable environment for accurate calibration. The authors leverage the unique optical geometry of the collimator system to introduce an angle invariance constraint, leading to the discovery that the relative motion between the calibration target and the camera complies with a spherical motion model. This insight effectively reduces the complex original 6 degrees of freedom (6DOF) relative motion to a simpler 3 degrees of freedom (3DOF) pure rotation motion, simplifying the calibration process. Based on this spherical motion constraint, the authors develop a closed-form linear solver that handles multiple images as well as a minimal solver for calibration using just two images. Additionally, they propose an innovative algorithm capable of calibrating the camera from a single collimator image by exploiting the angle invariance constraint, thereby eliminating the need for camera motion and enabling quicker and more flexible calibration. The proposed method's effectiveness is validated through both synthetic simulations and real-world experiments, demonstrating superior performance compared to existing baseline methods. To facilitate practical adoption, the authors have made the demonstration code publicly available on GitHub. <div>
arXiv:2512.16113v1 Announce Type: new 
Abstract: Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space</title>
<link>https://arxiv.org/abs/2512.16133</link>
<guid>https://arxiv.org/abs/2512.16133</guid>
<content:encoded><![CDATA[
<div> Keywords: cattle interaction detection, behavioral dataset, action latent space, contrastive learning, smart livestock management<br /><br />Summary:  
This paper presents a novel approach for automatically detecting behavioral interactions among grazing cattle using only a single image, which is critical for applications like estrus detection in smart livestock management. Unlike extensive research on human interaction detection, cattle present unique challenges due to rare occurrence of interactions and a lack of comprehensive behavioral datasets. To address this, the authors propose CattleAct, a data-efficient method that decomposes interactions into combinations of individual cattle actions. The method involves first learning an action latent space from a large-scale cattle action dataset. Rare interactions are then embedded by fine-tuning this pre-trained latent space through contrastive learning, resulting in a unified latent space that represents both individual actions and interactions. The authors also develop a practical system that integrates video and GPS data for real-world application. Experimental results from a commercial-scale pasture demonstrate that CattleAct outperforms baseline methods in accurately detecting cattle interactions. The implementation is publicly available on GitHub, promoting accessibility and potential adoption in the cattle industry for smarter livestock monitoring and management. <div>
arXiv:2512.16133v1 Announce Type: new 
Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT</title>
<link>https://arxiv.org/abs/2512.16140</link>
<guid>https://arxiv.org/abs/2512.16140</guid>
<content:encoded><![CDATA[
<div> Keywords: Dual-spectral CT, hybrid reconstruction, oblique projection modification technique, ResDynUNet++, deep learning

<br /><br />Summary:  
This paper proposes a hybrid reconstruction framework for dual-spectral computed tomography (DSCT) that combines iterative reconstruction techniques with deep learning models to improve image quality. The method consists of two complementary phases: a knowledge-driven phase and a data-driven phase. In the knowledge-driven phase, the oblique projection modification technique (OPMT) is employed to quickly generate an intermediate solution of the basis material images from the raw projection data, due to its fast convergence and reliable basis material decomposition capabilities. Following this, the data-driven phase introduces a novel neural network architecture named ResDynUNet++, which refines the intermediate solutions produced by OPMT. ResDynUNet++ enhances the standard UNet++ base by integrating residual dynamic convolution blocks that adaptively and input-specifically extract features while maintaining stable training through residual connections. This design specifically addresses challenges inherent to DSCT such as channel imbalance and large artifacts near interfaces, resulting in cleaner and more accurate reconstructed images. The framework was rigorously tested on both synthetic phantom data and real clinical datasets, demonstrating superior performance and validated efficacy compared to existing methods. This hybrid approach effectively leverages both domain knowledge and advanced deep learning for improved DSCT reconstruction outcomes. <div>
arXiv:2512.16140v1 Announce Type: new 
Abstract: We propose a hybrid reconstruction framework for dual-spectral CT (DSCT) that integrates iterative methods with deep learning models. The reconstruction process consists of two complementary components: a knowledge-driven module and a data-driven module. In the knowledge-driven phase, we employ the oblique projection modification technique (OPMT) to reconstruct an intermediate solution of the basis material images from the projection data. We select OPMT for this role because of its fast convergence, which allows it to rapidly generate an intermediate solution that successfully achieves basis material decomposition. Subsequently, in the data-driven phase, we introduce a novel neural network, ResDynUNet++, to refine this intermediate solution. The ResDynUNet++ is built upon a UNet++ backbone by replacing standard convolutions with residual dynamic convolution blocks, which combine the adaptive, input-specific feature extraction of dynamic convolution with the stable training of residual connections. This architecture is designed to address challenges like channel imbalance and near-interface large artifacts in DSCT, producing clean and accurate final solutions. Extensive experiments on both synthetic phantoms and real clinical datasets validate the efficacy and superior performance of the proposed method.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation</title>
<link>https://arxiv.org/abs/2512.16143</link>
<guid>https://arxiv.org/abs/2512.16143</guid>
<content:encoded><![CDATA[
<div> few-shot 3D segmentation, foundation models, SAM segment graph, geometric features, graph neural network<br /><br />Summary:<br /><br />This paper introduces SegGraph, a novel framework designed for few-shot 3D part segmentation that leverages 2D foundation models and explicitly incorporates geometric structures. Existing methods either overlook 3D geometric cues or fail to utilize high-quality grouping information from the Segment Anything Model (SAM), resulting in poor segmentation quality such as under-segmentation and inconsistent part labeling. SegGraph addresses these issues through a SAM segment graph-based propagation approach, where segments from SAM’s masks form nodes in a graph and edges represent spatial relationships including overlap and adjacency. This graph structure allows explicit modeling of mutual segment relationships, capturing global geometric context. Nodes in the graph adaptively modulate features obtained from 2D foundation models, and the features are propagated via a graph neural network to enhance 3D feature learning. To maintain semantic consistency within segments, SegGraph maps these segment features back to 3D points using a novel view-direction-weighted fusion technique that reduces the impact of lower-quality segments. Experimental results on the PartNet-E dataset demonstrate that SegGraph surpasses previous methods by at least 6.9% mean Intersection-over-Union (mIoU), with particularly strong results on small parts and boundaries, highlighting its effectiveness in understanding geometric details. The implementation code is publicly available on GitHub. <div>
arXiv:2512.16143v1 Announce Type: new 
Abstract: This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation</title>
<link>https://arxiv.org/abs/2512.16164</link>
<guid>https://arxiv.org/abs/2512.16164</guid>
<content:encoded><![CDATA[
<div> Keywords: Unsupervised Domain Adaptation, Vision-Language Models, Prompt Tuning, Dual Alignment, Class Mapping Mechanism  

<br /><br />Summary:  
Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, addressing challenges caused by domain discrepancies. Deploying Vision-Language Models (VLMs) with prompt tuning in UDA faces key issues because existing methods focus mainly on aligning marginal distributions but overlook conditional distribution discrepancies. This oversight leads to problems such as class prototype misalignment and reduced semantic discriminability. To tackle these, the paper introduces C-DGPA (Class-Centric Dual Alignment Generative Prompt Adaptation), which synergistically optimizes both marginal and conditional distribution alignments via a novel dual-branch architecture. The marginal distribution alignment branch uses a dynamic adversarial training framework to reduce domain gaps at the feature distribution level. Meanwhile, the conditional distribution alignment branch incorporates a Class Mapping Mechanism (CMM) to align class-specific semantic prompts and prevent over-dependence on the source domain. This dual alignment strategy allows the integration of domain knowledge into prompt learning effectively, fostering domain-invariant and semantically discriminative representations. Extensive experiments conducted on the OfficeHome, Office31, and VisDA-2017 benchmarks demonstrate that C-DGPA achieves new state-of-the-art results across all evaluated datasets, validating its effectiveness and superiority in downstream UDA tasks. <div>
arXiv:2512.16164v1 Announce Type: new 
Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Closing the Domain Gap with Event Cameras</title>
<link>https://arxiv.org/abs/2512.16178</link>
<guid>https://arxiv.org/abs/2512.16178</guid>
<content:encoded><![CDATA[
<div> Event cameras, domain gap, lighting conditions, day-night difference, performance consistency<br /><br />Summary:<br /><br />1. Traditional cameras used in end-to-end driving systems face significant performance degradation when there is a domain gap, particularly caused by different lighting conditions such as day and night.<br />2. This study explores the use of event cameras as an alternative sensor to address this issue without requiring extensive adjustments or retraining.<br />3. Event cameras capture changes in the scene asynchronously and have intrinsic properties that allow them to operate consistently across varying lighting environments.<br />4. Experimental results demonstrate that event cameras maintain more stable and consistent performance across day-night lighting differences compared to traditional grayscale frames.<br />5. The domain-shift penalties observed for event cameras are generally smaller or comparable to those of grayscale cameras, making event cameras a promising sensor choice for robust cross-domain autonomous driving applications. <div>
arXiv:2512.16178v1 Announce Type: new 
Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation</title>
<link>https://arxiv.org/abs/2512.16199</link>
<guid>https://arxiv.org/abs/2512.16199</guid>
<content:encoded><![CDATA[
<div> Keywords: Avatar4D, synthetic human motion, sports datasets, pose estimation, domain adaptation<br /><br />Summary:<br /><br />1. The paper introduces Avatar4D, a novel pipeline designed to generate customizable synthetic human motion datasets that are transferable to real-world applications without requiring manual annotations.<br />2. Avatar4D offers fine-grained control over multiple factors including body pose, appearance, camera viewpoint, and environmental context, providing greater flexibility compared to prior works which mostly handle general everyday motions.<br />3. The approach is validated specifically on sports scenarios, where human actions and movements show complex and domain-specific patterns, making motion understanding particularly challenging.<br />4. To support this validation, the authors create Syn2Sport, a large-scale synthetic dataset covering sports like baseball and ice hockey, featuring high-fidelity 4D human motion sequences with varied player appearances and diverse environments.<br />5. Several state-of-the-art pose estimation models are benchmarked on Syn2Sport, demonstrating the dataset’s utility for supervised learning, zero-shot transfer to real-world data, and generalization across different sports.<br />6. The study also assesses the alignment between synthetic and real-world data in feature space, highlighting Avatar4D’s potential to generate scalable, controllable, and transferable human motion datasets tailored for domain-specific tasks without relying on domain-specific real data. <div>
arXiv:2512.16199v1 Announce Type: new 
Abstract: We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation</title>
<link>https://arxiv.org/abs/2512.16201</link>
<guid>https://arxiv.org/abs/2512.16201</guid>
<content:encoded><![CDATA[
<div> Radiology Report Generation, Medical Vision-Language Models, Visual Grounding, Reinforcement Learning, Disease Findings

<br /><br />Summary: Radiology Report Generation (RRG) is essential for automating healthcare workflows, improving patient assessments, and reducing the workload of medical professionals. Existing Large Medical Vision-Language Models (Med-VLMs) have made progress, but challenges persist in generating reports that are both visually grounded and clinically accurate. Current methods often depend on large labeled datasets, expensive task-specific preference data, or retrieval-based techniques, which fail to sufficiently prevent hallucinations caused by poor cross-modal alignment. To overcome these challenges, the authors propose VALOR (Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation), a reinforcement learning-based post-alignment framework featuring Group-Relative Proximal Optimization (GRPO). VALOR’s training has two stages: first, enhancing Med-VLMs with textual rewards that promote the use of clinically precise terminology, and second, aligning the vision projection module to disease findings to focus attention on diagnostically relevant image regions. Extensive experiments performed on multiple benchmarks show that VALOR significantly improves both factual accuracy and visual grounding, substantially outperforming current state-of-the-art radiology report generation methods. This work highlights the importance of better visual-linguistic alignment for reliable and clinically accurate automated report generation. <div>
arXiv:2512.16201v1 Announce Type: new 
Abstract: Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Ad-hoc Categorization with Contextualized Feature Learning</title>
<link>https://arxiv.org/abs/2512.16202</link>
<guid>https://arxiv.org/abs/2512.16202</guid>
<content:encoded><![CDATA[
<div> Adaptive categorization, ad-hoc categories, CLIP, visual clustering, context tokens<br /><br />Summary:  
This paper addresses the problem of open ad-hoc categorization, where AI agents dynamically create categories based on a few labeled examples and a large amount of unlabeled data to discover underlying contexts and expand categories through semantic extension and visual clustering. The authors propose OAK, a novel model that introduces a small set of learnable context tokens input into a frozen CLIP model, trained using both CLIP's image-text alignment and a visual clustering objective from Generalized Category Discovery (GCD). OAK leverages the insight that ad-hoc and common categorization share similar perceptual mechanisms, enabling it to adaptively form meaningful categories in new contexts. Experiments on the Stanford and Clevr-4 datasets demonstrate that OAK achieves state-of-the-art accuracy and concept discovery performance, such as 87.4% novel accuracy on the Stanford Mood dataset, outperforming both CLIP and GCD by large margins (over 50%). Additionally, OAK generates interpretable saliency maps that emphasize relevant visual features according to the category type—for example, hands for Action, faces for Mood, and backgrounds for Location—enhancing transparency and trust. The approach supports adaptive, generalizable categorization suited for AI agents facing dynamic and changing visual scene understanding tasks. <div>
arXiv:2512.16202v1 Announce Type: new 
Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced 3D Shape Analysis via Information Geometry</title>
<link>https://arxiv.org/abs/2512.16213</link>
<guid>https://arxiv.org/abs/2512.16213</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D point clouds, Gaussian Mixture Models, statistical manifold, Modified Symmetric Kullback-Leibler divergence, shape analysis<br /><br />Summary:  
This paper addresses the challenges in comparing three-dimensional (3D) point clouds, which are crucial for accurate digital object representation in fields such as computer graphics, photogrammetry, computer vision, and robotics. Traditional metrics like the Hausdorff and Chamfer distances often struggle to capture the global statistical structure of point clouds and are sensitive to outliers. Likewise, existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models (GMMs) may be numerically unstable or unbounded. To overcome these limitations, the authors introduce an information geometric framework that models point clouds as GMMs situated on a statistical manifold. They prove that the space comprising these GMMs forms a valid statistical manifold and propose a new metric, the Modified Symmetric Kullback-Leibler (MSKL) divergence, which guarantees upper and lower bounds for numerical stability across all GMM comparisons. Experimental validation on datasets involving human poses (MPI-FAUST) and animal shapes (G-PCD) demonstrates that MSKL divergence yields stable, monotonic values directly correlated with geometric variations, outperforming both traditional geometric distances and previous KL divergence approximations. This approach offers a robust, theoretically sound method for point cloud shape analysis with improved numerical and geometric reliability. <div>
arXiv:2512.16213v1 Announce Type: new 
Abstract: Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2512.16219</link>
<guid>https://arxiv.org/abs/2512.16219</guid>
<content:encoded><![CDATA[
<div> Keywords: novel view synthesis, diffusion models, high-quality noise, encoder-decoder network, discretized Euler inversion<br /><br />Summary:<br /><br />This paper addresses the challenge in single-view novel view synthesis (NVS) using diffusion models, focusing on the quality of the initial noise that significantly affects the quality of generated views. The authors observe that certain high-quality initial noise patterns yield better generation results but note the absence of frameworks for learning such noise. To tackle this, they first design a discretized Euler inversion method that embeds image semantic information into random Gaussian noise, enabling the creation of paired datasets of random and high-quality noise. Building on this, they propose a novel learning framework based on an encoder-decoder network (EDN) that transforms random noise directly into high-quality noise. This EDN can be integrated seamlessly with existing NVS models like SV3D and MV-Adapter. Experimental results demonstrate that the inclusion of EDN consistently improves performance across multiple datasets, validating its effectiveness and versatility. The paper contributes both a new method for noise refinement and an applicable framework for enhancing diffusion-based NVS, with publicly available code to facilitate further research and application. <div>
arXiv:2512.16219v1 Announce Type: new 
Abstract: Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Compression Using Singular Value Decomposition</title>
<link>https://arxiv.org/abs/2512.16226</link>
<guid>https://arxiv.org/abs/2512.16226</guid>
<content:encoded><![CDATA[
<div> Keywords: image compression, singular value decomposition, low-rank approximation, compression ratio, image quality<br /><br />Summary: This study focuses on the application of Singular Value Decomposition (SVD) and low-rank matrix approximations for image compression. The goal is to reduce storage and bandwidth requirements by efficiently compressing images, which form a large part of internet data. The research evaluates the compression performance through metrics like relative Frobenius error and compression ratio. Both grayscale and multichannel images are tested to verify the method's general applicability across different image types. Findings indicate that low-rank approximations often yield images that are visually close to the original, suggesting reasonable preservation of image quality. However, when compared to established compression formats such as JPEG, JPEG2000, and WEBP, the SVD-based method consistently underperforms in terms of compression efficiency at similar error levels. In fact, at low error tolerances, the compressed file size resulting from SVD can surpass that of the original image, making it impractical for real-world image compression needs. Overall, although SVD offers some visual fidelity, it is not competitive with current industry-standard codecs for effective and practical image compression. <div>
arXiv:2512.16226v1 Announce Type: new 
Abstract: Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation</title>
<link>https://arxiv.org/abs/2512.16234</link>
<guid>https://arxiv.org/abs/2512.16234</guid>
<content:encoded><![CDATA[
<div> 3D human reaction generation, autoregressive framework, Bootstrap Contextual Encoding, real-time inference, motion fidelity<br /><br />Summary: This paper addresses three critical challenges in 3D human reaction generation: ensuring high motion fidelity, enabling real-time inference, and maintaining autoregressive adaptability for online scenarios. The authors propose ARMFlow, a novel MeanFlow-based autoregressive framework that captures temporal dependencies between actor and reactor motions. ARMFlow incorporates a causal context encoder and an MLP-based velocity predictor to improve generation accuracy. A key innovation is Bootstrap Contextual Encoding (BSCE), which, during training, uses generated history instead of ground-truth data, effectively reducing error accumulation in autoregressive outputs. To complement ARMFlow's online capabilities, the authors introduce ReMFlow, an offline variant that achieves state-of-the-art performance and the fastest inference speed among offline methods. ARMFlow enhances semantic alignment through a global contextual encoder, delivers high accuracy with low latency in single-step inference, and mitigates accumulated errors via BSCE. In experimental evaluations on InterHuman and InterX datasets, ARMFlow's single-step online generation surpasses existing online methods by more than 40% in FID score while matching the performance of state-of-the-art offline methods, despite relying on only partial sequence conditions. This work thus significantly advances real-time and accurate 3D human reaction generation. <div>
arXiv:2512.16234v1 Announce Type: new 
Abstract: 3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection</title>
<link>https://arxiv.org/abs/2512.16235</link>
<guid>https://arxiv.org/abs/2512.16235</guid>
<content:encoded><![CDATA[
<div> Keywords: dermatology, AI diagnosis, family history, multi-modal framework, clinical trials  
  
<br /><br />Summary:  
This article addresses the challenge of improving dermatological diagnosis by integrating family history data with clinical imaging using AI. It highlights the global burden of skin diseases affecting 1.9 billion people and the diagnostic difficulties caused by limited specialist availability and complex presentations. The research introduces a multi-modal AI framework combining deep learning-based image analysis and structured clinical data, notably detailed family history patterns. The AI system employs interpretable convolutional neural networks linked with clinical decision trees incorporating hereditary risk factors to enhance diagnostic accuracy. Although full prospective clinical trials are proposed for future work, preliminary validation with healthcare professionals shows improved diagnostic performance, especially for hereditary conditions like melanoma, psoriasis, and atopic dermatitis. Expert feedback suggests potential benefits in early detection and personalized treatment recommendations. The AI framework is designed for seamless integration into clinical workflows while maintaining transparency through explainable AI techniques. This approach aims to support both clinical diagnosis and clinical trial validation, ultimately facilitating real-world implementation of AI-assisted dermatological care. <div>
arXiv:2512.16235v1 Announce Type: new 
Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models</title>
<link>https://arxiv.org/abs/2512.16243</link>
<guid>https://arxiv.org/abs/2512.16243</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view crowd counting, semi-supervised learning, model ranking, uncertainty estimation, limited labeled data<br /><br />Summary:<br /><br />This paper addresses the challenge of occlusion in crowd counting across large scenes by leveraging multi-view images. However, datasets for multi-view counting are limited due to the difficulty in collecting and annotating such data. To overcome this limitation, the authors explore semi-supervised frameworks that require fewer labeled multi-view images. Two novel semi-supervised methods are proposed, both based on ranking multi-view fusion models that utilize variable numbers of camera views. The first method, termed the vanilla model, ranks model predictions, enforcing the constraint that predictions with fewer views should not exceed predictions with more views. The second method ranks estimated model uncertainties, with the constraint that uncertainty should decrease as the number of views increases, guided by the prediction errors of the models. These ranking constraints are integrated into the training process to improve performance when labeled data are scarce. Experimental results demonstrate that the proposed ranking-based semi-supervised approaches outperform other existing semi-supervised crowd counting methods, providing a promising direction for handling limited multi-view labeled data in crowd counting tasks. <div>
arXiv:2512.16243v1 Announce Type: new 
Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning</title>
<link>https://arxiv.org/abs/2512.16266</link>
<guid>https://arxiv.org/abs/2512.16266</guid>
<content:encoded><![CDATA[
<div> Keywords: Fluorescence lifetime imaging microscopy, pixel super-resolution, deep learning, conditional generative adversarial network, image quality

<br /><br />Summary:  
1. Fluorescence lifetime imaging microscopy (FLIM) offers quantitative metabolic and molecular contrast with high translational potential for label-free, real-time diagnostics but is limited clinically by long pixel dwell times and low signal-to-noise ratio (SNR).  
2. The study introduces FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with pixel sizes up to 5 times larger than standard.  
3. FLIM_PSR_k uses a conditional generative adversarial network (cGAN) for training, providing more robust super-resolution reconstruction and significantly shorter inference times compared to diffusion model-based alternatives.  
4. The framework enables faster image acquisition and improves SNR in autofluorescence-based FLIM images, overcoming key challenges of resolution-speed trade-offs and noise limitations.  
5. Blind testing on patient-derived tumor tissue confirms reliable super-resolution with a factor of k=5, achieving a 25-fold increase in the space-bandwidth product and revealing fine architectural features otherwise lost in lower-resolution inputs, validated by statistically significant image quality improvements.  
6. By enhancing spatial resolution effectively, FLIM_PSR_k supports faster, higher-resolution, and hardware-flexible FLIM implementations, including compatibility with low-numerical-aperture and miniaturized platforms, thus enhancing FLIM’s potential for clinical translation. <div>
arXiv:2512.16266v1 Announce Type: new 
Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering</title>
<link>https://arxiv.org/abs/2512.16270</link>
<guid>https://arxiv.org/abs/2512.16270</guid>
<content:encoded><![CDATA[
<div> Keywords: Text rendering, image editing, semantic consistency, multimodal models, text-guided generation<br /><br />Summary:<br /><br />This paper addresses the challenges in text rendering and editing within images, an area that has gained attention due to advances in large-scale diffusion and multimodal models but remains underexplored. The authors introduce TextEditBench, a benchmark specifically designed to evaluate text-centric regions in images, focusing not just on pixel-level changes but on reasoning-intensive scenarios requiring semantic, geometric, and contextual coherence. To better assess model performance, they propose a novel evaluation metric called Semantic Expectation (SE), which gauges a model's ability to preserve semantic consistency, contextual coherence, and alignment across modalities during text editing tasks. Through extensive experiments involving state-of-the-art image editing systems, they find that while current models can follow straightforward textual instructions, they struggle with more complex challenges such as context-dependent reasoning, maintaining physical consistency, and integrating edits in a layout-aware manner. TextEditBench thus fills a significant gap by establishing a new, focused testing ground that highlights the need to advance reasoning capabilities in text-guided image editing and multimodal generation systems. This benchmark and evaluation approach aim to push future research towards more intelligent and contextually aware text editing in visual content. <div>
arXiv:2512.16270v1 Announce Type: new 
Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFLAN: Generative Functional Layouts</title>
<link>https://arxiv.org/abs/2512.16275</link>
<guid>https://arxiv.org/abs/2512.16275</guid>
<content:encoded><![CDATA[
<div> Keywords: Floor plan generation, topological planning, geometric realization, graph neural network, Transformer  

<br /><br />Summary:  
This paper presents GFLAN, a novel generative framework designed to improve automated floor plan generation by explicitly separating the process into two stages: topological planning and geometric realization. The method addresses limitations found in prior deep learning approaches, which often fail to capture architectural reasoning such as prioritizing topological relations over geometric details, propagating functional constraints through adjacency, and forming circulation patterns from local connections. Stage A of GFLAN uses a specialized convolutional architecture with dual encoders to handle spatial context and layout state independently, sequentially placing room centroids within the building envelope based on discrete probability maps of feasible locations. Stage B builds a heterogeneous graph that links room nodes with boundary vertices and employs a Transformer-enhanced graph neural network to jointly predict precise room boundaries. By departing from traditional pixel-wise or wall-tracing methods, this two-stage approach provides a more principled and interpretable mechanism for synthesizing floor plans. The framework takes as input only the exterior boundary and front-door location, generating functional layouts that satisfy topological and geometric constraints more effectively than previous models. GFLAN represents a significant advancement in integrating combinatorial search, geometric constraints, and design functionality in automated architectural design. <div>
arXiv:2512.16275v1 Announce Type: new 
Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval</title>
<link>https://arxiv.org/abs/2512.16294</link>
<guid>https://arxiv.org/abs/2512.16294</guid>
<content:encoded><![CDATA[
<div> Multi-label learning, Contrastive learning, Remote-sensing retrieval, Semantic imbalance, Adaptive weighting  

<br /><br />Summary:  
This article addresses key challenges in multi-label remote-sensing image retrieval, namely semantic overlap among land-cover categories, imbalanced label distributions, and intricate inter-class co-occurrence patterns. To overcome these issues, the authors propose Multi-Label Adaptive Contrastive Learning (MACL), an enhanced contrastive learning framework tailored for multi-label remote sensing. MACL incorporates label-aware sampling to better handle semantic overlap by focusing on relevant categories during training. It also integrates frequency-sensitive weighting to balance representation learning across both frequent and rare classes, ensuring robustness against label imbalance. Additionally, dynamic-temperature scaling is employed to adaptively adjust the contrastive loss temperature, improving the learning dynamics. Extensive experiments on three benchmark datasets—DLRSD, ML-AID, and WHDLD—demonstrate that MACL consistently outperforms baseline methods based on traditional contrastive losses. The method effectively mitigates semantic imbalance issues and achieves more reliable retrieval performance in large-scale remote sensing archives. The authors plan to release their code, pretrained models, and evaluation scripts upon acceptance, facilitating reproducibility and further research in the field. Overall, MACL presents a significant advancement for multi-label remote-sensing image retrieval by addressing critical dataset challenges. <div>
arXiv:2512.16294v1 Announce Type: new 
Abstract: Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixelArena: A benchmark for Pixel-Precision Visual Intelligence</title>
<link>https://arxiv.org/abs/2512.16303</link>
<guid>https://arxiv.org/abs/2512.16303</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal large language models, image generation, semantic segmentation, Gemini 3 Pro Image, fine-grained generative capabilities<br /><br />Summary:  
This paper addresses the evaluation of multi-modal large language models that generate images, focusing on fine-grained generation rather than just aesthetics. The authors introduce PixelArena, a novel benchmark that uses semantic segmentation tasks to objectively assess image generation at the pixel level. Their experiments reveal that the Gemini 3 Pro Image model exhibits emergent capabilities in generating semantic masks with remarkable precision in zero-shot scenarios, demonstrating advanced visual intelligence and genuine generalization in new image generation challenges. The study includes a thorough qualitative and quantitative comparison of Gemini 3 Pro Image’s performance against other models, highlighting strengths and identifying failure cases. These findings mark a significant step forward in assessing and understanding the fine-grained visual reasoning abilities of multimodal models. Furthermore, the insights gained offer valuable directions for future research in multimodality, reasoning, interpretability, and benchmarking, emphasizing the importance of objective, task-based evaluations for image generation systems. Overall, the work contributes to advancing methodologies for evaluating and improving the intelligence of models that integrate language and visual output. <div>
arXiv:2512.16303v1 Announce Type: new 
Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation</title>
<link>https://arxiv.org/abs/2512.16313</link>
<guid>https://arxiv.org/abs/2512.16313</guid>
<content:encoded><![CDATA[
<div> All-in-one video restoration, temporal modeling, degradation-agnostic features, lightweight network, LaverNet<br /><br />Summary:<br /><br />Recent advancements in video restoration have focused on developing unified models capable of handling multiple types of degradation simultaneously, known as all-in-one video restoration. However, existing approaches face significant challenges when dealing with degradations that vary over time. Firstly, the presence of degradation can dominate the temporal modeling process, causing models to focus on artifacts rather than the true video content. Secondly, current methods rely on large and complex models to achieve such restoration, which obscures the inherent difficulties of the task. To overcome these issues, the authors introduce LaverNet, a lightweight all-in-one video restoration network comprising only 362K parameters. The key innovation in LaverNet is a novel propagation mechanism that selectively passes only degradation-agnostic features across frames, effectively reducing the negative impact of degradations on temporal modeling. Despite its compact size, which is less than 1% of parameters used by existing methods, LaverNet demonstrates strong performance on benchmark datasets, achieving results comparable or even superior to larger models. This work highlights that efficient and effective all-in-one video restoration is possible with a significantly smaller network architecture. <div>
arXiv:2512.16313v1 Announce Type: new 
Abstract: Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs</title>
<link>https://arxiv.org/abs/2512.16314</link>
<guid>https://arxiv.org/abs/2512.16314</guid>
<content:encoded><![CDATA[
<div> arXiv:2512.16314v1  
Keywords: UAV, fusion localization, ridge estimation, multicollinearity, laser ranging  

<br /><br />Summary:  
This paper addresses the challenge of accurately tracking and measuring targets using multiple sensors mounted on Unmanned Aerial Vehicles (UAVs). It proposes a fusion localization method based on ridge estimation, which leverages both sequential imagery providing rich scene information and precise laser ranging data to improve localization accuracy. The study highlights the difficulty posed by multicollinearity in the design matrix's column vectors during least squares estimation, particularly under harsh conditions such as long distances, small intersection angles, and large inclination angles. Multicollinearity causes ill-conditioned problems that result in significant instability and reduced robustness of the estimates. To resolve this, ridge estimation is introduced as a solution to mitigate multicollinearity when observations are limited. Experimental results validate that the proposed fusion localization method outperforms ground localization algorithms that rely on single information sources by achieving higher accuracy. Furthermore, implementing ridge estimation not only improves accuracy but also enhances robustness, especially under constrained observation scenarios. This study demonstrates the effectiveness of combining multimodal data and applying ridge estimation techniques for UAV-based target localization in challenging conditions. <div>
arXiv:2512.16314v1 Announce Type: new 
Abstract: Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing</title>
<link>https://arxiv.org/abs/2512.16325</link>
<guid>https://arxiv.org/abs/2512.16325</guid>
<content:encoded><![CDATA[
<div> Keywords: Quality of Information, vehicular mobile crowdsensing, sensing coverage, incentive mechanism, urban monitoring  

<br /><br />Summary:  
This paper tackles the challenge of optimizing Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems, where vehicles participate dynamically and sensing coverage and reliability are intertwined challenges. The authors propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, designed to maximize sensing coverage and reliability within budget limits. A novel metric called Aggregated Sensing Quality (ASQ) is introduced to quantitatively integrate both coverage and sensing reliability, providing a comprehensive measure of QoI. QUIDS features a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and intelligently allocates incentives under uncertain conditions, thereby further improving ASQ. Evaluations conducted with real-world metropolitan data demonstrate that QUIDS enhances ASQ by 38% compared to scenarios without dispatching and by 10% over existing state-of-the-art approaches. Additionally, it reduces reconstruction map errors by 39-74% across different algorithms, indicating significant improvements in data quality. By jointly optimizing coverage and reliability through a quality-informed incentive mechanism, QUIDS enables cost-effective and high-quality urban monitoring without needing dedicated infrastructure. The system is applicable to various smart-city scenarios including traffic and environmental sensing, facilitating more effective and efficient urban data collection. <div>
arXiv:2512.16325v1 Announce Type: new 
Abstract: This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Edge-to-Server Inference for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16349</link>
<guid>https://arxiv.org/abs/2512.16349</guid>
<content:encoded><![CDATA[
<div> Keywords: collaborative inference, vision-language models, edge computing, communication cost, region of interest  

<br /><br />Summary:  
This paper introduces a collaborative edge-to-server inference framework designed for vision-language models (VLMs) to minimize communication costs without sacrificing inference accuracy. Typically, edge devices send resized global images to servers for VLM processing, but resizing often loses fine details that degrade accuracy. To address this, the authors propose a two-stage approach: initially, the server processes the global image and identifies a region of interest (RoI) using the model's internal attention mechanisms. Then, by computing the min-entropy of output tokens, the system measures confidence and decides if the detailed local image of the RoI needs to be retransmitted from the edge device. If the min-entropy is above a threshold (indicating uncertainty), the server requests the detailed image, enabling refined inference by combining global and local inputs. This selective retransmission ensures that only necessary visual content is transmitted, optimizing bandwidth usage. Experimental results on multiple VLM architectures show that this framework significantly reduces communication overhead while maintaining comparable inference accuracy, validating its effectiveness in real-world deployments where preserving fine-grained visual information is critical. <div>
arXiv:2512.16349v1 Announce Type: new 
Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction</title>
<link>https://arxiv.org/abs/2512.16357</link>
<guid>https://arxiv.org/abs/2512.16357</guid>
<content:encoded><![CDATA[
<div> Keywords: Latent Diffusion Models, High Dynamic Range, Gain Map, HDR Reconstruction, Multi-exposure

<br /><br />Summary:  
This paper addresses the challenges of applying pre-trained Latent Diffusion Models (LDMs) to multi-exposure High Dynamic Range (HDR) image reconstruction. Traditional LDMs face limitations including restricted dynamic-range representation due to 8-bit latent compression, high inference costs from multi-step denoising, and the tendency to hallucinate content because of their generative nature. To overcome these, the authors propose GMODiff, a novel gain map-driven, one-step diffusion framework for HDR reconstruction. Instead of reconstructing full HDR images, GMODiff reformulates the problem as estimating a Gain Map (GM) conditionally, wherein the GM encodes the extended dynamic range while maintaining the bit depth of Low Dynamic Range (LDR) images. The approach initializes denoising from an informative regression-based estimate rather than pure noise, enabling the generation of high-quality GMs with a single denoising step. Furthermore, the method combines the strengths of regression-based models—which excel at preserving content fidelity—and LDMs, which enhance perceptual quality, by using regression priors to guide both denoising and latent decoding. This combined guidance effectively suppresses hallucinations and maintains structural accuracy. Extensive experiments show that GMODiff outperforms several state-of-the-art methods while being 100 times faster than prior LDM-based approaches. <div>
arXiv:2512.16357v1 Announce Type: new 
Abstract: Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation</title>
<link>https://arxiv.org/abs/2512.16360</link>
<guid>https://arxiv.org/abs/2512.16360</guid>
<content:encoded><![CDATA[
<div> pose-driven animation, multi-character, identity correspondence, graph matching, Mask-Query Attention<br /><br />Summary: This paper addresses the challenge of consistent pose-driven character animation in multi-character scenarios, focusing particularly on the problem of maintaining correct Identity Correspondence (IC) when characters swap positions. The authors present EverybodyDance, a novel framework designed specifically to enforce IC correctness in generated animations involving multiple characters. EverybodyDance introduces the Identity Matching Graph (IMG), which represents characters in both reference and generated frames as nodes in a weighted bipartite graph; the edge weights denote affinities computed via a new Mask-Query Attention (MQA) mechanism. By framing IC correctness as a graph structural metric, the approach optimizes this criterion during training to ensure accurate identity preservation. To further enhance performance in multi-character animation, the system incorporates identity-embedded guidance, multi-scale matching strategies, and pre-classified sampling techniques, all aimed at reinforcing identity alignment. For evaluation, the authors curate the Identity Correspondence Evaluation benchmark that specifically tests IC accuracy in multi-character settings. Extensive experimental results demonstrate that EverybodyDance significantly surpasses current leading methods in both identity correspondence and visual quality of animations, marking a substantial advancement in pose-driven multi-character animation. <div>
arXiv:2512.16360v1 Announce Type: new 
Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.16371</link>
<guid>https://arxiv.org/abs/2512.16371</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Video, Factorized Video Generation, Large Language Model, Video Composition, Temporal Synthesis

<br /><br />Summary:  
This paper addresses the challenges faced by state-of-the-art Text-to-Video (T2V) diffusion models, which often struggle to generate complex scenes and follow temporal instructions accurately. The authors identify that many errors arise from the generation of an initial frame that lacks semantic correctness and logical consistency. To solve this, they propose Factorized Video Generation (FVG), a three-stage pipeline that separates the video generation process into distinct tasks. The first stage, Reasoning, involves using a Large Language Model (LLM) to rewrite the input prompt to describe only the initial scene, resolving any temporal ambiguities. Next, the Composition stage employs a Text-to-Image (T2I) model to create a high-quality, compositionally accurate anchor frame based on the refined prompt. Finally, the Temporal Synthesis stage uses a finetuned video model to animate the scene while faithfully following the prompt. This factorized approach achieves state-of-the-art performance on the T2V CompBench benchmark and improves results on the VBench2 dataset. Additionally, the method allows for a 70% reduction in sampling steps without sacrificing performance, significantly speeding up the generation process. Overall, FVG presents a practical strategy for producing more efficient, robust, and controllable video synthesis. <div>
arXiv:2512.16371v1 Announce Type: new 
Abstract: State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Frequency Domain Alignment Network for Medical image segmentation</title>
<link>https://arxiv.org/abs/2512.16393</link>
<guid>https://arxiv.org/abs/2512.16393</guid>
<content:encoded><![CDATA[
<div> Adaptive Frequency Domain Alignment Network, Domain Adaptation, Medical Image Segmentation, Frequency Fusion, Cross-Domain Knowledge Transfer<br /><br />Summary:<br /><br />1. The paper addresses the scarcity of high-quality annotated medical image segmentation data, which is caused by the labor-intensive and time-consuming manual annotation process. <br />2. To overcome this limitation, the authors propose a novel domain adaptation framework named Adaptive Frequency Domain Alignment Network (AFDAN) that aligns features in the frequency domain.<br />3. AFDAN consists of three key modules: (a) an Adversarial Domain Learning Module for transferring features between source and target domains, (b) a Source-Target Frequency Fusion Module that blends frequency representations from both domains, and (c) a Spatial-Frequency Integration Module which integrates spatial and frequency features to improve segmentation performance.<br />4. The framework is designed to enable robust cross-domain knowledge transfer, thereby mitigating the impact of limited annotated data.<br />5. Extensive experiments on two datasets demonstrate AFDAN’s effectiveness, achieving a high Intersection over Union (IoU) of 90.9% for vitiligo segmentation on the new VITILIGO2025 dataset and an IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, both surpassing current state-of-the-art methods. <div>
arXiv:2512.16393v1 Announce Type: new 
Abstract: High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture</title>
<link>https://arxiv.org/abs/2512.16397</link>
<guid>https://arxiv.org/abs/2512.16397</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Splatting, 3D Neural Representations, Face Reconstruction, Relightable Texture, Text-driven Asset Creation

<br /><br />Summary:  
1. The paper introduces a novel method for reconstructing a neutral-pose human face from a limited set of uncalibrated images (only 11), using advanced three-dimensional neural representations.  
2. Gaussian Splatting is preferred over NeRFs due to its explicit nature, which is more conducive to applying constraints, facilitating better control over the reconstruction process.  
3. Semantic face regions are aligned using segmentation annotations, improving the coherence and accuracy of the reconstruction despite the sparse image input.  
4. The system softly constrains Gaussian primitives to an underlying triangulated surface, creating a structured 3D model that can be incorporated into standard graphics pipelines, allowing practical use in graphics applications.  
5. The accurate geometry enables transforming Gaussian Splats into texture space, treating them as view-dependent neural textures. This innovation allows high visual fidelity rendering of assets without modifying other scene assets or rendering parameters.  
6. A relightable Gaussian model disentangles texture from lighting, producing a detailed, delit high-resolution albedo texture compatible with standard graphics workflows.  
7. The approach supports training on images with varied, even incompatible, lighting conditions, increasing robustness through flexible regularization.  
8. Finally, the paper demonstrates the practical utility of the method by integrating it into a text-driven asset creation pipeline, showing its applicability in content generation and computer graphics. <div>
arXiv:2512.16397v1 Announce Type: new 
Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrepLLM: Native Boundary Representation Understanding with Large Language Models</title>
<link>https://arxiv.org/abs/2512.16413</link>
<guid>https://arxiv.org/abs/2512.16413</guid>
<content:encoded><![CDATA[
<div> Keywords: BrepLLM, Large Language Models, 3D Boundary Representation, Cross-modal Alignment, Mixture-of-Query Experts<br /><br />Summary: Current Large Language Models (LLMs) that work with token sequences struggle to directly process 3D Boundary Representation (Brep) models, which include complex geometric and topological information. To address this, the authors propose BrepLLM, the first framework designed to enable LLMs to parse and reason over raw Brep data, effectively bridging the gap between structured 3D geometry and natural language. BrepLLM utilizes a two-stage training pipeline: first, Cross-modal Alignment Pre-training, where an adaptive UV sampling strategy converts Breps into graph representations containing geometry and topology information. A hierarchical BrepEncoder then extracts features from faces, edges, and topology to produce a global token and node token sequence, which are aligned with text embeddings from a frozen CLIP text encoder via contrastive learning. The second stage involves Multi-stage LLM Fine-tuning, which integrates the pretrained BrepEncoder into an LLM and aligns node tokens using a progressive three-step strategy: training an MLP-based semantic mapper using 2D-LLM priors, fine-tuning the LLM, and introducing a Mixture-of-Query Experts (MQE) to model geometric diversity. Additionally, the authors created the Brep2Text dataset containing 269,444 Brep-text question-answer pairs. Experiments demonstrate that BrepLLM achieves state-of-the-art performance on 3D object classification and captioning tasks. <div>
arXiv:2512.16413v1 Announce Type: new 
Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CountZES: Counting via Zero-Shot Exemplar Selection</title>
<link>https://arxiv.org/abs/2512.16415</link>
<guid>https://arxiv.org/abs/2512.16415</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot object counting, exemplar selection, open-vocabulary detection, self-supervised learning, feature clustering  

<br /><br />Summary:  
The paper addresses the challenging task of zero-shot object counting (ZOC), where the goal is to count instances of previously unseen categories from class names alone. Current ZOC approaches either use open-vocabulary detectors that produce multi-instance candidates or random patch sampling which poorly isolates object instances. To overcome these limitations, the authors propose CountZES, a training-free framework that employs zero-shot exemplar selection to improve counting accuracy. CountZES operates through three complementary stages: Detection-Anchored Exemplar (DAE), which refines open-vocabulary detections to obtain precise single-instance exemplars; Density-Guided Exemplar (DGE), introducing a self-supervised method that selects exemplars based on density estimation to ensure statistical consistency and semantic compactness; and Feature-Consensus Exemplar (FCE), which uses feature-space clustering to enforce visual coherence across exemplars. This combined approach produces a diverse and complementary exemplar set balancing textual grounding, counting accuracy, and visual representativeness. Experimental results across multiple datasets, including natural, aerial, and medical images, demonstrate that CountZES outperforms existing zero-shot counting methods and generalizes well across different domains without requiring training. <div>
arXiv:2512.16415v1 Announce Type: new 
Abstract: Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt</title>
<link>https://arxiv.org/abs/2512.16443</link>
<guid>https://arxiv.org/abs/2512.16443</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image diffusion, subject consistency, semantic entanglement, training-free approach, text embeddings<br /><br />Summary:<br /><br />This paper addresses the challenge of maintaining subject consistency in text-to-image diffusion models, which are proficient at generating high-quality images from natural language prompts but often produce inconsistent subjects across multiple images. Existing solutions typically involve computationally expensive fine-tuning or image conditioning that requires optimization tailored to each subject. The authors critique 1Prompt1Story, a training-free method that concatenates scene descriptions into a single prompt and rescales token embeddings; while effective, it suffers from semantic leakage where embeddings of different frames entangle, causing misalignment between the text and generated images. To overcome this, the paper proposes a novel, training-free approach that refines text embeddings from a geometric perspective to suppress unwanted semantic overlap between frames. This method effectively reduces semantic entanglement and improves subject consistency without the need for model retraining or subject-specific fine-tuning. Extensive experiments demonstrate that the proposed approach significantly enhances both subject consistency and text alignment compared to existing baselines, making it a practical solution for visual storytelling applications using diffusion-based text-to-image generation models. <div>
arXiv:2512.16443v1 Announce Type: new 
Abstract: Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach</title>
<link>https://arxiv.org/abs/2512.16456</link>
<guid>https://arxiv.org/abs/2512.16456</guid>
<content:encoded><![CDATA[
<div> Keywords: human motion generation, gaze priming, diffusion model, reach success, motion datasets  

<br /><br />Summary:  
This paper addresses the complex challenge of generating realistic human motion, focusing specifically on the behaviour associated with priming an object or location for picking up or putting down. This behaviour involves spotting the target object or location from a distance, termed gaze priming, followed by approaching and reaching the target. The authors curated a novel dataset consisting of 23.7K gaze-primed human motion sequences by combining data from five publicly available datasets: HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. They leverage this dataset to pre-train a text-conditioned diffusion-based motion generation model, which is further fine-tuned conditioned on either the goal pose or location. The evaluation introduces a new metric called "Prime Success" alongside the existing "Reach Success" metric to assess how well the generated motion imitates natural human movement. Experimental results highlight that, on the largest dataset HD-EPIC, the model achieved a 60% rate in prime success and an 89% rate in reach success when conditioned on the goal object location, demonstrating the model's effectiveness in synthesizing realistic gaze-primed reaching motions. <div>
arXiv:2512.16456v1 Announce Type: new 
Abstract: Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning</title>
<link>https://arxiv.org/abs/2512.16461</link>
<guid>https://arxiv.org/abs/2512.16461</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D scene understanding, Vision-Language Models, point cloud geometry, temporal consistency, autonomous robotics<br /><br />Summary:  
1. SNOW (Scene Understanding with Open-World Knowledge) is a novel framework designed for unified 4D scene understanding in autonomous robotic systems.  
2. It addresses the limitations of Vision-Language Models (VLMs) by integrating semantic information with precise 3D geometric data and temporal dynamics to improve environment perception.  
3. The framework processes synchronized RGB images and 3D point clouds, utilizing HDBSCAN clustering to produce object-level proposals that direct the SAM2 segmentation model for more accurate region delineation.  
4. SNOW introduces Spatio-Temporal Tokenized Patch Encoding (STEP), which generates multimodal tokens encapsulating localized semantic, geometric, and temporal features.  
5. These tokens are incrementally fused into a 4D Scene Graph (4DSG), providing a structured and queryable world model that supports spatially and temporally grounded reasoning.  
6. A lightweight SLAM backend anchors STEP tokens in a global spatial reference, ensuring consistent mapping across time and enabling robust spatial grounding.  
7. Experimental evaluation demonstrates SNOW’s ability to achieve state-of-the-art performance on diverse benchmarks by leveraging structured 4D priors to enhance embodied reasoning and autonomous navigation.  
8. This work underlines the critical role of integrating open-world semantic knowledge with geometric and temporal cues for reliable real-world robotic perception and interaction. <div>
arXiv:2512.16461v1 Announce Type: new 
Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models</title>
<link>https://arxiv.org/abs/2512.16483</link>
<guid>https://arxiv.org/abs/2512.16483</guid>
<content:encoded><![CDATA[
arXiv:2512.16483v1 Announce Type: new 
Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment</title>
<link>https://arxiv.org/abs/2512.16484</link>
<guid>https://arxiv.org/abs/2512.16484</guid>
<content:encoded><![CDATA[
arXiv:2512.16484v1 Announce Type: new 
Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors</title>
<link>https://arxiv.org/abs/2512.16485</link>
<guid>https://arxiv.org/abs/2512.16485</guid>
<content:encoded><![CDATA[
arXiv:2512.16485v1 Announce Type: new 
Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images</title>
<link>https://arxiv.org/abs/2512.16493</link>
<guid>https://arxiv.org/abs/2512.16493</guid>
<content:encoded><![CDATA[
arXiv:2512.16493v1 Announce Type: new 
Abstract: The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.16494</link>
<guid>https://arxiv.org/abs/2512.16494</guid>
<content:encoded><![CDATA[
arXiv:2512.16494v1 Announce Type: new 
Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks</title>
<link>https://arxiv.org/abs/2512.16501</link>
<guid>https://arxiv.org/abs/2512.16501</guid>
<content:encoded><![CDATA[
arXiv:2512.16501v1 Announce Type: new 
Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization</title>
<link>https://arxiv.org/abs/2512.16504</link>
<guid>https://arxiv.org/abs/2512.16504</guid>
<content:encoded><![CDATA[
arXiv:2512.16504v1 Announce Type: new 
Abstract: The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images</title>
<link>https://arxiv.org/abs/2512.16511</link>
<guid>https://arxiv.org/abs/2512.16511</guid>
<content:encoded><![CDATA[
arXiv:2512.16511v1 Announce Type: new 
Abstract: Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16523</link>
<guid>https://arxiv.org/abs/2512.16523</guid>
<content:encoded><![CDATA[
arXiv:2512.16523v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.16561</link>
<guid>https://arxiv.org/abs/2512.16561</guid>
<content:encoded><![CDATA[
arXiv:2512.16561v1 Announce Type: new 
Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D Primitive-M\^ach\'e: Glueing Primitives for Persistent 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2512.16564</link>
<guid>https://arxiv.org/abs/2512.16564</guid>
<content:encoded><![CDATA[
arXiv:2512.16564v1 Announce Type: new 
Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.
  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.
  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.16567</link>
<guid>https://arxiv.org/abs/2512.16567</guid>
<content:encoded><![CDATA[
arXiv:2512.16567v1 Announce Type: new 
Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series</title>
<link>https://arxiv.org/abs/2512.16577</link>
<guid>https://arxiv.org/abs/2512.16577</guid>
<content:encoded><![CDATA[
arXiv:2512.16577v1 Announce Type: new 
Abstract: Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs</title>
<link>https://arxiv.org/abs/2512.16584</link>
<guid>https://arxiv.org/abs/2512.16584</guid>
<content:encoded><![CDATA[
arXiv:2512.16584v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks</title>
<link>https://arxiv.org/abs/2512.16586</link>
<guid>https://arxiv.org/abs/2512.16586</guid>
<content:encoded><![CDATA[
arXiv:2512.16586v1 Announce Type: new 
Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment</title>
<link>https://arxiv.org/abs/2512.16609</link>
<guid>https://arxiv.org/abs/2512.16609</guid>
<content:encoded><![CDATA[
arXiv:2512.16609v1 Announce Type: new 
Abstract: This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.16615</link>
<guid>https://arxiv.org/abs/2512.16615</guid>
<content:encoded><![CDATA[
arXiv:2512.16615v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation</title>
<link>https://arxiv.org/abs/2512.16620</link>
<guid>https://arxiv.org/abs/2512.16620</guid>
<content:encoded><![CDATA[
arXiv:2512.16620v1 Announce Type: new 
Abstract: Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeContext as Defense: Safe Image Editing in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.16625</link>
<guid>https://arxiv.org/abs/2512.16625</guid>
<content:encoded><![CDATA[
arXiv:2512.16625v1 Announce Type: new 
Abstract: In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARMAE: Masked Autoencoder for SAR Representation Learning</title>
<link>https://arxiv.org/abs/2512.16635</link>
<guid>https://arxiv.org/abs/2512.16635</guid>
<content:encoded><![CDATA[
arXiv:2512.16635v1 Announce Type: new 
Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion</title>
<link>https://arxiv.org/abs/2512.16636</link>
<guid>https://arxiv.org/abs/2512.16636</guid>
<content:encoded><![CDATA[
arXiv:2512.16636v1 Announce Type: new 
Abstract: Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering</title>
<link>https://arxiv.org/abs/2512.16670</link>
<guid>https://arxiv.org/abs/2512.16670</guid>
<content:encoded><![CDATA[
arXiv:2512.16670v1 Announce Type: new 
Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray</title>
<link>https://arxiv.org/abs/2512.16685</link>
<guid>https://arxiv.org/abs/2512.16685</guid>
<content:encoded><![CDATA[
arXiv:2512.16685v1 Announce Type: new 
Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?</title>
<link>https://arxiv.org/abs/2512.16688</link>
<guid>https://arxiv.org/abs/2512.16688</guid>
<content:encoded><![CDATA[
arXiv:2512.16688v1 Announce Type: new 
Abstract: The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDFoam: Signed-Distance Foam for explicit surface reconstruction</title>
<link>https://arxiv.org/abs/2512.16706</link>
<guid>https://arxiv.org/abs/2512.16706</guid>
<content:encoded><![CDATA[
arXiv:2512.16706v1 Announce Type: new 
Abstract: Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry</title>
<link>https://arxiv.org/abs/2512.16710</link>
<guid>https://arxiv.org/abs/2512.16710</guid>
<content:encoded><![CDATA[
arXiv:2512.16710v1 Announce Type: new 
Abstract: Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition</title>
<link>https://arxiv.org/abs/2512.16727</link>
<guid>https://arxiv.org/abs/2512.16727</guid>
<content:encoded><![CDATA[
arXiv:2512.16727v1 Announce Type: new 
Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.16740</link>
<guid>https://arxiv.org/abs/2512.16740</guid>
<content:encoded><![CDATA[
arXiv:2512.16740v1 Announce Type: new 
Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeNet: A Light Weight Model for Low Bitrate Image Compression</title>
<link>https://arxiv.org/abs/2512.16743</link>
<guid>https://arxiv.org/abs/2512.16743</guid>
<content:encoded><![CDATA[
arXiv:2512.16743v1 Announce Type: new 
Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation</title>
<link>https://arxiv.org/abs/2512.16767</link>
<guid>https://arxiv.org/abs/2512.16767</guid>
<content:encoded><![CDATA[
arXiv:2512.16767v1 Announce Type: new 
Abstract: Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowDet: Unifying Object Detection and Generative Transport Flows</title>
<link>https://arxiv.org/abs/2512.16771</link>
<guid>https://arxiv.org/abs/2512.16771</guid>
<content:encoded><![CDATA[
arXiv:2512.16771v1 Announce Type: new 
Abstract: We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kling-Omni Technical Report</title>
<link>https://arxiv.org/abs/2512.16776</link>
<guid>https://arxiv.org/abs/2512.16776</guid>
<content:encoded><![CDATA[
arXiv:2512.16776v1 Announce Type: new 
Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R3ST: A Synthetic 3D Dataset With Realistic Trajectories</title>
<link>https://arxiv.org/abs/2512.16784</link>
<guid>https://arxiv.org/abs/2512.16784</guid>
<content:encoded><![CDATA[
arXiv:2512.16784v1 Announce Type: new 
Abstract: Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals</title>
<link>https://arxiv.org/abs/2512.16791</link>
<guid>https://arxiv.org/abs/2512.16791</guid>
<content:encoded><![CDATA[
arXiv:2512.16791v1 Announce Type: new 
Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation</title>
<link>https://arxiv.org/abs/2512.16811</link>
<guid>https://arxiv.org/abs/2512.16811</guid>
<content:encoded><![CDATA[
arXiv:2512.16811v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenseBEV: Transforming BEV Grid Cells into 3D Objects</title>
<link>https://arxiv.org/abs/2512.16818</link>
<guid>https://arxiv.org/abs/2512.16818</guid>
<content:encoded><![CDATA[
arXiv:2512.16818v1 Announce Type: new 
Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Generation License Plate Detection and Recognition System using YOLOv8</title>
<link>https://arxiv.org/abs/2512.16826</link>
<guid>https://arxiv.org/abs/2512.16826</guid>
<content:encoded><![CDATA[
arXiv:2512.16826v1 Announce Type: new 
Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radiology Report Generation with Layer-Wise Anatomical Attention</title>
<link>https://arxiv.org/abs/2512.16841</link>
<guid>https://arxiv.org/abs/2512.16841</guid>
<content:encoded><![CDATA[
arXiv:2512.16841v1 Announce Type: new 
Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction</title>
<link>https://arxiv.org/abs/2512.16842</link>
<guid>https://arxiv.org/abs/2512.16842</guid>
<content:encoded><![CDATA[
arXiv:2512.16842v1 Announce Type: new 
Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation</title>
<link>https://arxiv.org/abs/2512.16853</link>
<guid>https://arxiv.org/abs/2512.16853</guid>
<content:encoded><![CDATA[
arXiv:2512.16853v1 Announce Type: new 
Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing</title>
<link>https://arxiv.org/abs/2512.16864</link>
<guid>https://arxiv.org/abs/2512.16864</guid>
<content:encoded><![CDATA[
arXiv:2512.16864v1 Announce Type: new 
Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pixel Seal: Adversarial-only training for invisible image and video watermarking</title>
<link>https://arxiv.org/abs/2512.16874</link>
<guid>https://arxiv.org/abs/2512.16874</guid>
<content:encoded><![CDATA[
arXiv:2512.16874v1 Announce Type: new 
Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation</title>
<link>https://arxiv.org/abs/2512.16880</link>
<guid>https://arxiv.org/abs/2512.16880</guid>
<content:encoded><![CDATA[
arXiv:2512.16880v1 Announce Type: new 
Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-PhyGs: Multi-Material Object Dynamics from Video</title>
<link>https://arxiv.org/abs/2512.16885</link>
<guid>https://arxiv.org/abs/2512.16885</guid>
<content:encoded><![CDATA[
arXiv:2512.16885v1 Announce Type: new 
Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</title>
<link>https://arxiv.org/abs/2512.16891</link>
<guid>https://arxiv.org/abs/2512.16891</guid>
<content:encoded><![CDATA[
arXiv:2512.16891v1 Announce Type: new 
Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</title>
<link>https://arxiv.org/abs/2512.16893</link>
<guid>https://arxiv.org/abs/2512.16893</guid>
<content:encoded><![CDATA[
arXiv:2512.16893v1 Announce Type: new 
Abstract: Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</title>
<link>https://arxiv.org/abs/2512.16900</link>
<guid>https://arxiv.org/abs/2512.16900</guid>
<content:encoded><![CDATA[
arXiv:2512.16900v1 Announce Type: new 
Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</title>
<link>https://arxiv.org/abs/2512.16905</link>
<guid>https://arxiv.org/abs/2512.16905</guid>
<content:encoded><![CDATA[
arXiv:2512.16905v1 Announce Type: new 
Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization</title>
<link>https://arxiv.org/abs/2512.16906</link>
<guid>https://arxiv.org/abs/2512.16906</guid>
<content:encoded><![CDATA[
arXiv:2512.16906v1 Announce Type: new 
Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</title>
<link>https://arxiv.org/abs/2512.16907</link>
<guid>https://arxiv.org/abs/2512.16907</guid>
<content:encoded><![CDATA[
arXiv:2512.16907v1 Announce Type: new 
Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SceneDiff: A Benchmark and Method for Multiview Object Change Detection</title>
<link>https://arxiv.org/abs/2512.16908</link>
<guid>https://arxiv.org/abs/2512.16908</guid>
<content:encoded><![CDATA[
arXiv:2512.16908v1 Announce Type: new 
Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</title>
<link>https://arxiv.org/abs/2512.16909</link>
<guid>https://arxiv.org/abs/2512.16909</guid>
<content:encoded><![CDATA[
arXiv:2512.16909v1 Announce Type: new 
Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFTok: Bridging the Performance Gap in Discrete Tokenizers</title>
<link>https://arxiv.org/abs/2512.16910</link>
<guid>https://arxiv.org/abs/2512.16910</guid>
<content:encoded><![CDATA[
arXiv:2512.16910v1 Announce Type: new 
Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</title>
<link>https://arxiv.org/abs/2512.16913</link>
<guid>https://arxiv.org/abs/2512.16913</guid>
<content:encoded><![CDATA[
arXiv:2512.16913v1 Announce Type: new 
Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</title>
<link>https://arxiv.org/abs/2512.16915</link>
<guid>https://arxiv.org/abs/2512.16915</guid>
<content:encoded><![CDATA[
arXiv:2512.16915v1 Announce Type: new 
Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaTooler-V: Adaptive Tool-Use for Images and Videos</title>
<link>https://arxiv.org/abs/2512.16918</link>
<guid>https://arxiv.org/abs/2512.16918</guid>
<content:encoded><![CDATA[
arXiv:2512.16918v1 Announce Type: new 
Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DVGT: Driving Visual Geometry Transformer</title>
<link>https://arxiv.org/abs/2512.16919</link>
<guid>https://arxiv.org/abs/2512.16919</guid>
<content:encoded><![CDATA[
arXiv:2512.16919v1 Announce Type: new 
Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EasyV2V: A High-quality Instruction-based Video Editing Framework</title>
<link>https://arxiv.org/abs/2512.16920</link>
<guid>https://arxiv.org/abs/2512.16920</guid>
<content:encoded><![CDATA[
arXiv:2512.16920v1 Announce Type: new 
Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</title>
<link>https://arxiv.org/abs/2512.16921</link>
<guid>https://arxiv.org/abs/2512.16921</guid>
<content:encoded><![CDATA[
arXiv:2512.16921v1 Announce Type: new 
Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Embedding Prediction Makes Strong Vision Learners</title>
<link>https://arxiv.org/abs/2512.16922</link>
<guid>https://arxiv.org/abs/2512.16922</guid>
<content:encoded><![CDATA[
arXiv:2512.16922v1 Announce Type: new 
Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Refocusing: Flexible Defocus Control from a Single Image</title>
<link>https://arxiv.org/abs/2512.16923</link>
<guid>https://arxiv.org/abs/2512.16923</guid>
<content:encoded><![CDATA[
arXiv:2512.16923v1 Announce Type: new 
Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</title>
<link>https://arxiv.org/abs/2512.16924</link>
<guid>https://arxiv.org/abs/2512.16924</guid>
<content:encoded><![CDATA[
arXiv:2512.16924v1 Announce Type: new 
Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D3G: Diverse Demographic Data Generation Increases Zero-Shot Image Classification Accuracy within Multimodal Models</title>
<link>https://arxiv.org/abs/2512.15747</link>
<guid>https://arxiv.org/abs/2512.15747</guid>
<content:encoded><![CDATA[
arXiv:2512.15747v1 Announce Type: cross 
Abstract: Image classification is a task essential for machine perception to achieve human-level image understanding. Multimodal models such as CLIP have been able to perform well on this task by learning semantic similarities across vision and language; however, despite these advances, image classification is still a challenging task. Models with low capacity often suffer from underfitting and thus underperform on fine-grained image classification. Along with this, it is important to ensure high-quality data with rich cross-modal representations of each class, which is often difficult to generate. When datasets do not enforce balanced demographics, the predictions will be biased toward the more represented class, while others will be neglected. We focus on how these issues can lead to harmful bias for zero-shot image classification, and explore how to combat these issues in demographic bias. We propose Diverse Demographic Data Generation (D3G), a training-free, zero-shot method of boosting classification accuracy while reducing demographic bias in pre-trained multimodal models. With this method, we utilize CLIP as our base multimodal model and Stable Diffusion XL as our generative model. We demonstrate that providing diverse demographic data at inference time improves performance for these models, and explore the impact of individual demographics on the resulting accuracy metric.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surely Large Multimodal Models (Don't) Excel in Visual Species Recognition?</title>
<link>https://arxiv.org/abs/2512.15748</link>
<guid>https://arxiv.org/abs/2512.15748</guid>
<content:encoded><![CDATA[
arXiv:2512.15748v1 Announce Type: cross 
Abstract: Visual Species Recognition (VSR) is pivotal to biodiversity assessment and conservation, evolution research, and ecology and ecosystem management. Training a machine-learned model for VSR typically requires vast amounts of annotated images. Yet, species-level annotation demands domain expertise, making it realistic for domain experts to annotate only a few examples. These limited labeled data motivate training an ''expert'' model via few-shot learning (FSL). Meanwhile, advanced Large Multimodal Models (LMMs) have demonstrated prominent performance on general recognition tasks. It is straightforward to ask whether LMMs excel in the highly specialized VSR task and whether they outshine FSL expert models. Somewhat surprisingly, we find that LMMs struggle in this task, despite using various established prompting techniques. LMMs even significantly underperform FSL expert models, which are as simple as finetuning a pretrained visual encoder on the few-shot images. However, our in-depth analysis reveals that LMMs can effectively post-hoc correct the expert models' incorrect predictions. Briefly, given a test image, when prompted with the top predictions from an FSL expert model, LMMs can recover the ground-truth label. Building on this insight, we derive a simple method called Post-hoc Correction (POC), which prompts an LMM to re-rank the expert model's top predictions using enriched prompts that include softmax confidence scores and few-shot visual examples. Across five challenging VSR benchmarks, POC outperforms prior art of FSL by +6.4% in accuracy without extra training, validation, or manual intervention. Importantly, POC generalizes to different pretrained backbones and LMMs, serving as a plug-and-play module to significantly enhance existing FSL methods.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundation Models in Biomedical Imaging: Turning Hype into Reality</title>
<link>https://arxiv.org/abs/2512.15808</link>
<guid>https://arxiv.org/abs/2512.15808</guid>
<content:encoded><![CDATA[
arXiv:2512.15808v1 Announce Type: cross 
Abstract: Foundation models (FMs) are driving a prominent shift in artificial intelligence across different domains, including biomedical imaging. These models are designed to move beyond narrow pattern recognition towards emulating sophisticated clinical reasoning, understanding complex spatial relationships, and integrating multimodal data with unprecedented flexibility. However, a critical gap exists between this potential and the current reality, where the clinical evaluation and deployment of FMs are hampered by significant challenges. Herein, we critically assess the current state-of-the-art, analyzing hype by examining the core capabilities and limitations of FMs in the biomedical domain. We also provide a taxonomy of reasoning, ranging from emulated sequential logic and spatial understanding to the integration of explicit symbolic knowledge, to evaluate whether these models exhibit genuine cognition or merely mimic surface-level patterns. We argue that a critical frontier lies beyond statistical correlation, in the pursuit of causal inference, which is essential for building robust models that understand cause and effect. Furthermore, we discuss the paramount issues in deployment stemming from trustworthiness, bias, and safety, dissecting the challenges of algorithmic bias, data bias and privacy, and model hallucinations. We also draw attention to the need for more inclusive, rigorous, and clinically relevant validation frameworks to ensure their safe and ethical application. We conclude that while the vision of autonomous AI-doctors remains distant, the immediate reality is the emergence of powerful technology and assistive tools that would benefit clinical practice. The future of FMs in biomedical imaging hinges not on scale alone, but on developing hybrid, causally aware, and verifiably safe systems that augment, rather than replace, human expertise.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioimageAIpub: a toolbox for AI-ready bioimaging data publishing</title>
<link>https://arxiv.org/abs/2512.15820</link>
<guid>https://arxiv.org/abs/2512.15820</guid>
<content:encoded><![CDATA[
arXiv:2512.15820v1 Announce Type: cross 
Abstract: Modern bioimage analysis approaches are data hungry, making it necessary for researchers to scavenge data beyond those collected within their (bio)imaging facilities. In addition to scale, bioimaging datasets must be accompanied with suitable, high-quality annotations and metadata. Although established data repositories such as the Image Data Resource (IDR) and BioImage Archive offer rich metadata, their contents typically cannot be directly consumed by image analysis tools without substantial data wrangling. Such a tedious assembly and conversion of (meta)data can account for a dedicated amount of time investment for researchers, hindering the development of more powerful analysis tools. Here, we introduce BioimageAIpub, a workflow that streamlines bioimaging data conversion, enabling a seamless upload to HuggingFace, a widely used platform for sharing machine learning datasets and models.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-like Working Memory from Artificial Intrinsic Plasticity Neurons</title>
<link>https://arxiv.org/abs/2512.15829</link>
<guid>https://arxiv.org/abs/2512.15829</guid>
<content:encoded><![CDATA[
arXiv:2512.15829v1 Announce Type: cross 
Abstract: Working memory enables the brain to integrate transient information for rapid decision-making. Artificial networks typically replicate this via recurrent or parallel architectures, yet incur high energy costs and noise sensitivity. Here we report IPNet, a hardware-software co-designed neuromorphic architecture realizing human-like working memory via neuronal intrinsic plasticity. Exploiting Joule-heating dynamics of Magnetic Tunnel Junctions (MTJs), IPNet physically emulates biological memory volatility. The memory behavior of the proposed architecture shows similar trends in n-back, free recall and memory interference tasks to that of reported human subjects. Implemented exclusively with MTJ neurons, the architecture with human-like working memory achieves 99.65% accuracy on 11-class DVS gesture datasets and maintains 99.48% on a novel 22-class time-reversed benchmark, outperforming RNN, LSTM, and 2+1D CNN baselines sharing identical backbones. For autonomous driving (DDD-20), IPNet reduces steering prediction error by 14.4% compared to ResNet-LSTM. Architecturally, we identify a 'Memory-at-the-Frontier' effect where performance is maximized at the sensing interface, validating a bio-plausible near-sensor processing paradigm. Crucially, all results rely on raw parameters from fabricated devices without optimization. Hardware-in-the-loop validation confirms the system's physical realizability. Separately, energy analysis reveals a reduction in memory power of 2,874x compared to LSTMs and 90,920x versus parallel 3D-CNNs. This capacitor-free design enables a compact ~1.5um2 footprint (28 nm CMOS): a >20-fold reduction over standard LIF neurons. Ultimately, we demonstrate that instantiating human-like working memory via intrinsic neuronal plasticity endows neural networks with the dual biological advantages of superior dynamic vision processing and minimal metabolic cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Video Planner Enables Generalizable Robot Control</title>
<link>https://arxiv.org/abs/2512.15840</link>
<guid>https://arxiv.org/abs/2512.15840</guid>
<content:encoded><![CDATA[
arXiv:2512.15840v1 Announce Type: cross 
Abstract: General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In search of truth: Evaluating concordance of AI-based anatomy segmentation models</title>
<link>https://arxiv.org/abs/2512.15921</link>
<guid>https://arxiv.org/abs/2512.15921</guid>
<content:encoded><![CDATA[
arXiv:2512.15921v1 Announce Type: cross 
Abstract: Purpose AI-based methods for anatomy segmentation can help automate characterization of large imaging datasets. The growing number of similar in functionality models raises the challenge of evaluating them on datasets that do not contain ground truth annotations. We introduce a practical framework to assist in this task. Approach We harmonize the segmentation results into a standard, interoperable representation, which enables consistent, terminology-based labeling of the structures. We extend 3D Slicer to streamline loading and comparison of these harmonized segmentations, and demonstrate how standard representation simplifies review of the results using interactive summary plots and browser-based visualization using OHIF Viewer. To demonstrate the utility of the approach we apply it to evaluating segmentation of 31 anatomical structures (lungs, vertebrae, ribs, and heart) by six open-source models - TotalSegmentator 1.5 and 2.6, Auto3DSeg, MOOSE, MultiTalent, and CADS - for a sample of Computed Tomography (CT) scans from the publicly available National Lung Screening Trial (NLST) dataset. Results We demonstrate the utility of the framework in enabling automating loading, structure-wise inspection and comparison across models. Preliminary results ascertain practical utility of the approach in allowing quick detection and review of problematic results. The comparison shows excellent agreement segmenting some (e.g., lung) but not all structures (e.g., some models produce invalid vertebrae or rib segmentations). Conclusions The resources developed are linked from https://imagingdatacommons.github.io/segmentation-comparison/ including segmentation harmonization scripts, summary plots, and visualization tools. This work assists in model evaluation in absence of ground truth, ultimately enabling informed model selection.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALVE: Sparse Autoencoder-Latent Vector Editing for Mechanistic Control of Neural Networks</title>
<link>https://arxiv.org/abs/2512.15938</link>
<guid>https://arxiv.org/abs/2512.15938</guid>
<content:encoded><![CDATA[
arXiv:2512.15938v1 Announce Type: cross 
Abstract: Deep neural networks achieve impressive performance but remain difficult to interpret and control. We present SALVE (Sparse Autoencoder-Latent Vector Editing), a unified "discover, validate, and control" framework that bridges mechanistic interpretability and model editing. Using an $\ell_1$-regularized autoencoder, we learn a sparse, model-native feature basis without supervision. We validate these features with Grad-FAM, a feature-level saliency mapping method that visually grounds latent features in input data. Leveraging the autoencoder's structure, we perform precise and permanent weight-space interventions, enabling continuous modulation of both class-defining and cross-class features. We further derive a critical suppression threshold, $\alpha_{crit}$, quantifying each class's reliance on its dominant feature, supporting fine-grained robustness diagnostics. Our approach is validated on both convolutional (ResNet-18) and transformer-based (ViT-B/16) models, demonstrating consistent, interpretable control over their behavior. This work contributes a principled methodology for turning feature discovery into actionable model edits, advancing the development of transparent and controllable AI systems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCR-VQGAN: A Scalable and Cost-Effective Tau PET Synthesis Approach for Alzheimer's Disease Imaging</title>
<link>https://arxiv.org/abs/2512.15947</link>
<guid>https://arxiv.org/abs/2512.15947</guid>
<content:encoded><![CDATA[
arXiv:2512.15947v1 Announce Type: cross 
Abstract: Tau positron emission tomography (PET) is a critical diagnostic modality for Alzheimer's disease (AD) because it visualizes and quantifies neurofibrillary tangles, a hallmark of AD pathology. However, its widespread clinical adoption is hindered by significant challenges, such as radiation exposure, limited availability, high clinical workload, and substantial financial costs. To overcome these limitations, we propose Multi-scale CBAM Residual Vector Quantized Generative Adversarial Network (MCR-VQGAN) to synthesize high-fidelity tau PET images from structural T1-weighted MRI scans. MCR-VQGAN improves standard VQGAN by integrating three key architectural enhancements: multi-scale convolutions, ResNet blocks, and Convolutional Block Attention Modules (CBAM). Using 222 paired structural T1-weighted MRI and tau PET scans from Alzheimer's Disease Neuroimaging Initiative (ADNI), we trained and compared MCR-VQGAN with cGAN, WGAN-GP, CycleGAN, and VQGAN. Our proposed model achieved superior image synthesis performance across all metrics: MSE of 0.0056 +/- 0.0061, PSNR of 24.39 +/- 4.49 dB, and SSIM of 0.9000 +/- 0.0453. To assess the clinical utility of the synthetic images, we trained and evaluated a CNN-based AD classifier. The classifier achieved comparable accuracy when tested on real (63.64%) and synthetic (65.91%) images. This result indicates that our synthesis process successfully preserves diagnostically relevant features without significant information loss. Our results demonstrate that MCR-VQGAN can offer a reliable and scalable surrogate for conventional tau PET imaging, potentially improving the accessibility and scalability of tau imaging biomarkers for AD research and clinical workflows.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Enabled Graph Analysis of Particulate Composites: Application to Solid-state Battery Cathodes</title>
<link>https://arxiv.org/abs/2512.16085</link>
<guid>https://arxiv.org/abs/2512.16085</guid>
<content:encoded><![CDATA[
arXiv:2512.16085v1 Announce Type: cross 
Abstract: Particulate composites underpin many solid-state chemical and electrochemical systems, where microstructural features such as multiphase boundaries and inter-particle connections strongly influence system performance. Advances in X-ray microscopy enable capturing large-scale, multimodal images of these complex microstructures with an unprecedentedly high throughput. However, harnessing these datasets to discover new physical insights and guide microstructure optimization remains a major challenge. Here, we develop a machine learning (ML) enabled framework that enables automated transformation of experimental multimodal X-ray images of multiphase particulate composites into scalable, topology-aware graphs for extracting physical insights and establishing local microstructure-property relationships at both the particle and network level. Using the multiphase particulate cathode of solid-state lithium batteries as an example, our ML-enabled graph analysis corroborates the critical role of triple phase junctions and concurrent ion/electron conduction channels in realizing desirable local electrochemical activity. Our work establishes graph-based microstructure representation as a powerful paradigm for bridging multimodal experimental imaging and functional understanding, and facilitating microstructure-aware data-driven materials design in a broad range of particulate composites.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tri-Dynamic Preprocessing Framework for UGC Video Compression</title>
<link>https://arxiv.org/abs/2512.16101</link>
<guid>https://arxiv.org/abs/2512.16101</guid>
<content:encoded><![CDATA[
arXiv:2512.16101v1 Announce Type: cross 
Abstract: In recent years, user generated content (UGC) has become the dominant force in internet traffic. However, UGC videos exhibit a higher degree of variability and diverse characteristics compared to traditional encoding test videos. This variance challenges the effectiveness of data-driven machine learning algorithms for optimizing encoding in the broader context of UGC scenarios. To address this issue, we propose a Tri-Dynamic Preprocessing framework for UGC. Firstly, we employ an adaptive factor to regulate preprocessing intensity. Secondly, an adaptive quantization level is employed to fine-tune the codec simulator. Thirdly, we utilize an adaptive lambda tradeoff to adjust the rate-distortion loss function. Experimental results on large-scale test sets demonstrate that our method attains exceptional performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Denoising Defense against Adversarial Attacks on Object Detection</title>
<link>https://arxiv.org/abs/2512.16123</link>
<guid>https://arxiv.org/abs/2512.16123</guid>
<content:encoded><![CDATA[
arXiv:2512.16123v1 Announce Type: cross 
Abstract: Deep learning-based object detection models play a critical role in real-world applications such as autonomous driving and security surveillance systems, yet they remain vulnerable to adversarial examples. In this work, we propose an autoencoder-based denoising defense to recover object detection performance degraded by adversarial perturbations. We conduct adversarial attacks using Perlin noise on vehicle-related images from the COCO dataset, apply a single-layer convolutional autoencoder to remove the perturbations, and evaluate detection performance using YOLOv5. Our experiments demonstrate that adversarial attacks reduce bbox mAP from 0.2890 to 0.1640, representing a 43.3% performance degradation. After applying the proposed autoencoder defense, bbox mAP improves to 0.1700 (3.7% recovery) and bbox mAP@50 increases from 0.2780 to 0.3080 (10.8% improvement). These results indicate that autoencoder-based denoising can provide partial defense against adversarial attacks without requiring model retraining.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-View Inference Attack: Machine Unlearning Amplifies Privacy Exposure</title>
<link>https://arxiv.org/abs/2512.16126</link>
<guid>https://arxiv.org/abs/2512.16126</guid>
<content:encoded><![CDATA[
arXiv:2512.16126v1 Announce Type: cross 
Abstract: Machine unlearning is a newly popularized technique for removing specific training data from a trained model, enabling it to comply with data deletion requests. While it protects the rights of users requesting unlearning, it also introduces new privacy risks. Prior works have primarily focused on the privacy of data that has been unlearned, while the risks to retained data remain largely unexplored. To address this gap, we focus on the privacy risks of retained data and, for the first time, reveal the vulnerabilities introduced by machine unlearning under the dual-view setting, where an adversary can query both the original and the unlearned models. From an information-theoretic perspective, we introduce the concept of {privacy knowledge gain} and demonstrate that the dual-view setting allows adversaries to obtain more information than querying either model alone, thereby amplifying privacy leakage. To effectively demonstrate this threat, we propose DVIA, a Dual-View Inference Attack, which extracts membership information on retained data using black-box queries to both models. DVIA eliminates the need to train an attack model and employs a lightweight likelihood ratio inference module for efficient inference. Experiments across different datasets and model architectures validate the effectiveness of DVIA and highlight the privacy risks inherent in the dual-view setting.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Guess, Escalate: Towards Explainable Uncertainty-Calibrated AI Forensic Agents</title>
<link>https://arxiv.org/abs/2512.16614</link>
<guid>https://arxiv.org/abs/2512.16614</guid>
<content:encoded><![CDATA[
arXiv:2512.16614v1 Announce Type: cross 
Abstract: AI is reshaping the landscape of multimedia forensics. We propose AI forensic agents: reliable orchestrators that select and combine forensic detectors, identify provenance and context, and provide uncertainty-aware assessments. We highlight pitfalls in current solutions and introduce a unified framework to improve the authenticity verification process.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.16724</link>
<guid>https://arxiv.org/abs/2512.16724</guid>
<content:encoded><![CDATA[
arXiv:2512.16724v1 Announce Type: cross 
Abstract: When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies</title>
<link>https://arxiv.org/abs/2512.16876</link>
<guid>https://arxiv.org/abs/2512.16876</guid>
<content:encoded><![CDATA[
arXiv:2512.16876v1 Announce Type: cross 
Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sceniris: A Fast Procedural Scene Generation Framework</title>
<link>https://arxiv.org/abs/2512.16896</link>
<guid>https://arxiv.org/abs/2512.16896</guid>
<content:encoded><![CDATA[
arXiv:2512.16896v1 Announce Type: cross 
Abstract: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</title>
<link>https://arxiv.org/abs/2512.16899</link>
<guid>https://arxiv.org/abs/2512.16899</guid>
<content:encoded><![CDATA[
arXiv:2512.16899v1 Announce Type: cross 
Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Resolution Action Recognition for Tiny Actions Challenge</title>
<link>https://arxiv.org/abs/2209.14711</link>
<guid>https://arxiv.org/abs/2209.14711</guid>
<content:encoded><![CDATA[
arXiv:2209.14711v2 Announce Type: replace 
Abstract: Tiny Actions Challenge focuses on understanding human activities in real-world surveillance. Basically, there are two main difficulties for activity recognition in this scenario. First, human activities are often recorded at a distance, and appear in a small resolution without much discriminative clue. Second, these activities are naturally distributed in a long-tailed way. It is hard to alleviate data bias for such heavy category imbalance. To tackle these problems, we propose a comprehensive recognition solution in this paper. First, we train video backbones with data balance, in order to alleviate overfitting in the challenge benchmark. Second, we design a dual-resolution distillation framework, which can effectively guide low-resolution action recognition by super-resolution knowledge. Finally, we apply model en-semble with post-processing, which can further boost per-formance on the long-tailed categories. Our solution ranks Top-1 on the leaderboard.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BoostDream: Efficient Refining for High-Quality Text-to-3D Generation from Multi-View Diffusion</title>
<link>https://arxiv.org/abs/2401.16764</link>
<guid>https://arxiv.org/abs/2401.16764</guid>
<content:encoded><![CDATA[
arXiv:2401.16764v4 Announce Type: replace 
Abstract: Witnessing the evolution of text-to-image diffusion models, significant strides have been made in text-to-3D generation. Currently, two primary paradigms dominate the field of text-to-3D: the feed-forward generation solutions, capable of swiftly producing 3D assets but often yielding coarse results, and the Score Distillation Sampling (SDS) based solutions, known for generating high-fidelity 3D assets albeit at a slower pace. The synergistic integration of these methods holds substantial promise for advancing 3D generation techniques. In this paper, we present BoostDream, a highly efficient plug-and-play 3D refining method designed to transform coarse 3D assets into high-quality. The BoostDream framework comprises three distinct processes: (1) We introduce 3D model distillation that fits differentiable representations from the 3D assets obtained through feed-forward generation. (2) A novel multi-view SDS loss is designed, which utilizes a multi-view aware 2D diffusion model to refine the 3D assets. (3) We propose to use prompt and multi-view consistent normal maps as guidance in refinement.Our extensive experiment is conducted on different differentiable 3D representations, revealing that BoostDream excels in generating high-quality 3D assets rapidly, overcoming the Janus problem compared to conventional SDS-based methods. This breakthrough signifies a substantial advancement in both the efficiency and quality of 3D generation processes.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition</title>
<link>https://arxiv.org/abs/2402.18951</link>
<guid>https://arxiv.org/abs/2402.18951</guid>
<content:encoded><![CDATA[
arXiv:2402.18951v2 Announce Type: replace 
Abstract: Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, foundation models with rich knowledge have recently shown their generalization power. However, how to apply such knowledge has not been fully explored for open-world video recognition. To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge. Second, we generate rich linguistic semantics as external textual knowledge in Chat stage. Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks. We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference</title>
<link>https://arxiv.org/abs/2405.14700</link>
<guid>https://arxiv.org/abs/2405.14700</guid>
<content:encoded><![CDATA[
arXiv:2405.14700v3 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a popular solution for adapting pre-trained Vision Transformer (ViT) models to downstream applications by updating only a small subset of parameters. While current PEFT methods have achieved fine-tuning efficiency, they overlook the efficiency of computation and GPU memory during inference, falling short of practical requirements. To address this limitation, we propose Sparse-Tuning, an efficient and effective framework that leverages popular token sparsification (TS) techniques to reduce information redundancy in images and videos, thereby significantly improving computational and memory efficiency. However, TS often compromises performance due to inevitable information loss. To address this limitation, we further introduce Dense Adapters (DA) to compensate for the information losses incurred by token sparsification. DA integrates comprehensive token information from shallow layers into the retained tokens of deeper layers, ensuring minimal performance degradation. Through the integration of TS techniques and DA, Sparse-Tuning achieves a significant reduction in computation and memory overhead while maintaining performance. Empirical results on VTAB-1K, three image datasets, and two video datasets show that Sparse-Tuning reduces GFLOPs to 66\% of the original ViT-B while achieving state-of-the-art performance compared to full fine-tuning and other PEFT baselines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMRel: Benchmarking Relation Understanding in Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2406.09121</link>
<guid>https://arxiv.org/abs/2406.09121</guid>
<content:encoded><![CDATA[
arXiv:2406.09121v3 Announce Type: replace 
Abstract: Though Multi-modal Large Language Models (MLLMs) have recently achieved significant progress, they often struggle to understand diverse and complicated inter-object relations. Specifically, the lack of large-scale and high-quality relation data has greatly hindered the progress of MLLMs in various vision-language perception tasks. We attempt to address this challenge by contributing the Multi-Modal Relation Understanding benchmark (MMRel), which features large-scale, high-quality, and diverse data on inter-object relations. MMRel has three distinctive attributes: (i) it contains 22,500 question-answer pairs spanning three distinct domains and around 400 relations, ensuring both scale and diversity; (ii) it provides manually verified, high-quality labels to ensure exceptional annotation accuracy; and (iii) it includes adversarial cases with highly unusual relations, offering a challenging setting for evaluating relation hallucination. These features make MMRel ideal for evaluating MLLMs on relation understanding, as well as for fine-tuning MLLMs to enhance relation comprehension capability. Extensive experiments on 28 MLLMs demonstrate the effectiveness of MMRel in both evaluating and enhancing MLLMs' relation understanding, and the accompanying analyses provide insights for future research. The benchmark has been made publicly available at: https://niejiahao1998.github.io/MMRel
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks</title>
<link>https://arxiv.org/abs/2407.20836</link>
<guid>https://arxiv.org/abs/2407.20836</guid>
<content:encoded><![CDATA[
arXiv:2407.20836v5 Announce Type: replace 
Abstract: Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. To this end, we propose a new method to attack AIGI detectors. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous AIGI detectors, e.g., transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as Frequency-based Post-train Bayesian Attack, or FPBA. Through FPBA, we demonstrate that adversarial attacks pose a real threat to AIGI detectors. FPBA can deliver successful black-box attacks across various detectors, generators, defense methods, and even evade cross-generator and compressed image detection, which are crucial real-world detection scenarios. Our code is available at https://github.com/onotoa/fpba.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildFit: Autonomous In-situ Model Adaptation for Resource-Constrained IoT Systems</title>
<link>https://arxiv.org/abs/2409.07796</link>
<guid>https://arxiv.org/abs/2409.07796</guid>
<content:encoded><![CDATA[
arXiv:2409.07796v4 Announce Type: replace 
Abstract: Resource-constrained IoT devices increasingly rely on deep learning models, however, these models experience significant accuracy drops due to domain shifts when encountering variations in lighting, weather, and seasonal conditions. While cloud-based retraining can address this issue, many IoT deployments operate with limited connectivity and energy constraints, making traditional fine-tuning approaches impractical. We explore this challenge through the lens of wildlife ecology, where camera traps must maintain accurate species classification across changing seasons, weather, and habitats without reliable connectivity. We introduce WildFit, an autonomous in-situ adaptation framework that leverages the key insight that background scenes change more frequently than the visual characteristics of monitored species. WildFit combines background-aware synthesis to generate training samples on-device with drift-aware fine-tuning that triggers model updates only when necessary to conserve resources. Our background-aware synthesis surpasses efficient baselines by 7.3% and diffusion models by 3.0% while being orders of magnitude faster, our drift-aware fine-tuning achieves Pareto optimality with 50% fewer updates and 1.5% higher accuracy, and the end-to-end system outperforms domain adaptation approaches by 20-35% while consuming only 11.2 Wh over 37 days-enabling battery-powered deployment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mat\'ern Kernels for Tunable Implicit Surface Reconstruction</title>
<link>https://arxiv.org/abs/2409.15466</link>
<guid>https://arxiv.org/abs/2409.15466</guid>
<content:encoded><![CDATA[
arXiv:2409.15466v3 Announce Type: replace 
Abstract: We propose to use the family of Mat\'ern kernels for implicit surface reconstruction, building upon the recent success of kernel methods for 3D reconstruction of oriented point clouds. As we show from a theoretical and practical perspective, Mat\'ern kernels have some appealing properties which make them particularly well suited for surface reconstruction -- outperforming state-of-the-art methods based on the arc-cosine kernel while being significantly easier to implement, faster to compute, and scalable. Being stationary, we demonstrate that Mat\'ern kernels allow for tunable surface reconstruction in the same way as Fourier feature mappings help coordinate-based MLPs overcome spectral bias. Moreover, we theoretically analyze Mat\'ern kernels' connection to SIREN networks as well as their relation to previously employed arc-cosine kernels. Finally, based on recently introduced Neural Kernel Fields, we present data-dependent Mat\'ern kernels and conclude that especially the Laplace kernel (being part of the Mat\'ern family) is extremely competitive, performing almost on par with state-of-the-art methods in the noise-free case while having a more than five times shorter training time.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Engineering Diagrams to Graphs: Digitizing P&amp;IDs with Transformers</title>
<link>https://arxiv.org/abs/2411.13929</link>
<guid>https://arxiv.org/abs/2411.13929</guid>
<content:encoded><![CDATA[
arXiv:2411.13929v2 Announce Type: replace 
Abstract: Digitizing engineering diagrams like Piping and Instrumentation Diagrams (P&amp;IDs) plays a vital role in maintainability and operational efficiency of process and hydraulic systems. Previous methods typically decompose the task into separate steps such as symbol detection and line detection, which can limit their ability to capture the structure in these diagrams. In this work, a transformer-based approach leveraging the Relationformer that addresses this limitation by jointly extracting symbols and their interconnections from P&amp;IDs is introduced. To evaluate our approach and compare it to a modular digitization approach, we present the first publicly accessible benchmark dataset for P&amp;ID digitization, annotated with graph-level ground truth. Experimental results on real-world diagrams show that our method significantly outperforms the modular baseline, achieving over 25% improvement in edge detection accuracy. This research contributes a reproducible evaluation framework and demonstrates the effectiveness of transformer models for structural understanding of complex engineering diagrams. The dataset is available under https://zenodo.org/records/14803338.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniDepthV2: Universal Monocular Metric Depth Estimation Made Simpler</title>
<link>https://arxiv.org/abs/2502.20110</link>
<guid>https://arxiv.org/abs/2502.20110</guid>
<content:encoded><![CDATA[
arXiv:2502.20110v2 Announce Type: replace 
Abstract: Accurate monocular metric depth estimation (MMDE) is crucial to solving downstream tasks in 3D perception and modeling. However, the remarkable accuracy of recent MMDE methods is confined to their training domains. These methods fail to generalize to unseen domains even in the presence of moderate domain gaps, which hinders their practical applicability. We propose a new model, UniDepthV2, capable of reconstructing metric 3D scenes from solely single images across domains. Departing from the existing MMDE paradigm, UniDepthV2 directly predicts metric 3D points from the input image at inference time without any additional information, striving for a universal and flexible MMDE solution. In particular, UniDepthV2 implements a self-promptable camera module predicting a dense camera representation to condition depth features. Our model exploits a pseudo-spherical output representation, which disentangles the camera and depth representations. In addition, we propose a geometric invariance loss that promotes the invariance of camera-prompted depth features. UniDepthV2 improves its predecessor UniDepth model via a new edge-guided loss which enhances the localization and sharpness of edges in the metric depth outputs, a revisited, simplified and more efficient architectural design, and an additional uncertainty-level output which enables downstream tasks requiring confidence. Thorough evaluations on ten depth datasets in a zero-shot regime consistently demonstrate the superior performance and generalization of UniDepthV2. Code and models are available at https://github.com/lpiccinelli-eth/UniDepth
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radar-Guided Polynomial Fitting for Metric Depth Estimation</title>
<link>https://arxiv.org/abs/2503.17182</link>
<guid>https://arxiv.org/abs/2503.17182</guid>
<content:encoded><![CDATA[
arXiv:2503.17182v3 Announce Type: replace 
Abstract: We propose POLAR, a novel radar-guided depth estimation method that introduces polynomial fitting to efficiently transform scaleless depth predictions from pretrained monocular depth estimation (MDE) models into metric depth maps. Unlike existing approaches that rely on complex architectures or expensive sensors, our method is grounded in a fundamental insight: although MDE models often infer reasonable local depth structure within each object or local region, they may misalign these regions relative to one another, making a linear scale and shift (affine) transformation insufficient given three or more of these regions. To address this limitation, we use polynomial coefficients predicted from cheap, ubiquitous radar data to adaptively adjust predictions non-uniformly across depth ranges. In this way, POLAR generalizes beyond affine transformations and is able to correct such misalignments by introducing inflection points. Importantly, our polynomial fitting framework preserves structural consistency through a novel training objective that enforces local monotonicity via first-derivative regularization. POLAR achieves state-of-the-art performance across three datasets, outperforming existing methods by an average of 24.9% in MAE and 33.2% in RMSE, while also achieving state-of-the-art efficiency in terms of latency and computational cost.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Core-Set Selection for Data-efficient Land Cover Segmentation</title>
<link>https://arxiv.org/abs/2505.01225</link>
<guid>https://arxiv.org/abs/2505.01225</guid>
<content:encoded><![CDATA[
arXiv:2505.01225v3 Announce Type: replace 
Abstract: The increasing accessibility of remotely sensed data and their potential to support large-scale decision-making have driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models rely on large datasets. However, the common assumption that larger training datasets lead to better performance tends to overlook issues related to data redundancy, noise, and the computational cost of processing massive datasets. Effective solutions must therefore consider not only the quantity but also the quality of data. Towards this, in this paper, we introduce six basic core-set selection approaches -- that rely on imagery only, labels only, or a combination of both -- and investigate whether they can identify high-quality subsets of data capable of maintaining -- or even surpassing -- the performance achieved when using full datasets for remote sensing semantic segmentation. We benchmark such approaches against two traditional baselines on three widely used land-cover classification datasets (DFC2022, Vaihingen, and Potsdam) using two different architectures (SegFormer and U-Net), thus establishing a general baseline for future works. Our experiments show that all proposed methods consistently outperform the baselines across multiple subset sizes, with some approaches even selecting core sets that surpass training on all available data. Notably, on DFC2022, a selected subset comprising only 25% of the training data yields slightly higher SegFormer performance than training with the entire dataset. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoAPT: Mixture of Adversarial Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.17509</link>
<guid>https://arxiv.org/abs/2505.17509</guid>
<content:encoded><![CDATA[
arXiv:2505.17509v2 Announce Type: replace 
Abstract: Large pre-trained Vision Language Models (VLMs) demonstrate excellent generalization capabilities but remain highly susceptible to adversarial examples, posing potential security risks. To improve the robustness of VLMs against adversarial examples, adversarial prompt tuning methods are proposed to align the text feature with the adversarial image feature without changing model parameters. However, when facing various adversarial attacks, a single learnable text prompt has insufficient generalization to align well with all adversarial image features, which ultimately results in overfitting. To address the above challenge, in this paper, we empirically find that increasing the number of learned prompts yields greater robustness improvements than simply extending the length of a single prompt. Building on this observation, we propose an adversarial tuning method named \textbf{Mixture of Adversarial Prompt Tuning (MoAPT)} to enhance the generalization against various adversarial attacks for VLMs. MoAPT aims to learn mixture text prompts to obtain more robust text features. To further enhance the adaptability, we propose a conditional weight router based on the adversarial images to predict the mixture weights of multiple learned prompts, which helps obtain sample-specific mixture text features aligning with different adversarial image features. Extensive experiments across 11 datasets under different settings show that our method can achieve better adversarial robustness than state-of-the-art approaches.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAEER: Visual Attention-Inspired Emotion Elicitation Reasoning</title>
<link>https://arxiv.org/abs/2505.24342</link>
<guid>https://arxiv.org/abs/2505.24342</guid>
<content:encoded><![CDATA[
arXiv:2505.24342v2 Announce Type: replace 
Abstract: Images shared online strongly influence emotions and public well-being. Understanding the emotions an image elicits is therefore vital for fostering healthier and more sustainable digital communities, especially during public crises. We study Visual Emotion Elicitation (VEE), predicting the set of emotions that an image evokes in viewers. We introduce VAEER, an interpretable multi-label VEE framework that combines attention-inspired cue extraction with knowledge-grounded reasoning. VAEER isolates salient visual foci and contextual signals, aligns them with structured affective knowledge, and performs per-emotion inference to yield transparent, emotion-specific rationales. Across three heterogeneous benchmarks, including social imagery and disaster-related photos, VAEER achieves state-of-the-art results with up to 19% per-emotion improvements and a 12.3% average gain over strong CNN and VLM baselines. Our findings highlight interpretable multi-label emotion elicitation as a scalable foundation for responsible visual media analysis and emotionally sustainable online ecosystems.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViStoryBench: Comprehensive Benchmark Suite for Story Visualization</title>
<link>https://arxiv.org/abs/2505.24862</link>
<guid>https://arxiv.org/abs/2505.24862</guid>
<content:encoded><![CDATA[
arXiv:2505.24862v4 Announce Type: replace 
Abstract: Story visualization aims to generate coherent image sequences that faithfully depict a narrative and align with character references. Despite progress in generative models, existing benchmarks are narrow in scope, often limited to short prompts, lacking character references, or single-image cases, and fail to capture real-world storytelling complexity. This hinders a nuanced understanding of model capabilities and limitations. We present \textbf{ViStoryBench}, a comprehensive benchmark designed to evaluate story visualization models across diverse narrative structures, visual styles, and character settings. The benchmark features richly annotated multi-shot scripts derived from curated stories spanning literature, film, and folklore. Large language models assist in story summarization and script generation, with all outputs human-verified to ensure coherence and fidelity. Character references are carefully curated to maintain intra-story consistency across varying artistic styles. To enable thorough evaluation, ViStoryBench introduces a set of automated metrics that assess character consistency, style similarity, prompt alignment, aesthetic quality, and generation artifacts such as copy-paste behavior. These metrics are validated through human studies, and used to benchmark a broad range of open-source and commercial models. ViStoryBench offers a multi-dimensional evaluation suite that facilitates systematic analysis and fosters future progress in visual storytelling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model</title>
<link>https://arxiv.org/abs/2506.09695</link>
<guid>https://arxiv.org/abs/2506.09695</guid>
<content:encoded><![CDATA[
arXiv:2506.09695v3 Announce Type: replace 
Abstract: Early diagnosis of Alzheimer's Disease (AD), particularly at the mild cognitive impairment stage, is essential for timely intervention. However, this process faces significant barriers, including reliance on subjective assessments and the high cost of advanced imaging techniques. While deep learning offers automated solutions to improve diagnostic accuracy, its widespread adoption remains constrained due to high energy requirements and computational demands, particularly in resource-limited settings. Spiking neural networks (SNNs) provide a promising alternative, as their brain-inspired design is well-suited to model the sparse and event-driven patterns characteristic of neural degeneration in AD. These networks offer the potential for developing interpretable, energy-efficient diagnostic tools. Despite their advantages, existing SNNs often suffer from limited expressiveness and challenges in stable training, which reduce their effectiveness in handling complex medical tasks. To address these shortcomings, we introduce FasterSNN, a hybrid neural architecture that combines biologically inspired Leaky Integrate-and-Fire (LIF) neurons with region-adaptive convolution and multi-scale spiking attention mechanisms. This approach facilitates efficient, sparse processing of 3D MRI data while maintaining high diagnostic accuracy. Experimental results on benchmark datasets reveal that FasterSNN delivers competitive performance with significantly enhanced efficiency and training stability, highlighting its potential for practical application in AD screening. Our source code is available at https://github.com/wuchangw/FasterSNN.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scene-aware SAR ship detection guided by unsupervised sea-land segmentation</title>
<link>https://arxiv.org/abs/2506.12775</link>
<guid>https://arxiv.org/abs/2506.12775</guid>
<content:encoded><![CDATA[
arXiv:2506.12775v2 Announce Type: replace 
Abstract: DL based Synthetic Aperture Radar (SAR) ship detection has tremendous advantages in numerous areas. However, it still faces some problems, such as the lack of prior knowledge, which seriously affects detection accuracy. In order to solve this problem, we propose a scene-aware SAR ship detection method based on unsupervised sea-land segmentation. This method follows a classical two-stage framework and is enhanced by two models: the unsupervised land and sea segmentation module (ULSM) and the land attention suppression module (LASM). ULSM and LASM can adaptively guide the network to reduce attention on land according to the type of scenes (inshore scene and offshore scene) and add prior knowledge (sea land segmentation information) to the network, thereby reducing the network's attention to land directly and enhancing offshore detection performance relatively. This increases the accuracy of ship detection and enhances the interpretability of the model. Specifically, in consideration of the lack of land sea segmentation labels in existing deep learning-based SAR ship detection datasets, ULSM uses an unsupervised approach to classify the input data scene into inshore and offshore types and performs sea-land segmentation for inshore scenes. LASM uses the sea-land segmentation information as prior knowledge to reduce the network's attention to land. We conducted our experiments using the publicly available SSDD dataset, which demonstrated the effectiveness of our network.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Deep Learning Framework for Brain Stroke Diagnosis Using Computed Tomography Images</title>
<link>https://arxiv.org/abs/2507.03558</link>
<guid>https://arxiv.org/abs/2507.03558</guid>
<content:encoded><![CDATA[
arXiv:2507.03558v3 Announce Type: replace 
Abstract: Brain stroke is a leading cause of mortality and long-term disability worldwide, underscoring the need for precise and rapid prediction techniques. Computed Tomography (CT) scan is considered one of the most effective methods for diagnosing brain strokes. Most stroke classification techniques use a single slice-level prediction mechanism, requiring radiologists to manually select the most critical CT slice from the original CT volume. Although clinical evaluations are often used in traditional diagnostic procedures, machine learning (ML) has opened up new avenues for improving stroke diagnosis. To supplement traditional diagnostic techniques, this study investigates machine learning models for early brain stroke prediction using CT scan images. This research proposes a novel machine learning approach to brain stroke detection, focusing on optimizing classification performance with pre-trained deep learning models and advanced optimization strategies. Pre-trained models, including DenseNet201, InceptionV3, MobileNetV2, ResNet50, and Xception, are used for feature extraction. Feature engineering techniques, including BFO, PCA, and LDA, further enhance model performance. These features are then classified using machine learning algorithms, including SVC, RF, XGB, DT, LR, KNN, and GNB. Our experiments demonstrate that the combination of MobileNetV2, LDA, and SVC achieved the highest classification accuracy of 97.93%, significantly outperforming other model-optimizer-classifier combinations. The results underline the effectiveness of integrating lightweight pre-trained models with robust optimization and classification techniques for brain stroke diagnosis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D-FCGS: Feedforward Compression of Dynamic Gaussian Splatting for Free-Viewpoint Videos</title>
<link>https://arxiv.org/abs/2507.05859</link>
<guid>https://arxiv.org/abs/2507.05859</guid>
<content:encoded><![CDATA[
arXiv:2507.05859v3 Announce Type: replace 
Abstract: Free-Viewpoint Video (FVV) enables immersive 3D experiences, but efficient compression of dynamic 3D representation remains a major challenge. Existing dynamic 3D Gaussian Splatting methods couple reconstruction with optimization-dependent compression and customized motion formats, limiting generalization and standardization. To address this, we propose D-FCGS, a novel Feedforward Compression framework for Dynamic Gaussian Splatting. Key innovations include: (1) a standardized Group-of-Frames (GoF) structure with I-P coding, leveraging sparse control points to extract inter-frame motion tensors; (2) a dual prior-aware entropy model that fuses hyperprior and spatial-temporal priors for accurate rate estimation; (3) a control-point-guided motion compensation mechanism and refinement network to enhance view-consistent fidelity. Trained on Gaussian frames derived from multi-view videos, D-FCGS generalizes across diverse scenes in a zero-shot fashion. Experiments show that it matches the rate-distortion performance of optimization-based methods, achieving over 40 times compression compared to the baseline while preserving visual quality across viewpoints. This work advances feedforward compression of dynamic 3DGS, facilitating scalable FVV transmission and storage for immersive applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis</title>
<link>https://arxiv.org/abs/2507.10171</link>
<guid>https://arxiv.org/abs/2507.10171</guid>
<content:encoded><![CDATA[
arXiv:2507.10171v3 Announce Type: replace 
Abstract: Concrete workability is essential for construction quality, with the slump test being the most widely used on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and highly operator-dependent, making it unsuitable for continuous or real-time monitoring during placement. To address these limitations, we present SlumpGuard, an AI-powered vision system that analyzes the natural discharge flow from a mixer-truck chute using a single fixed camera. The system performs automatic chute detection, pouring-event identification, and video-based slump classification, enabling quality monitoring without sensors, hardware installation, or manual intervention. We introduce the system design, construct a site-replicated dataset of over 6,000 video clips, and report extensive evaluations demonstrating reliable chute localization, accurate pouring detection, and robust slump prediction under diverse field conditions. An expert study further reveals significant disagreement in human visual estimates, highlighting the need for automated assessment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Building Heritage Assessment Using Street-Level Imagery</title>
<link>https://arxiv.org/abs/2508.11486</link>
<guid>https://arxiv.org/abs/2508.11486</guid>
<content:encoded><![CDATA[
arXiv:2508.11486v3 Announce Type: replace 
Abstract: Registration of heritage values in buildings is important to safeguard heritage values that can be lost in renovation and energy efficiency projects. However, registering heritage values is a cumbersome process. Novel artificial intelligence tools may improve efficiency in identifying heritage values in buildings compared to costly and time-consuming traditional inventories. In this study, OpenAI's large language model GPT was used to detect various aspects of cultural heritage value in facade images. Using GPT derived data and building register data, machine learning models were trained to classify multi-family and non-residential buildings in Stockholm, Sweden. Validation against a heritage expert-created inventory shows a macro F1-score of 0.71 using a combination of register data and features retrieved from GPT, and a score of 0.60 using only GPT-derived data. The methods presented can contribute to higher-quality datasets and support decision making.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation</title>
<link>https://arxiv.org/abs/2508.15216</link>
<guid>https://arxiv.org/abs/2508.15216</guid>
<content:encoded><![CDATA[
arXiv:2508.15216v2 Announce Type: replace 
Abstract: Accident prediction and timely warnings play a key role in improving road safety by reducing the risk of injury to road users and minimizing property damage. Advanced Driver Assistance Systems (ADAS) are designed to support human drivers and are especially useful when they can anticipate potential accidents before they happen. While many existing systems depend on a range of sensors such as LiDAR, radar, and GPS, relying solely on dash-cam video input presents a more challenging but a more cost-effective and easily deployable solution. In this work, we incorporate better spatio-temporal features and aggregate them through a recurrent network to improve upon state-of-the-art graph neural networks for predicting accidents from dash-cam videos. Experiments using three publicly available datasets show that our proposed STAGNet model achieves higher average precision and mean time-to-collision values than previous methods, both when cross-validated on a given dataset and when trained and tested on different datasets.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpatialVID: A Large-Scale Video Dataset with Spatial Annotations</title>
<link>https://arxiv.org/abs/2509.09676</link>
<guid>https://arxiv.org/abs/2509.09676</guid>
<content:encoded><![CDATA[
arXiv:2509.09676v2 Announce Type: replace 
Abstract: Significant progress has been made in spatial intelligence, spanning both spatial reconstruction and world exploration. However, the scalability and real-world fidelity of current models remain severely constrained by the scarcity of large-scale, high-quality training data. While several datasets provide camera pose information, they are typically limited in scale, diversity, and annotation richness, particularly for real-world dynamic scenes with ground-truth camera motion. To this end, we collect SpatialVID, a dataset consists of a large corpus of in-the-wild videos with diverse scenes, camera movements and dense 3D annotations such as per-frame camera poses, depth, and motion instructions. Specifically, we collect more than 21,000 hours of raw videos, and process them into 2.7 million clips through a hierarchical filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent annotation pipeline enriches these clips with detailed spatial and semantic information, including camera poses, depth maps, dynamic masks, structured captions, and serialized motion instructions. Analysis of SpatialVID's data statistics reveals a richness and diversity that directly fosters improved model generalization and performance, establishing it as a key asset for the video and 3D vision research community.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Segmentation of Polyps and Visual Explainability Analysis</title>
<link>https://arxiv.org/abs/2509.18159</link>
<guid>https://arxiv.org/abs/2509.18159</guid>
<content:encoded><![CDATA[
arXiv:2509.18159v5 Announce Type: replace 
Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as critical precursors according to the World Health Organization (WHO). Early and accurate segmentation of polyps during colonoscopy is essential for reducing CRC progression, yet manual delineation is labor-intensive and prone to observer variability. Deep learning methods have demonstrated strong potential for automated polyp analysis, but their limited interpretability remains a barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an explainable deep learning framework that integrates a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. To ensure rigorous benchmarking, the model was trained and evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results show a mean Dice coefficient of 0.8902 +/- 0.0125, a mean Intersection-over-Union (IoU) of 0.8023, and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.9722. Advanced quantitative analysis using an optimal threshold yielded a Sensitivity of 0.9058 and Precision of 0.9083. Additionally, Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, offering insight into the model's decision-making process. This study demonstrates that integrating segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.22737</link>
<guid>https://arxiv.org/abs/2509.22737</guid>
<content:encoded><![CDATA[
arXiv:2509.22737v2 Announce Type: replace 
Abstract: We introduce CompareBench, a benchmark for evaluating visual comparison reasoning in vision-language models (VLMs), a fundamental yet understudied skill. CompareBench consists of 1000 QA pairs across four tasks: quantity (600), temporal (100), geometric (200), and spatial (100). It is derived from two auxiliary datasets that we constructed: TallyBench (2000 counting images with QA) and HistCaps (515 historical images with bilingual captions). We evaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source models (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but also reveal critical limitations: even the strongest models consistently fail at temporal ordering and spatial relations, and they often make mistakes in basic counting and geometric comparisons that are trivial for humans. These findings demonstrate that visual comparison remains a systematic blind spot for current VLMs. By providing controlled, diverse, and diagnostic evaluation, CompareBench establishes a foundation for advancing more reliable multimodal reasoning.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Frames to Clips: Training-free Adaptive Key Clip Selection for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2510.02262</link>
<guid>https://arxiv.org/abs/2510.02262</guid>
<content:encoded><![CDATA[
arXiv:2510.02262v2 Announce Type: replace 
Abstract: Video Large Language Models (VLMs) have achieved strong performance on various vision-language tasks, yet their practical use is limited by the massive number of visual tokens produced from raw video frames, which quickly exhausts the model's context window. Existing solutions mitigate this issue by selecting a sparse set of frames, but such frame-wise selection discards essential temporal dynamics in long-form videos, leading to suboptimal reasoning about motion and event continuity. In this work, we systematically examine the role of temporal information and show that extending selection from isolated key frames to temporally coherent key clips improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we introduce frame resolution as a controllable factor in frame selection, enabling a trade-off between spatial resolution and clip length. Building on this idea, we propose an adaptive clip length module that dynamically balances these factors to ensure a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling by up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench, and MLVU, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling VLMs to real-world video understanding applications. Project webpage is available at https://guangyusun.com/f2c .
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</title>
<link>https://arxiv.org/abs/2510.12796</link>
<guid>https://arxiv.org/abs/2510.12796</guid>
<content:encoded><![CDATA[
arXiv:2510.12796v2 Announce Type: replace 
Abstract: Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep generative priors for 3D brain analysis</title>
<link>https://arxiv.org/abs/2510.15119</link>
<guid>https://arxiv.org/abs/2510.15119</guid>
<content:encoded><![CDATA[
arXiv:2510.15119v2 Announce Type: replace 
Abstract: Diffusion models have recently emerged as powerful generative models in medical imaging. However, it remains a major challenge to combine these data-driven models with domain knowledge to guide brain imaging problems. In neuroimaging, Bayesian inverse problems have long provided a successful framework for inference tasks, where incorporating domain knowledge of the imaging process enables robust performance without requiring extensive training data. However, the anatomical modeling component of these approaches typically relies on classical mathematical priors that often fail to capture the complex structure of brain anatomy. In this work, we present the first general-purpose application of diffusion models as priors for solving a wide range of medical imaging inverse problems. Our approach leverages a score-based diffusion prior trained extensively on diverse brain MRI data, paired with flexible forward models that capture common image processing tasks such as super-resolution, bias field correction, inpainting, and combinations thereof. We further demonstrate how our framework can refine outputs from existing deep learning methods to improve anatomical fidelity. Experiments on heterogeneous clinical and research MRI data show that our method achieves state-of-the-art performance producing consistent, high-quality solutions without requiring paired training datasets. These results highlight the potential of diffusion priors as versatile tools for brain MRI analysis.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models</title>
<link>https://arxiv.org/abs/2510.20322</link>
<guid>https://arxiv.org/abs/2510.20322</guid>
<content:encoded><![CDATA[
arXiv:2510.20322v3 Announce Type: replace 
Abstract: Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with M\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\% additional parameters. Code is available at https://github.com/godlin-sjtu/HyperET
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FARM: Fine-Tuning Geospatial Foundation Models for Intra-Field Crop Yield Regression</title>
<link>https://arxiv.org/abs/2510.26609</link>
<guid>https://arxiv.org/abs/2510.26609</guid>
<content:encoded><![CDATA[
arXiv:2510.26609v2 Announce Type: replace 
Abstract: Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces FARM: Fine-tuning Agricultural Regression Models, a deep learning framework designed for high-resolution, intra-field canola yield prediction. FARM leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level (30 m) yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, FARM achieves a Root Mean Squared Error (RMSE) of 0.44 and an R^2 of 0.81. Using an independent high-resolution yield monitor dataset, we further show that fine-tuning FARM on limited ground-truth labels outperforms training the same architecture from scratch, confirming the benefit of pre-training on large, upsampled county-level data for data-scarce precision agriculture. These results represent improvement over baseline architectures like 3D-CNN and DeepYield, which highlight the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, FARM offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Thinker: Interactive Thinking with Images</title>
<link>https://arxiv.org/abs/2511.04460</link>
<guid>https://arxiv.org/abs/2511.04460</guid>
<content:encoded><![CDATA[
arXiv:2511.04460v2 Announce Type: replace 
Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image interaction with long-horizon reasoning capabilities remains a long-standing challenge in this field. Recent advances in vision-centric reasoning explore a promising "Thinking with Images" paradigm for LMMs, marking a shift from image-assisted reasoning to image-interactive thinking. While this milestone enables models to focus on fine-grained image regions, progress remains constrained by limited visual tool spaces and task-specific workflow designs. To bridge this gap, we present V-Thinker, a general-purpose multimodal reasoning assistant that enables interactive, vision-centric thinking through end-to-end reinforcement learning. V-Thinker comprises two key components: (1) a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies interactive reasoning datasets across three dimensions-diversity, quality, and difficulty; and (2) a Visual Progressive Training Curriculum that first aligns perception via point-level supervision, then integrates interactive reasoning through a two-stage reinforcement learning framework. Furthermore, we introduce VTBench, an expert-verified benchmark targeting vision-centric interactive reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently outperforms strong LMM-based baselines in both general and interactive reasoning scenarios, providing valuable insights for advancing image-interactive reasoning applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConsistTalk: Intensity Controllable Temporally Consistent Talking Head Generation with Diffusion Noise Search</title>
<link>https://arxiv.org/abs/2511.06833</link>
<guid>https://arxiv.org/abs/2511.06833</guid>
<content:encoded><![CDATA[
arXiv:2511.06833v2 Announce Type: replace 
Abstract: Recent advancements in video diffusion models have significantly enhanced audio-driven portrait animation. However, current methods still suffer from flickering, identity drift, and poor audio-visual synchronization. These issues primarily stem from entangled appearance-motion representations and unstable inference strategies. In this paper, we introduce \textbf{ConsistTalk}, a novel intensity-controllable and temporally consistent talking head generation framework with diffusion noise search inference. First, we propose \textbf{an optical flow-guided temporal module (OFT)} that decouples motion features from static appearance by leveraging facial optical flow, thereby reducing visual flicker and improving temporal consistency. Second, we present an \textbf{Audio-to-Intensity (A2I) model} obtained through multimodal teacher-student knowledge distillation. By transforming audio and facial velocity features into a frame-wise intensity sequence, the A2I model enables joint modeling of audio and visual motion, resulting in more natural dynamics. This further enables fine-grained, frame-wise control of motion dynamics while maintaining tight audio-visual synchronization. Third, we introduce a \textbf{diffusion noise initialization strategy (IC-Init)}. By enforcing explicit constraints on background coherence and motion continuity during inference-time noise search, we achieve better identity preservation and refine motion dynamics compared to the current autoregressive strategy. Extensive experiments demonstrate that ConsistTalk significantly outperforms prior methods in reducing flicker, preserving identity, and delivering temporally stable, high-fidelity talking head videos.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization</title>
<link>https://arxiv.org/abs/2511.09117</link>
<guid>https://arxiv.org/abs/2511.09117</guid>
<content:encoded><![CDATA[
arXiv:2511.09117v2 Announce Type: replace 
Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using several recent versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, two state-of-the-art (SOTA) Generative Adversarial Network (GAN) methods, as well as our Conditional GAN (cGAN) baseline. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.12142</link>
<guid>https://arxiv.org/abs/2511.12142</guid>
<content:encoded><![CDATA[
arXiv:2511.12142v2 Announce Type: replace 
Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CompEvent: Complex-valued Event-RGB Fusion for Low-light Video Enhancement and Deblurring</title>
<link>https://arxiv.org/abs/2511.14469</link>
<guid>https://arxiv.org/abs/2511.14469</guid>
<content:encoded><![CDATA[
arXiv:2511.14469v2 Announce Type: replace 
Abstract: Low-light video deblurring poses significant challenges in applications like nighttime surveillance and autonomous driving due to dim lighting and long exposures. While event cameras offer potential solutions with superior low-light sensitivity and high temporal resolution, existing fusion methods typically employ staged strategies, limiting their effectiveness against combined low-light and motion blur degradations. To overcome this, we propose CompEvent, a complex neural network framework enabling holistic full-process fusion of event data and RGB frames for enhanced joint restoration. CompEvent features two core components: 1) Complex Temporal Alignment GRU, which utilizes complex-valued convolutions and processes video and event streams iteratively via GRU to achieve temporal alignment and continuous fusion; and 2) Complex Space-Frequency Learning module, which performs unified complex-valued signal processing in both spatial and frequency domains, facilitating deep fusion through spatial structures and system-level characteristics. By leveraging the holistic representation capability of complex-valued neural networks, CompEvent achieves full-process spatiotemporal fusion, maximizes complementary learning between modalities, and significantly strengthens low-light video deblurring capability. Extensive experiments demonstrate that CompEvent outperforms SOTA methods in addressing this challenging task.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization</title>
<link>https://arxiv.org/abs/2511.15705</link>
<guid>https://arxiv.org/abs/2511.15705</guid>
<content:encoded><![CDATA[
arXiv:2511.15705v2 Announce Type: replace 
Abstract: Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeAR: Coupled Neural Asset-Renderer Stack</title>
<link>https://arxiv.org/abs/2511.18600</link>
<guid>https://arxiv.org/abs/2511.18600</guid>
<content:encoded><![CDATA[
arXiv:2511.18600v2 Announce Type: replace 
Abstract: Neural asset authoring and neural rendering have traditionally evolved as disjoint paradigms: one generates digital assets for fixed graphics pipelines, while the other maps conventional assets to images. However, treating them as independent entities limits the potential for end-to-end optimization in fidelity and consistency. In this paper, we bridge this gap with NeAR, a Coupled Neural Asset--Renderer Stack. We argue that co-designing the asset representation and the renderer creates a robust "contract" for superior generation. On the asset side, we introduce the Lighting-Homogenized SLAT (LH-SLAT). Leveraging a rectified-flow model, NeAR lifts casually lit single images into a canonical, illumination-invariant latent space, effectively suppressing baked-in shadows and highlights. On the renderer side, we design a lighting-aware neural decoder tailored to interpret these homogenized latents. Conditioned on HDR environment maps and camera views, it synthesizes relightable 3D Gaussian splats in real-time without per-object optimization. We validate NeAR on four tasks: (1) G-buffer-based forward rendering, (2) random-lit reconstruction, (3) unknown-lit relighting, and (4) novel-view relighting. Extensive experiments demonstrate that our coupled stack outperforms state-of-the-art baselines in both quantitative metrics and perceptual quality. We hope this coupled asset-renderer perspective inspires future graphics stacks that view neural assets and renderers as co-designed components instead of independent entities.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</title>
<link>https://arxiv.org/abs/2511.23334</link>
<guid>https://arxiv.org/abs/2511.23334</guid>
<content:encoded><![CDATA[
arXiv:2511.23334v2 Announce Type: replace 
Abstract: Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 $\times$ 256) and decreases peak memory consumption by 83.8% (1024 $\times$ 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Domain Properties in Language-Driven Domain Generalization for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.03508</link>
<guid>https://arxiv.org/abs/2512.03508</guid>
<content:encoded><![CDATA[
arXiv:2512.03508v2 Announce Type: replace 
Abstract: Recent domain generalized semantic segmentation (DGSS) studies have achieved notable improvements by distilling semantic knowledge from Vision-Language Models (VLMs). However, they overlook the semantic misalignment between visual and textual contexts, which arises due to the rigidity of a fixed context prompt learned on a single source domain. To this end, we present a novel domain generalization framework for semantic segmentation, namely Domain-aware Prompt-driven Masked Transformer (DPMFormer). Firstly, we introduce domain-aware prompt learning to facilitate semantic alignment between visual and textual cues. To capture various domain-specific properties with a single source dataset, we propose domain-aware contrastive learning along with the texture perturbation that diversifies the observable domains. Lastly, to establish a framework resilient against diverse environmental changes, we have proposed the domain-robust consistency learning which guides the model to minimize discrepancies of prediction from original and the augmented images. Through experiments and analyses, we demonstrate the superiority of the proposed framework, which establishes a new state-of-the-art on various DGSS benchmarks. The code is available at https://github.com/jone1222/DPMFormer.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</title>
<link>https://arxiv.org/abs/2512.04677</link>
<guid>https://arxiv.org/abs/2512.04677</guid>
<content:encoded><![CDATA[
arXiv:2512.04677v3 Announce Type: replace 
Abstract: Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathrm{D}^\mathrm{3}$-Predictor: Noise-Free Deterministic Diffusion for Dense Prediction</title>
<link>https://arxiv.org/abs/2512.07062</link>
<guid>https://arxiv.org/abs/2512.07062</guid>
<content:encoded><![CDATA[
arXiv:2512.07062v3 Announce Type: replace 
Abstract: Although diffusion models with strong visual priors have emerged as powerful dense prediction backboens, they overlook a core limitation: the stochastic noise at the core of diffusion sampling is inherently misaligned with dense prediction that requires a deterministic mapping from image to geometry. In this paper, we show that this stochastic noise corrupts fine-grained spatial cues and pushes the model toward timestep-specific noise objectives, consequently destroying meaningful geometric structure mappings. To address this, we introduce $\mathrm{D}^\mathrm{3}$-Predictor, a noise-free deterministic framework built by reformulating a pretrained diffusion model without stochasticity noise. Instead of relying on noisy inputs to leverage diffusion priors, $\mathrm{D}^\mathrm{3}$-Predictor views the pretrained diffusion network as an ensemble of timestep-dependent visual experts and self-supervisedly aggregates their heterogeneous priors into a single, clean, and complete geometric prior. Meanwhile, we utilize task-specific supervision to seamlessly adapt this noise-free prior to dense prediction tasks. Extensive experiments on various dense prediction tasks demonstrate that $\mathrm{D}^\mathrm{3}$-Predictor achieves competitive or state-of-the-art performance in diverse scenarios. In addition, it requires less than half the training data previously used and efficiently performs inference in a single step. Our code, data, and checkpoints are publicly available at https://x-gengroup.github.io/HomePage_D3-Predictor/.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation is Required for Data-Efficient Perception</title>
<link>https://arxiv.org/abs/2512.08854</link>
<guid>https://arxiv.org/abs/2512.08854</guid>
<content:encoded><![CDATA[
arXiv:2512.08854v2 Announce Type: replace 
Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Logits to Hierarchies: Hierarchical Clustering made Simple</title>
<link>https://arxiv.org/abs/2410.07858</link>
<guid>https://arxiv.org/abs/2410.07858</guid>
<content:encoded><![CDATA[
arXiv:2410.07858v2 Announce Type: replace-cross 
Abstract: The hierarchical structure inherent in many real-world datasets makes the modeling of such hierarchies a crucial objective in both unsupervised and supervised machine learning. While recent advancements have introduced deep architectures specifically designed for hierarchical clustering, we adopt a critical perspective on this line of research. Our findings reveal that these methods face significant limitations in scalability and performance when applied to realistic datasets. Given these findings, we present an alternative approach and introduce a lightweight method that builds on pre-trained non-hierarchical clustering models. Remarkably, our approach outperforms specialized deep models for hierarchical clustering, and it is broadly applicable to any pre-trained clustering model that outputs logits, without requiring any fine-tuning. To highlight the generality of our approach, we extend its application to a supervised setting, demonstrating its ability to recover meaningful hierarchies from a pre-trained ImageNet classifier. Our results establish a practical and effective alternative to existing deep hierarchical clustering methods, with significant advantages in efficiency, scalability and performance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Backdoor Attacks on Neural Networks</title>
<link>https://arxiv.org/abs/2411.14516</link>
<guid>https://arxiv.org/abs/2411.14516</guid>
<content:encoded><![CDATA[
arXiv:2411.14516v2 Announce Type: replace-cross 
Abstract: Neural networks are often trained on proprietary datasets, making them attractive attack targets. We present a novel dataset extraction method leveraging an innovative training time backdoor attack, allowing a malicious federated learning server to systematically and deterministically extract complete client training samples through a simple indexing process. Unlike prior techniques, our approach guarantees exact data recovery rather than probabilistic reconstructions or hallucinations, provides precise control over which samples are memorized and how many, and shows high capacity and robustness. Infected models output data samples when they receive a patternbased index trigger, enabling systematic extraction of meaningful patches from each clients local data without disrupting global model utility. To address small model output sizes, we extract patches and then recombined them. The attack requires only a minor modification to the training code that can easily evade detection during client-side verification. Hence, this vulnerability represents a realistic FL supply-chain threat, where a malicious server can distribute modified training code to clients and later recover private data from their updates. Evaluations across classifiers, segmentation models, and large language models demonstrate that thousands of sensitive training samples can be recovered from client models with minimal impact on task performance, and a clients entire dataset can be stolen after multiple FL rounds. For instance, a medical segmentation dataset can be extracted with only a 3 percent utility drop. These findings expose a critical privacy vulnerability in FL systems, emphasizing the need for stronger integrity and transparency in distributed training pipelines.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws for Black box Adversarial Attacks</title>
<link>https://arxiv.org/abs/2411.16782</link>
<guid>https://arxiv.org/abs/2411.16782</guid>
<content:encoded><![CDATA[
arXiv:2411.16782v4 Announce Type: replace-cross 
Abstract: Adversarial examples exhibit cross-model transferability, enabling threatening black-box attacks on commercial models. Model ensembling, which attacks multiple surrogate models, is a known strategy to improve this transferability. However, prior studies typically use small, fixed ensembles, which leaves open an intriguing question of whether scaling the number of surrogate models can further improve black-box attacks. In this work, we conduct the first large-scale empirical study of this question. We show that by resolving gradient conflict with advanced optimizers, we discover a robust and universal log-linear scaling law through both theoretical analysis and empirical evaluations: the Attack Success Rate (ASR) scales linearly with the logarithm of the ensemble size $T$. We rigorously verify this law across standard classifiers, SOTA defenses, and MLLMs, and find that scaling distills robust, semantic features of the target class. Consequently, we apply this fundamental insight to benchmark SOTA MLLMs. This reveals both the attack's devastating power and a clear robustness hierarchy: we achieve 80\%+ transfer attack success rate on proprietary models like GPT-4o, while also highlighting the exceptional resilience of Claude-3.5-Sonnet. Our findings urge a shift in focus for robustness evaluation: from designing intricate algorithms on small ensembles to understanding the principled and powerful threat of scaling.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents</title>
<link>https://arxiv.org/abs/2502.01218</link>
<guid>https://arxiv.org/abs/2502.01218</guid>
<content:encoded><![CDATA[
arXiv:2502.01218v3 Announce Type: replace-cross 
Abstract: Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments on both simulated and real robots show that the pretrained features significantly enhance downstream manipulation tasks with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StructDiff: Structure-aware Diffusion Model for 3D Fine-grained Medical Image Synthesis</title>
<link>https://arxiv.org/abs/2503.09560</link>
<guid>https://arxiv.org/abs/2503.09560</guid>
<content:encoded><![CDATA[
arXiv:2503.09560v2 Announce Type: replace-cross 
Abstract: Solving medical imaging data scarcity through semantic image generation has attracted growing attention in recent years. However, existing generative models mainly focus on synthesizing whole-organ or large-tissue structures, showing limited capability in reproducing fine-grained anatomical details. Due to the stringent requirement of topological consistency and the complex 3D morphological heterogeneity of medical data, accurately reconstructing fine-grained anatomical details remains a significant challenge. To address these limitations, we propose StructDiff, a Structure-aware Diffusion Model for fine-grained 3D medical image synthesis, which enables precise generation of topologically complex anatomies. In addition to the conventional mask-based guidance, StructDiff further introduces a paired image-mask template to guide the generation process, providing structural constrains and offering explicit knowledge of mask-to-image correspondence. Moreover, a Mask Generation Module (MGM) is designed to enrich mask diversity and alleviate the scarcity of high-quality reference masks. Furthermore, we propose a Confidence-aware Adaptive Learning (CAL) strategy based on Skip-Sampling Variance (SSV), which mitigates uncertainty introduced by imperfect synthetic data when transferring to downstream tasks. Extensive experiments demonstrate that StructDiff achieves state-of-the-art performance in terms of topological consistency and visual realism, and significantly boosts downstream segmentation performance. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions</title>
<link>https://arxiv.org/abs/2507.21503</link>
<guid>https://arxiv.org/abs/2507.21503</guid>
<content:encoded><![CDATA[
arXiv:2507.21503v3 Announce Type: replace-cross 
Abstract: Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransUNet-GradCAM: A Hybrid Transformer-U-Net with Self-Attention and Explainable Visualizations for Foot Ulcer Segmentation</title>
<link>https://arxiv.org/abs/2508.03758</link>
<guid>https://arxiv.org/abs/2508.03758</guid>
<content:encoded><![CDATA[
arXiv:2508.03758v4 Announce Type: replace-cross 
Abstract: Automated segmentation of diabetic foot ulcers (DFUs) plays a critical role in clinical diagnosis, therapeutic planning, and longitudinal wound monitoring. However, this task remains challenging due to the heterogeneous appearance, irregular morphology, and complex backgrounds associated with ulcer regions in clinical photographs. Traditional convolutional neural networks (CNNs), such as U-Net, provide strong localization capabilities but struggle to model long-range spatial dependencies due to their inherently limited receptive fields. To address this, we employ the TransUNet architecture, a hybrid framework that integrates the global attention mechanism of Vision Transformers (ViTs) into the U-Net structure. This combination allows the model to extract global contextual features while maintaining fine-grained spatial resolution. We trained the model on the public Foot Ulcer Segmentation Challenge (FUSeg) dataset using a robust augmentation pipeline and a hybrid loss function to mitigate class imbalance. On the internal validation set, the model achieved a Dice Similarity Coefficient (F1-score) of 0.8886 using an optimized threshold of 0.4843. Crucially, to assess generalizability, we performed external validation on two independent datasets: the AZH Wound Care Center dataset (n=278) and the Medetec dataset (n=152). Without any retraining, the model achieved Dice scores of 0.6209 and 0.7850, respectively, demonstrating robust zero-shot transferability to unseen clinical domains. Furthermore, clinical utility analysis revealed a strong correlation (Pearson r = 0.9749) between predicted and ground-truth wound areas. These outcomes demonstrate that our approach effectively integrates global and local feature extraction, offering a reliable, effective, and explainable solution for automated foot ulcer assessment.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Team Westwood Solution for MIDOG 2025 Challenge: An Ensemble-CNN-Based Approach For Mitosis Detection And Classification</title>
<link>https://arxiv.org/abs/2509.02600</link>
<guid>https://arxiv.org/abs/2509.02600</guid>
<content:encoded><![CDATA[
arXiv:2509.02600v3 Announce Type: replace-cross 
Abstract: This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial mitosis candidate screening with high sensitivity, followed by a random forest classifier ensembling predictions of three convolutional neural networks (CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the atypical mitosis classification, we trained another random forest classifier ensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and InceptionV3. On the preliminary test set, our solution achieved an F1 score of 0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for track 2 atypical mitosis classification. On the final test set, our solution achieved an F1 score of 0.6972 for track 1 mitosis detection, and a balanced accuracy of 0.8242 for track 2 atypical mitosis classification.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI</title>
<link>https://arxiv.org/abs/2510.05684</link>
<guid>https://arxiv.org/abs/2510.05684</guid>
<content:encoded><![CDATA[
arXiv:2510.05684v2 Announce Type: replace-cross 
Abstract: Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-localization on a 3D map by fusing global and local features from a monocular camera</title>
<link>https://arxiv.org/abs/2510.26170</link>
<guid>https://arxiv.org/abs/2510.26170</guid>
<content:encoded><![CDATA[
arXiv:2510.26170v2 Announce Type: replace-cross 
Abstract: Self-localization on a 3D map by using an inexpensive monocular camera is required to realize autonomous driving. Self-localization based on a camera often uses a convolutional neural network (CNN) that can extract local features that are calculated by nearby pixels. However, when dynamic obstacles, such as people, are present, CNN does not work well. This study proposes a new method combining CNN with Vision Transformer, which excels at extracting global features that show the relationship of patches on whole image. Experimental results showed that, compared to the state-of-the-art method (SOTA), the accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times higher than that without dynamic obstacles. Moreover, the self-localization error of our method is 20.1% smaller than that of SOTA on public datasets. Additionally, our robot using our method can localize itself with 7.51cm error on average, which is more accurate than SOTA.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAFM: Dynamic Adaptive Fusion for Multi-Model Collaboration in Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2511.05020</link>
<guid>https://arxiv.org/abs/2511.05020</guid>
<content:encoded><![CDATA[
arXiv:2511.05020v2 Announce Type: replace-cross 
Abstract: Composed Image Retrieval (CIR) is a cross-modal task that aims to retrieve target images from large-scale databases using a reference image and a modification text. Most existing methods rely on a single model to perform feature fusion and similarity matching. However, this paradigm faces two major challenges. First, one model alone can't see the whole picture and the tiny details at the same time; it has to handle different tasks with the same weights, so it often misses the small but important links between image and text. Second, the absence of dynamic weight allocation prevents adaptive leveraging of complementary model strengths, so the resulting embedding drifts away from the target and misleads the nearest-neighbor search in CIR. To address these limitations, we propose Dynamic Adaptive Fusion (DAFM) for multi-model collaboration in CIR. Rather than optimizing a single method in isolation, DAFM exploits the complementary strengths of heterogeneous models and adaptively rebalances their contributions. This not only maximizes retrieval accuracy but also ensures that the performance gains are independent of the fusion order, highlighting the robustness of our approach. Experiments on the CIRR and FashionIQ benchmarks demonstrate consistent improvements. Our method achieves a Recall@10 of 93.21 and an Rmean of 84.43 on CIRR, and an average Rmean of 67.48 on FashionIQ, surpassing recent strong baselines by up to 4.5%. These results confirm that dynamic multi-model collaboration provides an effective and general solution for CIR.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2511.11688</link>
<guid>https://arxiv.org/abs/2511.11688</guid>
<content:encoded><![CDATA[
arXiv:2511.11688v2 Announce Type: replace-cross 
Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning</title>
<link>https://arxiv.org/abs/2511.13654</link>
<guid>https://arxiv.org/abs/2511.13654</guid>
<content:encoded><![CDATA[
arXiv:2511.13654v2 Announce Type: replace-cross 
Abstract: In this paper, we present the first detailed analysis of how training hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the training hyperparameter space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.22862</link>
<guid>https://arxiv.org/abs/2511.22862</guid>
<content:encoded><![CDATA[
arXiv:2511.22862v2 Announce Type: replace-cross 
Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at https://github.com/Luchicken/BriMPR.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</title>
<link>https://arxiv.org/abs/2512.02920</link>
<guid>https://arxiv.org/abs/2512.02920</guid>
<content:encoded><![CDATA[
arXiv:2512.02920v2 Announce Type: replace-cross 
Abstract: We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\%$, which is a $3.7\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\%$ under higher precipitation, by $22\%$ on higher-speed roads such as motorways, and by $29\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.
]]></content:encoded>
<pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.02498</link>
<guid>https://arxiv.org/abs/2512.02498</guid>
<content:encoded><![CDATA[
<div> keywords: Document Layout Parsing, Vision-Language Model, dots_ocr, multilingual corpus, XDocParse<br /><br />Summary:<br /><br />Document Layout Parsing acts as an essential interface enabling AI to interpret vast amounts of structured knowledge by integrating layout detection, text recognition, and relational understanding. This is especially vital for advancing next-generation Vision-Language Models. Existing approaches often depend on fragmented, multi-stage pipelines that cause error propagation and fail to exploit joint training benefits. The paper introduces dots_ocr, a unified Vision-Language Model that jointly learns these three key tasks in an end-to-end manner, overcoming prior limitations. This innovation is supported by a scalable data engine capable of synthesizing a vast multilingual corpus, thereby enhancing the model’s robustness across varied tasks, languages, layouts, and domains. The proposed framework achieves state-of-the-art results on the OmniDocBench benchmark, demonstrating its effectiveness. Additionally, the authors present XDocParse, a new challenging benchmark spanning 126 languages to foster research in global document intelligence. On XDocParse, dots_ocr attains approximately a 10% relative improvement over previous methods, showcasing strong multilingual capabilities and establishing a new standard for document layout parsing in diverse linguistic contexts. <div>
arXiv:2512.02498v4 Announce Type: replace 
Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots_ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this benchmark, dots_ocr achieves state-of-the-art performance, delivering an approximately 10% relative improvement and demonstrating strong multilingual capability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Continual Compositional Zero-Shot Learning</title>
<link>https://arxiv.org/abs/2512.09172</link>
<guid>https://arxiv.org/abs/2512.09172</guid>
<content:encoded><![CDATA[
<div> Compositional Zero-Shot Learning, Continual Learning, Vision-Language Models, Prompt-based Learning, Knowledge Distillation<br /><br />Summary:<br /><br />This paper addresses the challenge of continual adaptation of vision-language models (VLMs) to new attributes, objects, and their unique compositions within Compositional Zero-Shot Learning (CZSL), while avoiding forgetting previously learned knowledge. Unlike classical continual learning, the task is complicated by overlapping attributes and objects across learning sessions, with only their compositions remaining unique. The authors propose PromptCCZSL, the first prompt-based framework for continual compositional zero-shot learning built on a frozen VLM backbone. PromptCCZSL uses recency-weighted multi-teacher knowledge distillation to retain prior knowledge. It employs session-aware compositional prompts for fusing multimodal features of new compositions and session-agnostic prompts for attributes and objects to maintain global semantic consistency. A Cosine Anchor Loss (CAL) further stabilizes prior knowledge retention. To promote effective current session adaptation, an Orthogonal Projection Loss (OPL) ensures distinction between new and previous embeddings, preventing overlap, while an Intra-Session Diversity Loss (IDL) encourages diversity among new embeddings for richer, discriminative representations. The authors also introduce a comprehensive evaluation protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA demonstrate that PromptCCZSL significantly outperforms prior VLM-based and non-VLM baselines, setting a new state-of-the-art in closed-world CCZSL scenarios. <div>
arXiv:2512.09172v2 Announce Type: replace 
Abstract: We tackle continual adaptation of vision-language models to new attributes, objects, and their compositions in Compositional Zero-Shot Learning (CZSL), while preventing forgetting of prior knowledge. Unlike classical continual learning where classes are disjoint, CCZSL is more complex as attributes and objects may reoccur across sessions while compositions remain unique. Built on a frozen VLM backbone, we propose the first Prompt-based Continual Compositional Zero-Shot Learning (PromptCCZSL) framework that retains prior knowledge through recency-weighted multi-teacher distillation. It employs session-aware compositional prompts to fuse multimodal features for new compositions, while attribute and object prompts are learned through session-agnostic fusion to maintain global semantic consistency, which is further stabilized by a Cosine Anchor Loss (CAL) to preserve prior knowledge. To enhance adaptation in the current session, an Orthogonal Projection Loss (OPL) ensures that new attribute and object embeddings remain distinct from previous ones, preventing overlap, while an Intra-Session Diversity Loss (IDL) promotes variation among current-session embeddings for richer, more discriminative representations. We also introduce a comprehensive protocol that jointly measures catastrophic forgetting and compositional generalization. Extensive experiments on UT-Zappos and C-GQA benchmarks demonstrate that PromptCCZSL achieves substantial improvements over prior VLM-based and non-VLM baselines, setting a new benchmark for CCZSL in closed-world settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation</title>
<link>https://arxiv.org/abs/2512.14755</link>
<guid>https://arxiv.org/abs/2512.14755</guid>
<content:encoded><![CDATA[
<div> Change detection, Synthetic Aperture Radar, Very-High-Resolution imagery, Foundation models, Label transfer<br /><br />Summary:<br /><br />1. The article addresses change detection for linear infrastructure monitoring, which requires reliable, high-resolution data with consistent acquisition intervals; however, optical Very-High-Resolution (VHR) imagery is limited by weather conditions like clouds. 2. Synthetic Aperture Radar (SAR) offers all-weather imaging capabilities but poses challenges in annotation due to its complexity. 3. The authors introduce SkyCap, a novel bitemporal VHR dataset combining optical SkySat and SAR Capella Space scenes through archive matching and precise co-registration. 4. They employ an optical-to-SAR label transfer technique to generate SAR amplitude change detection (ACD) labels without needing expert SAR annotations. 5. Continued pretraining of the SARATR-X model on this SAR data is performed to develop SAR-specific foundation models, which are benchmarked against optical foundation models on the SkyCap dataset under various preprocessing schemes. 6. Results show that the optical foundation model MTP(ViT-B+RVSA), when preprocessed with dB+Z-score normalization, achieves the best performance (F1_c = 45.06), surpassing SAR-specific models trained directly on Capella data. 7. The study also reveals a strong dependency of model performance on preprocessing alignment with pretraining data statistics and indicates that optical model rankings in change detection do not directly translate to SAR ACD tasks. 8. This work represents the first evaluation of foundation models on VHR SAR amplitude change detection, contributing valuable insights for future research in this domain. <div>
arXiv:2512.14755v1 Announce Type: new 
Abstract: Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2512.14757</link>
<guid>https://arxiv.org/abs/2512.14757</guid>
<content:encoded><![CDATA[
<div> Keywords: socially compliant navigation, vision language models, reinforcement fine-tuning, semantic similarity reward, Mixture-of-Experts model<br /><br />Summary:<br /> This paper addresses the challenge of developing robots capable of socially compliant navigation in human-populated environments, emphasizing not only safety but also social norms, human comfort, and contextual appropriateness. The authors identify that while large vision language models (VLMs) have potential for this task, their high computational costs hinder real-time deployment on resource-limited robotic platforms. To overcome this, they propose SocialNav-MoE, an efficient Mixture-of-Experts VLM designed for socially compliant navigation, fine-tuned through reinforcement learning techniques. A novel semantic similarity reward (SSR) is introduced to enhance the reinforcement fine-tuning process, improving decision-making effectiveness over traditional hard-level and character-level rewards. The study further investigates the impact of various small language models (Phi, Qwen, StableLM), routing strategies, and vision encoders (CLIP and SigLIP, both frozen and fine-tuned) on navigation performance. Experimental results on the SNEI dataset demonstrate that SocialNav-MoE achieves a strong balance between navigation accuracy and computational efficiency. The findings indicate the SSR function is particularly effective in reinforcing compliance with social navigation norms. The authors plan to release their source code upon acceptance, facilitating further research and application. <div>
arXiv:2512.14757v1 Announce Type: new 
Abstract: For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics</title>
<link>https://arxiv.org/abs/2512.14758</link>
<guid>https://arxiv.org/abs/2512.14758</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Music Recognition, Jianpu, Chinese Folk Songs, MusicXML, Expert System<br /><br />Summary:<br /><br />This paper addresses the underexplored area of Optical Music Recognition (OMR) for Chinese Jianpu (numbered notation) and its associated lyrics, contrasting with the predominant focus on Western staff notation in large-scale OMR research. The authors propose a modular expert-system pipeline designed to convert printed Jianpu scores with lyrics into machine-readable formats such as MusicXML and MIDI, eliminating the need for large sets of annotated training data. Their approach blends traditional computer vision techniques—including phrase correlation and skeleton analysis—with unsupervised deep-learning modules for extracting image feature embeddings, achieving a hybrid system that balances interpretability and accuracy. The method was evaluated on The Anthology of Chinese Folk Songs, covering a large-scale melody-only dataset of over 5,000 songs (more than 300,000 notes) and a curated subset including lyrics with over 1,400 songs (exceeding 100,000 notes). Experimental results demonstrate that the system attains high precision in recognizing both melodies, with a note-wise F1 score of 0.951, and aligned lyrics, with a character-wise F1 score of 0.931. This work significantly advances large-scale digitization and accessibility of Chinese Jianpu musical resources. <div>
arXiv:2512.14758v1 Announce Type: new 
Abstract: Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion</title>
<link>https://arxiv.org/abs/2512.14760</link>
<guid>https://arxiv.org/abs/2512.14760</guid>
<content:encoded><![CDATA[
<div> underwater image enhancement, diffusion model, color correction, chromatic prior, cross-domain consistency loss  

<br /><br />Summary:  
Underwater images often suffer from severe degradation due to wavelength-dependent light absorption and scattering, leading to color distortion, low contrast, and loss of fine details that impair vision-based underwater applications. To tackle these issues, the paper proposes AquaDiff, a diffusion-based framework aimed at enhancing underwater images by correcting chromatic distortions while maintaining both structural and perceptual fidelity. AquaDiff incorporates a chromatic prior-guided color compensation strategy integrated within a conditional diffusion process. This process utilizes cross-attention mechanisms to dynamically fuse the degraded input images with noisy latent states during each denoising step. The model’s enhanced denoising backbone leverages residual dense blocks and multi-resolution attention modules to effectively capture global color contexts as well as local details. Moreover, AquaDiff introduces a novel cross-domain consistency loss that enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity simultaneously. Extensive experiments across multiple challenging underwater benchmarks demonstrate that AquaDiff surpasses various state-of-the-art methods, including traditional, CNN-, GAN-, and other diffusion-based techniques, by delivering superior color correction and competitive overall image quality under diverse underwater conditions. <div>
arXiv:2512.14760v1 Announce Type: new 
Abstract: Underwater images are severely degraded by wavelength-dependent light absorption and scattering, resulting in color distortion, low contrast, and loss of fine details that hinder vision-based underwater applications. To address these challenges, we propose AquaDiff, a diffusion-based underwater image enhancement framework designed to correct chromatic distortions while preserving structural and perceptual fidelity. AquaDiff integrates a chromatic prior-guided color compensation strategy with a conditional diffusion process, where cross-attention dynamically fuses degraded inputs and noisy latent states at each denoising step. An enhanced denoising backbone with residual dense blocks and multi-resolution attention captures both global color context and local details. Furthermore, a novel cross-domain consistency loss jointly enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity. Extensive experiments on multiple challenging underwater benchmarks demonstrate that AquaDiff provides good results as compared to the state-of-the-art traditional, CNN-, GAN-, and diffusion-based methods, achieving superior color correction and competitive overall image quality across diverse underwater conditions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification</title>
<link>https://arxiv.org/abs/2512.14770</link>
<guid>https://arxiv.org/abs/2512.14770</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, Visual Question Answering, hallucination, uncertainty estimation, dual-assessment<br /><br />Summary:<br />Vision-language models (VLMs) have shown great promise in the task of Visual Question Answering (VQA), where the goal is to provide accurate answers based on visual and textual input. However, these models suffer from hallucination issues, producing confident but incorrect answers that damage trustworthiness. To counter this, the study introduces Dual-Assessment for VLM Reliability (DAVR), a novel framework designed to enhance answer reliability by providing comprehensive uncertainty estimation. DAVR employs a dual-pathway architecture: the first pathway uses dual selector modules that combine the latent features from the VLM with question-answer embeddings to evaluate the reliability of generated responses. The second pathway incorporates external reference models that conduct factual cross-checks to identify and reduce hallucinations. This two-fold approach effectively addresses both internal uncertainty and external factual accuracy. The framework was tested in the Reliable VQA Challenge at ICCV-CLVL 2025, where it achieved a top-ranking performance, recording a leading Φ₁₀₀ score of 39.64 and a 100-AUC of 97.22. These results illustrate the strong capability of DAVR in improving the trustworthiness and reliability of VLM-generated answers for visual question answering tasks. <div>
arXiv:2512.14770v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $\Phi_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering</title>
<link>https://arxiv.org/abs/2512.14870</link>
<guid>https://arxiv.org/abs/2512.14870</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Large Language Models, VideoQA, multi-evidence integration, Minimum Required Frame-Set, compositional video understanding<br /><br />Summary:<br /><br />1. HERBench is a new Video Question Answering (VideoQA) benchmark specifically designed to evaluate the ability of Video Large Language Models (Video-LLMs) to integrate multiple, temporally separated pieces of evidence rather than relying on a single salient cue.<br /><br />2. Each of the 26,000 multiple-choice questions in HERBench requires aggregating at least three distinct non-overlapping evidential cues from different video segments, ensuring that neither language priors nor a single frame can suffice to answer correctly.<br /><br />3. HERBench includes twelve compositional tasks that examine various reasoning skills, including identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting.<br /><br />4. The authors introduce the Minimum Required Frame-Set (MRFS) metric, which quantifies the minimal number of frames a model must combine to answer a question, finding that HERBench demands significantly more integration (mean MRFS 5.5) compared to prior datasets (2.6-4.2).<br /><br />5. Evaluation of 13 state-of-the-art Video-LLMs on HERBench reveals poor performance with accuracies between 31-42%, only marginally higher than random guessing, driven by two key failure modes: retrieval deficits (missing key evidence in frame selection) and fusion deficits (failure to integrate evidence even when fully retrieved).<br /><br />6. HERBench establishes a rigorous, quantifiable challenge to push progress toward robust, compositional understanding of videos that require cross-time evidence aggregation. <div>
arXiv:2512.14870v1 Announce Type: new 
Abstract: Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Isolated Sign Language Recognition with Segmentation and Pose Estimation</title>
<link>https://arxiv.org/abs/2512.14876</link>
<guid>https://arxiv.org/abs/2512.14876</guid>
<content:encoded><![CDATA[
<div> Sign language recognition, ASL, pose estimation, ResNet-Transformer, computational efficiency<br /><br />Summary: This paper addresses the challenge of isolated sign language recognition (ISLR) for American Sign Language (ASL), focusing on overcoming issues such as scarce per-sign data, high variability among signers, and large computational costs. The authors propose a novel model designed to reduce computational demands while maintaining robustness to different signers. Their approach involves a multi-stage pipeline: first, a pose estimation system extracts detailed hand and face joint coordinates from video inputs to capture essential visual cues. Second, a segmentation module is employed to isolate relevant information from the video, filtering out noise and irrelevant background data. Third, the processed data is fed into a hybrid ResNet-Transformer backbone, which effectively models both spatial features and temporal dependencies inherent in sign language gestures. This combination allows the system to learn complex patterns in ASL efficiently and accurately. Overall, the work contributes an innovative approach that balances computational efficiency with high recognition performance, making ISLR more accessible and potentially benefiting the ASL community by bridging communication gaps with automated understanding systems. <div>
arXiv:2512.14876v1 Announce Type: new 
Abstract: The recent surge in large language models has automated translations of spoken and written languages. However, these advances remain largely inaccessible to American Sign Language (ASL) users, whose language relies on complex visual cues. Isolated sign language recognition (ISLR) - the task of classifying videos of individual signs - can help bridge this gap but is currently limited by scarce per-sign data, high signer variability, and substantial computational costs. We propose a model for ISLR that reduces computational requirements while maintaining robustness to signer variation. Our approach integrates (i) a pose estimation pipeline to extract hand and face joint coordinates, (ii) a segmentation module that isolates relevant information, and (iii) a ResNet-Transformer backbone to jointly model spatial and temporal dependencies.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris</title>
<link>https://arxiv.org/abs/2512.14878</link>
<guid>https://arxiv.org/abs/2512.14878</guid>
<content:encoded><![CDATA[
<div> Keywords: animal re-identification, dermatoglyphic descriptors, cross-modal retrieval, data augmentation, ecological monitoring<br /><br />Summary:  
This study introduces an innovative approach to animal re-identification (Re-ID) by integrating precise dermatoglyphic textual descriptors, traditionally used in forensics, into ecological monitoring methods. Unlike conventional AI tools that rely primarily on image-based inputs for species with distinctive morphologies, this methodology encodes animal coat topology using human-interpretable language tags. The researchers utilized a substantial dataset consisting of 84,264 manually labeled minutiae from 3,355 images of 185 tigers (Panthera tigris) to evaluate the effectiveness of combining visual and textual data for identity retrieval across modalities. To address limitations posed by data scarcity, a novel text-image co-synthesis pipeline was developed, generating virtual individuals with dozens of life-like visuals paired with corresponding dermatoglyphic text. Benchmarking tests against real-world scenarios demonstrate that this augmentation significantly improves AI accuracy in cross-modal retrieval tasks. Importantly, this approach supports explainability by enabling textual-to-visual identity recovery based on human-verifiable matching criteria. The findings represent a substantial advancement in unifying descriptive modalities, highlighting the potential of language-guided biometrics to overcome the constraints of vision-only Re-ID systems and enhance ecological species monitoring with an interpretable and effective framework. <div>
arXiv:2512.14878v1 Announce Type: new 
Abstract: Biologists have long combined visuals with textual field notes to re-identify (Re-ID) animals. Contemporary AI tools automate this for species with distinctive morphological features but remain largely image-based. Here, we extend Re-ID methodologies by incorporating precise dermatoglyphic textual descriptors-an approach used in forensics but new to ecology. We demonstrate that these specialist semantics abstract and encode animal coat topology using human-interpretable language tags. Drawing on 84,264 manually labelled minutiae across 3,355 images of 185 tigers (Panthera tigris), we evaluate this visual-textual methodology, revealing novel capabilities for cross-modal identity retrieval. To optimise performance, we developed a text-image co-synthesis pipeline to generate 'virtual individuals', each comprising dozens of life-like visuals paired with dermatoglyphic text. Benchmarking against real-world scenarios shows this augmentation significantly boosts AI accuracy in cross-modal retrieval while alleviating data scarcity. We conclude that dermatoglyphic language-guided biometrics can overcome vision-only limitations, enabling textual-to-visual identity recovery underpinned by human-verifiable matchings. This represents a significant advance towards explainability in Re-ID and a language-driven unification of descriptive modalities in ecological monitoring.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vibe Spaces for Creatively Connecting and Expressing Visual Concepts</title>
<link>https://arxiv.org/abs/2512.14884</link>
<guid>https://arxiv.org/abs/2512.14884</guid>
<content:encoded><![CDATA[
<div> Keywords: Vibe Blending, visual concepts, latent space, Vibe Space, creative evaluation<br /><br />Summary:  
1. The paper introduces a new task called Vibe Blending, aimed at generating coherent and meaningful hybrid images by connecting distinct visual concepts through their shared "vibe" or attributes.  
2. Current generative methods face challenges in identifying and navigating nonlinear latent space paths that link distant concepts to produce smooth blends.  
3. To address this, the authors propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesic paths in feature spaces such as CLIP, enabling semantically consistent and smooth transitions between different visual concepts.  
4. The study also presents a cognitively inspired evaluation framework that integrates human judgments, reasoning from large language models (LLMs), and a geometric path-based difficulty score to measure the creativity and coherence of the generated blends.  
5. Experimental results demonstrate that Vibe Space outperforms existing methods, producing blends that humans perceive as more creative and coherent, highlighting its effectiveness for creative visual concept generation. <div>
arXiv:2512.14884v1 Announce Type: new 
Abstract: Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis</title>
<link>https://arxiv.org/abs/2512.14922</link>
<guid>https://arxiv.org/abs/2512.14922</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, Gleason grading, prostate cancer, robustness, PANDA-PLUS-Bench  

<br /><br />Summary:  
This article addresses the challenge of artificial intelligence foundation models potentially overfitting to specimen-specific artifacts instead of generalizable biological features in prostate cancer Gleason grading, a critical task influencing treatment decisions. To tackle this problem, the authors introduce PANDA-PLUS-Bench, a novel, expert-annotated benchmark dataset composed of nine whole slide images from unique patients with varying Gleason patterns. The dataset offers tissue patches at two resolutions (512x512 and 224x224 pixels) under eight augmentation conditions, explicitly designed to identify and quantify models' failure to distinguish biological signals from slide-level confounders. Seven foundation models were evaluated using this benchmark, revealing considerable disparities in robustness. Notably, Virchow2, despite low slide-level encoding, showed poor cross-slide accuracy, while HistoEncoder, trained specifically on prostate tissue data, achieved the best overall performance, suggesting that tissue-specific training benefits both biological feature extraction and slide-specific signature recognition. All models demonstrated measurable accuracy gaps between within-slide and cross-slide evaluations, with differences ranging from approximately 20 to 27 percentage points. To facilitate further research, the authors provide an open-source Google Colab notebook that enables standardized benchmarking of additional models against PANDA-PLUS-Bench. This work fills an essential gap by offering a dedicated resource for evaluating foundation model robustness in the clinically vital domain of prostate cancer Gleason grading. <div>
arXiv:2512.14922v1 Announce Type: new 
Abstract: Artificial intelligence foundation models are increasingly deployed for prostate cancer Gleason grading, where GP3/GP4 distinction directly impacts treatment decisions. However, these models may achieve high validation accuracy by learning specimen-specific artifacts rather than generalizable biological features, limiting real-world clinical utility. We introduce PANDA-PLUS-Bench, a curated benchmark dataset derived from expert-annotated prostate biopsies designed specifically to quantify this failure mode. The benchmark comprises nine carefully selected whole slide images from nine unique patients containing diverse Gleason patterns, with non-overlapping tissue patches extracted at both 512x512 and 224x224 pixel resolutions across eight augmentation conditions. Using this benchmark, we evaluate seven foundation models on their ability to separate biological signal from slide-level confounders. Our results reveal substantial variation in robustness across models: Virchow2 achieved the lowest slide-level encoding among large-scale models (81.0%) yet exhibited the second-lowest cross-slide accuracy (47.2%). HistoEncoder, trained specifically on prostate tissue, demonstrated the highest cross-slide accuracy (59.7%) and the strongest slide-level encoding (90.3%), suggesting tissue-specific training may enhance both biological feature capture and slide-specific signatures. All models exhibited measurable within-slide vs. cross-slide accuracy gaps, though the magnitude varied from 19.9 percentage points to 26.9 percentage points. We provide an open-source Google Colab notebook enabling researchers to evaluate additional foundation models against our benchmark using standardized metrics. PANDA-PLUS-Bench addresses a critical gap in foundation model evaluation by providing a purpose-built resource for robustness assessment in the clinically important context of Gleason grading.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Pre-trained Segmentation Models using Post-Processing</title>
<link>https://arxiv.org/abs/2512.14937</link>
<guid>https://arxiv.org/abs/2512.14937</guid>
<content:encoded><![CDATA[
<div> Gliomas, multiparametric MRI, segmentation, post-processing, sustainability<br /><br />Summary:<br /><br />1. Gliomas are the most common and lethal malignant brain tumors in adults, with survival rates under 15 months despite aggressive treatment.<br /><br />2. Accurate multiparametric MRI (mpMRI) segmentation of gliomas is essential for surgical planning, radiotherapy, and disease monitoring.<br /><br />3. Although deep learning models have improved automated segmentation accuracy, large-scale pretrained models often generalize poorly, leading to systematic errors such as false positives, label swaps, and slice discontinuities.<br /><br />4. Challenges are exacerbated by unequal GPU access and the high environmental cost associated with training large-scale models.<br /><br />5. The authors propose adaptive post-processing techniques to refine glioma segmentation results from large pretrained tumor segmentation models.<br /><br />6. These techniques were tested on BraTS 2025 segmentation challenges, improving the ranking metric by 14.9% in the sub-Saharan Africa challenge and 0.9% in the adult glioma challenge.<br /><br />7. This work encourages a shift from creating increasingly complex model architectures towards efficient, clinically relevant post-processing methods that are precise, computationally equitable, and more sustainable. <div>
arXiv:2512.14937v1 Announce Type: new 
Abstract: Gliomas are the most common malignant brain tumors in adults and are among the most lethal. Despite aggressive treatment, the median survival rate is less than 15 months. Accurate multiparametric MRI (mpMRI) tumor segmentation is critical for surgical planning, radiotherapy, and disease monitoring. While deep learning models have improved the accuracy of automated segmentation, large-scale pre-trained models generalize poorly and often underperform, producing systematic errors such as false positives, label swaps, and slice discontinuities in slices. These limitations are further compounded by unequal access to GPU resources and the growing environmental cost of large-scale model training. In this work, we propose adaptive post-processing techniques to refine the quality of glioma segmentations produced by large-scale pretrained models developed for various types of tumors. We demonstrated the techniques in multiple BraTS 2025 segmentation challenge tasks, with the ranking metric improving by 14.9 % for the sub-Saharan Africa challenge and 0.9% for the adult glioma challenge. This approach promotes a shift in brain tumor segmentation research from increasingly complex model architectures to efficient, clinically aligned post-processing strategies that are precise, computationally fair, and sustainable.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation</title>
<link>https://arxiv.org/abs/2512.14938</link>
<guid>https://arxiv.org/abs/2512.14938</guid>
<content:encoded><![CDATA[
<div> Keywords: TalkVerse, audio-driven talking video generation, large-scale dataset, DiT model, zero-shot video dubbing<br /><br />Summary:  
The paper introduces TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation, designed to enable fair and reproducible comparison across methods. The dataset contains 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours, curated from over 60k hours of video using a transparent pipeline including scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations such as 2D skeletons and structured visual/audio-style captions. Leveraging this dataset, the authors present a reproducible 5 billion parameter DiT baseline based on Wan2.2-5B. Their model utilizes a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context to achieve minute-long video generation with low drift. The approach delivers lip-sync and visual quality comparable to the 14B Wan-S2V model but requires 10 times lower inference cost. To improve storytelling in long videos, an MLLM director module is integrated to rewrite prompts based on audio and visual cues. Additionally, the model supports zero-shot video dubbing through controlled latent noise injection. The dataset, training recipes, and 5B model checkpoints are open-sourced to facilitate further research in audio-driven human video generation. <div>
arXiv:2512.14938v1 Announce Type: new 
Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Puzzle Curriculum GRPO for Vision-Centric Reasoning</title>
<link>https://arxiv.org/abs/2512.14944</link>
<guid>https://arxiv.org/abs/2512.14944</guid>
<content:encoded><![CDATA[
<div> Keywords: Puzzle Curriculum GRPO, Vision Language Models, reinforcement learning, self-supervised puzzles, reasoning consistency<br /><br />Summary:<br /><br />This paper addresses key challenges in reinforcement learning (RL) approaches for Vision Language Models (VLMs), specifically issues with dependence on expensive annotations or external verifiers, flat and sparse reward structures, and the logical inconsistency between chain-of-thought reasoning and final answers. The authors propose Puzzle Curriculum GRPO (PC-GRPO), a supervision-free RL method that utilizes Verifiable Rewards (RLVR) without requiring labels or external verification. PC-GRPO introduces three self-supervised puzzle environments—PatchFit, Rotation (binary rewards), and Jigsaw (graded partial credit)—to provide meaningful rewards and reduce sparsity. To improve training, it employs a difficulty-aware curriculum which dynamically adjusts sample weights, focusing on medium difficulty puzzles to maximize learning. The paper also monitors Reasoning-Answer Consistency (RAC) during post-training, revealing RAC initially improves but then declines; their curriculum delays this drop, and implementing consistency-enforcing reward mechanisms further enhances RAC, which correlates with better downstream task accuracy. Experiments conducted on Qwen-7B and Qwen-3B VLMs across multiple benchmarks demonstrate that PC-GRPO significantly improves reasoning abilities, training stability, and final task performance. Overall, this work offers a scalable, interpretable, and annotation-free reinforcement learning framework for advancing visual reasoning in VLMs. <div>
arXiv:2512.14944v1 Announce Type: new 
Abstract: Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities</title>
<link>https://arxiv.org/abs/2512.14961</link>
<guid>https://arxiv.org/abs/2512.14961</guid>
<content:encoded><![CDATA[
<div> Keywords: person recognition, trimodal system, multi-task learning, cross-attention fusion, robustness to modality loss  

<br /><br />Summary: This paper addresses the challenges of person recognition in real-world scenarios where audio, visual, or behavioral data modalities are often missing or degraded. The authors propose a Trimodal person identification framework that integrates voice, face, and gesture modalities to improve robustness. Their method employs multi-task learning to independently process each modality and utilizes cross-attention alongside gated fusion mechanisms to enable effective interaction across these modalities. A key innovation is a confidence-weighted fusion strategy that dynamically adjusts to missing or low-quality data, ensuring strong performance even when only one or two modalities are available. The system is evaluated on a newly introduced interview-based multimodal dataset named CANDOR, marking the first benchmark for this dataset, where it achieves an impressive 99.18% Top-1 accuracy. Additionally, the model is tested on the well-known VoxCeleb1 dataset, attaining 99.92% accuracy in bimodal settings. The results highlight the proposed system's superiority over traditional unimodal and late-fusion approaches. The code and data accompanying this work have been made publicly available, promoting further research and application development in robust multimodal person recognition systems. <div>
arXiv:2512.14961v1 Announce Type: new 
Abstract: Person recognition systems often rely on audio, visual, or behavioral cues, but real-world conditions frequently result in missing or degraded modalities. To address this challenge, we propose a Trimodal person identification framework that integrates voice, face, and gesture modalities, while remaining robust to modality loss. Our approach leverages multi-task learning to process each modality independently, followed by a cross-attention and gated fusion mechanisms to facilitate interaction across modalities. Moreover, a confidence-weighted fusion strategy dynamically adapts to missing and low-quality data, ensuring optimal classification even in Unimodal or Bimodal scenarios. We evaluate our method on CANDOR, a newly introduced interview-based multimodal dataset, which we benchmark for the first time. Our results demonstrate that the proposed Trimodal system achieves 99.18% Top-1 accuracy on person identification tasks, outperforming conventional Unimodal and late-fusion approaches. In addition, we evaluate our model on the VoxCeleb1 dataset as a benchmark and reach 99.92% accuracy in Bimodal mode. Moreover, we show that our system maintains high accuracy even when one or two modalities are unavailable, making it a robust solution for real-world person recognition applications. The code and data for this work are publicly available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Where is the Watermark? Interpretable Watermark Detection at the Block Level</title>
<link>https://arxiv.org/abs/2512.14994</link>
<guid>https://arxiv.org/abs/2512.14994</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, image watermarking, discrete wavelet transform, interpretability, robustness

<br /><br />Summary:  
This paper addresses the challenges posed by generative AI in creating realistic digital content, emphasizing concerns about authenticity, ownership, and misuse. Traditional image watermarking schemes typically provide global detection scores without clarifying where or how watermarks exist within images, limiting transparency and interpretability. To overcome this, the authors propose a post-hoc image watermarking method that combines localized embedding with region-level interpretability, allowing more detailed insight into watermarked areas. Their approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy, enabling generation of detection maps that identify watermarked or altered regions. Experiments demonstrate that the method achieves strong robustness against common image transformations such as cropping, even up to half the image, while maintaining sensitivity to semantic manipulations. Importantly, the watermark remains imperceptible to users, preserving image quality. Compared to prior post-hoc watermarking methods, this technique provides improved interpretability of detections without sacrificing robustness. Overall, the work presents a compelling balance between invisible watermark embedding, reliable tampering detection, and meaningful interpretability, enhancing user trust and utility in watermark verification for digitally generated images. <div>
arXiv:2512.14994v1 Announce Type: new 
Abstract: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle</title>
<link>https://arxiv.org/abs/2512.14998</link>
<guid>https://arxiv.org/abs/2512.14998</guid>
<content:encoded><![CDATA[
<div> Keywords: precision livestock farming, social behavior, pose estimation, interaction classification, computer vision<br /><br />Summary:<br /><br />1. The study addresses the challenge of accurately assessing social interactions among livestock in commercial barns, which is critical for monitoring herd welfare. Traditional methods relying on static proximity thresholds fail to distinguish between affiliative (friendly) and agonistic (aggressive) behaviors in complex environments.<br /><br />2. The authors introduce a novel pose-based computational framework that leverages spatiotemporal geometry of anatomical keypoints rather than simple distance or pixel appearance cues to classify social interactions.<br /><br />3. The system integrates a series of computer vision components: YOLOv11 for object detection achieving a mean average precision (mAP@0.50) of 96.24%, a supervised individual identification module with 98.24% accuracy, ByteTrack for multi-object tracking with 81.96% accuracy, and ZebraPose for 27-point anatomical keypoint estimation.<br /><br />4. A support vector machine classifier trained on dynamic pose-derived distance features successfully differentiates affiliative from agonistic behaviors, reaching 77.51% accuracy on annotated clips from a commercial dairy barn.<br /><br />5. Compared to proximity-only baselines, the pose-based approach shows substantial improvement in discriminating social behaviors, especially affiliative interactions, confirming its potential for real-time, automated social network analysis in precision livestock farming using commodity hardware. <div>
arXiv:2512.14998v1 Announce Type: new 
Abstract: Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation</title>
<link>https://arxiv.org/abs/2512.15006</link>
<guid>https://arxiv.org/abs/2512.15006</guid>
<content:encoded><![CDATA[
<div> Video Question Generation, Expert Knowledge, Question Quality, EgoExoAsk Dataset, Question-to-Answer Retrieval<br /><br />Summary:<br /><br />1. This paper focuses on improving Video Question Generation (VQG) by emphasizing the quality of questions generated to elicit unseen knowledge from human experts rather than just evaluating answerability. <br /><br />2. It introduces a novel evaluation protocol that simulates a question-answering communication process with human experts, using a question-to-answer retrieval mechanism to quantitatively assess question effectiveness. <br /><br />3. To support this approach, the authors have created EgoExoAsk, a new dataset consisting of 27,666 question-answer pairs derived from expert commentary annotations on the Ego-Exo4D video dataset. <br /><br />4. The EgoExoAsk training set is used to train the retriever model, and the benchmark evaluation is conducted on validation segments of Ego-Exo4D videos, ensuring a realistic and challenging testing scenario. <br /><br />5. Experimental results show that the proposed evaluation metric correlates with model configurations accessing richer contextual information, validating the effectiveness of the protocol. The EgoExoAsk dataset and resources are publicly available for further research and continuous improvement of VQG models. <div>
arXiv:2512.15006v1 Announce Type: new 
Abstract: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Agnostic Preference Optimization for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.15009</link>
<guid>https://arxiv.org/abs/2512.15009</guid>
<content:encoded><![CDATA[
<div> Preference optimization, medical image segmentation, model-agnostic, Dropout, boundary adherence<br /><br />Summary: Preference optimization provides a scalable supervision paradigm based on relative preference signals but has been limited in medical image segmentation due to model-specific designs and low-diversity prediction sampling. This paper introduces MAPO (Model-Agnostic Preference Optimization), a novel training framework that leverages Dropout-driven stochastic segmentation hypotheses to generate preference-consistent gradients without requiring direct ground-truth supervision. MAPO is designed to be fully architecture- and dimensionality-agnostic, enabling compatibility with various segmentation pipelines including both 2D and 3D CNNs as well as Transformer-based models. Extensive evaluations on a wide range of medical datasets demonstrate that MAPO consistently improves boundary adherence in segmentation results, which is critical for medical accuracy. Furthermore, it notably reduces overfitting, leading to better generalization on unseen data. Additionally, MAPO achieves more stable optimization dynamics compared to traditional supervised training methods, contributing to more reliable and robust model performance. Through these advantages, MAPO targets key challenges in medical image segmentation, offering a flexible and effective alternative supervision strategy that can be broadly adopted across different architectures and data dimensionalities. <div>
arXiv:2512.15009v1 Announce Type: new 
Abstract: Preference optimization offers a scalable supervision paradigm based on relative preference signals, yet prior attempts in medical image segmentation remain model-specific and rely on low-diversity prediction sampling. In this paper, we propose MAPO (Model-Agnostic Preference Optimization), a training framework that utilizes Dropout-driven stochastic segmentation hypotheses to construct preference-consistent gradients without direct ground-truth supervision. MAPO is fully architecture- and dimensionality-agnostic, supporting 2D/3D CNN and Transformer-based segmentation pipelines. Comprehensive evaluations across diverse medical datasets reveal that MAPO consistently enhances boundary adherence, reduces overfitting, and yields more stable optimization dynamics compared to conventional supervised training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance</title>
<link>https://arxiv.org/abs/2512.15048</link>
<guid>https://arxiv.org/abs/2512.15048</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, super-resolution, multi-view attention, epipolar constraint, auxiliary view selection<br /><br />Summary:<br /><br />1. The paper addresses the limitation of 3D Gaussian Splatting (3DGS) models trained on low-resolution images, which underperform when rendering high-resolution outputs. <br />2. Existing single-image super-resolution methods for 3DGS lack cross-view consistency and fail to effectively fuse information from multiple views, while video-based SR methods require sequential frames and are not suitable for unstructured multi-view data.<br />3. The authors propose MVGSR, a novel framework designed to enhance 3DGS super-resolution by integrating multi-view information while maintaining geometric consistency and high-frequency detail.<br />4. A key contribution is the Auxiliary View Selection Method based on camera poses, enabling flexible handling of arbitrarily organized multi-view datasets without reliance on temporal continuity or frame ordering.<br />5. MVGSR incorporates, for the first time, an epipolar-constrained multi-view attention mechanism that selectively aggregates consistent information from auxiliary views, improving fidelity and geometric coherence.<br />6. Extensive experiments demonstrate that MVGSR outperforms prior state-of-the-art methods in both object-centric and scene-level 3DGS super-resolution benchmarks, validating the effectiveness of the proposed strategies. <div>
arXiv:2512.15048v1 Announce Type: new 
Abstract: Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement</title>
<link>https://arxiv.org/abs/2512.15055</link>
<guid>https://arxiv.org/abs/2512.15055</guid>
<content:encoded><![CDATA[
<div> Keywords: event camera, LED markers, high-frequency deformation, noise filtering, monocular measurement  

<br /><br />Summary:  
This paper addresses the challenge of measuring high-frequency deformations in large-scale structures, which are often caused by complex loads and difficult to capture using traditional high-speed cameras due to harsh lighting and cost limitations. It introduces a novel measurement method leveraging an event camera in combination with blinking LED markers. The approach begins by filtering observation noise based on the unique characteristics of the event stream generated by LED blinking and utilizing spatiotemporal correlation to enhance signal quality. Subsequently, the method distinguishes between motion-induced events and those generated by LED blinking, enabling accurate extraction of LED markers even at high speeds. This separation is crucial for capturing fast-moving markers without confusion from structural motion. Finally, the monocular event camera system is used to measure the high-frequency planar deformations of the observed structure. Experiments validate the accuracy and effectiveness of the proposed method, demonstrating its potential as a low-cost, robust alternative to traditional high-speed cameras for deformation measurement under challenging conditions. <div>
arXiv:2512.15055v1 Announce Type: new 
Abstract: Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank</title>
<link>https://arxiv.org/abs/2512.15066</link>
<guid>https://arxiv.org/abs/2512.15066</guid>
<content:encoded><![CDATA[
<div> Keywords: ultrasound segmentation, wavelet filtering, memory bank, long video tracking, small object detection<br /><br />Summary:<br /><br />1. The paper addresses challenges in medical ultrasound video analysis, focusing on accurate lesion and organ segmentation essential for disease diagnosis and surgical planning.<br />2. Ultrasound videos suffer from low contrast and noisy backgrounds causing segmentation errors, especially at object boundaries and for small objects.<br />3. To overcome these issues, the authors propose a memory bank-based wavelet filtering and fusion network (MWNet) utilizing an encoder-decoder architecture to extract fine spatial features and high-frequency information.<br />4. Innovations include memory-based wavelet convolution capturing category and adjacent details, cascaded wavelet compression for multiscale frequency fusion and receptive field expansion, and a long short-term memory bank with cross-attention for tracking objects in long videos.<br />5. An HF-aware feature fusion module using adaptive wavelet filters is incorporated in the decoder to leverage boundary-sensitive high-frequency details.<br />6. Extensive benchmarking on four ultrasound video datasets (including thyroid nodules, thyroid gland, and heart datasets) shows significant improvements over state-of-the-art methods.<br />7. The method excels particularly in segmenting small thyroid nodules in long-duration videos.<br />8. The implementation code is publicly available at https://github.com/XiAooZ/MWNet, promoting reproducibility and further research. <div>
arXiv:2512.15066v1 Announce Type: new 
Abstract: Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMMD: A pose-guided multi-view multi-modal diffusion for person generation</title>
<link>https://arxiv.org/abs/2512.15069</link>
<guid>https://arxiv.org/abs/2512.15069</guid>
<content:encoded><![CDATA[
<div> Keywords: human image generation, multimodal diffusion, pose control, multi-view synthesis, text-conditioned

<br /><br />Summary: Generating consistent and photorealistic human images with controllable pose and appearance is critical for various applications such as virtual try-on, image editing, and digital human creation. The paper introduces Pose-guided Multi-view Multimodal Diffusion (PMMD), a novel diffusion-based framework that synthesizes person images by conditioning on multiple visual references, pose maps, and text prompts. PMMD features a multimodal encoder that effectively integrates visual views, pose information, and semantic text descriptions, minimizing cross-modal discrepancies and enhancing identity preservation. To improve local detail while maintaining the global structure of synthesized images, the authors propose a Residual Cross-View Attention (ResCVA) module. Additionally, a cross-modal fusion module is designed to incorporate image semantics and textual cues cohesively throughout the denoising diffusion process. Experimental results on the DeepFashion MultiModal dataset demonstrate that PMMD surpasses existing methods in terms of pose consistency, garment detail preservation, and controllability over appearance. The proposed framework successfully addresses common challenges like occlusions, garment style drift, and pose misalignment. The authors have made the project’s code and resources publicly available, facilitating further research and development in multimodal image synthesis with controllable human appearance and pose. <div>
arXiv:2512.15069v1 Announce Type: new 
Abstract: Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uni-Parser Technical Report</title>
<link>https://arxiv.org/abs/2512.15098</link>
<guid>https://arxiv.org/abs/2512.15098</guid>
<content:encoded><![CDATA[
<div> Keywords: Uni-Parser, document parsing, multi-expert architecture, GPU load balancing, large-scale cloud deployment

<br /><br />Summary: This technical report presents Uni-Parser, an advanced document parsing engine designed specifically for industrial-scale processing of scientific literature and patents. The system distinguishes itself from traditional pipeline-based methods by utilizing a modular, loosely coupled multi-expert architecture that maintains precise cross-modal alignments among text, equations, tables, figures, and chemical structures. This design also ensures easy extensibility to accommodate new modalities. Uni-Parser integrates adaptive GPU load balancing, distributed inference capabilities, and dynamic orchestration of modules, allowing for configurable modes that support either a holistic parsing approach or modality-specific parsing. Optimized for deployment on large-scale cloud infrastructure, Uni-Parser can process up to 20 PDF pages per second using eight NVIDIA RTX 4090D GPUs, delivering cost-efficient inference on a massive scale. Such scalability enables various downstream applications, including literature retrieval, summarization, and the extraction of specialized data such as chemical structures, reaction schemes, and bioactivity information. Additionally, Uni-Parser supports the curation of large corpora essential for training the next generation of large language models and AI-driven scientific discovery systems, thereby facilitating advancements in AI4Science research and applications. <div>
arXiv:2512.15098v1 Announce Type: new 
Abstract: This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets</title>
<link>https://arxiv.org/abs/2512.15110</link>
<guid>https://arxiv.org/abs/2512.15110</guid>
<content:encoded><![CDATA[
<div> Keywords: Nano Banana Pro, text-to-image generation, low-level vision, zero-shot evaluation, visual quality<br /><br />Summary:<br /><br />1. The paper evaluates Nano Banana Pro, a cutting-edge text-to-image generation model, for its capability in traditional low-level vision tasks.<br />2. A comprehensive zero-shot evaluation was performed on 14 different low-level vision tasks across 40 diverse datasets using simple text prompts without any fine-tuning.<br />3. Nano Banana Pro was benchmarked against state-of-the-art specialist models designed specifically for these tasks.<br />4. Results show a performance dichotomy: Nano Banana Pro outperforms specialists in subjective visual quality, often creating plausible high-frequency details that specialists miss.<br />5. However, it underperforms on standard reference-based quantitative metrics due to the stochastic and generative nature of the model, which challenges pixel-level consistency.<br />6. The study concludes that while Nano Banana Pro is a promising zero-shot all-rounder for low-level vision, bridging the gap to achieve the high fidelity and accuracy of dedicated domain specialists remains an open challenge. <div>
arXiv:2512.15110v1 Announce Type: new 
Abstract: The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding</title>
<link>https://arxiv.org/abs/2512.15126</link>
<guid>https://arxiv.org/abs/2512.15126</guid>
<content:encoded><![CDATA[
<div> 3D animation, proxy representation, appearance synthesis, motion control, low-power platforms<br /><br />Summary:<br /><br />This paper addresses the challenges in 3D animation production, which is traditionally labor-intensive, requires expert knowledge, and is computationally expensive. It highlights that existing AI-generated content (AIGC) approaches either replicate the expensive full 3D pipelines or use video-synthesis methods that compromise 3D control and interactivity. The authors focus on single-image 3D animation generation and identify a key trade-off between rendering quality and 3D control as a fundamental bottleneck. To overcome this, they propose a lightweight 3D animation framework that separates geometric control from appearance synthesis. The core innovation is a 2D-3D aligned proxy representation employing a coarse 3D estimate to guide structural aspects, while learned image-space generative models handle detailed appearance and view synthesis. This design allows for precise 3D-aware motion control analogous to classical systems but without the need for accurate geometry or costly optimization. Additionally, the method supports coherent background animation naturally. Experimental results show the framework delivers efficient animation generation on low-power devices and surpasses video-based 3D animation methods in preserving identity, maintaining geometric and textural consistency, and offering fine-grained, interactive user control. <div>
arXiv:2512.15126v1 Announce Type: new 
Abstract: 3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.
  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Borrowing from anything: A generalizable framework for reference-guided instance editing</title>
<link>https://arxiv.org/abs/2512.15138</link>
<guid>https://arxiv.org/abs/2512.15138</guid>
<content:encoded><![CDATA[
<div> Keywords: reference-guided instance editing, semantic disentanglement, Spatial Alignment Module, Adaptive Residual Scaling Module, Progressive Attention Fusion  

<br /><br />Summary:  
This paper addresses the challenge of semantic entanglement in reference-guided instance editing, where intrinsic appearance features of a reference are mixed with extrinsic attributes, complicating the editing process. The authors propose GENIE, a Generalizable Instance Editing framework designed to achieve explicit disentanglement between intrinsic and extrinsic information. GENIE incorporates three main components: first, a Spatial Alignment Module (SAM) corrects spatial misalignments between the reference and the target, ensuring better correspondence. Second, an Adaptive Residual Scaling Module (ARSM) identifies and amplifies salient intrinsic cues in the reference image while suppressing irrelevant extrinsic attributes, effectively learning what information should be borrowed. Third, a Progressive Attention Fusion (PAF) mechanism governs how the borrowed appearance is rendered onto the target, preserving the target's original structure. The framework is tested extensively on the challenging AnyInsertion dataset, where it demonstrates state-of-the-art performance in terms of fidelity and robustness. By explicitly disentangling features and addressing how to transfer and apply appearance details properly, GENIE sets a new benchmark in instance editing tasks that rely on reference guidance. This work significantly advances capabilities in controlled and semantically faithful image editing. <div>
arXiv:2512.15138v1 Announce Type: new 
Abstract: Reference-guided instance editing is fundamentally limited by semantic entanglement, where a reference's intrinsic appearance is intertwined with its extrinsic attributes. The key challenge lies in disentangling what information should be borrowed from the reference, and determining how to apply it appropriately to the target. To tackle this challenge, we propose GENIE, a Generalizable Instance Editing framework capable of achieving explicit disentanglement. GENIE first corrects spatial misalignments with a Spatial Alignment Module (SAM). Then, an Adaptive Residual Scaling Module (ARSM) learns what to borrow by amplifying salient intrinsic cues while suppressing extrinsic attributes, while a Progressive Attention Fusion (PAF) mechanism learns how to render this appearance onto the target, preserving its structure. Extensive experiments on the challenging AnyInsertion dataset demonstrate that GENIE achieves state-of-the-art fidelity and robustness, setting a new standard for disentanglement-based instance editing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning</title>
<link>https://arxiv.org/abs/2512.15153</link>
<guid>https://arxiv.org/abs/2512.15153</guid>
<content:encoded><![CDATA[
<div> Keywords: Human Action Assessment, Chain-of-Thought Explanation, Video Dataset, Fitness and Martial Arts, Explainable Model<br /><br />Summary: This paper introduces a novel task called Human Action Form Assessment (AFA), aimed at evaluating whether human actions are performed in a standard manner and providing detailed, reasonable feedback for improvement. Existing video understanding methods mainly focus on identifying what and where the action occurs but lack the capability to assess action quality or standardization. To address the scarcity of datasets labeled for action standardization and explainability, the authors present a new large-scale dataset named CoT-AFA, which includes diverse fitness and martial arts videos with multi-level annotations supporting comprehensive analysis. A key innovation is the integration of a Chain-of-Thought explanation paradigm, where feedback is not isolated but involves a full reasoning process—from action step identification to outcome analysis and concrete solution proposals. The paper also proposes the Explainable Fitness Assessor framework that employs two parallel streams and a dynamic gating mechanism to combine visual and semantic inputs, enhancing assessment and explanation capabilities. Experimental results show significant improvements over baseline methods, including a 16.0% increase in CIDEr for explanation generation, 2.7% higher accuracy in action classification, and 2.1% boost in quality assessment. The dataset and code are publicly available, offering strong potential for advancing explainable human action evaluation research. <div>
arXiv:2512.15153v1 Announce Type: new 
Abstract: Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence</title>
<link>https://arxiv.org/abs/2512.15160</link>
<guid>https://arxiv.org/abs/2512.15160</guid>
<content:encoded><![CDATA[
<div> spatial cognition, multimodal reasoning, reinforcement learning, 3D perception, video keyframe selection<br /><br />Summary: The paper introduces EagleVision, a novel dual-stage framework aimed at improving spatial intelligence in vision-language models by addressing key challenges in spatial Chain-of-Thought (CoT) reasoning. First, it tackles the problem of building a global spatial perception within strict token budgets by employing a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact, geometry- and semantics-aware set of keyframes from long video sequences. Second, EagleVision formalizes spatial CoT as BEV-grounded pose querying, where the agent iteratively predicts object or camera poses on a bird’s-eye view (BEV) plane and retrieves nearest real video frames to verify these predictions. Third, the framework is trained purely through reinforcement learning guided by a spatial grounding reward, which quantitatively measures consistency between predicted poses and observed views, therefore explicitly associating 3D hypotheses with visual evidence. EagleVision demonstrates significant improvements over existing open-source vision-language models on the VSI-Bench benchmark, showcasing strong and generalizable spatial understanding. By integrating macro-level perception with micro-level verification and a well-designed reward function, EagleVision advances multimodal reasoning methods in spatial tasks without relying on black-box 3D reconstructions or compromising evidence traceability. <div>
arXiv:2512.15160v1 Announce Type: new 
Abstract: Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for "thinking with images" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis</title>
<link>https://arxiv.org/abs/2512.15171</link>
<guid>https://arxiv.org/abs/2512.15171</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal classification, renal biopsy, ultra-scale learning, glomerular disease, feature fusion

<br /><br />Summary: The paper presents CMUS-Net, a novel cross-modal ultra-scale learning network designed for automatic classification of multiple glomerular diseases using three types of renal biopsy images obtained from different modalities: transmission electron microscopy (TEM), optical microscopy (OM), and immunofluorescence microscopy (IM). The main challenge addressed is the significant scale difference between nanoscale TEM image features and microscale OM/IM features, which complicates effective feature fusion in existing models. CMUS-Net bridges this gap by integrating multiple ultrastructural information. A sparse multi-instance learning module is introduced to effectively aggregate features from TEM images, while a cross-modal scale attention module enhances feature interaction and boosts the extraction of pathological semantic information. Additionally, the framework employs multiple loss functions to balance the importance of different modalities, leading to improved classification accuracy. Evaluation on an in-house dataset demonstrates high performance, with an accuracy of 95.37±2.41%, AUC of 99.05±0.53%, and F1-score of 95.32±2.41%. The method excels over other multi-modal and multi-scale approaches and shows strong generalization in membranous nephropathy staging. The approach aligns with standard renal biopsy diagnostic procedures and marks the first instance of automatic multi-disease classification using tri-modal, dual-scale renal biopsy images. The related code is publicly available on GitHub. <div>
arXiv:2512.15171v1 Announce Type: new 
Abstract: Constructing a multi-modal automatic classification model based on three types of renal biopsy images can assist pathologists in glomerular multi-disease identification. However, the substantial scale difference between transmission electron microscopy (TEM) image features at the nanoscale and optical microscopy (OM) or immunofluorescence microscopy (IM) images at the microscale poses a challenge for existing multi-modal and multi-scale models in achieving effective feature fusion and improving classification accuracy. To address this issue, we propose a cross-modal ultra-scale learning network (CMUS-Net) for the auxiliary diagnosis of multiple glomerular diseases. CMUS-Net utilizes multiple ultrastructural information to bridge the scale difference between nanometer and micrometer images. Specifically, we introduce a sparse multi-instance learning module to aggregate features from TEM images. Furthermore, we design a cross-modal scale attention module to facilitate feature interaction, enhancing pathological semantic information. Finally, multiple loss functions are combined, allowing the model to weigh the importance among different modalities and achieve precise classification of glomerular diseases. Our method follows the conventional process of renal biopsy pathology diagnosis and, for the first time, performs automatic classification of multiple glomerular diseases including IgA nephropathy (IgAN), membranous nephropathy (MN), and lupus nephritis (LN) based on images from three modalities and two scales. On an in-house dataset, CMUS-Net achieves an ACC of 95.37+/-2.41%, an AUC of 99.05+/-0.53%, and an F1-score of 95.32+/-2.41%. Extensive experiments demonstrate that CMUS-Net outperforms other well-known multi-modal or multi-scale methods and show its generalization capability in staging MN. Code is available at https://github.com/SMU-GL-Group/MultiModal_lkx/tree/main.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving</title>
<link>https://arxiv.org/abs/2512.15181</link>
<guid>https://arxiv.org/abs/2512.15181</guid>
<content:encoded><![CDATA[
<div> Keywords: automated driving, object detection, safety evaluation, criticality metrics, DeepAccident dataset<br /><br />Summary: This paper addresses the paramount goal of ensuring safety in automated driving through accurate environmental perception. It emphasizes the need for safety-specific performance metrics in evaluating object detection systems, particularly focusing on distinguishing relevant objects from non-relevant ones via criticality or relevance metrics. The authors conduct the first comprehensive analysis of criticality metrics for safety evaluation by reviewing existing literature and identifying a range of applicable metrics. The effectiveness of these metrics is empirically validated using the DeepAccident dataset, which contains diverse safety-critical scenarios. To further enhance evaluation accuracy, the paper proposes two novel application strategies: bidirectional criticality rating and multi-metric aggregation. These strategies collectively improve the criticality classification accuracy by up to 100%. The findings highlight the significant potential of these approaches to advance the safety evaluation framework for object detection systems in automated vehicles, ultimately contributing to safer autonomous driving technologies. <div>
arXiv:2512.15181v1 Announce Type: new 
Abstract: Ensuring safety is the primary objective of automated driving, which necessitates a comprehensive and accurate perception of the environment. While numerous performance evaluation metrics exist for assessing perception capabilities, incorporating safety-specific metrics is essential to reliably evaluate object detection systems. A key component for safety evaluation is the ability to distinguish between relevant and non-relevant objects - a challenge addressed by criticality or relevance metrics. This paper presents the first in-depth analysis of criticality metrics for safety evaluation of object detection systems. Through a comprehensive review of existing literature, we identify and assess a range of applicable metrics. Their effectiveness is empirically validated using the DeepAccident dataset, which features a variety of safety-critical scenarios. To enhance evaluation accuracy, we propose two novel application strategies: bidirectional criticality rating and multi-metric aggregation. Our approach demonstrates up to a 100% improvement in terms of criticality classification accuracy, highlighting its potential to significantly advance the safety evaluation of object detection systems in automated vehicles.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust and Calibrated Detection of Authentic Multimedia Content</title>
<link>https://arxiv.org/abs/2512.15182</link>
<guid>https://arxiv.org/abs/2512.15182</guid>
<content:encoded><![CDATA[
<div> Keywords: deepfake detection, generative models, resynthesis framework, adversarial robustness, inversion techniques  

<br /><br />Summary:  
Generative models can create highly realistic synthetic content, known as deepfakes, which pose significant challenges to verifying digital media authenticity. Existing deepfake detection methods are unreliable primarily due to two major issues: first, distinguishing inauthentic content after the fact is often impossible, particularly for memorized samples, resulting in an unbounded false positive rate (FPR). Second, current detectors lack robustness since adversaries can easily adapt to them using limited computational resources, thus evading detection with near-perfect accuracy. To overcome these limitations, the authors propose a novel resynthesis framework designed to determine if a content sample is authentically original or if its authenticity can be plausibly denied. Their approach focuses on a high-precision and low-recall operating point aimed at defending against compute-restricted adversaries. The first key contribution of their work is demonstrating that their calibrated resynthesis method reliably verifies authentic samples while maintaining controllable and low false positive rates. The second contribution shows that their method achieves robustness to adversarial attacks under the same computational constraints, outperforming prior detection methods which fail under such conditions. Additionally, their approach is versatile, supporting multiple data modalities and integrating state-of-the-art inversion techniques for enhanced performance and generalizability. <div>
arXiv:2512.15182v1 Announce Type: new 
Abstract: Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ERIENet: An Efficient RAW Image Enhancement Network under Low-Light Environment</title>
<link>https://arxiv.org/abs/2512.15186</link>
<guid>https://arxiv.org/abs/2512.15186</guid>
<content:encoded><![CDATA[
<div> Keywords: RAW image enhancement, low-light imaging, multi-scale processing, green channel guidance, real-time performance<br /><br />Summary:  
This paper addresses the challenge of enhancing low-light RAW images efficiently and effectively. Firstly, the authors highlight that RAW images outperform sRGB images in low-light enhancement but existing methods typically process multi-scale information sequentially, limiting model efficiency and speed. Secondly, current approaches often neglect the unique advantages of the green channel in RAW data, which contains richer information for image reconstruction. To tackle these issues, the authors propose ERIENet, an efficient RAW Image Enhancement Network designed to process multi-scale features in parallel using a novel channel-aware residual dense block. This architecture reduces computational cost and boosts speed, enabling real-time processing. Additionally, ERIENet incorporates a green channel guidance branch specifically to leverage the abundant green channel information, improving reconstruction accuracy without significantly increasing parameter count or computation. Experiments conducted on standard low-light enhancement datasets demonstrate that ERIENet outperforms state-of-the-art methods in both image quality and efficiency. Notably, it achieves a high processing speed of over 146 frames per second for 4K-resolution images on a single NVIDIA GeForce RTX 3090 GPU with 24GB memory, making it suitable for practical, real-time applications in low-light imaging scenarios. <div>
arXiv:2512.15186v1 Announce Type: new 
Abstract: RAW images have shown superior performance than sRGB images in many image processing tasks, especially for low-light image enhancement. However, most existing methods for RAW-based low-light enhancement usually sequentially process multi-scale information, which makes it difficult to achieve lightweight models and high processing speeds. Besides, they usually ignore the green channel superiority of RAW images, and fail to achieve better reconstruction performance with good use of green channel information. In this work, we propose an efficient RAW Image Enhancement Network (ERIENet), which parallelly processes multi-scale information with efficient convolution modules, and takes advantage of rich information in green channels to guide the reconstruction of images. Firstly, we introduce an efficient multi-scale fully-parallel architecture with a novel channel-aware residual dense block to extract feature maps, which reduces computational costs and achieves real-time processing speed. Secondly, we introduce a green channel guidance branch to exploit the rich information within the green channels of the input RAW image. It increases the quality of reconstruction results with few parameters and computations. Experiments on commonly used low-light image enhancement datasets show that ERIENet outperforms state-of-the-art methods in enhancing low-light RAW images with higher effiency. It also achieves an optimal speed of over 146 frame-per-second (FPS) for 4K-resolution images on a single NVIDIA GeForce RTX 3090 with 24G memory.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion</title>
<link>https://arxiv.org/abs/2512.15211</link>
<guid>https://arxiv.org/abs/2512.15211</guid>
<content:encoded><![CDATA[
<div> Infrared image fusion, visible image fusion, no-reference metrics, noise trap, Target-Background Contrast (TBC)<br /><br />Summary:<br /><br />Infrared and visible image fusion is crucial for low-altitude UAV reconnaissance, enhancing target detection and tracking by combining thermal saliency with background texture details. Traditional no-reference quality metrics like Entropy (EN) and Average Gradient (AG) perform poorly in complex low-light conditions because they cannot distinguish between actual image details and high-frequency sensor noise. This limitation results in a "Noise Trap," where noisy images receive misleadingly high scores, adversely affecting fusion algorithm performance. To overcome this challenge, the authors propose the Target-Background Contrast (TBC) metric, inspired by Weber's Law. TBC shifts focus from global image statistics to the relative contrast between salient targets and the background, effectively penalizing noise while rewarding clear target visibility. Experimental results on the DroneVehicle dataset demonstrate that TBC aligns more closely with human visual perception under low-light and noisy conditions, providing a more reliable and accurate quality assessment standard for infrared-visible image fusion in UAV applications. This work addresses a critical limitation in existing evaluation metrics and offers a practical solution for improving image fusion outcomes in challenging environments. <div>
arXiv:2512.15211v1 Announce Type: new 
Abstract: Infrared and visible image fusion is a pivotal technology in low-altitude UAV reconnaissance missions, providing high-quality data support for downstream tasks such as target detection and tracking by integrating thermal saliency with background texture details.However, traditional no-reference metrics fail(Specifically,like Entropy (EN) and Average Gradient (AG)) in complex low-light environments. They often misinterpret high-frequency sensor noise as valid detail. This creates a "Noise Trap," paradoxically assigning higher scores to noisy images and misguiding fusion algorithms.To address this, we propose the Target-Background Contrast (TBC) metric. Inspired by Weber's Law, TBC focuses on the relative contrast of salient targets rather than global statistics. Unlike traditional metrics, TBC penalizes background noise and rewards target visibility. Experiments on the DroneVehicle dataset demonstrate that TBC aligns better with human perception and provides a reliable standard for low-altitude scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Camera to World: A Plug-and-Play Module for Human Mesh Transformation</title>
<link>https://arxiv.org/abs/2512.15212</link>
<guid>https://arxiv.org/abs/2512.15212</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D human mesh, camera rotation, world coordinate system, Mesh-Plug, root joint orientation  

<br /><br />Summary:  
1. The paper addresses the challenge of reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images, which is difficult due to the unknown camera rotation information.  
2. Existing methods typically assume zero camera rotation and reconstruct meshes in the camera coordinate system, but this leads to errors when the mesh is transformed to the world coordinate system.  
3. To solve this, the authors propose Mesh-Plug, a plug-and-play module designed to accurately convert human meshes from camera coordinates to world coordinates by estimating the camera rotation.  
4. The approach is human-centered, leveraging both RGB images and depth maps rendered from the initial mesh in order to predict camera pitch angle without relying on environmental cues.  
5. Mesh-Plug first trains a camera rotation prediction module focused on the human body’s spatial configuration, then uses these estimated parameters in a mesh adjustment module that refines the root joint orientation and body pose simultaneously.  
6. Extensive experiments on benchmark datasets SPEC-SYN and SPEC-MTP demonstrate that Mesh-Plug outperforms current state-of-the-art methods, achieving more accurate alignment of 3D human meshes in the world coordinate system. <div>
arXiv:2512.15212v1 Announce Type: new 
Abstract: Reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images remains challenging due to the lack of camera rotation information. While existing methods achieve promising results in the camera coordinate system by assuming zero camera rotation, this simplification leads to significant errors when transforming the reconstructed mesh to the world coordinate system. To address this challenge, we propose Mesh-Plug, a plug-and-play module that accurately transforms human meshes from camera coordinates to world coordinates. Our key innovation lies in a human-centered approach that leverages both RGB images and depth maps rendered from the initial mesh to estimate camera rotation parameters, eliminating the dependency on environmental cues. Specifically, we first train a camera rotation prediction module that focuses on the human body's spatial configuration to estimate camera pitch angle. Then, by integrating the predicted camera parameters with the initial mesh, we design a mesh adjustment module that simultaneously refines the root joint orientation and body pose. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on the benchmark datasets SPEC-SYN and SPEC-MTP.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLCFormer: Spectral-Local Context Transformer with Physics-Grounded Flare Synthesis for Nighttime Flare Removal</title>
<link>https://arxiv.org/abs/2512.15221</link>
<guid>https://arxiv.org/abs/2512.15221</guid>
<content:encoded><![CDATA[
<div> lens flare, nighttime, transformer, frequency domain, flare removal  

<br /><br />Summary:  
Lens flare is a common artifact in nighttime photography caused by strong light sources scattering within camera lenses, resulting in hazy streaks, halos, and glare that degrade image quality. Existing methods struggle to effectively remove nonuniform scattered flares, limiting their performance in complex real-world lighting conditions. To solve this problem, the authors propose SLCFormer, a spectral-local context transformer framework designed specifically for nighttime lens flare removal. SLCFormer includes two main components: the Frequency Fourier and Excitation Module (FFEM), which captures global contextual information in the frequency domain to model flare characteristics, and the Directionally-Enhanced Spatial Module (DESM), which enhances local spatial structures and directional features for precise flare removal. Additionally, they develop a ZernikeVAE-based scatter flare generation pipeline that synthesizes physically realistic flare artifacts with spatially varying point spread functions (PSFs), combining principles of optical physics with data-driven learning. Experiments conducted on the Flare7K++ dataset show that SLCFormer achieves state-of-the-art performance with superior quantitative metrics and visual quality compared to existing methods. Furthermore, the method generalizes robustly to real nighttime scenes containing complex flare patterns, demonstrating its practical effectiveness and adaptability. <div>
arXiv:2512.15221v1 Announce Type: new 
Abstract: Lens flare is a common nighttime artifact caused by strong light sources scattering within camera lenses, leading to hazy streaks, halos, and glare that degrade visual quality. However, existing methods usually fail to effectively address nonuniform scattered flares, which severely reduces their applicability to complex real-world scenarios with diverse lighting conditions. To address this issue, we propose SLCFormer, a novel spectral-local context transformer framework for effective nighttime lens flare removal. SLCFormer integrates two key modules: the Frequency Fourier and Excitation Module (FFEM), which captures efficient global contextual representations in the frequency domain to model flare characteristics, and the Directionally-Enhanced Spatial Module (DESM) for local structural enhancement and directional features in the spatial domain for precise flare removal. Furthermore, we introduce a ZernikeVAE-based scatter flare generation pipeline to synthesize physically realistic scatter flares with spatially varying PSFs, bridging optical physics and data-driven training. Extensive experiments on the Flare7K++ dataset demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches in both quantitative metrics and perceptual visual quality, and generalizing robustly to real nighttime scenes with complex flare artifacts.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Null-LoRA: Low-Rank Adaptation on Null Space</title>
<link>https://arxiv.org/abs/2512.15233</link>
<guid>https://arxiv.org/abs/2512.15233</guid>
<content:encoded><![CDATA[
<div> keywords: Null-LoRA, parameter-efficient fine-tuning, low-rank adaptation, null space, large-scale models<br /><br />Summary: Parameter-efficient fine-tuning methods such as LoRA have become prominent for adapting large-scale models efficiently to downstream tasks by modifying a limited number of parameters. Traditional approaches perform low-rank adaptation across the entire parameter space, which may include redundant or ineffective updates. Inspired by the existence of non-trivial null spaces in pre-trained models, the authors propose Null-space based Low-Rank Adaptation (Null-LoRA), a novel technique that freezes certain parts of the low-rank matrices to reduce redundancy and improve the effective rank of updates. By restricting the incremental updates entirely within the null space, Null-LoRA maximizes the use of parameter updates specific to new tasks, enhancing parameter efficiency. Extensive experiments on image-text retrieval and visual question answering benchmarks demonstrate that Null-LoRA outperforms state-of-the-art fine-tuning methods while requiring fewer parameters. This method thus represents a significant advancement in parameter-efficient transfer learning by leveraging the structural properties of pre-trained models to optimize adaptation without increasing complexity. <div>
arXiv:2512.15233v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning methods have gained considerable popularity for adapting large-scale models to downstream tasks, particularly LoRA and its variants. Existing methods perform low-rank adaptation over the full parameter space. However, fine-tuning within a subspace can achieve comparable effectiveness. Inspired by the observation that pre-trained models possess non-trivial null spaces, we propose Null-space based Low-Rank Adaptation (Null-LoRA). Null-LoRA effectively reduces redundancy and enhances effective rank by freezing portions of the low-rank matrices. To further improve parameter efficiency, Null-LoRA constrains the entire incremental update within the null space, maximizing the utilization of incremental updates to adapt to new task paradigms. Null-LoRA surpasses the state of the art with fewer parameters in extensive experiments across image-text retrieval and visual question answering tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification</title>
<link>https://arxiv.org/abs/2512.15249</link>
<guid>https://arxiv.org/abs/2512.15249</guid>
<content:encoded><![CDATA[
<div> Medical AI, intersectional bias, diagnostic certainty, fairness, multimodal vision-language models  

<br /><br />Summary:  
This study addresses the challenge of intersectional biases in medical AI systems, specifically multimodal vision-language models (VLMs), which tend to show reduced confidence and accuracy in diagnosing marginalized patient subgroups. Such biases result from demographically skewed data and varied diagnostic certainty distributions, often causing higher missed diagnosis rates among these groups. Traditional fairness interventions either fail to fully close these gaps or degrade overall diagnostic performance by forcing statistical parity. To overcome these issues, the authors propose a novel training framework named Cross-Modal Alignment Consistency (CMAC-MMD) that standardizes diagnostic certainty across intersectional subgroups without needing sensitive demographic data during inference, thus preserving patient privacy. The framework was evaluated on large-scale datasets: 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), as well as 10,000 fundus images for glaucoma detection (Harvard-FairVLMed). Performance was stratified by intersectional attributes such as age, gender, and race. Results showed that CMAC-MMD substantially reduced the missed diagnosis gap (difference in True Positive Rate, ΔTPR) while improving overall diagnostic accuracy (AUC): in dermatology, ΔTPR dropped from 0.50 to 0.26 and AUC rose from 0.94 to 0.97; in glaucoma detection, ΔTPR decreased from 0.41 to 0.31 with a slight AUC improvement from 0.71 to 0.72. This work presents a scalable and privacy-preserving approach to build equitable, high-stakes clinical AI systems that maintain or improve accuracy alongside reducing bias. <div>
arXiv:2512.15249v1 Announce Type: new 
Abstract: Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $\Delta$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $\Delta$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15254</link>
<guid>https://arxiv.org/abs/2512.15254</guid>
<content:encoded><![CDATA[
<div> Counting, Vision-Language Models, Object Enumeration, Visual Scenes, Benchmark

<br /><br />Summary:  
This study addresses the challenge of counting objects in visual scenes, a significant problem in computer vision. Traditional methods depend on specialized counting architectures trained on datasets with fixed object categories. However, the emergence of large-scale multimodal vision-language models (VLMs) offers a more flexible option for open-set object counting. The authors systematically compare state-of-the-art specialized counting models with generalist VLMs using two established counting datasets and a newly designed benchmark that allows fine-grained control over visual test properties. Results indicate that many VLMs can perform approximate enumeration of objects, sometimes equaling or outperforming specialized models. A key discovery is that counting accuracy improves when VLMs are prompted to create intermediate outputs, such as object locations and verbal labels, before enumerating items. Despite these advances, no model currently demonstrates reliable counting performance in highly complex visual scenes. This highlights the necessity for continued research aimed at developing AI systems capable of dependable object counting in real-world environments. <div>
arXiv:2512.15254v1 Announce Type: new 
Abstract: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement</title>
<link>https://arxiv.org/abs/2512.15261</link>
<guid>https://arxiv.org/abs/2512.15261</guid>
<content:encoded><![CDATA[
<div> Pan-sharpening, Multimodal Diffusion Transformer, Cross-modal fusion, Zero-shot super-resolution, Multimodal interleaved scanning<br /><br />Summary:<br /><br />1. The paper addresses pan-sharpening, aiming to generate high-resolution multispectral (HRMS) images by effectively fusing a high-resolution panchromatic (PAN) image with a low-resolution multispectral (MS) image.<br />2. Traditional CNN-based methods rely on channel-wise concatenation and fixed convolutions, which lack adaptability to varying spatial and spectral features, while existing cross-attention approaches, though enabling global interaction, suffer from high computational cost and diluted fine-grained correspondences.<br />3. The authors leverage recent Multimodal Diffusion Transformer (MMDiT) advances, which use in-context conditioning rather than cross-attention, to enable more direct and efficient cross-modal information exchange.<br />4. The proposed framework, MMMamba, is built on the Mamba architecture, ensuring linear computational complexity and strong cross-modal fusion capability, and it uniquely supports image super-resolution tasks in a zero-shot manner.<br />5. Additionally, the paper introduces a novel multimodal interleaved (MI) scanning mechanism that facilitates effective interactions between PAN and MS modalities, with extensive experiments demonstrating the framework’s superior performance over current state-of-the-art methods across several benchmarks and tasks. <div>
arXiv:2512.15261v1 Announce Type: new 
Abstract: Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.15310</link>
<guid>https://arxiv.org/abs/2512.15310</guid>
<content:encoded><![CDATA[
<div> Zero Shot Weakly Supervised Semantic Segmentation, SynthSeg Agents, Large Language Models, Synthetic Training Data, Semantic Segmentation

<br /><br />Summary:  
1. This paper introduces Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), a novel approach aiming to perform semantic segmentation without using any real training images, relying solely on synthetic data generated through Large Language Models (LLMs).  
2. The proposed framework, SynthSeg Agents, consists of two main components: the Self Refine Prompt Agent and the Image Generation Agent. The former iteratively creates diverse and semantically rich image prompts using refinement, memory, and exploration techniques, guided by CLIP similarity metrics and diversity filtering.  
3. The Image Generation Agent utilizes Vision Language Models (VLMs) to convert these prompts into synthetic images. High-quality images are selected through a frozen CLIP-based scoring model, ensuring semantic relevance and image fidelity.  
4. After synthetic data generation, a ViT-based classifier is trained to relabel the entire dataset with enhanced semantic precision, improving the training data's quality further.  
5. Experiments on benchmark datasets PASCAL VOC 2012 and COCO 2014 demonstrate that SynthSeg Agents achieve competitive semantic segmentation performance without using any real images during training, showcasing the potential for cost-efficient and scalable approaches driven by LLM-based agents. <div>
arXiv:2512.15310v1 Announce Type: new 
Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation</title>
<link>https://arxiv.org/abs/2512.15311</link>
<guid>https://arxiv.org/abs/2512.15311</guid>
<content:encoded><![CDATA[
<div> LiDAR image representation, cross-modality distillation, Bird's-Eye-View segmentation, panoramic camera, autonomous driving<br /><br />Summary:<br /><br />This paper introduces a novel cross-modality distillation framework designed specifically for single-panoramic-camera Bird's-Eye-View (BEV) segmentation, aiming to improve perception in autonomous driving systems. The authors propose a unique LiDAR image representation that fuses range, intensity, and ambient data channels, combined with a voxel-aligned view transformer to maintain spatial details while enabling efficient BEV processing. A Teacher network that fuses high-capacity LiDAR and camera inputs extracts rich spatial and semantic features, which are distilled into a lightweight Student network relying solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate the Teacher model outperforms existing camera-based BEV methods by a 25.6% IoU margin. The Student network, after distillation, achieves an 8.5% IoU improvement along with state-of-the-art inference speed of 31.2 FPS, making it suitable for real-time applications. Further evaluations on KITTI-360 with fisheye cameras validate the framework’s generalizability across different camera setups. Overall, this approach effectively reduces sensor complexity and deployment costs, providing a practical, efficient, and robust solution for BEV segmentation in real-world autonomous driving contexts. <div>
arXiv:2512.15311v1 Announce Type: new 
Abstract: We present the first cross-modality distillation framework specifically tailored for single-panoramic-camera Bird's-Eye-View (BEV) segmentation. Our approach leverages a novel LiDAR image representation fused from range, intensity and ambient channels, together with a voxel-aligned view transformer that preserves spatial fidelity while enabling efficient BEV processing. During training, a high-capacity LiDAR and camera fusion Teacher network extracts both rich spatial and semantic features for cross-modality knowledge distillation into a lightweight Student network that relies solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate that our teacher model significantly outperforms existing camera-based BEV segmentation methods, achieving a 25.6\% IoU improvement. Meanwhile, the distilled Student network attains competitive performance with an 8.5\% IoU gain and state-of-the-art inference speed of 31.2 FPS. Moreover, evaluations on KITTI-360 (two fisheye cameras) confirm that our distillation framework generalises to diverse camera setups, underscoring its feasibility and robustness. This approach reduces sensor complexity and deployment costs while providing a practical solution for efficient, low-cost BEV segmentation in real-world autonomous driving.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment</title>
<link>https://arxiv.org/abs/2512.15315</link>
<guid>https://arxiv.org/abs/2512.15315</guid>
<content:encoded><![CDATA[
<div> Motion artifacts, MRI quality, supervised contrastive learning, interpretability, affinity scores<br /><br />Summary:  
Motion artifacts significantly degrade MRI image quality, leading to increased patient recalls and inefficiencies. Existing automated quality assessment techniques typically offer only binary decisions (artifact present or not) and lack interpretability, making it challenging for clinicians to understand the severity of motion artifacts. To address this, the authors propose AutoMAC-MRI, an explainable framework designed to grade motion artifacts across diverse MRI contrasts and orientations. The method leverages supervised contrastive learning to develop a discriminative representation of motion severity in the feature space. Within this space, AutoMAC-MRI calculates grade-specific affinity scores, which measure an image’s proximity to defined motion severity grades, providing transparency and interpretability for the assigned grades. The framework was evaluated using over 5000 expert-annotated brain MRI slices collected from multiple contrasts and views. Experimental results demonstrate strong alignment between the affinity scores and expert labels, validating the scores as an interpretable indicator of motion severity. By combining accurate grade prediction with per-grade affinity scoring, AutoMAC-MRI facilitates inline MRI quality control, which has the potential to reduce unnecessary rescans and enhance workflow efficiency in clinical settings. <div>
arXiv:2512.15315v1 Announce Type: new 
Abstract: Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.15319</link>
<guid>https://arxiv.org/abs/2512.15319</guid>
<content:encoded><![CDATA[
<div> Few-shot anomaly detection, prototypical learning, context-aware segmentation, feature adaptation, FSAD performance<br /><br />Summary:<br /><br />Few-shot anomaly detection (FSAD) aims to identify anomalies within a specific category using a very limited number of normal samples. Existing FSAD approaches depend heavily on pre-trained features, but often neglect the domain gap between those features and the target anomaly detection scenario. To tackle this, the paper proposes PCSNet, a Prototypical Learning Guided Context-Aware Segmentation Network designed to reduce this domain gap and improve the descriptiveness of features in target settings, thereby boosting FSAD performance. PCSNet consists of two key components: a Prototypical Feature Adaptation (PFA) sub-network and a Context-Aware Segmentation (CAS) sub-network. The PFA module generates prototypical features that ensure normal class features remain compact and distinctly separated from anomalies, aided by a pixel-level disparity classification loss that highlights subtle anomalies. The CAS sub-network provides pixel-wise anomaly localization, utilizing pseudo anomalies during training to enhance detection accuracy. Experimental evaluation on benchmark datasets MVTec and MPDD demonstrates that PCSNet achieves impressive image-level AUROC scores of 94.9% and 80.2%, respectively, in an 8-shot setup. Additionally, real-world tests in automotive plastic part inspection validate PCSNet’s effectiveness in practical applications with limited training data. The authors provide the source code for reproducibility at their GitHub repository. <div>
arXiv:2512.15319v1 Announce Type: new 
Abstract: Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. Existing FSAD methods largely rely on pre-trained feature representations to detect anomalies, but the inherent domain gap between pre-trained representations and target FSAD scenarios is often overlooked. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap, thereby improving feature descriptiveness in target scenarios and enhancing FSAD performance. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples. Code is available at https://github.com/yuxin-jiang/PCSNet.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MECAD: A multi-expert architecture for continual anomaly detection</title>
<link>https://arxiv.org/abs/2512.15323</link>
<guid>https://arxiv.org/abs/2512.15323</guid>
<content:encoded><![CDATA[
<div> MECAD, continual anomaly detection, multi-expert architecture, coreset selection, incremental learning  

<br /><br />Summary:  
This paper introduces MECAD, a new approach for continual anomaly detection utilizing a multi-expert architecture. The system dynamically assigns experts to specific object classes by analyzing feature similarity, which allows for more specialized detection tailored to different categories. To retain knowledge of previously encountered classes, MECAD incorporates efficient memory management strategies. Key to its incremental learning capability is the use of an optimized coreset selection method coupled with a specialized replay buffer, enabling model updates without full retraining. Experimental evaluation on the MVTec AD dataset, which includes 15 diverse object categories, shows that the optimal configuration with five experts achieves an average AUROC score of 0.8259. This performance not only demonstrates strong anomaly detection ability but also significantly reduces knowledge degradation compared to single-expert models. The framework optimally balances computational efficiency, retention of specialized knowledge, and adaptability. Such a balance makes MECAD particularly suitable for industrial applications where product types and related anomaly patterns continuously evolve, requiring systems that can learn incrementally and maintain high detection performance over time. <div>
arXiv:2512.15323v1 Announce Type: new 
Abstract: In this paper we propose MECAD, a novel approach for continual anomaly detection using a multi-expert architecture. Our system dynamically assigns experts to object classes based on feature similarity and employs efficient memory management to preserve the knowledge of previously seen classes. By leveraging an optimized coreset selection and a specialized replay buffer mechanism, we enable incremental learning without requiring full model retraining. Our experimental evaluation on the MVTec AD dataset demonstrates that the optimal 5-expert configuration achieves an average AUROC of 0.8259 across 15 diverse object categories while significantly reducing knowledge degradation compared to single-expert approaches. This framework balances computational efficiency, specialized knowledge retention, and adaptability, making it well-suited for industrial environments with evolving product types.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.15326</link>
<guid>https://arxiv.org/abs/2512.15326</guid>
<content:encoded><![CDATA[
<div> Keywords: knowledge distillation, anomaly detection, image restoration, masked reverse knowledge distillation, overgeneralization  

<br /><br />Summary:  
This paper addresses the overgeneralization issue in knowledge distillation-based image anomaly detection and localization, which arises due to similarities between input and supervisory signals. To overcome this, the authors propose a novel approach called masked reverse knowledge distillation (MRKD). MRKD leverages two key techniques: image-level masking (ILM) and feature-level masking (FLM). ILM aids in capturing global contextual information by distinguishing input signals from supervisory signals, effectively transforming image reconstruction into an image restoration task. FLM further enhances the approach by injecting synthetic feature-level anomalies, ensuring that the learned representations retain sufficient local details. Together, these strategies strengthen the network's capacity to capture image context while reducing the likelihood of overgeneralization. Experimental results on the MVTec anomaly detection dataset demonstrate that MRKD achieves outstanding performance, with an image-level AU-ROC of 98.9%, pixel-level AU-ROC of 98.4%, and AU-PRO of 95.3%. Additionally, comprehensive ablation studies confirm the effectiveness of MRKD in mitigating overgeneralization, highlighting its superiority over existing methods in anomaly detection and localization tasks. <div>
arXiv:2512.15326v1 Announce Type: new 
Abstract: Knowledge distillation is an effective image anomaly detection and localization scheme. However, a major drawback of this scheme is its tendency to overly generalize, primarily due to the similarities between input and supervisory signals. In order to address this issue, this paper introduces a novel technique called masked reverse knowledge distillation (MRKD). By employing image-level masking (ILM) and feature-level masking (FLM), MRKD transforms the task of image reconstruction into image restoration. Specifically, ILM helps to capture global information by differentiating input signals from supervisory signals. On the other hand, FLM incorporates synthetic feature-level anomalies to ensure that the learned representations contain sufficient local information. With these two strategies, MRKD is endowed with stronger image context capture capacity and is less likely to be overgeneralized. Experiments on the widely-used MVTec anomaly detection dataset demonstrate that MRKD achieves impressive performance: image-level 98.9% AU-ROC, pixel-level 98.4% AU-ROC, and 95.3% AU-PRO. In addition, extensive ablation experiments have validated the superiority of MRKD in mitigating the overgeneralization problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-based module for accurately reading linear scales in a laboratory</title>
<link>https://arxiv.org/abs/2512.15327</link>
<guid>https://arxiv.org/abs/2512.15327</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-based models, quantitative measurements, linear scale reading, syringe measurement, robotic autonomy<br /><br />Summary:<br />1. Vision-based models have rapidly advanced and can perform tasks such as object detection, image classification, and instance segmentation with high accuracy.<br />2. However, models capable of extracting precise quantitative measurements from images, akin to human visual interpretation, remain uncommon.<br />3. For robots to operate fully autonomously in laboratory settings, they must develop basic skills including navigation, object handling, sample preparation, and reading measurements from instruments.<br />4. The paper proposes a human-inspired method to read measurements from linear scales, using syringes and measuring cylinders as test cases.<br />5. To handle syringes in random orientations, image transformations are applied to correct the orientation.<br />6. The system isolates the region containing the linear scale to enhance efficiency and robustness.<br />7. Feature extraction identifies key elements such as major markers, digit labels, and the level indicator's position.<br />8. These features are combined to compute the final measurement reading.<br />9. The system's readings were compared with human readings of the same instances, showing accurate correspondence, validating the approach for robotic autonomous measurement reading tasks. <div>
arXiv:2512.15327v1 Announce Type: new 
Abstract: Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics</title>
<link>https://arxiv.org/abs/2512.15340</link>
<guid>https://arxiv.org/abs/2512.15340</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D conversational head generation, causal modeling, multimodal fusion, TIMAR, diffusion head  

<br /><br />Summary:  
This paper introduces TIMAR (Turn-level Interleaved Masked AutoRegression), a novel causal framework designed for generating 3D conversational head movements that capture natural human interaction dynamics. 1) It addresses the bidirectional dynamics of human conversation by modeling speech and nonverbal cues such as head nods, gaze shifts, and facial expressions in an interleaved, turn-level manner rather than treating talking and listening as separate processes. 2) TIMAR utilizes multimodal fusion within each conversational turn, integrating audio and visual context effectively, and applies turn-level causal attention to maintain temporal coherence and accumulate conversational history. 3) A lightweight diffusion-based prediction module generates continuous and expressive 3D head dynamics that reflect both coordination between interlocutors and individual expressive variability. 4) The model was evaluated on the DualTalk benchmark, where it demonstrated significant improvements by reducing Fréchet Distance and mean squared error by 15-30%, indicating better realism and accuracy in head movement synthesis. 5) Furthermore, TIMAR maintains its performance on out-of-distribution data, highlighting robust generalization. The authors plan to release the source code on GitHub, facilitating further research and applications in expressive avatars and interactive robotics. <div>
arXiv:2512.15340v1 Announce Type: new 
Abstract: Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fr\'echet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models</title>
<link>https://arxiv.org/abs/2512.15347</link>
<guid>https://arxiv.org/abs/2512.15347</guid>
<content:encoded><![CDATA[
<div> Group Relative Policy Optimization, Reward Clustering, Optimal Variance Filtering, Proactive GRPO, Trajectory Pruning

<br /><br />Summary:  
This paper addresses the challenge of computational inefficiency in Group Relative Policy Optimization (GRPO) caused by large group sizes. First, the authors identify a "reward clustering" phenomenon where many trajectories tend to converge around the group mean reward, which limits optimization improvements. Second, they propose Optimal Variance Filtering (OVF), a heuristic method that selects a high-variance subset of trajectories, which can achieve better performance than using the whole unfiltered group. However, OVF as a static, post-sampling filter still incurs computational waste by sampling many trajectories that are ultimately discarded. To tackle this, the authors introduce Pro-GRPO, a dynamic framework that incorporates latent feature-based trajectory pruning directly into the sampling process through early termination of clustered reward trajectories. This reduces unnecessary computations. Additionally, Pro-GRPO uses an "Expand-and-Prune" strategy: it initially increases the sampling group size to enhance diversity and then applies multi-step OVF on latent representations to avoid heavy computational costs. Extensive experiments on diffusion- and flow-based generative models show that Pro-GRPO is a general and efficient approach to improve GRPO optimization while minimizing computational overhead. <div>
arXiv:2512.15347v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an "Expand-and-Prune" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis</title>
<link>https://arxiv.org/abs/2512.15369</link>
<guid>https://arxiv.org/abs/2512.15369</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D semantic segmentation, bridges, infrastructure inspection, domain gap, deep learning

<br /><br />Summary:  
This article introduces a novel dataset specifically designed for 3D semantic segmentation of bridges, addressing a critical need in infrastructure inspection and maintenance. The dataset includes high-resolution 3D scans of diverse bridge structures from multiple countries, with detailed semantic labeling for each structural component. Its primary goal is to facilitate accurate and automated segmentation of bridge parts, thereby improving structural health monitoring practices. To assess the dataset's usefulness, the authors evaluate the performance of three state-of-the-art 3D deep learning architectures on this task. Additionally, the paper investigates the domain gap caused by varying sensor technologies used to acquire the data, providing insights into how sensor diversity affects model accuracy. The study quantifies this domain gap and observes that performance degradation can be as high as 11.4% in mean Intersection over Union (mIoU) metrics when models are tested across data from different sensors. Despite this, all tested architectures demonstrate robust performance on the new dataset, indicating its practical value for advancing automated bridge inspection methods. This work contributes to improving reliability and automation in monitoring the structural integrity of bridges, which carries significant importance for public safety and infrastructure maintenance. <div>
arXiv:2512.15369v1 Announce Type: new 
Abstract: We propose a novel dataset that has been specifically designed for 3D semantic segmentation of bridges and the domain gap analysis caused by varying sensors. This addresses a critical need in the field of infrastructure inspection and maintenance, which is essential for modern society. The dataset comprises high-resolution 3D scans of a diverse range of bridge structures from various countries, with detailed semantic labels provided for each. Our initial objective is to facilitate accurate and automated segmentation of bridge components, thereby advancing the structural health monitoring practice. To evaluate the effectiveness of existing 3D deep learning models on this novel dataset, we conduct a comprehensive analysis of three distinct state-of-the-art architectures. Furthermore, we present data acquired through diverse sensors to quantify the domain gap resulting from sensor variations. Our findings indicate that all architectures demonstrate robust performance on the specified task. However, the domain gap can potentially lead to a decline in the performance of up to 11.4% mIoU.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emotion Recognition in Signers</title>
<link>https://arxiv.org/abs/2512.15376</link>
<guid>https://arxiv.org/abs/2512.15376</guid>
<content:encoded><![CDATA[
<div> keywords: sign language, emotion recognition, cross-lingual, dataset, hand motion  

<br /><br />Summary:  
This paper tackles two main challenges in recognizing emotions in sign language: the overlap between grammatical and affective facial expressions, and the scarcity of data for model training. To address these, the authors introduce eJSL, a new benchmark dataset for Japanese Sign Language emotion recognition, alongside BOBSL, a large British Sign Language dataset with subtitles, enabling a cross-lingual approach. The eJSL dataset consists of 1,092 video clips, featuring two signers expressing 78 distinct utterances across seven emotional states. The study empirically shows that leveraging textual emotion recognition from spoken language data helps mitigate the issue of limited sign language data. Additionally, it demonstrates that selecting appropriate temporal segments significantly impacts recognition performance. The incorporation of hand motion information further enhances the accuracy of emotion recognition in signers. Overall, the paper establishes a stronger baseline for emotion recognition in sign language than current spoken language large language models (LLMs), highlighting the benefits of cross-modal and cross-lingual strategies combined with fine-tuned temporal and motion cues. <div>
arXiv:2512.15376v1 Announce Type: new 
Abstract: Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball</title>
<link>https://arxiv.org/abs/2512.15386</link>
<guid>https://arxiv.org/abs/2512.15386</guid>
<content:encoded><![CDATA[
arXiv:2512.15386v1 Announce Type: new 
Abstract: Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt. To benchmark this task, a new self-curated dataset comprising 100,000 basketball video clips, over 300 hours of footage, and more than 2,000 manually annotated rebound events is presented. Comprehensive baseline results are reported using state-of-the-art action anticipation methods, representing the first application of deep learning techniques to basketball rebound prediction. Additionally, two complementary tasks, rebound classification and rebound spotting, are explored, demonstrating that this dataset supports a wide range of video understanding applications in basketball, for which no comparable datasets currently exist. Experimental results highlight both the feasibility and inherent challenges of anticipating rebounds, providing valuable insights into predictive modeling for dynamic multi-agent sports scenarios. By forecasting team possession before rebounds occur, this work enables applications in real-time automated broadcasting and post-game analysis tools to support decision-making.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering</title>
<link>https://arxiv.org/abs/2512.15396</link>
<guid>https://arxiv.org/abs/2512.15396</guid>
<content:encoded><![CDATA[
arXiv:2512.15396v1 Announce Type: new 
Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning</title>
<link>https://arxiv.org/abs/2512.15410</link>
<guid>https://arxiv.org/abs/2512.15410</guid>
<content:encoded><![CDATA[
arXiv:2512.15410v1 Announce Type: new 
Abstract: Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry</title>
<link>https://arxiv.org/abs/2512.15423</link>
<guid>https://arxiv.org/abs/2512.15423</guid>
<content:encoded><![CDATA[
arXiv:2512.15423v1 Announce Type: new 
Abstract: Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-GUI Technical Report</title>
<link>https://arxiv.org/abs/2512.15431</link>
<guid>https://arxiv.org/abs/2512.15431</guid>
<content:encoded><![CDATA[
arXiv:2512.15431v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning</title>
<link>https://arxiv.org/abs/2512.15433</link>
<guid>https://arxiv.org/abs/2512.15433</guid>
<content:encoded><![CDATA[
arXiv:2512.15433v1 Announce Type: new 
Abstract: Face recognition systems store face templates for efficient matching. Once leaked, these templates pose a threat: inverting them can yield photorealistic surrogates that compromise privacy and enable impersonation. Although existing research has achieved relatively realistic face template inversion, the reconstructed facial images exhibit over-smoothed facial-part attributes (eyes, nose, mouth) and limited transferability. To address this problem, we present CLIP-FTI, a CLIP-driven fine-grained attribute conditioning framework for face template inversion. Our core idea is to use the CLIP model to obtain the semantic embeddings of facial features, in order to realize the reconstruction of specific facial feature attributes. Specifically, facial feature attribute embeddings extracted from CLIP are fused with the leaked template via a cross-modal feature interaction network and projected into the intermediate latent space of a pretrained StyleGAN. The StyleGAN generator then synthesizes face images with the same identity as the templates but with more fine-grained facial feature attributes. Experiments across multiple face recognition backbones and datasets show that our reconstructions (i) achieve higher identification accuracy and attribute similarity, (ii) recover sharper component-level attribute semantics, and (iii) improve cross-model attack transferability compared to prior reconstruction attacks. To the best of our knowledge, ours is the first method to use additional information besides the face template attack to realize face template inversion and obtains SOTA results.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence</title>
<link>https://arxiv.org/abs/2512.15445</link>
<guid>https://arxiv.org/abs/2512.15445</guid>
<content:encoded><![CDATA[
arXiv:2512.15445v1 Announce Type: new 
Abstract: Automated extraction of individual plant branches from time-series imagery is essential for high-throughput phenotyping, yet it remains computationally challenging due to non-rigid growth dynamics and severe identity fragmentation within entangled canopies. To overcome these stage-dependent ambiguities, we propose ST-DETrack, a spatiotemporal-fusion dual-decoder network designed to preserve branch identity from budding to flowering. Our architecture integrates a spatial decoder, which leverages geometric priors such as position and angle for early-stage tracking, with a temporal decoder that exploits motion consistency to resolve late-stage occlusions. Crucially, an adaptive gating mechanism dynamically shifts reliance between these spatial and temporal cues, while a biological constraint based on negative gravitropism mitigates vertical growth ambiguities. Validated on a Brassica napus dataset, ST-DETrack achieves a Branch Matching Accuracy (BMA) of 93.6%, significantly outperforming spatial and temporal baselines by 28.9 and 3.3 percentage points, respectively. These results demonstrate the method's robustness in maintaining long-term identity consistency amidst complex, dynamic plant architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception</title>
<link>https://arxiv.org/abs/2512.15480</link>
<guid>https://arxiv.org/abs/2512.15480</guid>
<content:encoded><![CDATA[
arXiv:2512.15480v1 Announce Type: new 
Abstract: Wildlife object detection plays a vital role in biodiversity conservation, ecological monitoring, and habitat protection. However, this task is often challenged by environmental variability, visual similarities among species, and intra-class diversity. This study investigates the effectiveness of two individual deep learning architectures ResNet-101 and Inception v3 for wildlife object detection under such complex conditions. The models were trained and evaluated on a wildlife image dataset using a standardized preprocessing approach, which included resizing images to a maximum dimension of 800 pixels, converting them to RGB format, and transforming them into PyTorch tensors. A ratio of 70:30 training and validation split was used for model development. The ResNet-101 model achieved a classification accuracy of 94% and a mean Average Precision (mAP) of 0.91, showing strong performance in extracting deep hierarchical features. The Inception v3 model performed slightly better, attaining a classification accuracy of 95% and a mAP of 0.92, attributed to its efficient multi-scale feature extraction through parallel convolutions. Despite the strong results, both models exhibited challenges when detecting species with similar visual characteristics or those captured under poor lighting and occlusion. Nonetheless, the findings confirm that both ResNet-101 and Inception v3 are effective models for wildlife object detection tasks and provide a reliable foundation for conservation-focused computer vision applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting</title>
<link>https://arxiv.org/abs/2512.15488</link>
<guid>https://arxiv.org/abs/2512.15488</guid>
<content:encoded><![CDATA[
arXiv:2512.15488v1 Announce Type: new 
Abstract: Estimating 3D human poses from 2D images remains challenging due to occlusions and projective ambiguity. Multi-view learning-based approaches mitigate these issues but often fail to generalize to real-world scenarios, as large-scale multi-view datasets with 3D ground truth are scarce and captured under constrained conditions. To overcome this limitation, recent methods rely on 2D pose estimation combined with 2D-to-3D pose lifting trained on synthetic data. Building on our previous MPL framework, we propose RUMPL, a transformer-based 3D pose lifter that introduces a 3D ray-based representation of 2D keypoints. This formulation makes the model independent of camera calibration and the number of views, enabling universal deployment across arbitrary multi-view configurations without retraining or fine-tuning. A new View Fusion Transformer leverages learned fused-ray tokens to aggregate information along rays, further improving multi-view consistency. Extensive experiments demonstrate that RUMPL reduces MPJPE by up to 53% compared to triangulation and over 60% compared to transformer-based image-representation baselines. Results on new benchmarks, including in-the-wild multi-view and multi-person datasets, confirm its robustness and scalability. The framework's source code is available at https://github.com/aghasemzadeh/OpenRUMPL
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge</title>
<link>https://arxiv.org/abs/2512.15505</link>
<guid>https://arxiv.org/abs/2512.15505</guid>
<content:encoded><![CDATA[
arXiv:2512.15505v1 Announce Type: new 
Abstract: The LUMIR challenge represents an important benchmark for evaluating deformable image registration methods on large-scale neuroimaging data. While the challenge demonstrates that modern deep learning methods achieve competitive accuracy on T1-weighted MRI, it also claims exceptional zero-shot generalization to unseen contrasts and resolutions, assertions that contradict established understanding of domain shift in deep learning. In this paper, we perform an independent re-evaluation of these zero-shot claims using rigorous evaluation protocols while addressing potential sources of instrumentation bias. Our findings reveal a more nuanced picture: (1) deep learning methods perform comparably to iterative optimization on in-distribution T1w images and even on human-adjacent species (macaque), demonstrating improved task understanding; (2) however, performance degrades significantly on out-of-distribution contrasts (T2, T2*, FLAIR), with Cohen's d scores ranging from 0.7-1.5, indicating substantial practical impact on downstream clinical workflows; (3) deep learning methods face scalability limitations on high-resolution data, failing to run on 0.6 mm isotropic images, while iterative methods benefit from increased resolution; and (4) deep methods exhibit high sensitivity to preprocessing choices. These results align with the well-established literature on domain shift and suggest that claims of universal zero-shot superiority require careful scrutiny. We advocate for evaluation protocols that reflect practical clinical and research workflows rather than conditions that may inadvertently favor particular method classes.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.15508</link>
<guid>https://arxiv.org/abs/2512.15508</guid>
<content:encoded><![CDATA[
arXiv:2512.15508v1 Announce Type: new 
Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, "Off The Grid" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics</title>
<link>https://arxiv.org/abs/2512.15512</link>
<guid>https://arxiv.org/abs/2512.15512</guid>
<content:encoded><![CDATA[
arXiv:2512.15512v1 Announce Type: new 
Abstract: Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations</title>
<link>https://arxiv.org/abs/2512.15524</link>
<guid>https://arxiv.org/abs/2512.15524</guid>
<content:encoded><![CDATA[
arXiv:2512.15524v1 Announce Type: new 
Abstract: Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration</title>
<link>https://arxiv.org/abs/2512.15528</link>
<guid>https://arxiv.org/abs/2512.15528</guid>
<content:encoded><![CDATA[
arXiv:2512.15528v1 Announce Type: new 
Abstract: Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain</title>
<link>https://arxiv.org/abs/2512.15531</link>
<guid>https://arxiv.org/abs/2512.15531</guid>
<content:encoded><![CDATA[
arXiv:2512.15531v1 Announce Type: new 
Abstract: The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BLANKET: Anonymizing Faces in Infant Video Recordings</title>
<link>https://arxiv.org/abs/2512.15542</link>
<guid>https://arxiv.org/abs/2512.15542</guid>
<content:encoded><![CDATA[
arXiv:2512.15542v1 Announce Type: new 
Abstract: Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random face, compatible with the original identity, is generated via inpainting using a diffusion model. Second, the new identity is seamlessly incorporated into each video frame through temporally consistent face swapping with authentic expression transfer. The method is evaluated on a dataset of short video recordings of babies and is compared to the popular anonymization method, DeepPrivacy2. Key metrics assessed include the level of de-identification, preservation of facial attributes, impact on human pose estimation (as an example of a downstream task), and presence of artifacts. Both methods alter the identity, and our method outperforms DeepPrivacy2 in all other respects. The code is available as an easy-to-use anonymization demo at https://github.com/ctu-vras/blanket-infant-face-anonym.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.15560</link>
<guid>https://arxiv.org/abs/2512.15560</guid>
<content:encoded><![CDATA[
arXiv:2512.15560v1 Announce Type: new 
Abstract: The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation</title>
<link>https://arxiv.org/abs/2512.15564</link>
<guid>https://arxiv.org/abs/2512.15564</guid>
<content:encoded><![CDATA[
arXiv:2512.15564v1 Announce Type: new 
Abstract: Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors</title>
<link>https://arxiv.org/abs/2512.15577</link>
<guid>https://arxiv.org/abs/2512.15577</guid>
<content:encoded><![CDATA[
arXiv:2512.15577v1 Announce Type: new 
Abstract: In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion</title>
<link>https://arxiv.org/abs/2512.15581</link>
<guid>https://arxiv.org/abs/2512.15581</guid>
<content:encoded><![CDATA[
arXiv:2512.15581v1 Announce Type: new 
Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision</title>
<link>https://arxiv.org/abs/2512.15599</link>
<guid>https://arxiv.org/abs/2512.15599</guid>
<content:encoded><![CDATA[
arXiv:2512.15599v1 Announce Type: new 
Abstract: We introduce FlexAvatar, a method for creating high-quality and complete 3D head avatars from a single image. A core challenge lies in the limited availability of multi-view data and the tendency of monocular training to yield incomplete 3D head reconstructions. We identify the root cause of this issue as the entanglement between driving signal and target viewpoint when learning from monocular videos. To address this, we propose a transformer-based 3D portrait animation model with learnable data source tokens, so-called bias sinks, which enables unified training across monocular and multi-view datasets. This design leverages the strengths of both data sources during inference: strong generalization from monocular data and full 3D completeness from multi-view supervision. Furthermore, our training procedure yields a smooth latent avatar space that facilitates identity interpolation and flexible fitting to an arbitrary number of input observations. In extensive evaluations on single-view, few-shot, and monocular avatar creation tasks, we verify the efficacy of FlexAvatar. Many existing methods struggle with view extrapolation while FlexAvatar generates complete 3D head avatars with realistic facial animations. Website: https://tobias-kirschstein.github.io/flexavatar/
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition</title>
<link>https://arxiv.org/abs/2512.15603</link>
<guid>https://arxiv.org/abs/2512.15603</guid>
<content:encoded><![CDATA[
arXiv:2512.15603v1 Announce Type: new 
Abstract: Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Multi-view Camera Calibration from Dense Matches</title>
<link>https://arxiv.org/abs/2512.15608</link>
<guid>https://arxiv.org/abs/2512.15608</guid>
<content:encoded><![CDATA[
arXiv:2512.15608v1 Announce Type: new 
Abstract: Estimating camera intrinsics and extrinsics is a fundamental problem in computer vision, and while advances in structure-from-motion (SfM) have improved accuracy and robustness, open challenges remain. In this paper, we introduce a robust method for pose estimation and calibration. We consider a set of rigid cameras, each observing the scene from a different perspective, which is a typical camera setup in animal behavior studies and forensic analysis of surveillance footage. Specifically, we analyse the individual components in a structure-from-motion (SfM) pipeline, and identify design choices that improve accuracy. Our main contributions are: (1) we investigate how to best subsample the predicted correspondences from a dense matcher to leverage them in the estimation process. (2) We investigate selection criteria for how to add the views incrementally. In a rigorous quantitative evaluation, we show the effectiveness of our changes, especially for cameras with strong radial distortion (79.9% ours vs. 40.4 vanilla VGGT). Finally, we demonstrate our correspondence subsampling in a global SfM setting where we initialize the poses using VGGT. The proposed pipeline generalizes across a wide range of camera setups, and could thus become a useful tool for animal behavior and forensic analysis.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images</title>
<link>https://arxiv.org/abs/2512.15618</link>
<guid>https://arxiv.org/abs/2512.15618</guid>
<content:encoded><![CDATA[
arXiv:2512.15618v1 Announce Type: new 
Abstract: With the rapidly growing population of resident space objects (RSOs) in the near-Earth space environment, detailed information about their condition and capabilities is needed to provide Space Domain Awareness (SDA). Space-based sensing will enable inspection of RSOs at shorter ranges, independent of atmospheric effects, and from all aspects. The use of a sub-THz inverse synthetic aperture radar (ISAR) imaging and sensing system for SDA has been proposed in previous work, demonstrating the achievement of sub-cm image resolution at ranges of up to 100 km. This work focuses on recognition of external structures by use of sequential feature detection and tracking throughout the aligned ISAR images of the satellites. The Hough transform is employed to detect linear features, which are tracked throughout the sequence. ISAR imagery is generated via a metaheuristic simulator capable of modelling encounters for a variety of deployment scenarios. Initial frame-to-frame alignment is achieved through a series of affine transformations to facilitate later association between image features. A gradient-by-ratio method is used for edge detection within individual ISAR images, and edge magnitude and direction are subsequently used to inform a double-weighted Hough transform to detect features with high accuracy. Feature evolution during sequences of frames is analysed. It is shown that the use of feature tracking within sequences with the proposed approach will increase confidence in feature detection and classification, and an example use-case of robust detection of shadowing as a feature is presented.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence</title>
<link>https://arxiv.org/abs/2512.15621</link>
<guid>https://arxiv.org/abs/2512.15621</guid>
<content:encoded><![CDATA[
arXiv:2512.15621v1 Announce Type: new 
Abstract: Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: "what would happen given a specific future action". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Physically-Based Sky-Modeling For Image Based Lighting</title>
<link>https://arxiv.org/abs/2512.15632</link>
<guid>https://arxiv.org/abs/2512.15632</guid>
<content:encoded><![CDATA[
arXiv:2512.15632v1 Announce Type: new 
Abstract: Accurate environment maps are a key component for rendering photorealistic outdoor scenes with coherent illumination. They enable captivating visual arts, immersive virtual reality, and a wide range of engineering and scientific applications. Recent works have extended sky-models to be more comprehensive and inclusive of cloud formations but, as we demonstrate, existing methods fall short in faithfully recreating natural skies. Though in recent years the visual quality of DNN-generated High Dynamic Range Imagery (HDRI) has greatly improved, the environment maps generated by DNN sky-models do not re-light scenes with the same tones, shadows, and illumination as physically captured HDR imagery. In this work, we demonstrate progress in HDR literature to be tangential to sky-modelling as current works cannot support both photorealism and the 22 f-stops required for the Full Dynamic Range (FDR) of outdoor illumination. We achieve this by proposing AllSky, a flexible all-weather sky-model learned directly from physically captured HDRI which we leverage to study the input modalities, tonemapping, conditioning, and evaluation of sky-models. Per user-controlled positioning of the sun and cloud formations, AllSky expands on current functionality by allowing for intuitive user control over environment maps and achieves state-of-the-art sky-model performance. Through our proposed evaluation, we demonstrate existing DNN sky-models are not interchangeable with physically captured HDRI or parametric sky-models, with current limitations being prohibitive of scalability and accurate illumination in downstream applications
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning</title>
<link>https://arxiv.org/abs/2512.15635</link>
<guid>https://arxiv.org/abs/2512.15635</guid>
<content:encoded><![CDATA[
arXiv:2512.15635v1 Announce Type: new 
Abstract: We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization</title>
<link>https://arxiv.org/abs/2512.15644</link>
<guid>https://arxiv.org/abs/2512.15644</guid>
<content:encoded><![CDATA[
arXiv:2512.15644v1 Announce Type: new 
Abstract: Foreground-conditioned inpainting, which aims at generating a harmonious background for a given foreground subject based on the text prompt, is an important subfield in controllable image generation. A common challenge in current methods, however, is the occurrence of Spatial Relationship Hallucinations between the foreground subject and the generated background, including inappropriate scale, positional relationships, and viewpoints. Critically, the subjective nature of spatial rationality makes it challenging to quantify, hindering the use of traditional reward-based RLHF methods. To address this issue, we propose InpaintDPO, the first Direct Preference Optimization (DPO) based framework dedicated to spatial rationality in foreground-conditioned inpainting, ensuring plausible spatial relationships between foreground and background elements. To resolve the gradient conflicts in standard DPO caused by identical foreground in win-lose pairs, we propose MaskDPO, which confines preference optimization exclusively to the background to enhance background spatial relationships, while retaining the inpainting loss in the foreground region for robust foreground preservation. To enhance coherence at the foreground-background boundary, we propose Conditional Asymmetric Preference Optimization, which samples pairs with differentiated cropping operations and applies global preference optimization to promote contextual awareness and enhance boundary coherence. Finally, based on the observation that winning samples share a commonality in plausible spatial relationships, we propose Shared Commonality Preference Optimization to enhance the model's understanding of spatial commonality across high-quality winning samples, further promoting shared spatial rationality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift</title>
<link>https://arxiv.org/abs/2512.15647</link>
<guid>https://arxiv.org/abs/2512.15647</guid>
<content:encoded><![CDATA[
arXiv:2512.15647v1 Announce Type: new 
Abstract: Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 42.7% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 9.0%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label-dominated training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?</title>
<link>https://arxiv.org/abs/2512.15649</link>
<guid>https://arxiv.org/abs/2512.15649</guid>
<content:encoded><![CDATA[
arXiv:2512.15649v1 Announce Type: new 
Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stylized Synthetic Augmentation further improves Corruption Robustness</title>
<link>https://arxiv.org/abs/2512.15675</link>
<guid>https://arxiv.org/abs/2512.15675</guid>
<content:encoded><![CDATA[
arXiv:2512.15675v1 Announce Type: new 
Abstract: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</title>
<link>https://arxiv.org/abs/2512.15693</link>
<guid>https://arxiv.org/abs/2512.15693</guid>
<content:encoded><![CDATA[
arXiv:2512.15693v1 Announce Type: new 
Abstract: The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</title>
<link>https://arxiv.org/abs/2512.15701</link>
<guid>https://arxiv.org/abs/2512.15701</guid>
<content:encoded><![CDATA[
arXiv:2512.15701v1 Announce Type: new 
Abstract: Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</title>
<link>https://arxiv.org/abs/2512.15702</link>
<guid>https://arxiv.org/abs/2512.15702</guid>
<content:encoded><![CDATA[
arXiv:2512.15702v1 Announce Type: new 
Abstract: Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</title>
<link>https://arxiv.org/abs/2512.15707</link>
<guid>https://arxiv.org/abs/2512.15707</guid>
<content:encoded><![CDATA[
arXiv:2512.15707v1 Announce Type: new 
Abstract: Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-View Foundation Models</title>
<link>https://arxiv.org/abs/2512.15708</link>
<guid>https://arxiv.org/abs/2512.15708</guid>
<content:encoded><![CDATA[
arXiv:2512.15708v1 Announce Type: new 
Abstract: Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</title>
<link>https://arxiv.org/abs/2512.15711</link>
<guid>https://arxiv.org/abs/2512.15711</guid>
<content:encoded><![CDATA[
arXiv:2512.15711v1 Announce Type: new 
Abstract: We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</title>
<link>https://arxiv.org/abs/2512.15713</link>
<guid>https://arxiv.org/abs/2512.15713</guid>
<content:encoded><![CDATA[
arXiv:2512.15713v1 Announce Type: new 
Abstract: In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Pursuit of Pixel Supervision for Visual Pre-training</title>
<link>https://arxiv.org/abs/2512.15715</link>
<guid>https://arxiv.org/abs/2512.15715</guid>
<content:encoded><![CDATA[
arXiv:2512.15715v1 Announce Type: new 
Abstract: At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatia: Video Generation with Updatable Spatial Memory</title>
<link>https://arxiv.org/abs/2512.15716</link>
<guid>https://arxiv.org/abs/2512.15716</guid>
<content:encoded><![CDATA[
arXiv:2512.15716v1 Announce Type: new 
Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM as a Neural Architect: Controlled Generation of Image Captioning Models Under Strict API Contracts</title>
<link>https://arxiv.org/abs/2512.14706</link>
<guid>https://arxiv.org/abs/2512.14706</guid>
<content:encoded><![CDATA[
arXiv:2512.14706v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) traditionally requires significant human expertise or automated trial-and-error to design deep learning models. We present NN-Caption, an LLM-guided neural architecture search pipeline that generates runnable image-captioning models by composing CNN encoders from LEMUR's classification backbones with sequence decoders (LSTM/GRU/Transformer) under a strict Net API. Using DeepSeek-R1-0528-Qwen3-8B as the primary generator, we present the prompt template and examples of generated architectures. We evaluate on MS COCO with BLEU-4. The LLM generated dozens of captioning models, with over half successfully trained and producing meaningful captions. We analyse the outcomes of using different numbers of input model snippets (5 vs. 10) in the prompt, finding a slight drop in success rate when providing more candidate components. We also report training dynamics (caption accuracy vs. epochs) and the highest BLEU-4 attained. Our results highlight the promise of LLM-guided NAS: the LLM not only proposes architectures but also suggests hyperparameters and training practices. We identify the challenges encountered (e.g., code hallucinations or API compliance issues) and detail how prompt rules and iterative code fixes addressed them. This work presents a pipeline that integrates prompt-based code generation with automatic evaluation, and adds dozens of novel captioning models to the open LEMUR dataset to facilitate reproducible benchmarking and downstream AutoML research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SepsisSuite: Beyond Risk Stratification -- A Comparative Analysis of Deep Fusion vs. Expert Stacking for Prescriptive Sepsis AI</title>
<link>https://arxiv.org/abs/2512.14712</link>
<guid>https://arxiv.org/abs/2512.14712</guid>
<content:encoded><![CDATA[
arXiv:2512.14712v1 Announce Type: cross 
Abstract: Sepsis accounts for nearly 20% of global ICU admissions, yet conventional prediction models often fail to effectively integrate heterogeneous data streams, remaining either siloed by modality or reliant on brittle early fusion. In this work, we present a rigorous architectural comparison between End-to-End Deep Fusion and Context-Aware Stacking for sepsis tasks. We initially hypothesized that a novel Quad-Modal Hierarchical Gated Attention Network -- termed SepsisFusionFormer -- would resolve complex cross-modal interactions between vitals, text, and imaging. However, experiments on MIMIC-IV revealed that SepsisFusionFormer suffered from "attention starvation" in the small antibiotic cohort ($N \approx 2,100$), resulting in overfitting (AUC 0.66). This counterintuitive result informed the design of SepsisLateFusion, a "leaner" Context-Aware Mixture-of-Experts (MoE) architecture. By treating modalities as orthogonal experts -- the "Historian" (Static), the "Monitor" (Temporal), and the "Reader" (NLP) -- and dynamically gating them via a CatBoost meta-learner, we achieved State-of-the-Art (SOTA) performance: 0.915 AUC for prediction 4 hours prior to clinical onset. By calibrating the decision threshold for clinical safety, we reduced missed cases by 48% relative to the default operating point, thus opening a true preventative window for timely intervention over reactive alerts. Furthermore, for the novel prescriptive task of multi-class antibiotic selection, we demonstrate that a Quad-Modal Ensemble achieved the highest performance (0.72 AUC). These models are integrated into SepsisSuite, a deployment-ready Python framework for clinical decision support. SepsisSuite is available for free at: https://github.com/RyanCartularo/SepsisSuite-Info
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT</title>
<link>https://arxiv.org/abs/2512.14732</link>
<guid>https://arxiv.org/abs/2512.14732</guid>
<content:encoded><![CDATA[
arXiv:2512.14732v1 Announce Type: cross 
Abstract: Incidental findings in CT scans, though often benign, can have significant clinical implications and should be reported following established guidelines. Traditional manual inspection by radiologists is time-consuming and variable. This paper proposes a novel framework that leverages large language models (LLMs) and foundational vision-language models (VLMs) in a plan-and-execute agentic approach to improve the efficiency and precision of incidental findings detection, classification, and reporting for abdominal CT scans. Given medical guidelines for abdominal organs, the process of managing incidental findings is automated through a planner-executor framework. The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections, via VLMs, segmentation models, and image processing subroutines.
  We demonstrate the effectiveness of our approach through experiments on a CT abdominal benchmark for three organs, in a fully automatic end-to-end manner. Our results show that the proposed framework outperforms existing pure VLM-based approaches in terms of accuracy and efficiency.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyFi: Toward Pyramid-like Financial Image Understanding for VLMs via Adversarial Agents</title>
<link>https://arxiv.org/abs/2512.14735</link>
<guid>https://arxiv.org/abs/2512.14735</guid>
<content:encoded><![CDATA[
arXiv:2512.14735v1 Announce Type: cross 
Abstract: This paper proposes PyFi, a novel framework for pyramid-like financial image understanding that enables vision language models (VLMs) to reason through question chains in a progressive, simple-to-complex manner. At the core of PyFi is PyFi-600K, a dataset comprising 600K financial question-answer pairs organized into a reasoning pyramid: questions at the base require only basic perception, while those toward the apex demand increasing levels of capability in financial visual understanding and expertise. This data is scalable because it is synthesized without human annotations, using PyFi-adv, a multi-agent adversarial mechanism under the Monte Carlo Tree Search (MCTS) paradigm, in which, for each image, a challenger agent competes with a solver agent by generating question chains that progressively probe deeper capability levels in financial visual reasoning. Leveraging this dataset, we present fine-grained, hierarchical, and comprehensive evaluations of advanced VLMs in the financial domain. Moreover, fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on the pyramid-structured question chains enables these models to answer complex financial questions by decomposing them into sub-questions with gradually increasing reasoning demands, yielding average accuracy improvements of 19.52% and 8.06%, respectively, on the dataset. All resources of code, dataset and models are available at: https://github.com/AgenticFinLab/PyFi .
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnification-Aware Distillation (MAD): A Self-Supervised Framework for Unified Representation Learning in Gigapixel Whole-Slide Images</title>
<link>https://arxiv.org/abs/2512.14796</link>
<guid>https://arxiv.org/abs/2512.14796</guid>
<content:encoded><![CDATA[
arXiv:2512.14796v1 Announce Type: cross 
Abstract: Whole-slide images (WSIs) contain tissue information distributed across multiple magnification levels, yet most self-supervised methods treat these scales as independent views. This separation prevents models from learning representations that remain stable when resolution changes, a key requirement for practical neuropathology workflows. This study introduces Magnification-Aware Distillation (MAD), a self-supervised strategy that links low-magnification context with spatially aligned high-magnification detail, enabling the model to learn how coarse tissue structure relates to fine cellular patterns. The resulting foundation model, MAD-NP, is trained entirely through this cross-scale correspondence without annotations. A linear classifier trained only on 10x embeddings maintains 96.7% of its performance when applied to unseen 40x tiles, demonstrating strong resolution-invariant representation learning. Segmentation outputs remain consistent across magnifications, preserving anatomical boundaries and minimizing noise. These results highlight the feasibility of scalable, magnification-robust WSI analysis using a unified embedding space
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence for the Assessment of Peritoneal Carcinosis during Diagnostic Laparoscopy for Advanced Ovarian Cancer</title>
<link>https://arxiv.org/abs/2512.14797</link>
<guid>https://arxiv.org/abs/2512.14797</guid>
<content:encoded><![CDATA[
arXiv:2512.14797v1 Announce Type: cross 
Abstract: Advanced Ovarian Cancer (AOC) is often diagnosed at an advanced stage with peritoneal carcinosis (PC). Fagotti score (FS) assessment at diagnostic laparoscopy (DL) guides treatment planning by estimating surgical resectability, but its subjective and operator-dependent nature limits reproducibility and widespread use. Videos of patients undergoing DL with concomitant FS assessments at a referral center were retrospectively collected and divided into a development dataset, for data annotation, AI training and evaluation, and an independent test dataset, for internal validation. In the development dataset, FS-relevant frames were manually annotated for anatomical structures and PC. Deep learning models were trained to automatically identify FS-relevant frames, segment structures and PC, and predict video-level FS and indication to surgery (ItS). AI performance was evaluated using Dice score for segmentation, F1-scores for anatomical stations (AS) and ItS prediction, and root mean square error (RMSE) for final FS estimation. In the development dataset, the segmentation model trained on 7,311 frames, achieved Dice scores of 70$\pm$3% for anatomical structures and 56$\pm$3% for PC. Video-level AS classification achieved F1-scores of 74$\pm$3% and 73$\pm$4%, FS prediction showed normalized RMSE values of 1.39$\pm$0.18 and 1.15$\pm$0.08, and ItS reached F1-scores of 80$\pm$8% and 80$\pm$2% in the development (n=101) and independent test datasets (n=50), respectively. This is the first AI model to predict the feasibility of cytoreductive surgery providing automated FS estimation from DL videos. Its reproducible and reliable performance across datasets suggests that AI can support surgeons through standardized intraoperative tumor burden assessment and clinical decision-making in AOC.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task Matrices: Linear Maps for Cross-Model Finetuning Transfer</title>
<link>https://arxiv.org/abs/2512.14880</link>
<guid>https://arxiv.org/abs/2512.14880</guid>
<content:encoded><![CDATA[
arXiv:2512.14880v1 Announce Type: cross 
Abstract: Results in interpretability suggest that large vision and language models learn implicit linear encodings when models are biased by in-context prompting. However, the existence of similar linear representations in more general adaptation regimes has not yet been demonstrated. In this work, we develop the concept of a task matrix, a linear transformation from a base to finetuned embedding state. We demonstrate that for vision and text models and ten different datasets, a base model augmented with a task matrix achieves results surpassing linear probes, sometimes approaching finetuned levels. Our results validate the existence of cross-layer linear encodings between pretrained and finetuned architectures. Moreover, we show that a data-based approximation for such encodings is both efficient and generalizable to multiple domains. We make our implementation publicly available.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Large Language Models on Multimodal Chemistry Olympiad Exams</title>
<link>https://arxiv.org/abs/2512.14989</link>
<guid>https://arxiv.org/abs/2512.14989</guid>
<content:encoded><![CDATA[
arXiv:2512.14989v1 Announce Type: cross 
Abstract: Multimodal scientific reasoning remains a significant challenge for large language models (LLMs), particularly in chemistry, where problem-solving relies on symbolic diagrams, molecular structures, and structured visual data. Here, we systematically evaluate 40 proprietary and open-source multimodal LLMs, including GPT-5, o3, Gemini-2.5-Pro, and Qwen2.5-VL, on a curated benchmark of Olympiad-style chemistry questions drawn from over two decades of U.S. National Chemistry Olympiad (USNCO) exams. These questions require integrated visual and textual reasoning across diverse modalities. We find that many models struggle with modality fusion, where in some cases, removing the image even improves accuracy, indicating misalignment in vision-language integration. Chain-of-Thought prompting consistently enhances both accuracy and visual grounding, as demonstrated through ablation studies and occlusion-based interpretability. Our results reveal critical limitations in the scientific reasoning abilities of current MLLMs, providing actionable strategies for developing more robust and interpretable multimodal systems in chemistry. This work provides a timely benchmark for measuring progress in domain-specific multimodal AI and underscores the need for further advances at the intersection of artificial intelligence and scientific reasoning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Gaussian Parameterization for Direct Atomic Structure Identification in Electron Tomography</title>
<link>https://arxiv.org/abs/2512.15034</link>
<guid>https://arxiv.org/abs/2512.15034</guid>
<content:encoded><![CDATA[
arXiv:2512.15034v1 Announce Type: cross 
Abstract: Atomic electron tomography (AET) enables the determination of 3D atomic structures by acquiring a sequence of 2D tomographic projection measurements of a particle and then computationally solving for its underlying 3D representation. Classical tomography algorithms solve for an intermediate volumetric representation that is post-processed into the atomic structure of interest. In this paper, we reformulate the tomographic inverse problem to solve directly for the locations and properties of individual atoms. We parameterize an atomic structure as a collection of Gaussians, whose positions and properties are learnable. This representation imparts a strong physical prior on the learned structure, which we show yields improved robustness to real-world imaging artifacts. Simulated experiments and a proof-of-concept result on experimentally-acquired data confirm our method's potential for practical applications in materials characterization and analysis with Transmission Electron Microscopy (TEM). Our code is available at https://github.com/nalinimsingh/gaussian-atoms.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles</title>
<link>https://arxiv.org/abs/2512.15047</link>
<guid>https://arxiv.org/abs/2512.15047</guid>
<content:encoded><![CDATA[
arXiv:2512.15047v1 Announce Type: cross 
Abstract: 3D Scene Graphs (3DSGs) constitute a powerful representation of the physical world, distinguished by their abilities to explicitly model the complex spatial, semantic, and functional relationships between entities, rendering a foundational understanding that enables agents to interact intelligently with their environment and execute versatile behaviors. Embodied navigation, as a crucial component of such capabilities, leverages the compact and expressive nature of 3DSGs to enable long-horizon reasoning and planning in complex, large-scale environments. However, prior works rely on a static-world assumption, defining traversable space solely based on static spatial layouts and thereby treating interactable obstacles as non-traversable. This fundamental limitation severely undermines their effectiveness in real-world scenarios, leading to limited reachability, low efficiency, and inferior extensibility. To address these issues, we propose HERO, a novel framework for constructing Hierarchical Traversable 3DSGs, that redefines traversability by modeling operable obstacles as pathways, capturing their physical interactivity, functional semantics, and the scene's relational hierarchy. The results show that, relative to its baseline, HERO reduces PL by 35.1% in partially obstructed environments and increases SR by 79.4% in fully obstructed ones, demonstrating substantially higher efficiency and reachability.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-learners for few-shot weakly-supervised optic disc and cup segmentation on fundus images</title>
<link>https://arxiv.org/abs/2512.15061</link>
<guid>https://arxiv.org/abs/2512.15061</guid>
<content:encoded><![CDATA[
arXiv:2512.15061v1 Announce Type: cross 
Abstract: This study develops meta-learners for few-shot weakly-supervised segmentation (FWS) to address the challenge of optic disc (OD) and optic cup (OC) segmentation for glaucoma diagnosis with limited labeled fundus images. We significantly improve existing meta-learners by introducing Omni meta-training which balances data usage and diversifies the number of shots. We also develop their efficient versions that reduce computational costs. In addition, we develop sparsification techniques that generate more customizable and representative scribbles and other sparse labels. After evaluating multiple datasets, we find that Omni and efficient versions outperform the original versions, with the best meta-learner being Efficient Omni ProtoSeg (EO-ProtoSeg). It achieves intersection over union (IoU) scores of 88.15% for OD and 71.17% for OC on the REFUGE dataset using just one sparsely labeled image, outperforming few-shot and semi-supervised methods which require more labeled images. Its best performance reaches 86.80% for OD and 71.78%for OC on DRISHTIGS, 88.21% for OD and 73.70% for OC on REFUGE, 80.39% for OD and 52.65% for OC on REFUGE. EO-ProtoSeg is comparable to unsupervised domain adaptation methods yet much lighter with less than two million parameters and does not require any retraining.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEV-Patch-PF: Particle Filtering with BEV-Aerial Feature Matching for Off-Road Geo-Localization</title>
<link>https://arxiv.org/abs/2512.15111</link>
<guid>https://arxiv.org/abs/2512.15111</guid>
<content:encoded><![CDATA[
arXiv:2512.15111v1 Announce Type: cross 
Abstract: We propose BEV-Patch-PF, a GPS-free sequential geo-localization system that integrates a particle filter with learned bird's-eye-view (BEV) and aerial feature maps. From onboard RGB and depth images, we construct a BEV feature map. For each 3-DoF particle pose hypothesis, we crop the corresponding patch from an aerial feature map computed from a local aerial image queried around the approximate location. BEV-Patch-PF computes a per-particle log-likelihood by matching the BEV feature to the aerial patch feature. On two real-world off-road datasets, our method achieves 7.5x lower absolute trajectory error (ATE) on seen routes and 7.0x lower ATE on unseen routes than a retrieval-based baseline, while maintaining accuracy under dense canopy and shadow. The system runs in real time at 10 Hz on an NVIDIA Tesla T4, enabling practical robot deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EPSM: A Novel Metric to Evaluate the Safety of Environmental Perception in Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.15195</link>
<guid>https://arxiv.org/abs/2512.15195</guid>
<content:encoded><![CDATA[
arXiv:2512.15195v1 Announce Type: cross 
Abstract: Extensive evaluation of perception systems is crucial for ensuring the safety of intelligent vehicles in complex driving scenarios. Conventional performance metrics such as precision, recall and the F1-score assess the overall detection accuracy, but they do not consider the safety-relevant aspects of perception. Consequently, perception systems that achieve high scores in these metrics may still cause misdetections that could lead to severe accidents. Therefore, it is important to evaluate not only the overall performance of perception systems, but also their safety. We therefore introduce a novel safety metric for jointly evaluating the most critical perception tasks, object and lane detection. Our proposed framework integrates a new, lightweight object safety metric that quantifies the potential risk associated with object detection errors, as well as an lane safety metric including the interdependence between both tasks that can occur in safety evaluation. The resulting combined safety score provides a unified, interpretable measure of perception safety performance. Using the DeepAccident dataset, we demonstrate that our approach identifies safety critical perception errors that conventional performance metrics fail to capture. Our findings emphasize the importance of safety-centric evaluation methods for perception systems in autonomous driving.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Preprocessing for Image Compression with Pre-trained Diffusion Models</title>
<link>https://arxiv.org/abs/2512.15270</link>
<guid>https://arxiv.org/abs/2512.15270</guid>
<content:encoded><![CDATA[
arXiv:2512.15270v1 Announce Type: cross 
Abstract: Preprocessing is a well-established technique for optimizing compression, yet existing methods are predominantly Rate-Distortion (R-D) optimized and constrained by pixel-level fidelity. This work pioneers a shift towards Rate-Perception (R-P) optimization by, for the first time, adapting a large-scale pre-trained diffusion model for compression preprocessing. We propose a two-stage framework: first, we distill the multi-step Stable Diffusion 2.1 into a compact, one-step image-to-image model using Consistent Score Identity Distillation (CiD). Second, we perform a parameter-efficient fine-tuning of the distilled model's attention modules, guided by a Rate-Perception loss and a differentiable codec surrogate. Our method seamlessly integrates with standard codecs without any modification and leverages the model's powerful generative priors to enhance texture and mitigate artifacts. Experiments show substantial R-P gains, achieving up to a 30.13% BD-rate reduction in DISTS on the Kodak dataset and delivering superior subjective visual quality.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Preprocessing Framework for Video Machine Vision under Compression</title>
<link>https://arxiv.org/abs/2512.15331</link>
<guid>https://arxiv.org/abs/2512.15331</guid>
<content:encoded><![CDATA[
arXiv:2512.15331v1 Announce Type: cross 
Abstract: There has been a growing trend in compressing and transmitting videos from terminals for machine vision tasks. Nevertheless, most video coding optimization method focus on minimizing distortion according to human perceptual metrics, overlooking the heightened demands posed by machine vision systems. In this paper, we propose a video preprocessing framework tailored for machine vision tasks to address this challenge. The proposed method incorporates a neural preprocessor which retaining crucial information for subsequent tasks, resulting in the boosting of rate-accuracy performance. We further introduce a differentiable virtual codec to provide constraints on rate and distortion during the training stage. We directly apply widely used standard codecs for testing. Therefore, our solution can be easily applied to real-world scenarios. We conducted extensive experiments evaluating our compression method on two typical downstream tasks with various backbone networks. The experimental results indicate that our approach can save over 15% of bitrate compared to using only the standard codec anchor version.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.15372</link>
<guid>https://arxiv.org/abs/2512.15372</guid>
<content:encoded><![CDATA[
arXiv:2512.15372v1 Announce Type: cross 
Abstract: Vision transformers in vision-language models apply uniform computational effort across all images, expending 175.33 GFLOPs (ViT-L/14) whether analysing a straightforward product photograph or a complex street scene. We propose ICAR (Image Complexity-Aware Retrieval), which enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both reduced-compute and full-compute processing. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance with 0.959 correlation with human judgement (Pearson) and 4.4x speedup. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% practical speedup while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training</title>
<link>https://arxiv.org/abs/2512.15411</link>
<guid>https://arxiv.org/abs/2512.15411</guid>
<content:encoded><![CDATA[
arXiv:2512.15411v1 Announce Type: cross 
Abstract: While leveraging abundant human videos and simulated robot data poses a scalable solution to the scarcity of real-world robot data, the generalization capability of existing vision-language-action models (VLAs) remains limited by mismatches in camera views, visual appearance, and embodiment morphologies. To overcome this limitation, we propose MiVLA, a generalizable VLA empowered by human-robot mutual imitation pre-training, which leverages inherent behavioral similarity between human hands and robotic arms to build a foundation of strong behavioral priors for both human actions and robotic control. Specifically, our method utilizes kinematic rules with left/right hand coordinate systems for bidirectional alignment between human and robot action spaces. Given human or simulated robot demonstrations, MiVLA is trained to forecast behavior trajectories for one embodiment, and imitate behaviors for another one unseen in the demonstration. Based on this mutual imitation, it integrates the behavioral fidelity of real-world human data with the manipulative diversity of simulated robot data into a unified model, thereby enhancing the generalization capability for downstream tasks. Extensive experiments conducted on both simulation and real-world platforms with three robots (ARX, PiPer and LocoMan), demonstrate that MiVLA achieves strong improved generalization capability, outperforming state-of-the-art VLAs (e.g., $\boldsymbol{\pi}_{0}$, $\boldsymbol{\pi}_{0.5}$ and H-RDT) by 25% in simulation, and 14% in real-world robot control tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoFlow: Solution Flow Models for One-Step Generative Modeling</title>
<link>https://arxiv.org/abs/2512.15657</link>
<guid>https://arxiv.org/abs/2512.15657</guid>
<content:encoded><![CDATA[
arXiv:2512.15657v1 Announce Type: cross 
Abstract: The multi-step denoising process in diffusion and Flow Matching models causes major efficiency issues, which motivates research on few-step generation. We present Solution Flow Models (SoFlow), a framework for one-step generation from scratch. By analyzing the relationship between the velocity function and the solution function of the velocity ordinary differential equation (ODE), we propose a Flow Matching loss and a solution consistency loss to train our models. The Flow Matching loss allows our models to provide estimated velocity fields for Classifier-Free Guidance (CFG) during training, which improves generation performance. Notably, our consistency loss does not require the calculation of the Jacobian-vector product (JVP), a common requirement in recent works that is not well-optimized in deep learning frameworks like PyTorch. Experimental results indicate that, when trained from scratch using the same Diffusion Transformer (DiT) architecture and an equal number of training epochs, our models achieve better FID-50K scores than MeanFlow models on the ImageNet 256x256 dataset.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</title>
<link>https://arxiv.org/abs/2512.15692</link>
<guid>https://arxiv.org/abs/2512.15692</guid>
<content:encoded><![CDATA[
arXiv:2512.15692v1 Announce Type: cross 
Abstract: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving</title>
<link>https://arxiv.org/abs/2312.09245</link>
<guid>https://arxiv.org/abs/2312.09245</guid>
<content:encoded><![CDATA[
arXiv:2312.09245v3 Announce Type: replace 
Abstract: Large language models (LLMs) have opened up new possibilities for intelligent agents, endowing them with human-like thinking and cognitive abilities. In this work, we delve into the potential of large language models (LLMs) in autonomous driving (AD). We introduce DriveMLM, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators. To this end, (1) we bridge the gap between the language decisions and the vehicle control commands by standardizing the decision states according to the off-the-shelf motion planning module. (2) We employ a multimodal LLM (MLLM) to model the behavior planning module of a module AD system, which uses driving rules, user commands, and inputs from various sensors (e.g., camera, lidar) as input and makes driving decisions and provide explanations; This model can plug-and-play in existing AD systems such as Autopilot and Apollo for close-loop driving. (3) We design an effective data engine to collect a dataset that includes decision state and corresponding explanation annotation for model training and evaluation. We conduct extensive experiments and show that replacing the decision-making modules of the Autopilot and Apollo with DriveMLM resulted in significant improvements of 3.2 and 4.7 points on the CARLA Town05 Long respectively, demonstrating the effectiveness of our model. We hope this work can serve as a baseline for autonomous driving with LLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASSR-NeRF: Arbitrary-Scale Super-Resolution on Voxel Grid for High-Quality Radiance Fields Reconstruction</title>
<link>https://arxiv.org/abs/2406.20066</link>
<guid>https://arxiv.org/abs/2406.20066</guid>
<content:encoded><![CDATA[
arXiv:2406.20066v2 Announce Type: replace 
Abstract: NeRF-based methods reconstruct 3D scenes by building a radiance field with implicit or explicit representations. While NeRF-based methods can perform novel view synthesis (NVS) at arbitrary scale, the performance in high-resolution novel view synthesis (HRNVS) with low-resolution (LR) optimization often results in oversmoothing. On the other hand, single-image super-resolution (SR) aims to enhance LR images to HR counterparts but lacks multi-view consistency. To address these challenges, we propose Arbitrary-Scale Super-Resolution NeRF (ASSR-NeRF), a novel framework for super-resolution novel view synthesis (SRNVS). We propose an attention-based VoxelGridSR model to directly perform 3D super-resolution (SR) on the optimized volume. Our model is trained on diverse scenes to ensure generalizability. For unseen scenes trained with LR views, we then can directly apply our VoxelGridSR to further refine the volume and achieve multi-view consistent SR. We demonstrate quantitative and qualitatively that the proposed method achieves significant performance in SRNVS.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynJAC: Synthetic-data-driven Joint-granular Adaptation and Calibration for Domain Specific Scanned Document Key Information Extraction</title>
<link>https://arxiv.org/abs/2410.01609</link>
<guid>https://arxiv.org/abs/2410.01609</guid>
<content:encoded><![CDATA[
arXiv:2410.01609v2 Announce Type: replace 
Abstract: Visually Rich Documents (VRDs), comprising elements such as charts, tables, and paragraphs, convey complex information across diverse domains. However, extracting key information from these documents remains labour-intensive, particularly for scanned formats with inconsistent layouts and domain-specific requirements. Despite advances in pretrained models for VRD understanding, their dependence on large annotated datasets for fine-tuning hinders scalability. This paper proposes \textbf{SynJAC} (Synthetic-data-driven Joint-granular Adaptation and Calibration), a method for key information extraction in scanned documents. SynJAC leverages synthetic, machine-generated data for domain adaptation and employs calibration on a small, manually annotated dataset to mitigate noise. By integrating fine-grained and coarse-grained document representation learning, SynJAC significantly reduces the need for extensive manual labelling while achieving competitive performance. Extensive experiments demonstrate its effectiveness in domain-specific and scanned VRD scenarios.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cascaded Dual Vision Transformer for Accurate Facial Landmark Detection</title>
<link>https://arxiv.org/abs/2411.07167</link>
<guid>https://arxiv.org/abs/2411.07167</guid>
<content:encoded><![CDATA[
arXiv:2411.07167v2 Announce Type: replace 
Abstract: Facial landmark detection is a fundamental problem in computer vision for many downstream applications. This paper introduces a new facial landmark detector based on vision transformers, which consists of two unique designs: Dual Vision Transformer (D-ViT) and Long Skip Connections (LSC). Based on the observation that the channel dimension of feature maps essentially represents the linear bases of the heatmap space, we propose learning the interconnections between these linear bases to model the inherent geometric relations among landmarks via Channel-split ViT. We integrate such channel-split ViT into the standard vision transformer (i.e., spatial-split ViT), forming our Dual Vision Transformer to constitute the prediction blocks. We also suggest using long skip connections to deliver low-level image features to all prediction blocks, thereby preventing useful information from being discarded by intermediate supervision. Extensive experiments are conducted to evaluate the performance of our proposal on the widely used benchmarks, i.e., WFLW, COFW, and 300W, demonstrating that our model outperforms the previous SOTAs across all three benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Robust and Accurate Adversarial Camouflage Generation against Vehicle Detectors</title>
<link>https://arxiv.org/abs/2411.10029</link>
<guid>https://arxiv.org/abs/2411.10029</guid>
<content:encoded><![CDATA[
arXiv:2411.10029v2 Announce Type: replace 
Abstract: Adversarial camouflage is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance. One promising approach involves using differentiable neural renderers to facilitate adversarial camouflage optimization through gradient back-propagation. However, existing methods often struggle to capture environmental characteristics during the rendering process or produce adversarial textures that can precisely map to the target vehicle. Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios. To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA. The core of RAUCA is a novel neural rendering component, End-to-End Neural Renderer Plus (E2E-NRP), which can accurately optimize and project vehicle textures and render images with environmental characteristics such as lighting and weather. In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the E2E-NRP to enhance the attack robustness. Experimental results on six popular object detectors show that RAUCA-final outperforms existing methods in both simulation and real-world settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>If you can describe it, they can see it: Cross-Modal Learning of Visual Concepts from Textual Descriptions</title>
<link>https://arxiv.org/abs/2411.15611</link>
<guid>https://arxiv.org/abs/2411.15611</guid>
<content:encoded><![CDATA[
arXiv:2411.15611v2 Announce Type: replace 
Abstract: Humans can visualize new and unknown concepts from their natural language description, based on their experience and previous knowledge. Insipired by this, we present a way to extend this ability to Vision-Language Models (VLMs), teaching them novel concepts by only using a textual description. We refer to this approach as Knowledge Transfer (KT). Our hypothesis is that the knowledge of a pre-trained VLM can be re-used to represent previously unknown concepts. Provided with a textual description of the novel concept, KT works by aligning relevant features of the visual encoder, obtained through model inversion, to its text representation. Differently from approaches relying on visual examples or external generative models, KT transfers knowledge within the same VLM by injecting visual knowledge directly from the text. Through an extensive evaluation on several VLM tasks, including classification, segmentation, image-text retrieval, and captioning, we show that: 1) KT can efficiently introduce new visual concepts from a single textual description; 2) the same principle can be used to refine the representation of existing concepts; and 3) KT significantly improves the performance of zero-shot VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do MLLMs Exhibit Human-like Perceptual Behaviors? HVSBench: A Benchmark for MLLM Alignment with Human Perceptual Behavior</title>
<link>https://arxiv.org/abs/2412.09603</link>
<guid>https://arxiv.org/abs/2412.09603</guid>
<content:encoded><![CDATA[
arXiv:2412.09603v3 Announce Type: replace 
Abstract: While Multimodal Large Language Models (MLLMs) excel at many vision tasks, it is unknown if they exhibit human-like perceptual behaviors. To evaluate this, we introduce HVSBench, the first large-scale benchmark with over 85,000 samples designed to test MLLM alignment with the human visual system (HVS). The benchmark covers 13 categories across 5 key fields: Prominence, Subitizing, Prioritizing, Free-Viewing, and Searching. Our comprehensive evaluation reveals a significant perceptual gap: even state-of-the-art MLLMs achieve only moderate results. In contrast, human participants demonstrate strong performance, significantly outperforming all models. This underscores the high quality of HVSBench and the need for more human-aligned AI. We believe our benchmark will be a critical tool for developing the next generation of explainable MLLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MS-Temba: Multi-Scale Temporal Mamba for Understanding Long Untrimmed Videos</title>
<link>https://arxiv.org/abs/2501.06138</link>
<guid>https://arxiv.org/abs/2501.06138</guid>
<content:encoded><![CDATA[
arXiv:2501.06138v3 Announce Type: replace 
Abstract: Temporal Action Detection (TAD) in untrimmed videos poses significant challenges, particularly for Activities of Daily Living (ADL) requiring models to (1) process long-duration videos, (2) capture temporal variations in actions, and (3) simultaneously detect dense overlapping actions. Existing CNN and Transformer-based approaches, struggle to jointly capture fine-grained detail and long-range structure at scale. State-space Model (SSM) based Mamba offers powerful long-range modeling, but naive application to TAD collapses fine-grained temporal structure and fails to account for the challenges inherent to TAD. To this end, we propose Multi-Scale Temporal Mamba (MS-Temba), which extends Mamba to TAD with newly introduced dilated SSMs. Each Temba block, comprising dilated SSMs coupled with our proposed additional losses, enables the learning of discriminative representations across temporal scales. A lightweight Multi-scale Mamba Fuser then unifies these multi-scale features via SSM-based aggregation, yielding precise action-boundary localization. With only 17M parameters, MS-Temba achieves state-of-the-art performance on densely labeled ADL benchmarks TSU & Charades, and further generalizes to long-form video summarization, setting new state-of-the-art results on TVSum & SumMe.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Cycle Structured Pruning via Stability-Driven Subnetwork Search</title>
<link>https://arxiv.org/abs/2501.13439</link>
<guid>https://arxiv.org/abs/2501.13439</guid>
<content:encoded><![CDATA[
arXiv:2501.13439v2 Announce Type: replace 
Abstract: Existing structured pruning methods typically rely on multi-stage training procedures that incur high computational costs. Pruning at initialization aims to reduce this burden but often suffers from degraded performance. To address these limitations, we propose an efficient one-cycle structured pruning framework that integrates pre-training, pruning, and fine-tuning into a single training cycle without sacrificing accuracy. The key idea is to identify an optimal sub-network during the early stages of training, guided by norm-based group saliency criteria and structured sparsity regularization. We introduce a novel pruning indicator that detects a stable pruning epoch by measuring the similarity between pruning sub-networks across consecutive training epochs. In addition, group sparsity regularization accelerates convergence, further reducing overall training time. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet using VGG, ResNet, and MobileNet architectures demonstrate that the proposed method achieves state-of-the-art accuracy while being among the most efficient structured pruning frameworks in terms of training cost. Code is available at https://github.com/ghimiredhikura/OCSPruner.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain-of-Evidence Multimodal Reasoning for Few-shot Temporal Action Localization</title>
<link>https://arxiv.org/abs/2504.13460</link>
<guid>https://arxiv.org/abs/2504.13460</guid>
<content:encoded><![CDATA[
arXiv:2504.13460v4 Announce Type: replace 
Abstract: Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the action localization task. To address these issues, in this work, we propose a new few-shot temporal action localization method by Chain-of-Evidence multimodal reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level, we design a Chain-of-Evidence (CoE) reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoE text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3, THUMOS14 and our newly collected Human-related Anomaly Localization Dataset. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. Our source code and data are available at https://github.com/MICLAB-BUPT/VAL-VLM.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[
arXiv:2505.22657v2 Announce Type: replace 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation</title>
<link>https://arxiv.org/abs/2505.24431</link>
<guid>https://arxiv.org/abs/2505.24431</guid>
<content:encoded><![CDATA[
arXiv:2505.24431v2 Announce Type: replace 
Abstract: 3D point cloud anomaly detection is essential for robust vision systems but is challenged by pose variations and complex geometric anomalies. Existing patch-based methods often suffer from geometric fidelity issues due to discrete voxelization or projection-based representations, limiting fine-grained anomaly localization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel framework that integrates 3D anomaly detection and repair by learning a continuous, pose-invariant shape representation. PASDF leverages a Pose Alignment Module for canonicalization and a SDF Network to dynamically incorporate pose, enabling implicit learning of high-fidelity anomaly repair templates from the continuous SDF. This facilitates precise pixel-level anomaly localization through an Anomaly-Aware Scoring Module. Crucially, the continuous 3D representation in PASDF extends beyond detection, facilitating in-situ anomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate state-of-the-art performance, achieving high object-level AUROC scores of 80.2% and 90.0%, respectively. These results highlight the effectiveness of continuous geometric representations in advancing 3D anomaly detection and facilitating practical anomaly region repair. The code is available at https://github.com/ZZZBBBZZZ/PASDF to support further research.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO Challenge</title>
<link>https://arxiv.org/abs/2506.02976</link>
<guid>https://arxiv.org/abs/2506.02976</guid>
<content:encoded><![CDATA[
arXiv:2506.02976v3 Announce Type: replace 
Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated detection and monitoring of age-related macular degeneration (AMD) through the analysis of optical coherence tomography (OCT) images. Designed to evaluate algorithmic performance in detecting neovascular activity changes within AMD, the challenge incorporated unique multi-modal datasets. The primary dataset, sourced from Brest, France, was used by participating teams to train and test their models. The final ranking was determined based on performance on this dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate population and device shifts from submitted solutions. Two tasks were involved in the MARIO challenge. The first one was the classification of evolution between two consecutive 2D OCT B-scans. The second one was the prediction of future AMD evolution over three months for patients undergoing anti-vascular endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with the top 12 finalists presenting their methods. This paper outlines the challenge's structure, tasks, data characteristics, and winning methodologies, setting a benchmark for AMD monitoring using OCT, infrared imaging, and clinical data (such as the number of visits, age, gender, etc.). The results of this challenge indicate that artificial intelligence (AI) performs as well as a physician in measuring AMD progression (Task 1) but is not yet able of predicting future evolution (Task 2).
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Gaslighting Negation Attacks Against Reasoning Models</title>
<link>https://arxiv.org/abs/2506.09677</link>
<guid>https://arxiv.org/abs/2506.09677</guid>
<content:encoded><![CDATA[
arXiv:2506.09677v2 Announce Type: replace 
Abstract: Recent advances in reasoning-centric models promise improved robustness through mechanisms such as chain-of-thought prompting and test-time scaling. However, their ability to withstand gaslighting negation attacks-adversarial prompts that confidently deny correct answers-remains underexplored. In this paper, we conduct a systematic evaluation of three state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average) following gaslighting negation attacks, indicating that even top-tier reasoning models struggle to preserve correct answers under manipulative user feedback. Built upon the insights of the evaluation and to further probe this vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark specifically designed to evaluate reasoning models' susceptibility to defend their belief under gaslighting negation attacks. Constructed by filtering and curating 1,025 challenging samples from the existing benchmarks, GaslightingBench-R induces even more dramatic failures, with accuracy drops exceeding 53% on average. Our findings highlight a fundamental gap between step-by-step reasoning and resistance to adversarial manipulation, calling for new robustness strategies that safeguard reasoning models against gaslighting negation attacks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binarization-Aware Adjuster: A Theoretical Framework for Bridging Continuous Optimization and Discrete Inference with Application to Edge Detection</title>
<link>https://arxiv.org/abs/2506.12460</link>
<guid>https://arxiv.org/abs/2506.12460</guid>
<content:encoded><![CDATA[
arXiv:2506.12460v2 Announce Type: replace 
Abstract: In machine learning, discrete decision-making tasks exhibit a fundamental inconsistency between training and inference: models are optimized using continuous-valued outputs, yet evaluated through discrete predictions. This discrepancy arises from the non-differentiability of discretization operations, weakening the alignment between optimization objectives and practical decision outcomes. To address this, we present a theoretical framework for constructing a Binarization-Aware Adjuster (BAA) that integrates binarization behavior directly into gradient-based learning. Central to the approach is a Distance Weight Function (DWF) that dynamically modulates pixel-wise loss contributions based on prediction correctness and proximity to the decision boundary, thereby emphasizing decision-critical regions while de-emphasizing confidently correct samples. Furthermore, a self-adaptive threshold estimation procedure is introduced to better match optimization dynamics with inference conditions. As one of its applications, we implement experiments on the edge detection (ED) task, which also demonstrate the effectiveness of the proposed method experimentally. Beyond binary decision tasks and ED, the proposed framework provides a general strategy for aligning continuous optimization with discrete evaluation and can be extended to multi-valued decision processes in broader structured prediction problems.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGIC: Few-Shot Mask-Guided Anomaly Inpainting with Prompt Perturbation, Spatially Adaptive Guidance, and Context Awareness</title>
<link>https://arxiv.org/abs/2507.02314</link>
<guid>https://arxiv.org/abs/2507.02314</guid>
<content:encoded><![CDATA[
arXiv:2507.02314v3 Announce Type: replace 
Abstract: Few-shot anomaly generation is a key challenge in industrial quality control. Although diffusion models are promising, existing methods struggle: global prompt-guided approaches corrupt normal regions, and existing inpainting-based methods often lack the in-distribution diversity essential for robust downstream models. We propose MAGIC, a fine-tuned inpainting framework that generates high-fidelity anomalies that strictly adhere to the mask while maximizing this diversity. MAGIC introduces three complementary components: (i) Gaussian prompt perturbation, which prevents model overfitting in the few-shot setting by learning and sampling from a smooth manifold of realistic anomalies, (ii) spatially adaptive guidance that applies distinct guidance strengths to the anomaly and background regions, and (iii) context-aware mask alignment to relocate masks for plausible placement within the host object. Under consistent identical evaluation protocol, MAGIC outperforms state-of-the-art methods on diverse anomaly datasets in downstream tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Navigation Refinement: Achieving Lane-Level Guidance by Associating Standard-Definition and Online Perception Maps</title>
<link>https://arxiv.org/abs/2507.07487</link>
<guid>https://arxiv.org/abs/2507.07487</guid>
<content:encoded><![CDATA[
arXiv:2507.07487v3 Announce Type: replace 
Abstract: Lane-level navigation is critical for geographic information systems and navigation-based tasks, offering finer-grained guidance than road-level navigation by standard definition (SD) maps. However, it currently relies on expansive global HD maps that cannot adapt to dynamic road conditions. Recently, online perception (OP) maps have become research hotspots, providing real-time geometry as an alternative, but lack the global topology needed for navigation. To address these issues, Online Navigation Refinement (ONR), a new mission is introduced that refines SD-map-based road-level routes into accurate lane-level navigation by associating SD maps with OP maps. The map-to-map association to handle many-to-one lane-to-road mappings under two key challenges: (1) no public dataset provides lane-to-road correspondences; (2) severe misalignment from spatial fluctuations, semantic disparities, and OP map noise invalidates traditional map matching. For these challenges, We contribute: (1) Online map association dataset (OMA), the first ONR benchmark with 30K scenarios and 2.6M annotated lane vectors; (2) MAT, a transformer with path-aware attention to aligns topology despite spatial fluctuations and semantic disparities and spatial attention for integrates noisy OP features via global context; and (3) NR P-R, a metric evaluating geometric and semantic alignment. Experiments show that MAT outperforms existing methods at 34 ms latency, enabling low-cost and up-to-date lane-level navigation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation</title>
<link>https://arxiv.org/abs/2507.18625</link>
<guid>https://arxiv.org/abs/2507.18625</guid>
<content:encoded><![CDATA[
arXiv:2507.18625v2 Announce Type: replace 
Abstract: Graphical user interface (UI) software has undergone a fundamental transformation from traditional two-dimensional (2D) desktop/web/mobile interfaces to spatial three-dimensional (3D) environments. While existing work has made remarkable success in automated 2D software generation, such as HTML/CSS and mobile app interface code synthesis, the generation of 3D software still remains under-explored. Current methods for 3D software generation usually generate the 3D environments as a whole and cannot modify or control specific elements in the software. Furthermore, these methods struggle to handle the complex spatial and semantic constraints inherent in the real world. To address the challenges, we present Scenethesis, a novel requirement-sensitive 3D software synthesis approach that maintains formal traceability between user specifications and generated 3D software. Scenethesis is built upon ScenethesisLang, a domain-specific language that serves as a granular constraint-aware intermediate representation (IR) to bridge natural language requirements and executable 3D software. It serves both as a comprehensive scene description language enabling fine-grained modification of 3D software elements and as a formal constraint-expressive specification language capable of expressing complex spatial constraints. By decomposing 3D software synthesis into stages operating on ScenethesisLang, Scenethesis enables independent verification, targeted modification, and systematic constraint satisfaction. Our evaluation demonstrates that Scenethesis accurately captures over 80% of user requirements and satisfies more than 90% of hard constraints while handling over 100 constraints simultaneously. Furthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual evaluation scores compared to the state-of-the-art method.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts</title>
<link>https://arxiv.org/abs/2508.06032</link>
<guid>https://arxiv.org/abs/2508.06032</guid>
<content:encoded><![CDATA[
arXiv:2508.06032v2 Announce Type: replace 
Abstract: Existing methods for human parsing into body parts and clothing often use fixed mask categories with broad labels that obscure fine-grained clothing types. Recent open-vocabulary segmentation approaches leverage pretrained text-to-image (T2I) diffusion model features for strong zero-shot transfer, but typically group entire humans into a single person category, failing to distinguish diverse clothing or detailed body parts. To address this, we propose Spectrum, a unified network for part-level pixel parsing (body parts and clothing) and instance-level grouping. While diffusion-based open-vocabulary models generalize well across tasks, their internal representations are not specialized for detailed human parsing. We observe that, unlike diffusion models with broad representations, image-driven 3D texture generators maintain faithful correspondence to input images, enabling stronger representations for parsing diverse clothing and body parts. Spectrum introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model (obtained by fine-tuning a T2I model on 3D human texture maps) for improved alignment with body parts and clothing. From an input image, we extract human-part internal features via the I2Tx diffusion model and generate semantically valid masks aligned to diverse clothing categories through prompt-guided grounding. Once trained, Spectrum produces semantic segmentation maps for every visible body part and clothing category, ignoring standalone garments or irrelevant objects, for any number of humans in the scene. We conduct extensive cross-dataset experiments, separately assessing body parts, clothing parts, unseen clothing categories, and full-body masks, and demonstrate that Spectrum consistently outperforms baseline methods in prompt-based segmentation.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation</title>
<link>https://arxiv.org/abs/2508.07981</link>
<guid>https://arxiv.org/abs/2508.07981</guid>
<content:encoded><![CDATA[
arXiv:2508.07981v4 Announce Type: replace 
Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation</title>
<link>https://arxiv.org/abs/2508.08179</link>
<guid>https://arxiv.org/abs/2508.08179</guid>
<content:encoded><![CDATA[
arXiv:2508.08179v2 Announce Type: replace 
Abstract: Human motion generation has found widespread applications in AR/VR, film, sports, and medical rehabilitation, offering a cost-effective alternative to traditional motion capture systems. However, evaluating the fidelity of such generated motions is a crucial, multifaceted task. Although previous approaches have attempted at motion fidelity evaluation using human perception or physical constraints, there remains an inherent gap between human-perceived fidelity and physical feasibility. Moreover, the subjective and coarse binary labeling of human perception further undermines the development of a robust data-driven metric. We address these issues by introducing a physical labeling method. This method evaluates motion fidelity by calculating the minimum modifications needed for a motion to align with physical laws. With this approach, we are able to produce fine-grained, continuous physical alignment annotations that serve as objective ground truth. With these annotations, we propose PP-Motion, a novel data-driven metric to evaluate both physical and perceptual fidelity of human motion. To effectively capture underlying physical priors, we employ Pearson's correlation loss for the training of our metric. Additionally, by incorporating a human-based perceptual fidelity loss, our metric can capture fidelity that simultaneously considers both human perception and physical alignment. Experimental results demonstrate that our metric, PP-Motion, not only aligns with physical laws but also aligns better with human perception of motion fidelity than previous work.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FitPro: A Zero-Shot Framework for Interactive Text-based Pedestrian Retrieval in Open World</title>
<link>https://arxiv.org/abs/2509.16674</link>
<guid>https://arxiv.org/abs/2509.16674</guid>
<content:encoded><![CDATA[
arXiv:2509.16674v3 Announce Type: replace 
Abstract: Text-based Pedestrian Retrieval (TPR) deals with retrieving specific target pedestrians in visual scenes according to natural language descriptions. Although existing methods have achieved progress under constrained settings, interactive retrieval in the open-world scenario still suffers from limited model generalization and insufficient semantic understanding. To address these challenges, we propose FitPro, an open-world interactive zero-shot TPR framework with enhanced semantic comprehension and cross-scene adaptability. FitPro has three innovative components: Feature Contrastive Decoding (FCD), Incremental Semantic Mining (ISM), and Query-aware Hierarchical Retrieval (QHR). The FCD integrates prompt-guided contrastive decoding to generate high-quality structured pedestrian descriptions from denoised images, effectively alleviating semantic drift in zero-shot scenarios. The ISM constructs holistic pedestrian representations from multi-view observations to achieve global semantic modeling in multi-turn interactions, thereby improving robustness against viewpoint shifts and fine-grained variations in descriptions. The QHR dynamically optimizes the retrieval pipeline according to query types, enabling efficient adaptation to multi-modal and multi-view inputs. Extensive experiments on five public datasets and two evaluation protocols demonstrate that FitPro significantly overcomes the generalization limitations and semantic modeling constraints of existing methods in interactive retrieval, paving the way for practical deployment.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking and Mitigating Sycophancy in Medical Vision Language Models</title>
<link>https://arxiv.org/abs/2509.21979</link>
<guid>https://arxiv.org/abs/2509.21979</guid>
<content:encoded><![CDATA[
arXiv:2509.21979v3 Announce Type: replace 
Abstract: Visual language models (VLMs) have the potential to transform medical workflows. However, the deployment is limited by sycophancy. Despite this serious threat to patient safety, a systematic benchmark remains lacking. This paper addresses this gap by introducing a Medical benchmark that applies multiple templates to VLMs in a hierarchical medical visual question answering task. We find that current VLMs are highly susceptible to visual cues, with failure rates showing a correlation to model size or overall accuracy. we discover that perceived authority and user mimicry are powerful triggers, suggesting a bias mechanism independent of visual data. To overcome this, we propose a Visual Information Purification for Evidence based Responses (VIPER) strategy that proactively filters out non-evidence-based social cues, thereby reinforcing evidence based reasoning. VIPER reduces sycophancy while maintaining interpretability and consistently outperforms baseline methods, laying the necessary foundation for the robust and secure integration of VLMs.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation</title>
<link>https://arxiv.org/abs/2510.03769</link>
<guid>https://arxiv.org/abs/2510.03769</guid>
<content:encoded><![CDATA[
arXiv:2510.03769v3 Announce Type: replace 
Abstract: The increasing size and complexity of medical imaging datasets, particularly in 3D formats, present significant barriers to collaborative research and transferability. This study investigates whether the ZFP compression technique can mitigate these challenges without compromising the performance of automated cerebrovascular segmentation, a critical first step in intracranial aneurysm detection. We apply ZFP in both its error tolerance and fixed-rate modes to a large scale, and one of the most recent, datasets in the literature, 3D medical dataset containing ground-truth vascular segmentations. The segmentation quality on the compressed volumes is rigorously compared to the uncompressed baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance mode--while maintaining a high degree of fidelity, with the mean Dice coefficient remaining high at 0.87656. These results demonstrate that ZFP is a viable and powerful tool for enabling more efficient and accessible research on large-scale medical datasets, fostering broader collaboration across the community.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models</title>
<link>https://arxiv.org/abs/2510.15022</link>
<guid>https://arxiv.org/abs/2510.15022</guid>
<content:encoded><![CDATA[
arXiv:2510.15022v2 Announce Type: replace 
Abstract: Low-rank Adaptation (LoRA) models have revolutionized the personalization of pre-trained diffusion models by enabling fine-tuning through low-rank, factorized weight matrices specifically optimized for attention layers. These models facilitate the generation of highly customized content across a variety of objects, individuals, and artistic styles without the need for extensive retraining. Despite the availability of over 100K LoRA adapters on platforms like Civit.ai, users often face challenges in navigating, selecting, and effectively utilizing the most suitable adapters due to their sheer volume, diversity, and lack of structured organization. This paper addresses the problem of selecting the most relevant and diverse LoRA models from this vast database by framing the task as a combinatorial optimization problem and proposing a novel submodular framework. Our quantitative and qualitative experiments demonstrate that our method generates diverse outputs across a wide range of domains.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset</title>
<link>https://arxiv.org/abs/2510.15742</link>
<guid>https://arxiv.org/abs/2510.15742</guid>
<content:encoded><![CDATA[
arXiv:2510.15742v2 Announce Type: replace 
Abstract: Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping</title>
<link>https://arxiv.org/abs/2510.26569</link>
<guid>https://arxiv.org/abs/2510.26569</guid>
<content:encoded><![CDATA[
arXiv:2510.26569v2 Announce Type: replace 
Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall. The dataset and code are available at https://github.com/ostadabbas/AdSum204.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUSE: Multi-Scale Dense Self-Distillation for Nucleus Detection and Classification</title>
<link>https://arxiv.org/abs/2511.05170</link>
<guid>https://arxiv.org/abs/2511.05170</guid>
<content:encoded><![CDATA[
arXiv:2511.05170v2 Announce Type: replace 
Abstract: Nucleus detection and classification (NDC) in histopathology analysis is a fundamental task that underpins a wide range of high-level pathology applications. However, existing methods heavily rely on labor-intensive nucleus-level annotations and struggle to fully exploit large-scale unlabeled data for learning discriminative nucleus representations. In this work, we propose MUSE (MUlti-scale denSE self-distillation), a novel self-supervised learning method tailored for NDC. At its core is NuLo (Nucleus-based Local self-distillation), a coordinate-guided mechanism that enables flexible local self-distillation based on predicted nucleus positions. By removing the need for strict spatial alignment between augmented views, NuLo allows critical cross-scale alignment, thus unlocking the capacity of models for fine-grained nucleus-level representation. To support MUSE, we design a simple yet effective encoder-decoder architecture and a large field-of-view semi-supervised fine-tuning strategy that together maximize the value of unlabeled pathology images. Extensive experiments on three widely used benchmarks demonstrate that MUSE effectively addresses the core challenges of histopathological NDC. The resulting models not only surpass state-of-the-art supervised baselines but also outperform generic pathology foundation models.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abstract 3D Perception for Spatial Intelligence in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.10946</link>
<guid>https://arxiv.org/abs/2511.10946</guid>
<content:encoded><![CDATA[
arXiv:2511.10946v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) struggle with 3D-related tasks such as spatial cognition and physical understanding, which are crucial for real-world applications like robotics and embodied agents. We attribute this to a modality gap between the 3D tasks and the 2D training of VLM, which led to inefficient retrieval of 3D information from 2D input. To bridge this gap, we introduce SandboxVLM, a simple yet effective framework that leverages abstract bounding boxes to encode geometric structure and physical kinematics for VLM. Specifically, we design a 3D Sandbox reconstruction and perception pipeline comprising four stages: generating multi-view priors with abstract control, proxy elevation, multi-view voting and clustering, and 3D-aware reasoning. Evaluated in zero-shot settings across multiple benchmarks and VLM backbones, our approach consistently improves spatial intelligence, achieving an 8.3\% gain on SAT Real compared with baseline methods for instance. These results demonstrate that equipping VLMs with a 3D abstraction substantially enhances their 3D reasoning ability without additional training, suggesting new possibilities for general-purpose embodied intelligence.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity (Extension)</title>
<link>https://arxiv.org/abs/2511.12061</link>
<guid>https://arxiv.org/abs/2511.12061</guid>
<content:encoded><![CDATA[
arXiv:2511.12061v2 Announce Type: replace 
Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching</title>
<link>https://arxiv.org/abs/2511.12998</link>
<guid>https://arxiv.org/abs/2511.12998</guid>
<content:encoded><![CDATA[
arXiv:2511.12998v2 Announce Type: replace 
Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPG-INR: Target Prior-Guided Implicit 3D CT Reconstruction for Enhanced Sparse-view Imaging</title>
<link>https://arxiv.org/abs/2511.18806</link>
<guid>https://arxiv.org/abs/2511.18806</guid>
<content:encoded><![CDATA[
arXiv:2511.18806v2 Announce Type: replace 
Abstract: X-ray imaging, based on penetration, enables detailed visualization of internal structures. Building on this capability, existing implicit 3D reconstruction methods have adapted the NeRF model and its variants for internal CT reconstruction. However, these approaches often neglect the significance of objects' anatomical priors for implicit learning, limiting both reconstruction precision and learning efficiency, particularly in ultra-sparse view scenarios. To address these challenges, we propose a novel 3D CT reconstruction framework that employs a 'target prior' derived from the object's projection data to enhance implicit learning. Our approach integrates positional and structural encoding to facilitate voxel-wise implicit reconstruction, utilizing the target prior to guide voxel sampling and enrich structural encoding. This dual strategy significantly boosts both learning efficiency and reconstruction quality. Additionally, we introduce a CUDA-based algorithm for rapid estimation of high-quality 3D target priors from sparse-view projections. Experiments utilizing projection data from a complex abdominal dataset demonstrate that the proposed model substantially enhances learning efficiency, outperforming the current leading model, NAF, by a factor of ten. In terms of reconstruction quality, it also exceeds the most accurate model, NeRP, achieving PSNR improvements of 3.57 dB, 5.42 dB, and 5.70 dB with 10, 20, and 30 projections, respectively. The code is available at https://github.com/qlcao171/TPG-INR.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Centric Open-Future Task Discovery: Formulation, Benchmark, and Scalable Tree-Based Search</title>
<link>https://arxiv.org/abs/2511.18929</link>
<guid>https://arxiv.org/abs/2511.18929</guid>
<content:encoded><![CDATA[
arXiv:2511.18929v3 Announce Type: replace 
Abstract: Recent progress in robotics and embodied AI is largely driven by Large Multimodal Models (LMMs). However, a key challenge remains underexplored: how can we advance LMMs to discover tasks that assist humans in open-future scenarios, where human intentions are highly concurrent and dynamic. In this work, we formalize the problem of Human-centric Open-future Task Discovery (HOTD), focusing particularly on identifying tasks that reduce human effort across plausible futures. To facilitate this study, we propose HOTD-Bench, which features over 2K real-world videos, a semi-automated annotation pipeline, and a simulation-based protocol tailored for open-set future evaluation. Additionally, we propose the Collaborative Multi-Agent Search Tree (CMAST) framework, which decomposes complex reasoning through a multi-agent system and structures the reasoning process through a scalable search tree module. In our experiments, CMAST achieves the best performance on the HOTD-Bench, significantly surpassing existing LMMs. It also integrates well with existing LMMs, consistently improving performance.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History-Augmented Contrastive Learning With Soft Mixture of Experts for Blind Super-Resolution of Planetary Remote Sensing Images</title>
<link>https://arxiv.org/abs/2511.20045</link>
<guid>https://arxiv.org/abs/2511.20045</guid>
<content:encoded><![CDATA[
arXiv:2511.20045v2 Announce Type: replace 
Abstract: Blind Super-Resolution (BSR) in planetary remote sensing constitutes a highly ill-posed inverse problem, characterized by unknown degradation patterns and a complete absence of ground-truth supervision. Existing unsupervised approaches often struggle with optimization instability and distribution shifts, relying on greedy strategies or generic priors that fail to preserve distinct morphological semantics. To address these challenges, we propose History-Augmented Contrastive Mixture of Experts (HAC-MoE), a novel unsupervised framework that decouples kernel estimation from image reconstruction without external kernel priors. The framework is founded on three key innovations: (1) A Contrastive Kernel Sampling mechanism that mitigates the distribution bias inherent in random Gaussian sampling, ensuring the generation of plausible kernel priors via similarity constraints; (2) A History-Augmented Contrastive Learning strategy that leverages historical model states as negative self-priors. We provide a theoretical analysis demonstrating that this mechanism induces strong convexity in the feature space, thereby stabilizing the unsupervised optimization trajectory and preventing overfitting; and (3) A Morphology-Aware Soft Mixture-of-Experts (MA-MoE) estimator that dynamically modulates spectral-spatial features to adaptively reconstruct diverse planetary topographies. To facilitate rigorous evaluation, we introduce Ceres-50, a benchmark dataset encapsulating diverse geological features under realistic degradation simulations. Extensive experiments demonstrate that HAC-MoE achieves state-of-the-art performance in reconstruction quality and kernel estimation accuracy, offering a solution for scientific observation in data-sparse extraterrestrial environments.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</title>
<link>https://arxiv.org/abs/2511.22039</link>
<guid>https://arxiv.org/abs/2511.22039</guid>
<content:encoded><![CDATA[
arXiv:2511.22039v2 Announce Type: replace 
Abstract: This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM3D: Deformable Mamba via Offset-Guided Gaussian Sequencing for Point Cloud Understanding</title>
<link>https://arxiv.org/abs/2512.03424</link>
<guid>https://arxiv.org/abs/2512.03424</guid>
<content:encoded><![CDATA[
arXiv:2512.03424v2 Announce Type: replace 
Abstract: State Space Models (SSMs) demonstrate significant potential for long-sequence modeling, but their reliance on input order conflicts with the irregular nature of point clouds. Existing approaches often rely on predefined serialization strategies, which cannot adjust based on diverse geometric structures. To overcome this limitation, we propose \textbf{DM3D}, a deformable Mamba architecture for point cloud understanding. Specifically, DM3D introduces an offset-guided Gaussian sequencing mechanism that unifies local resampling and global reordering within a deformable scan. The Gaussian-based KNN Resampling (GKR) enhances structural awareness by adaptively reorganizing neighboring points, while the Gaussian-based Differentiable Reordering (GDR) enables end-to-end optimization of serialization order. Furthermore, a Tri-Path Frequency Fusion module enhances feature complementarity and reduces aliasing. Together, these components enable structure-adaptive serialization of point clouds. Extensive experiments on benchmark datasets show that DM3D achieves state-of-the-art performance in classification, few-shot learning, and part segmentation, demonstrating that adaptive serialization effectively unlocks the potential of SSMs for point cloud understanding. The code will be released at https://github.com/L1277471578/DM3D.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model</title>
<link>https://arxiv.org/abs/2512.05277</link>
<guid>https://arxiv.org/abs/2512.05277</guid>
<content:encoded><![CDATA[
arXiv:2512.05277v2 Announce Type: replace 
Abstract: Temporal understanding in autonomous driving (AD) remains a significant challenge, even for recent state-of-the-art (SoTA) Vision-Language Models (VLMs). Prior work has introduced datasets and benchmarks aimed at improving temporal reasoning, but these have emphasized other video content, including sports, cooking, and movies. No existing benchmark focuses exclusively on the unique challenges of temporal understanding in ego-centric AD footage. To fill this gap, the Temporal Understanding in Autonomous Driving (TAD) benchmark is presented, which evaluates VLMs' ability to capture the dynamic relationships between actions in AD. TAD comprises nearly 6,000 question-answer (QA) pairs, spanning 7 human-designed tasks. In addition, an evaluation is performed that consists of 9 closed- and open-source generalist models as well as SoTA AD specialist models. When applied to TAD, current SoTA models demonstrated substandard accuracies, largely due to imperfect fine-grained motion understanding. To improve motion understanding and overall accuracy on TAD, two novel training-free solutions are proposed: Scene-CoT, that leverages Chain-of-Thought (CoT) and TCogMap, which incorporates an ego-centric temporal cognitive map. The proposed approaches are integrated with existing VLMs and improve average accuracy on TAD by up to 17.72%. By introducing TAD, benchmarking multiple SoTA models, and proposing effective enhancements, this work aims to catalyze future research on temporal understanding in AD. The benchmark and evaluation code are available at \href{https://huggingface.co/datasets/vbdai/TAD}{Hugging Face} and \href{https://github.com/vbdi/tad_bench}{Github}, respectively.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning</title>
<link>https://arxiv.org/abs/2403.13522</link>
<guid>https://arxiv.org/abs/2403.13522</guid>
<content:encoded><![CDATA[
arXiv:2403.13522v3 Announce Type: replace-cross 
Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning (CIL) without available historical training samples as exemplars. Compared with its exemplar-based CIL counterpart that stores exemplars, EFCIL suffers more from forgetting issues. Recently, a new EFCIL branch named Analytic Continual Learning (ACL) introduces a gradient-free paradigm via Recursive Least-Square, achieving a forgetting-resistant classifier training with a frozen backbone during CIL. However, existing ACL suffers from ineffective representations and insufficient utilization of backbone knowledge. In this paper, we propose a representation-enhanced analytic learning (REAL) to address these problems. To enhance the representation, REAL constructs a dual-stream base pretraining followed by representation enhancing distillation process. The dual-stream base pretraining combines self-supervised contrastive learning for general features and supervised learning for class-specific knowledge, followed by the representation enhancing distillation to merge both streams, enhancing representations for subsequent CIL paradigm. To utilize more knowledge from the backbone, REAL presents a feature fusion buffer to multi-layer backbone features, providing informative features for the subsequent classifier training. Our method can be incorporated into existing ACL techniques and provides more competitive performance. Empirical results demonstrate that, REAL achieves state-of-the-art performance on CIFAR-100, ImageNet-100 and ImageNet-1k benchmarks, outperforming exemplar-free methods and rivaling exemplar-based approaches.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pretraining to Privacy: Federated Ultrasound Foundation Model with Self-Supervised Learning</title>
<link>https://arxiv.org/abs/2411.16380</link>
<guid>https://arxiv.org/abs/2411.16380</guid>
<content:encoded><![CDATA[
arXiv:2411.16380v2 Announce Type: replace-cross 
Abstract: Ultrasound imaging is widely used in clinical diagnosis due to its non-invasive nature and real-time capabilities. However, traditional ultrasound diagnostics relies heavily on physician expertise and is often hampered by suboptimal image quality, leading to potential diagnostic errors. While artificial intelligence (AI) offers a promising solution to enhance clinical diagnosis by detecting abnormalities across various imaging modalities, existing AI methods for ultrasound face two major challenges. First, they typically require vast amounts of labeled medical data, raising serious concerns regarding patient privacy. Second, most models are designed for specific tasks, which restricts their broader clinical utility. To overcome these challenges, we present UltraFedFM, an innovative privacy-preserving ultrasound foundation model. UltraFedFM is collaboratively pre-trained using federated learning across 16 distributed medical institutions in 9 countries, leveraging a dataset of over 1 million ultrasound images covering 19 organs and 10 ultrasound modalities. This extensive and diverse data, combined with a secure training framework, enables UltraFedFM to exhibit strong generalization and diagnostic capabilities. It achieves an average area under the receiver operating characteristic curve (AUROC) of 0.927 for disease diagnosis and a dice similarity coefficient (DSC) of 0.878 for lesion segmentation. Notably, UltraFedFM surpasses the diagnostic accuracy of mid-level ultrasonographers (4-8 years of experience) and matches the performance of expert-level sonographers (10+ years of experience) in the joint diagnosis of 8 common systemic diseases.c These findings indicate that UltraFedFM can significantly enhance clinical diagnostics while safeguarding patient privacy, marking a significant advancement in AI-driven ultrasound imaging for future clinical applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedicoSAM: Robust Improvement of SAM for Medical Imaging</title>
<link>https://arxiv.org/abs/2501.11734</link>
<guid>https://arxiv.org/abs/2501.11734</guid>
<content:encoded><![CDATA[
arXiv:2501.11734v2 Announce Type: replace-cross 
Abstract: Medical image segmentation is an important analysis task in clinical practice and research. Deep learning has massively advanced the field, but current approaches are mostly based on models trained for a specific task. Training such models or adapting them to a new condition is costly due to the need for (manually) labeled data. The emergence of vision foundation models, especially Segment Anything, offers a path to universal segmentation for medical images, overcoming these issues. Here, we study how to improve Segment Anything for medical images by comparing different finetuning strategies on a large and diverse dataset. We evaluate the finetuned models on a wide range of interactive and (automatic) semantic segmentation tasks. We find that the performance can be clearly improved for interactive segmentation. However, semantic segmentation does not benefit from pretraining on medical images. Our best model, MedicoSAM, is publicly available at https://github.com/computational-cell-analytics/medico-sam. We show that it is compatible with existing tools for data annotation and believe that it will be of great practical value.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event Camera Meets Mobile Embodied Perception: Abstraction, Algorithm, Acceleration, Application</title>
<link>https://arxiv.org/abs/2503.22943</link>
<guid>https://arxiv.org/abs/2503.22943</guid>
<content:encoded><![CDATA[
arXiv:2503.22943v4 Announce Type: replace-cross 
Abstract: With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in achieving high-accuracy and low-latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution and low latency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, lack of stable, persistent semantic information, and large data volume pose challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature from 2014 to 2025 and presents a comprehensive overview of event-based mobile sensing, encompassing its fundamental principles, event \textit{abstraction} methods, \textit{algorithm} advancements, and both hardware and software \textit{acceleration} strategies. We discuss key \textit{applications} of event cameras in mobile sensing, including visual odometry, object tracking, optical flow, and 3D reconstruction, while highlighting challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving the event camera with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms. To support ongoing research, we provide an open-source \textit{Online Sheet} with recent developments. We hope this survey serves as a reference, facilitating the adoption of event-based vision across diverse applications.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models</title>
<link>https://arxiv.org/abs/2506.07400</link>
<guid>https://arxiv.org/abs/2506.07400</guid>
<content:encoded><![CDATA[
arXiv:2506.07400v3 Announce Type: replace-cross 
Abstract: The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoSelect: Robust Learning with Noisy Labels via Dynamics Temporal Memory</title>
<link>https://arxiv.org/abs/2507.18183</link>
<guid>https://arxiv.org/abs/2507.18183</guid>
<content:encoded><![CDATA[
arXiv:2507.18183v2 Announce Type: replace-cross 
Abstract: Training deep neural networks on real-world datasets is often hampered by the presence of noisy labels, which can be memorized by over-parameterized models, leading to significant degradation in generalization performance. While existing methods for learning with noisy labels (LNL) have made considerable progress, they fundamentally suffer from static snapshot evaluations and fail to leverage the rich temporal dynamics of learning evolution. In this paper, we propose ChronoSelect (chrono denoting its temporal nature), a novel framework featuring an innovative four-stage memory architecture that compresses prediction history into compact temporal distributions. Our unique sliding update mechanism with controlled decay maintains only four dynamic memory units per sample, progressively emphasizing recent patterns while retaining essential historical knowledge. This enables precise three-way sample partitioning into clean, boundary, and noisy subsets through temporal trajectory analysis and dual-branch consistency. Theoretical guarantees prove the mechanism's convergence and stability under noisy conditions. Extensive experiments demonstrate ChronoSelect's state-of-the-art performance across synthetic and real-world benchmarks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Registering the 4D Millimeter Wave Radar Point Clouds Via Generalized Method of Moments</title>
<link>https://arxiv.org/abs/2508.02187</link>
<guid>https://arxiv.org/abs/2508.02187</guid>
<content:encoded><![CDATA[
arXiv:2508.02187v2 Announce Type: replace-cross 
Abstract: 4D millimeter wave radars (4D radars) are new emerging sensors that provide point clouds of objects with both position and radial velocity measurements. Compared to LiDARs, they are more affordable and reliable sensors for robots' perception under extreme weather conditions. On the other hand, point cloud registration is an essential perception module that provides robot's pose feedback information in applications such as Simultaneous Localization and Mapping (SLAM). Nevertheless, the 4D radar point clouds are sparse and noisy compared to those of LiDAR, and hence we shall confront great challenges in registering the radar point clouds. To address this issue, we propose a point cloud registration framework for 4D radars based on Generalized Method of Moments. The method does not require explicit point-to-point correspondences between the source and target point clouds, which is difficult to compute for sparse 4D radar point clouds. Moreover, we show the consistency of the proposed method. Experiments on both synthetic and real-world datasets show that our approach achieves higher accuracy and robustness than benchmarks, and the accuracy is even comparable to LiDAR-based frameworks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control-Augmented Autoregressive Diffusion for Data Assimilation</title>
<link>https://arxiv.org/abs/2510.06637</link>
<guid>https://arxiv.org/abs/2510.06637</guid>
<content:encoded><![CDATA[
arXiv:2510.06637v2 Announce Type: replace-cross 
Abstract: Despite recent advances in test-time scaling and finetuning of diffusion models, guidance in Auto-Regressive Diffusion Models (ARDMs) remains underexplored. We introduce an amortized framework that augments a pretrained ARDM with a lightweight controller network, trained offline by previewing future rollouts to output stepwise controls that anticipate upcoming observations under a terminal-cost objective. Our approach is motivated by viewing guided generation as an entropy-regularized stochastic optimal control problem over ARDM trajectories: we learn a reusable policy that injects small control corrections inside each denoising sub-step while remaining anchored to the pretrained dynamics. We evaluate this framework in the context of data assimilation (DA) for chaotic spatiotemporal partial differential equations (PDEs), where existing methods can be computationally prohibitive and prone to forecast drift under sparse observations. At inference, DA reduces to a single causal forward rollout with on-the-fly corrections, requiring neither adjoint computations nor gradient-based optimization, and yields an order-of-magnitude speedup over strong diffusion-based DA baselines. Across two canonical PDEs and six observation regimes, our method consistently improves stability, accuracy, and physics-aware fidelity over state-of-the-art baselines. We will release code and checkpoints publicly.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</title>
<link>https://arxiv.org/abs/2510.12691</link>
<guid>https://arxiv.org/abs/2510.12691</guid>
<content:encoded><![CDATA[
arXiv:2510.12691v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.01140</link>
<guid>https://arxiv.org/abs/2511.01140</guid>
<content:encoded><![CDATA[
arXiv:2511.01140v2 Announce Type: replace-cross 
Abstract: Medical imaging often operates under limited labeled data, especially in rare disease and low resource clinical environments. Existing multimodal and meta learning approaches improve performance in these settings but lack a theoretical explanation of why or when they succeed. This paper presents a unified theoretical framework for few shot multimodal medical imaging that jointly characterizes sample complexity, uncertainty quantification, and interpretability. Using PAC learning, VC theory, and PAC Bayesian analysis, we derive bounds that describe the minimum number of labeled samples required for reliable performance and show how complementary modalities reduce effective capacity through an information gain term. We further introduce a formal metric for explanation stability, proving that explanation variance decreases at an inverse n rate. A sequential Bayesian interpretation of Chain of Thought reasoning is also developed to show stepwise posterior contraction. To illustrate these ideas, we implement a controlled multimodal dataset and evaluate an additive CNN MLP fusion model under few shot regimes, confirming predicted multimodal gains, modality interference at larger sample sizes, and shrinking predictive uncertainty. Together, the framework provides a principled foundation for designing data efficient, uncertainty aware, and interpretable diagnostic models in low resource settings.
]]></content:encoded>
<pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complex Mathematical Expression Recognition: Benchmark, Large-Scale Dataset and Strong Baseline</title>
<link>https://arxiv.org/abs/2512.13731</link>
<guid>https://arxiv.org/abs/2512.13731</guid>
<content:encoded><![CDATA[
<div> Keywords: Mathematical Expression Recognition, complex expressions, datasets, Structured Mathematical Language, CMERNet<br /><br />Summary:<br />Mathematical Expression Recognition (MER) has advanced in recognizing simple expressions but struggles with complex expressions that involve many tokens and multi-line layouts. To address this, the authors introduce CMER-Bench, a benchmark dataset that categorizes mathematical expressions into three difficulty levels: easy, moderate, and complex. They use this benchmark to evaluate current MER models as well as multimodal large language models (MLLMs), finding that performance significantly declines on complex expressions due to a lack of complex samples in existing datasets. To overcome this limitation, the authors create two new large-scale datasets, MER-17M and CMER-3M, which focus on complex mathematical expressions and provide diverse samples for robust model training. Furthermore, they develop a novel expression tokenizer and propose a new representation called Structured Mathematical Language, which explicitly captures the hierarchical and spatial structures of mathematical expressions, going beyond traditional LaTeX representations. Building on these innovations, the authors design CMERNet, an encoder-decoder based model trained on the CMER-3M dataset. Despite having only 125 million parameters, CMERNet significantly outperforms existing MER models and general-purpose MLLMs on CMER-Bench, demonstrating superior accuracy and robustness in recognizing complex mathematical expressions. <div>
arXiv:2512.13731v1 Announce Type: new 
Abstract: Mathematical Expression Recognition (MER) has made significant progress in recognizing simple expressions, but the robust recognition of complex mathematical expressions with many tokens and multiple lines remains a formidable challenge. In this paper, we first introduce CMER-Bench, a carefully constructed benchmark that categorizes expressions into three difficulty levels: easy, moderate, and complex. Leveraging CMER-Bench, we conduct a comprehensive evaluation of existing MER models and general-purpose multimodal large language models (MLLMs). The results reveal that while current methods perform well on easy and moderate expressions, their performance degrades significantly when handling complex mathematical expressions, mainly because existing public training datasets are primarily composed of simple samples. In response, we propose MER-17M and CMER-3M that are large-scale datasets emphasizing the recognition of complex mathematical expressions. The datasets provide rich and diverse samples to support the development of accurate and robust complex MER models. Furthermore, to address the challenges posed by the complicated spatial layout of complex expressions, we introduce a novel expression tokenizer, and a new representation called Structured Mathematical Language, which explicitly models the hierarchical and spatial structure of expressions beyond LaTeX format. Based on these, we propose a specialized model named CMERNet, built upon an encoder-decoder architecture and trained on CMER-3M. Experimental results show that CMERNet, with only 125 million parameters, significantly outperforms existing MER models and MLLMs on CMER-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-AI Collaboration Mechanism Study on AIGC Assisted Image Production for Special Coverage</title>
<link>https://arxiv.org/abs/2512.13739</link>
<guid>https://arxiv.org/abs/2512.13739</guid>
<content:encoded><![CDATA[
<div> Artificial Intelligence Generated Content, journalism, semantic alignment, human-in-the-loop, cultural specificity  

<br /><br />Summary: This paper addresses the challenges of using Artificial Intelligence Generated Content (AIGC) to assist image production in journalism, focusing on issues such as misinformation, authenticity, semantic fidelity, and interpretability. It emphasizes that many AIGC tools function as opaque "black boxes," complicating efforts to ensure content accuracy and semantic alignment. The study explores methods for controllable image production specifically tailored for special coverage in journalism. Two experiments are conducted with a Chinese media agency to test these methods. The first experiment evaluates cross-platform adaptability through standardized prompts used across three different scenes, uncovering variations in semantic alignment, cultural specificity, and visual realism caused by both training data biases and platform filtering mechanisms. The second experiment develops a modular human-in-the-loop pipeline integrating components for precise segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulation (Style-LoRA, Prompt-to-Prompt). This pipeline incorporates editorial checks such as CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials to maintain fidelity and traceability. The paper proposes a human-AI collaborative framework for AIGC-assisted image production in journalism and suggests evaluation metrics including Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA). <div>
arXiv:2512.13739v1 Announce Type: new 
Abstract: Artificial Intelligence Generated Content (AIGC) assisting image production triggers controversy in journalism while attracting attention from media agencies. Key issues involve misinformation, authenticity, semantic fidelity, and interpretability. Most AIGC tools are opaque "black boxes," hindering the dual demands of content accuracy and semantic alignment and creating ethical, sociotechnical, and trust dilemmas. This paper explores pathways for controllable image production in journalism's special coverage and conducts two experiments with projects from China's media agency: (1) Experiment 1 tests cross-platform adaptability via standardized prompts across three scenes, revealing disparities in semantic alignment, cultural specificity, and visual realism driven by training-corpus bias and platform-level filtering. (2) Experiment 2 builds a human-in-the-loop modular pipeline combining high-precision segmentation (SAM, GroundingDINO), semantic alignment (BrushNet), and style regulating (Style-LoRA, Prompt-to-Prompt), ensuring editorial fidelity through CLIP-based semantic scoring, NSFW/OCR/YOLO filtering, and verifiable content credentials. Traceable deployment preserves semantic representation. Consequently, we propose a human-AI collaboration mechanism for AIGC assisted image production in special coverage and recommend evaluating Character Identity Stability (CIS), Cultural Expression Accuracy (CEA), and User-Public Appropriateness (U-PA).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models</title>
<link>https://arxiv.org/abs/2512.13742</link>
<guid>https://arxiv.org/abs/2512.13742</guid>
<content:encoded><![CDATA[
<div> Keywords: Medical image classification, Large language models, Clinical reasoning, MobileCoAtNet, Endoscopic images<br /><br />Summary:<br /><br />This study addresses the gap between medical image classification and clinical reasoning by integrating deep learning (DL) models with large language models (LLMs). The authors introduce MobileCoAtNet, a hybrid model tailored for endoscopic images, achieving high accuracy in classifying eight stomach-related diseases. They leverage the classifier’s outputs to prompt multiple LLMs to generate structured clinical explanations involving causes, symptoms, treatments, lifestyle, and follow-up care. To evaluate these explanations, two expert-verified benchmarks were developed as gold standards. Thirty-two LLMs were assessed against these benchmarks, revealing that stronger image classification enhances the quality of LLM-generated narratives, but none reach human-level consistency or stability. The best-performing LLMs still show variability in reasoning when input prompts change, highlighting current limitations. The research demonstrates the potential of combining DL with LLMs to create useful clinical narratives, while cautioning that existing LLMs are not yet reliable for critical medical decisions. The proposed framework clarifies LLMs’ constraints and suggests directions for developing safer and more stable clinical reasoning systems. Additionally, the authors have made the source code and datasets publicly available for further research and validation. <div>
arXiv:2512.13742v1 Announce Type: new 
Abstract: Medical image classifiers detect gastrointestinal diseases well, but they do not explain their decisions. Large language models can generate clinical text, yet they struggle with visual reasoning and often produce unstable or incorrect explanations. This leaves a gap between what a model sees and the type of reasoning a clinician expects. We introduce a framework that links image classification with structured clinical reasoning. A new hybrid model, MobileCoAtNet, is designed for endoscopic images and achieves high accuracy across eight stomach-related classes. Its outputs are then used to drive reasoning by several LLMs. To judge this reasoning, we build two expert-verified benchmarks covering causes, symptoms, treatment, lifestyle, and follow-up care. Thirty-two LLMs are evaluated against these gold standards. Strong classification improves the quality of their explanations, but none of the models reach human-level stability. Even the best LLMs change their reasoning when prompts vary. Our study shows that combining DL with LLMs can produce useful clinical narratives, but current LLMs remain unreliable for high-stakes medical decisions. The framework provides a clearer view of their limits and a path for building safer reasoning systems. The complete source code and datasets used in this study are available at https://github.com/souravbasakshuvo/DL3M.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Text Prevails: Vision May Undermine Multimodal Medical Decision Making</title>
<link>https://arxiv.org/abs/2512.13747</link>
<guid>https://arxiv.org/abs/2512.13747</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal large language models, medical decision making, Alzheimer's disease classification, MIMIC-CXR, vision-text reasoning<br /><br />Summary:<br /><br />1. Advanced multimodal large language models (MLLMs) have shown strong zero-shot capabilities in vision-language tasks but face significant challenges in the biomedical domain, particularly with fundamental Medical Decision Making (MDM) tasks.<br />2. The study evaluates MLLMs on two difficult datasets: a three-stage Alzheimer's disease classification task, where categories differ subtly, and the MIMIC-CXR dataset involving classification of 14 overlapping chest radiograph conditions.<br />3. Empirical results reveal that text-only reasoning consistently surpasses vision-only or combined vision-text approaches, with multimodal inputs occasionally performing worse than text alone.<br />4. To address these limitations, three strategies are tested: (a) in-context learning using reason-annotated exemplars, (b) employing vision captioning followed by text-only inference, and (c) few-shot fine-tuning of the vision encoder with classification supervision.<br />5. The findings demonstrate that current MLLMs lack robust grounded visual understanding in medical contexts and highlight promising directions to enhance multimodal medical decision-making systems. <div>
arXiv:2512.13747v1 Announce Type: new 
Abstract: With the rapid progress of large language models (LLMs), advanced multimodal large language models (MLLMs) have demonstrated impressive zero-shot capabilities on vision-language tasks. In the biomedical domain, however, even state-of-the-art MLLMs struggle with basic Medical Decision Making (MDM) tasks. We investigate this limitation using two challenging datasets: (1) three-stage Alzheimer's disease (AD) classification (normal, mild cognitive impairment, dementia), where category differences are visually subtle, and (2) MIMIC-CXR chest radiograph classification with 14 non-mutually exclusive conditions. Our empirical study shows that text-only reasoning consistently outperforms vision-only or vision-text settings, with multimodal inputs often performing worse than text alone. To mitigate this, we explore three strategies: (1) in-context learning with reason-annotated exemplars, (2) vision captioning followed by text-only inference, and (3) few-shot fine-tuning of the vision tower with classification supervision. These findings reveal that current MLLMs lack grounded visual understanding and point to promising directions for improving multimodal decision making in healthcare.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR: STacked AutoRegressive Scheme for Unified Multimodal Learning</title>
<link>https://arxiv.org/abs/2512.13752</link>
<guid>https://arxiv.org/abs/2512.13752</guid>
<content:encoded><![CDATA[
<div> Multimodal learning, autoregressive model, unified multimodal understanding, task-progressive learning, image representation<br /><br />Summary:<br /><br />This paper introduces STAR, a novel framework designed for unified multimodal learning, addressing the challenges of simultaneously achieving both multimodal understanding and generation. The method decomposes the learning process into three progressive stages: understanding, generation, and editing, which helps isolate and optimize each task effectively. A key innovation is the use of a STacked AutoRegressive (AR) scheme, where the base AR model's parameters are frozen while additional isomorphic AR modules are sequentially stacked, preventing interference between tasks and enabling scalability of capabilities. To enhance image representation granularity, STAR employs a high-capacity vector quantization (VQ) mechanism. Furthermore, an implicit reasoning mechanism is integrated to improve the model's ability to generate high-quality content, especially under complex scenarios. Experimental validation demonstrates that STAR surpasses previous models by achieving state-of-the-art results on multiple benchmarks: GenEval with a score of 0.91, DPG-Bench with 87.44, and ImgEdit with 4.34. These results collectively confirm STAR’s effectiveness in providing a unified framework that balances multimodal comprehension and generative performance without compromising on either aspect. <div>
arXiv:2512.13752v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) play a pivotal role in advancing the quest for general artificial intelligence. However, achieving unified target for multimodal understanding and generation remains challenging due to optimization conflicts and performance trade-offs. To effectively enhance generative performance while preserving existing comprehension capabilities, we introduce STAR: a STacked AutoRegressive scheme for task-progressive unified multimodal learning. This approach decomposes multimodal learning into multiple stages: understanding, generation, and editing. By freezing the parameters of the fundamental autoregressive (AR) model and progressively stacking isomorphic AR modules, it avoids cross-task interference while expanding the model's capabilities. Concurrently, we introduce a high-capacity VQ to enhance the granularity of image representations and employ an implicit reasoning mechanism to improve generation quality under complex conditions. Experiments demonstrate that STAR achieves state-of-the-art performance on GenEval (0.91), DPG-Bench (87.44), and ImgEdit (4.34), validating its efficacy for unified multimodal learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-aware UNet and super-resolution deep residual networks for spatial downscaling</title>
<link>https://arxiv.org/abs/2512.13753</link>
<guid>https://arxiv.org/abs/2512.13753</guid>
<content:encoded><![CDATA[
<div> Keywords: satellite data, spatial downscaling, deep learning, tropospheric ozone, temporal encoding  

<br /><br />Summary:  
This article addresses the challenge of improving the spatial resolution of satellite data on atmospheric pollutants, specifically for tropospheric ozone, which is often limited by coarse spatial resolution. The study investigates two popular deep learning architectures for spatial downscaling: the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet. Both architectures are enhanced with a lightweight temporal module designed to incorporate the observation time into the model, leveraging either sinusoidal or radial basis function (RBF) encoding methods. This temporal information is fused with spatial features within the networks. The extended models are evaluated through a case study focused on ozone downscaling over Italy. Results show that these time-aware extensions improve the downscaling performance, providing more accurate high-resolution ozone fields compared to baseline models without temporal modules. Additionally, the temporal modules contribute to faster convergence during training while only modestly increasing computational complexity. The findings demonstrate the value of integrating temporal encoding in deep learning architectures aimed at spatial downscaling of environmental satellite data, enhancing their application for local-scale environmental analysis and decision-making. <div>
arXiv:2512.13753v1 Announce Type: new 
Abstract: Satellite data of atmospheric pollutants are often available only at coarse spatial resolution, limiting their applicability in local-scale environmental analysis and decision-making. Spatial downscaling methods aim to transform the coarse satellite data into high-resolution fields. In this work, two widely used deep learning architectures, the super-resolution deep residual network (SRDRN) and the encoder-decoder-based UNet, are considered for spatial downscaling of tropospheric ozone. Both methods are extended with a lightweight temporal module, which encodes observation time using either sinusoidal or radial basis function (RBF) encoding, and fuses the temporal features with the spatial representations in the networks. The proposed time-aware extensions are evaluated against their baseline counterparts in a case study on ozone downscaling over Italy. The results suggest that, while only slightly increasing computational complexity, the temporal modules significantly improve downscaling performance and convergence speed.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nexels: Neurally-Textured Surfels for Real-Time Novel View Synthesis with Sparse Geometries</title>
<link>https://arxiv.org/abs/2512.13796</link>
<guid>https://arxiv.org/abs/2512.13796</guid>
<content:encoded><![CDATA[
<div> Gaussian splatting, novel view synthesis, surfels, neural field, textured primitives<br /><br />Summary:<br /><br />This paper addresses the limitations of Gaussian splatting in novel view synthesis, particularly its need for millions of primitives to accurately model textured scenes even when geometry is simple. The authors propose a new representation that decouples geometry and appearance to create a more compact and efficient model. Geometry is represented using surfels, while appearance is modeled with a combination of a global neural field and per-primitive colors. This approach textures a fixed number of primitives per pixel, ensuring low computational overhead. Compared to 3D Gaussian splatting, the proposed method delivers comparable perceptual quality while drastically reducing resource usage—using 9.7 times fewer primitives and 5.5 times less memory on outdoor scenes, and 31 times fewer primitives and 3.7 times less memory on indoor scenes. Additionally, the method renders at twice the speed of existing textured primitive techniques while improving visual quality. Overall, the representation offers a significant improvement in efficiency and rendering speed without sacrificing visual fidelity, making it highly suitable for applications involving complex textured scene synthesis. <div>
arXiv:2512.13796v1 Announce Type: new 
Abstract: Though Gaussian splatting has achieved impressive results in novel view synthesis, it requires millions of primitives to model highly textured scenes, even when the geometry of the scene is simple. We propose a representation that goes beyond point-based rendering and decouples geometry and appearance in order to achieve a compact representation. We use surfels for geometry and a combination of a global neural field and per-primitive colours for appearance. The neural field textures a fixed number of primitives for each pixel, ensuring that the added compute is low. Our representation matches the perceptual quality of 3D Gaussian splatting while using $9.7\times$ fewer primitives and $5.5\times$ less memory on outdoor scenes and using $31\times$ fewer primitives and $3.7\times$ less memory on indoor scenes. Our representation also renders twice as fast as existing textured primitives while improving upon their visual quality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VajraV1 -- The most accurate Real Time Object Detector of the YOLO family</title>
<link>https://arxiv.org/abs/2512.13834</link>
<guid>https://arxiv.org/abs/2512.13834</guid>
<content:encoded><![CDATA[
<div> Keywords: VajraV1, real-time object detection, YOLO, mAP, COCO validation set<br /><br />Summary:<br /><br />This technical report introduces VajraV1, a new model architecture for real-time object detection that builds upon and enhances existing YOLO-based detectors. VajraV1 incorporates effective design elements from previous YOLO versions (YOLOv10 to YOLOv13) to improve both accuracy and inference speed. The model is evaluated on the COCO validation dataset, where it shows significant performance gains across multiple variants. VajraV1-Nano achieves a mean Average Precision (mAP) of 44.3%, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7%, while maintaining comparable latency to YOLOv12-N and YOLOv11-N. The Small variant of VajraV1 reaches 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium attains 52.7% mAP, slightly better by 0.2% than YOLOv12-M. The Large model variant achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. Finally, VajraV1-Xlarge delivers a state-of-the-art 56.2% mAP, outperforming all existing real-time object detectors. Overall, VajraV1 demonstrates significant improvements in detection accuracy across different model sizes while maintaining competitive inference speeds, positioning it as a leading real-time object detection architecture. <div>
arXiv:2512.13834v1 Announce Type: new 
Abstract: Recent years have seen significant advances in real-time object detection, with the release of YOLOv10, YOLO11, YOLOv12, and YOLOv13 between 2024 and 2025. This technical report presents the VajraV1 model architecture, which introduces architectural enhancements over existing YOLO-based detectors. VajraV1 combines effective design choices from prior YOLO models to achieve state-of-the-art accuracy among real-time object detectors while maintaining competitive inference speed.
  On the COCO validation set, VajraV1-Nano achieves 44.3% mAP, outperforming YOLOv12-N by 3.7% and YOLOv13-N by 2.7% at latency competitive with YOLOv12-N and YOLOv11-N. VajraV1-Small achieves 50.4% mAP, exceeding YOLOv12-S and YOLOv13-S by 2.4%. VajraV1-Medium achieves 52.7% mAP, outperforming YOLOv12-M by 0.2%. VajraV1-Large achieves 53.7% mAP, surpassing YOLOv13-L by 0.3%. VajraV1-Xlarge achieves 56.2% mAP, outperforming all existing real-time object detectors.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoLingo: Motion-Language Alignment for Text-to-Motion Generation</title>
<link>https://arxiv.org/abs/2512.13840</link>
<guid>https://arxiv.org/abs/2512.13840</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-motion, diffusion, latent space, cross-attention, human motion generation<br /><br />Summary: We introduce MoLingo, an advanced text-to-motion (T2M) model that generates realistic human motion through denoising in a continuous latent space. Unlike previous methods that perform latent diffusion either on the entire latent at once or auto-regressively over multiple latents, this work focuses on optimizing diffusion specifically for continuous motion latents. The paper addresses two main challenges: first, building a semantically aligned latent space where motion representations that share similar textual meanings remain close together, improving the efficiency and effectiveness of diffusion. This is achieved by training a semantic-aligned motion encoder using frame-level text labels. Second, it investigates how to inject text conditioning optimally so that the generated motion aligns closely with the text description. The study compares single-token conditioning against a multi-token cross-attention mechanism, concluding that cross-attention significantly improves both motion realism and text-motion correspondence. By combining semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, MoLingo establishes a new state of the art on standard human motion generation benchmarks and user studies. The authors plan to publicly release their code and models to support further research and applications. <div>
arXiv:2512.13840v1 Announce Type: new 
Abstract: We introduce MoLingo, a text-to-motion (T2M) model that generates realistic, lifelike human motion by denoising in a continuous latent space. Recent works perform latent space diffusion, either on the whole latent at once or auto-regressively over multiple latents. In this paper, we study how to make diffusion on continuous motion latents work best. We focus on two questions: (1) how to build a semantically aligned latent space so diffusion becomes more effective, and (2) how to best inject text conditioning so the motion follows the description closely. We propose a semantic-aligned motion encoder trained with frame-level text labels so that latents with similar text meaning stay close, which makes the latent space more diffusion-friendly. We also compare single-token conditioning with a multi-token cross-attention scheme and find that cross-attention gives better motion realism and text-motion alignment. With semantically aligned latents, auto-regressive generation, and cross-attention text conditioning, our model sets a new state of the art in human motion generation on standard metrics and in a user study. We will release our code and models for further research and downstream usage.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging</title>
<link>https://arxiv.org/abs/2512.13855</link>
<guid>https://arxiv.org/abs/2512.13855</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Segmentation Models, Parameter-Efficient Fine-Tuning, Telescopic Adapters, Medical Imaging, Transformer Layers<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting Vision Language Segmentation Models (VLSMs), such as CLIPSeg, to medical imaging domains, where conventional fine-tuning demands extensive computational resources. The authors identify limitations in existing Parameter-Efficient Fine-Tuning (PEFT) methods, which apply uniform adapter sizes to all transformer layers, resulting in inefficient parameter use and suboptimal adaptation. To solve this, they propose Telescopic Adapters, a novel PEFT framework that scales adapter capacity progressively from shallow to deep transformer layers, based on layer depth and semantic relevance. This depth-aware scaling hypothesizes that deeper layers require greater adaptation capacity, a claim validated through comprehensive ablation studies. Implemented by integrating lightweight bottleneck modules into both vision and text encoders of CLIPSeg, the approach trains only 613k parameters, reducing the parameter count by 244 times compared to full fine-tuning. Experiments conducted on five diverse medical datasets—including polyp segmentation, skin lesion detection, and breast ultrasound imaging—demonstrate superior segmentation performance using Telescopic Adapters. The method enables effective VLSM adaptation in resource-constrained clinical settings while maintaining competitive accuracy, establishing a new efficient paradigm for medical imaging model fine-tuning. <div>
arXiv:2512.13855v1 Announce Type: new 
Abstract: Adapting Vision Language Segmentation Models (VLSMs) to medical imaging domains requires significant computational overhead when using conventional fine-tuning approaches. Existing Parameter-Efficient Fine-Tuning (PEFT) methods apply uniform adapter dimensions across all transformer layers, leading to suboptimal parameter allocation and reduced adaptation efficiency. We introduce Telescopic Adapters, a novel PEFT framework that employs depth-aware scaling to progressively increase adapter capacity from shallow to deep transformer layers. Our method integrates lightweight bottleneck modules within CLIPSeg's vision and text encoders, with adapter dimensions dynamically scaled based on layer depth and semantic relevance. Using only 613k trainable parameters--244x fewer than end-to-end fine-tuning, Telescopic Adapters achieve superior performance across five diverse medical datasets spanning polyp segmentation, skin lesion detection, and breast ultrasound imaging. Comprehensive ablation studies demonstrate that deeper layers require substantially more adaptation capacity than shallow layers, validating our telescopic scaling hypothesis. Our approach establishes a new paradigm for efficient medical VLSM fine-tuning, enabling deployment in resource-constrained clinical environments while maintaining competitive segmentation accuracy.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coarse-to-Fine Hierarchical Alignment for UAV-based Human Detection using Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13869</link>
<guid>https://arxiv.org/abs/2512.13869</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV human detection, synthetic data, domain adaptation, diffusion model, hierarchical alignment<br /><br />Summary: Training object detectors for UAV-based human detection is challenged by constantly changing target distributions and limited labeled real-world images. To address this, synthetic simulators generate annotated data with low cost but suffer from a domain gap between synthetic and real images. The paper proposes Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework aimed at narrowing this domain gap while preserving original synthetic labels. CFHA involves (1) Global Style Transfer, which uses a diffusion model to align the color, illumination, and texture of synthetic images to real image style based on a small real reference set; (2) Local Refinement, employing a super-resolution diffusion model to enhance photorealistic details of small objects like humans while maintaining shape and boundary integrity; and (3) Hallucination Removal, filtering out synthetic human instances whose visual attributes mismatch real-world data to refine appearance closer to the target distribution. Experimental results on UAV Sim2Real detection benchmarks demonstrate significant improvements in detection accuracy, with up to +14.1 mAP50 gain on the Semantic-Drone benchmark. Ablation studies validate the complementary roles of global and local alignment stages and emphasize the importance of hierarchical domain adaptation. The framework and code are publicly available at the provided GitHub repository. <div>
arXiv:2512.13869v1 Announce Type: new 
Abstract: Training object detectors demands extensive, task-specific annotations, yet this requirement becomes impractical in UAV-based human detection due to constantly shifting target distributions and the scarcity of labeled images. As a remedy, synthetic simulators are adopted to generate annotated data, with a low annotation cost. However, the domain gap between synthetic and real images hinders the model from being effectively applied to the target domain. Accordingly, we introduce Coarse-to-Fine Hierarchical Alignment (CFHA), a three-stage diffusion-based framework designed to transform synthetic data for UAV-based human detection, narrowing the domain gap while preserving the original synthetic labels. CFHA explicitly decouples global style and local content domain discrepancies and bridges those gaps using three modules: (1) Global Style Transfer -- a diffusion model aligns color, illumination, and texture statistics of synthetic images to the realistic style, using only a small real reference set; (2) Local Refinement -- a super-resolution diffusion model is used to facilitate fine-grained and photorealistic details for the small objects, such as human instances, preserving shape and boundary integrity; (3) Hallucination Removal -- a module that filters out human instances whose visual attributes do not align with real-world data to make the human appearance closer to the target distribution. Extensive experiments on public UAV Sim2Real detection benchmarks demonstrate that our methods significantly improve the detection accuracy compared to the non-transformed baselines. Specifically, our method achieves up to $+14.1$ improvement of mAP50 on Semantic-Drone benchmark. Ablation studies confirm the complementary roles of the global and local stages and highlight the importance of hierarchical alignment. The code is released at \href{https://github.com/liwd190019/CFHA}{this url}.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAGE: Training Smart Any-Horizon Agents for Long Video Reasoning with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13874</link>
<guid>https://arxiv.org/abs/2512.13874</guid>
<content:encoded><![CDATA[
<div> any-horizon reasoning, multi-turn reasoning, video understanding, reinforcement learning, synthetic data generation  

<br /><br />Summary:  
The paper introduces SAGE, an agent system designed to mimic human flexible video reasoning by supporting any-horizon video understanding, meaning it can handle both long and short videos effectively by performing multi-turn reasoning for complex videos and single-turn reasoning for simpler tasks. To develop this system, the authors create a synthetic data generation pipeline using Gemini-2.5-Flash, which is used to train SAGE-MM, the critical orchestrator module of SAGE. They also propose a reinforcement learning (RL) post-training method that is crucial for enabling the any-horizon reasoning capability of SAGE-MM. To evaluate the system's performance in realistic scenarios, the authors curate SAGE-Bench, a benchmark comprising videos averaging over 700 seconds with real-world entertainment use cases. Experimental results demonstrate that SAGE, along with the data pipeline and RL training, significantly improves video reasoning performance, achieving up to 6.1% gains on open-ended video reasoning tasks overall, and up to 8.2% improvement specifically on videos longer than 10 minutes. This work sets a new direction toward resource-efficient and flexible video understanding models that better align with natural human reasoning across varied video durations. <div>
arXiv:2512.13874v1 Announce Type: new 
Abstract: As humans, we are natural any-horizon reasoners, i.e., we can decide whether to iteratively skim long videos or watch short ones in full when necessary for a given task. With this in mind, one would expect video reasoning models to reason flexibly across different durations. However, SOTA models are still trained to predict answers in a single turn while processing a large number of frames, akin to watching an entire long video, requiring significant resources. This raises the question: Is it possible to develop performant any-horizon video reasoning systems? Inspired by human behavior, we first propose SAGE, an agent system that performs multi-turn reasoning on long videos while handling simpler problems in a single turn. Secondly, we introduce an easy synthetic data generation pipeline using Gemini-2.5-Flash to train the orchestrator, SAGE-MM, which lies at the core of SAGE. We further propose an effective RL post-training recipe essential for instilling any-horizon reasoning ability in SAGE-MM. Thirdly, we curate SAGE-Bench with an average duration of greater than 700 seconds for evaluating video reasoning ability in real-world entertainment use cases. Lastly, we empirically validate the effectiveness of our system, data, and RL recipe, observing notable improvements of up to 6.1% on open-ended video reasoning tasks, as well as an impressive 8.2% improvement on videos longer than 10 minutes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Route-DETR: Pairwise Query Routing in Transformers for Object Detection</title>
<link>https://arxiv.org/abs/2512.13876</link>
<guid>https://arxiv.org/abs/2512.13876</guid>
<content:encoded><![CDATA[
<div> Keywords: DETR, object detection, adaptive routing, decoder self-attention, query competition  

<br /><br />Summary:  
Detection Transformer (DETR) is an end-to-end object detection framework that removes the need for hand-crafted components like non-maximum suppression but faces challenges due to inefficient query competition, where multiple queries focus on the same object and cause redundant computations. Route-DETR is proposed to solve this by using adaptive pairwise routing within decoder self-attention layers, effectively differentiating between competing queries (aimed at the same object) and complementary queries (aimed at different objects). It incorporates dual routing mechanisms: suppressor routes that reduce duplication by modulating attention between competing queries, and delegator routes that promote exploration of diverse regions. These mechanisms are realized through learnable low-rank attention biases allowing asymmetric interactions among queries. A dual-branch training approach applies these routing biases only during training, keeping inference unchanged to avoid extra computational cost. Evaluations on COCO and Cityscapes datasets show that Route-DETR consistently improves results across multiple DETR baselines, achieving a +1.7% mean average precision (mAP) gain over the DINO baseline on ResNet-50 and reaching a state-of-the-art 57.6% mAP with Swin-L backbone. This confirms Route-DETR’s effectiveness in addressing query competition in DETR for enhanced object detection performance. <div>
arXiv:2512.13876v1 Announce Type: new 
Abstract: Detection Transformer (DETR) offers an end-to-end solution for object detection by eliminating hand-crafted components like non-maximum suppression. However, DETR suffers from inefficient query competition where multiple queries converge to similar positions, leading to redundant computations. We present Route-DETR, which addresses these issues through adaptive pairwise routing in decoder self-attention layers. Our key insight is distinguishing between competing queries (targeting the same object) versus complementary queries (targeting different objects) using inter-query similarity, confidence scores, and geometry. We introduce dual routing mechanisms: suppressor routes that modulate attention between competing queries to reduce duplication, and delegator routes that encourage exploration of different regions. These are implemented via learnable low-rank attention biases enabling asymmetric query interactions. A dual-branch training strategy incorporates routing biases only during training while preserving standard attention for inference, ensuring no additional computational cost. Experiments on COCO and Cityscapes demonstrate consistent improvements across multiple DETR baselines, achieving +1.7% mAP gain over DINO on ResNet-50 and reaching 57.6% mAP on Swin-L, surpassing prior state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KLO-Net: A Dynamic K-NN Attention U-Net with CSP Encoder for Efficient Prostate Gland Segmentation from MRI</title>
<link>https://arxiv.org/abs/2512.13902</link>
<guid>https://arxiv.org/abs/2512.13902</guid>
<content:encoded><![CDATA[
<div> Prostate segmentation, MRI, K-Nearest Neighbor attention, Cross Stage Partial, deep learning<br /><br />Summary:  
This paper addresses the challenge of real-time prostate MRI segmentation on clinical workstations, where computational load and memory constraints limit performance. The authors propose KLO-Net, a novel dynamic K-Nearest Neighbor (K-NN) attention U-Net combined with a Cross Stage Partial (CSP) encoder to improve efficiency and segmentation accuracy. Unlike traditional K-NN attention, the dynamic K-NN attention mechanism in KLO-Net adaptively determines the number of attention connections for each spatial location within a slice, enhancing model flexibility and localization. The CSP blocks integrated into the architecture reduce memory consumption and computational overhead, making the model more suitable for real-time application. To validate the effectiveness of their approach, the authors conduct comprehensive experiments and ablation studies on two public MRI datasets—PROMISE12 and PROSTATEx. The results show that KLO-Net achieves a favorable balance of computational efficiency and accurate prostate gland segmentation compared to existing methods. The study highlights the benefits of combining adaptive attention mechanisms with efficient network design to tackle anatomical variability and resource constraints in clinical prostate MRI segmentation tasks. <div>
arXiv:2512.13902v1 Announce Type: new 
Abstract: Real-time deployment of prostate MRI segmentation on clinical workstations is often bottlenecked by computational load and memory footprint. Deep learning-based prostate gland segmentation approaches remain challenging due to anatomical variability. To bridge this efficiency gap while still maintaining reliable segmentation accuracy, we propose KLO-Net, a dynamic K-Nearest Neighbor attention U-Net with Cross Stage Partial, i.e., CSP, encoder for efficient prostate gland segmentation from MRI scan. Unlike the regular K-NN attention mechanism, the proposed dynamic K-NN attention mechanism allows the model to adaptively determine the number of attention connections for each spatial location within a slice. In addition, CSP blocks address the computational load to reduce memory consumption. To evaluate the model's performance, comprehensive experiments and ablation studies are conducted on two public datasets, i.e., PROMISE12 and PROSTATEx, to validate the proposed architecture. The detailed comparative analysis demonstrates the model's advantage in computational efficiency and segmentation quality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An evaluation of SVBRDF Prediction from Generative Image Models for Appearance Modeling of 3D Scenes</title>
<link>https://arxiv.org/abs/2512.13950</link>
<guid>https://arxiv.org/abs/2512.13950</guid>
<content:encoded><![CDATA[
<div> Keywords: SVBRDF, deep generative models, texture atlas, multiview coherence, UNet<br /><br />Summary:<br /><br />1. The paper addresses the integration of deep generative models in digital content creation, focusing on the synthesis of realistic RGB images aligned with 3D scene geometry for texturing purposes.<br /><br />2. It explores the use of SVBRDF (Spatially Varying Bidirectional Reflectance Distribution Function) prediction networks, which estimate material parameters from RGB images, facilitating detailed appearance modeling.<br /><br />3. By combining conditional image generation and SVBRDF prediction, the authors propose a pipeline to rapidly generate SVBRDF maps from multiple views that can be merged into a coherent SVBRDF texture atlas of a scene.<br /><br />4. The paper discusses challenges such as multiview incoherence caused by single-view SVBRDF prediction, which can lead to inconsistent textures across views in the final atlas.<br /><br />5. Additional information derived from generated RGB images, conditioned on various modalities, can improve SVBRDF estimation beyond what is possible from standard photographs.<br /><br />6. Through a comparative study of different neural network architectures and conditioning strategies, the authors identify designs that achieve both high accuracy and multiview coherence.<br /><br />7. Interestingly, their findings show that a relatively simple and standard UNet architecture performs competitively with more complex models in this context.<br /><br />8. The work offers insights for practitioners aiming to build fast and coherent appearance modeling pipelines leveraging recent advances in generative image synthesis and material prediction.<br /><br />9. The project page provides additional resources and evaluation results: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation <div>
arXiv:2512.13950v1 Announce Type: new 
Abstract: Digital content creation is experiencing a profound change with the advent of deep generative models. For texturing, conditional image generators now allow the synthesis of realistic RGB images of a 3D scene that align with the geometry of that scene. For appearance modeling, SVBRDF prediction networks recover material parameters from RGB images. Combining these technologies allows us to quickly generate SVBRDF maps for multiple views of a 3D scene, which can be merged to form a SVBRDF texture atlas of that scene. In this paper, we analyze the challenges and opportunities for SVBRDF prediction in the context of such a fast appearance modeling pipeline. On the one hand, single-view SVBRDF predictions might suffer from multiview incoherence and yield inconsistent texture atlases. On the other hand, generated RGB images, and the different modalities on which they are conditioned, can provide additional information for SVBRDF estimation compared to photographs. We compare neural architectures and conditions to identify designs that achieve high accuracy and coherence. We find that, surprisingly, a standard UNet is competitive with more complex designs. Project page: http://repo-sam.inria.fr/nerphys/svbrdf-evaluation
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.13953</link>
<guid>https://arxiv.org/abs/2512.13953</guid>
<content:encoded><![CDATA[
<div> Keywords: unbranding, trademark removal, text-to-image diffusion, brand recognition, vision language models<br /><br />Summary:<br /><br />The paper introduces the task of unbranding, aimed at the fine-grained removal of trademarks and subtle structural brand features from images generated by text-to-image diffusion models, while maintaining semantic coherence. The motivation stems from growing concerns over unauthorized reproduction of trademarked content, which extends beyond explicit logos to implicit brand identifiers like distinctive design elements. To support research in this area, the authors create a comprehensive benchmark dataset specifically designed for unbranding. They also identify limitations in existing brand detectors that mostly focus on explicit logos and do not handle abstract trade dress features such as the shape of product containers. To address this evaluation gap, the paper proposes a novel metric based on Vision Language Models (VLMs) using a question-answering approach to detect both explicit and implicit brand characteristics in images. The study also highlights the trend that newer, higher-fidelity text-to-image models (e.g., SDXL, FLUX) tend to generate brand identifiers more readily compared to older models, underscoring the increasing importance of unbranding techniques. Experimental results validated by the proposed VLM metric demonstrate that unbranding is a distinct and practically relevant problem that calls for specialized removal methods. The project page provides further resources and data for ongoing research. <div>
arXiv:2512.13953v1 Announce Type: new 
Abstract: The rapid progress of text-to-image diffusion models raises significant concerns regarding the unauthorized reproduction of trademarked content. While prior work targets general concepts (e.g., styles, celebrities), it fails to address specific brand identifiers. Crucially, we note that brand recognition is multi-dimensional, extending beyond explicit logos to encompass distinctive structural features (e.g., a car's front grille). To tackle this, we introduce unbranding, a novel task for the fine-grained removal of both trademarks and subtle structural brand features, while preserving semantic coherence. To facilitate research, we construct a comprehensive benchmark dataset. Recognizing that existing brand detectors are limited to logos and fail to capture abstract trade dress (e.g., the shape of a Coca-Cola bottle), we introduce a novel evaluation metric based on Vision Language Models (VLMs). This VLM-based metric uses a question-answering framework to probe images for both explicit logos and implicit, holistic brand characteristics. Furthermore, we observe that as model fidelity increases, with newer systems (SDXL, FLUX) synthesizing brand identifiers more readily than older models (Stable Diffusion), the urgency of the unbranding challenge is starkly highlighted. Our results, validated by our VLM metric, confirm unbranding is a distinct, practically relevant problem requiring specialized techniques. Project Page: https://gmum.github.io/UNBRANDING/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality-Driven and Diversity-Aware Sample Expansion for Robust Marine Obstacle Segmentation</title>
<link>https://arxiv.org/abs/2512.13970</link>
<guid>https://arxiv.org/abs/2512.13970</guid>
<content:encoded><![CDATA[
<div> Keywords: marine obstacle detection, diffusion models, data augmentation, segmentation robustness, synthetic samples<br /><br />Summary: Marine obstacle detection is challenging due to adverse environmental conditions like sun glitter, fog, and dynamic wave patterns, which degrade image quality. To address limited training data caused by scarcity and repetitive marine datasets, the authors propose a novel sample expansion pipeline that enhances training data diversity without retraining diffusion models. This pipeline consists of two main components: a class-aware style bank that generates high-entropy, semantically meaningful prompts, and an adaptive annealing sampler that introduces controlled perturbations during early conditioning. A COD-guided proportional controller dynamically regulates these perturbations to boost sample diversity while preserving the semantic layout fidelity. The synthetic images produced at inference time by this method improve segmentation robustness across different marine obstacle benchmarks. Adding these controlled synthetic samples to the training data consistently enhances segmentation performance across multiple network backbones. Additionally, this approach increases visual variation particularly in rare and texture-sensitive classes, addressing the limitations of traditional mask-conditioned diffusion models which tend to output low-diversity images under low-entropy conditioning. This contribution represents an effective strategy for generating diverse, semantically aligned synthetic training data to improve marine obstacle detection. <div>
arXiv:2512.13970v1 Announce Type: new 
Abstract: Marine obstacle detection demands robust segmentation under challenging conditions, such as sun glitter, fog, and rapidly changing wave patterns. These factors degrade image quality, while the scarcity and structural repetition of marine datasets limit the diversity of available training data. Although mask-conditioned diffusion models can synthesize layout-aligned samples, they often produce low-diversity outputs when conditioned on low-entropy masks and prompts, limiting their utility for improving robustness. In this paper, we propose a quality-driven and diversity-aware sample expansion pipeline that generates training data entirely at inference time, without retraining the diffusion model. The framework combines two key components:(i) a class-aware style bank that constructs high-entropy, semantically grounded prompts, and (ii) an adaptive annealing sampler that perturbs early conditioning, while a COD-guided proportional controller regulates this perturbation to boost diversity without compromising layout fidelity. Across marine obstacle benchmarks, augmenting training data with these controlled synthetic samples consistently improves segmentation performance across multiple backbones and increases visual variation in rare and texture-sensitive classes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XAI-Driven Diagnosis of Generalization Failure in State-Space Cerebrovascular Segmentation Models: A Case Study on Domain Shift Between RSNA and TopCoW Datasets</title>
<link>https://arxiv.org/abs/2512.13977</link>
<guid>https://arxiv.org/abs/2512.13977</guid>
<content:encoded><![CDATA[
<div> Keywords: domain shift, Explainable AI, cerebrovascular segmentation, State-Space Models, attention mechanism<br /><br />Summary:<br /><br />1. The article addresses the critical issue of domain shift in deploying deep learning models for medical imaging, highlighting how models that perform well on their training data catastrophically fail on external datasets, thus limiting trustworthy AI applications.  
2. The authors propose that overcoming this challenge requires moving beyond simple performance metrics and incorporating Explainable AI (XAI) methods to deeply understand model behavior, particularly in medical image analysis.  
3. They focus on diagnosing the generalization failure of the State-Space Model UMamaba on cerebrovascular segmentation, comparing two datasets: the Source (RSNA CTA Aneurysm) and Target (TopCoW Circle of Willis CT), which differ notably in Z-resolution and background noise.  
4. A dramatic drop in model performance was documented, with the Dice score declining from 0.8604 on the Source dataset to 0.2902 on the Target dataset, indicating severe generalization failure.  
5. The core contribution is the use of Seg-XRes-CAM, an XAI technique, to quantify the model’s attention by measuring the overlap between its attention maps and both the Ground Truth and the model’s own incorrect prediction masks. Quantitative results show that the model’s attention shifted away from true anatomical vessels (IoU ~ 0.101) but remained aligned with its erroneous predictions (IoU ~ 0.282), demonstrating reliance on spurious correlations. This insight confirms the value of XAI in diagnosing dataset bias and model failure in emerging neural architectures. <div>
arXiv:2512.13977v1 Announce Type: new 
Abstract: The clinical deployment of deep learning models in medical imaging is severely hindered by domain shift. This challenge, where a high-performing model fails catastrophically on external datasets, is a critical barrier to trustworthy AI. Addressing this requires moving beyond simple performance metrics toward deeper understanding, making Explainable AI (XAI) an essential diagnostic tool in medical image analysis. We present a rigorous, two-phase approach to diagnose the generalization failure of state-of-the-art State-Space Models (SSMs), specifically UMamaba, applied to cerebrovascular segmentation. We first established a quantifiable domain gap between our Source (RSNA CTA Aneurysm) and Target (TopCoW Circle of Willis CT) datasets, noting significant differences in Z-resolution and background noise. The model's Dice score subsequently plummeted from 0.8604 (Source) to 0.2902 (Target). In the second phase, which is our core contribution, we utilized Seg-XRes-CAM to diagnose the cause of this failure. We quantified the model's focus by measuring the overlap between its attention maps and the Ground Truth segmentations, and between its attention maps and its own Prediction Mask. Our analysis proves the model failed to generalize because its attention mechanism abandoned true anatomical features in the Target domain. Quantitative metrics confirm the model's focus shifted away from the Ground Truth vessels (IoU~0.101 at 0.3 threshold) while still aligning with its own wrong predictions (IoU~0.282 at 0.3 threshold). This demonstrates the model learned spurious correlations, confirming XAI is a powerful diagnostic tool for identifying dataset bias in emerging architectures.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FocalComm: Hard Instance-Aware Multi-Agent Perception</title>
<link>https://arxiv.org/abs/2512.13982</link>
<guid>https://arxiv.org/abs/2512.13982</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent collaborative perception, hard instance mining, feature-level fusion, pedestrian detection, autonomous driving<br /><br />Summary: Multi-agent collaborative perception (CP) enhances autonomous driving safety by improving 3D perception, especially for vulnerable road users like pedestrians. Existing CP methods often focus on vehicle detection metrics and struggle with detecting smaller, safety-critical objects such as pedestrians, where errors can be dangerous. Current approaches typically rely on exchanging full feature sets among agents, which is inefficient and can lead to higher false negatives. To address these challenges, the paper introduces FocalComm, a novel CP framework that prioritizes the exchange of hard-instance-oriented features among collaborative agents. FocalComm incorporates two main innovations: first, a learnable progressive hard instance mining (HIM) module that enables each agent to extract features specifically targeting hard instances; second, a query-based feature-level fusion technique that dynamically weights and integrates these challenging features during collaboration. Experimental results demonstrate that FocalComm significantly outperforms state-of-the-art CP methods on two real-world datasets (V2X-Real and DAIR-V2X) across both vehicle-centric and infrastructure-centric collaboration settings. Notably, FocalComm achieves considerable improvements in pedestrian detection within the V2X-Real dataset, highlighting its potential to enhance safety in autonomous driving applications by reducing detection failures of critical small objects. <div>
arXiv:2512.13982v1 Announce Type: new 
Abstract: Multi-agent collaborative perception (CP) is a promising paradigm for improving autonomous driving safety, particularly for vulnerable road users like pedestrians, via robust 3D perception. However, existing CP approaches often optimize for vehicle detection performance metrics, underperforming on smaller, safety-critical objects such as pedestrians, where detection failures can be catastrophic. Furthermore, previous CP methods rely on full feature exchange rather than communicating only salient features that help reduce false negatives. To this end, we present FocalComm, a novel collaborative perception framework that focuses on exchanging hard-instance-oriented features among connected collaborative agents. FocalComm consists of two key novel designs: (1) a learnable progressive hard instance mining (HIM) module to extract hard instance-oriented features per agent, and (2) a query-based feature-level (intermediate) fusion technique that dynamically weights these identified features during collaboration. We show that FocalComm outperforms state-of-the-art collaborative perception methods on two challenging real-world datasets (V2X-Real and DAIR-V2X) across both vehicle-centric and infrastructure-centric collaborative setups. FocalComm also shows a strong performance gain in pedestrian detection in V2X-Real.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repurposing 2D Diffusion Models for 3D Shape Completion</title>
<link>https://arxiv.org/abs/2512.13991</link>
<guid>https://arxiv.org/abs/2512.13991</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D shape completion, diffusion models, Shape Atlas, point clouds, 2D representation

<br /><br />Summary:  
The paper proposes a novel framework that adapts existing 2D diffusion models for the task of 3D shape completion from incomplete point clouds. Recognizing the limitation caused by the scarcity of high-quality 3D datasets and the modality gap between 3D inputs and 2D latent diffusion spaces, the authors introduce the Shape Atlas, a compact 2D representation of 3D geometry designed to bridge this gap. This representation allows the full generative power of pretrained 2D diffusion models to be leveraged effectively for 3D shape reconstruction tasks. The approach also aligns the conditioning modalities between input and output, enabling more effective and accurate shape completion results. The unified 2D formulation facilitates learning from limited 3D datasets and successfully preserves intricate detail in completed shapes. Experimental validation on benchmark datasets, PCN and ShapeNet-55, demonstrates the efficacy and competitiveness of the proposed method. Beyond shape completion, the framework's practical application extends to generating artist-created meshes from the completed point clouds, showcasing its potential utility in creative workflows and digital content creation environments. <div>
arXiv:2512.13991v1 Announce Type: new 
Abstract: We present a framework that adapts 2D diffusion models for 3D shape completion from incomplete point clouds. While text-to-image diffusion models have achieved remarkable success with abundant 2D data, 3D diffusion models lag due to the scarcity of high-quality 3D datasets and a persistent modality gap between 3D inputs and 2D latent spaces. To overcome these limitations, we introduce the Shape Atlas, a compact 2D representation of 3D geometry that (1) enables full utilization of the generative power of pretrained 2D diffusion models, and (2) aligns the modalities between the conditional input and output spaces, allowing more effective conditioning. This unified 2D formulation facilitates learning from limited 3D data and produces high-quality, detail-preserving shape completions. We validate the effectiveness of our results on the PCN and ShapeNet-55 datasets. Additionally, we show the downstream application of creating artist-created meshes from our completed point clouds, further demonstrating the practicality of our method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse-LaViDa: Sparse Multimodal Discrete Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.14008</link>
<guid>https://arxiv.org/abs/2512.14008</guid>
<content:encoded><![CDATA[
<div> Masked discrete diffusion models, inference speed, token truncation, register tokens, LaViDa-O

<br /><br />Summary: Masked Discrete Diffusion Models (MDMs) are effective across multimodal tasks such as image understanding, generation, and editing but suffer from slow inference speeds due to processing redundant masked tokens at every sampling step. This paper introduces Sparse-LaViDa, a new framework designed to speed up MDM sampling by dynamically truncating unnecessary masked tokens during inference. To preserve the quality of generated outputs despite truncation, specialized register tokens are introduced to compactly represent these removed tokens. The framework also addresses the training-inference mismatch by designing a specialized attention mask that aligns the training procedure with the truncated sampling strategy used during inference. Sparse-LaViDa builds upon the state-of-the-art unified MDM LaViDa-O and demonstrates an up to 2x speedup across multiple domains, including text-to-image generation, image editing, and mathematical reasoning. Importantly, this acceleration is achieved without sacrificing generation quality, making Sparse-LaViDa an efficient and effective enhancement for masked discrete diffusion models. <div>
arXiv:2512.14008v1 Announce Type: new 
Abstract: Masked Discrete Diffusion Models (MDMs) have achieved strong performance across a wide range of multimodal tasks, including image understanding, generation, and editing. However, their inference speed remains suboptimal due to the need to repeatedly process redundant masked tokens at every sampling step. In this work, we propose Sparse-LaViDa, a novel modeling framework that dynamically truncates unnecessary masked tokens at each inference step to accelerate MDM sampling. To preserve generation quality, we introduce specialized register tokens that serve as compact representations for the truncated tokens. Furthermore, to ensure consistency between training and inference, we design a specialized attention mask that faithfully matches the truncated sampling procedure during training. Built upon the state-of-the-art unified MDM LaViDa-O, Sparse-LaViDa achieves up to a 2x speedup across diverse tasks including text-to-image generation, image editing, and mathematical reasoning, while maintaining generation quality.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KFS-Bench: Comprehensive Evaluation of Key Frame Sampling in Long Video Understanding</title>
<link>https://arxiv.org/abs/2512.14017</link>
<guid>https://arxiv.org/abs/2512.14017</guid>
<content:encoded><![CDATA[
<div> Key frame sampling, long video QA, multi-scene annotations, sampling quality metric, question-video relevance  

<br /><br />Summary:  
This paper introduces KFS-Bench, the first dedicated benchmark for key frame sampling in long video question answering (QA). It uniquely features multi-scene annotations that enable direct and robust evaluation of sampling strategies, overcoming the limitations of prior approaches that only assessed frame selection quality indirectly through QA accuracy. Key frame sampling is critical for efficient understanding of long-form videos, as selecting informative frames allows multimodal large language models (MLLMs) to enhance both accuracy and computational efficiency in QA tasks. KFS-Bench provides ground-truth annotations identifying multiple disjoint scenes required for each question, facilitating precise analysis of how different sampling methods capture essential content throughout long videos. Using this benchmark, the authors conduct a comprehensive study of existing key frame sampling techniques, revealing that sampling precision alone is insufficient, and that scene coverage and sampling balance are also crucial factors influencing QA outcomes. Based on these insights, they propose a novel sampling quality metric that correlates strongly with QA accuracy by jointly considering these factors. Additionally, they develop a new sampling method that leverages question-video relevance to balance sampling diversity and question-frame similarity, thereby improving coverage of relevant scenes. This adaptively balanced approach achieves superior performance in both key frame selection quality and QA effectiveness. The benchmark and code are publicly available on GitHub. <div>
arXiv:2512.14017v1 Announce Type: new 
Abstract: We propose KFS-Bench, the first benchmark for key frame sampling in long video question answering (QA), featuring multi-scene annotations to enable direct and robust evaluation of sampling strategies. Key frame sampling is crucial for efficient long-form video understanding. In long video QA, selecting informative frames enables multimodal large language models (MLLMs) to improve both accuracy and efficiency. KFS-Bench addresses the limitation of prior works that only indirectly assess frame selection quality via QA accuracy. By providing ground-truth annotations of multiple disjoint scenes required per question, KFS-Bench allows us to directly analyze how different sampling approaches capture essential content across an entire long video. Using KFS-Bench, we conduct a comprehensive study of key frame sampling methods and identify that not only sampling precision but also scene coverage and sampling balance are the key factors influencing QA performance. Regarding all the factors, we design a novel sampling quality metric that correlates with QA accuracy. Furthermore, we develop a novel key frame sampling method that leverages question-video relevance to balance sampling diversity against question-frame similarity, thereby improving coverage of relevant scenes. Our adaptively balanced sampling approach achieves superior performance in both key frame sampling and QA performance. The benchmark is available at https://github.com/NEC-VID/KFS-Bench.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning Perspective of Scene Understanding in Autonomous Robots</title>
<link>https://arxiv.org/abs/2512.14020</link>
<guid>https://arxiv.org/abs/2512.14020</guid>
<content:encoded><![CDATA[
<div> Deep learning, scene understanding, autonomous robots, object detection, visual SLAM  

<br /><br />Summary:  
This paper reviews deep learning applications in scene understanding specifically targeted at autonomous robots. It covers key innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM, highlighting how these advances contribute to improved environmental perception. The review emphasizes the superiority of learning-based methods over traditional geometric models, particularly in handling challenges like occlusions and textureless surfaces for real-time depth perception. It stresses the enhancement of semantic reasoning to facilitate better understanding and interpretation of surroundings, which is critical for autonomous operation. Integration of these perception modules is shown to improve robot effectiveness in dynamic and unstructured environments, leading to more robust decision-making, navigation, and interaction capabilities. Finally, the paper identifies existing challenges and outlines future research directions aiming to further advance learning-based scene understanding, ensuring autonomous robots become increasingly reliable and capable in complex real-world scenarios. <div>
arXiv:2512.14020v1 Announce Type: new 
Abstract: This paper provides a review of deep learning applications in scene understanding in autonomous robots, including innovations in object detection, semantic and instance segmentation, depth estimation, 3D reconstruction, and visual SLAM. It emphasizes how these techniques address limitations of traditional geometric models, improve depth perception in real time despite occlusions and textureless surfaces, and enhance semantic reasoning to understand the environment better. When these perception modules are integrated into dynamic and unstructured environments, they become more effective in decisionmaking, navigation and interaction. Lastly, the review outlines the existing problems and research directions to advance learning-based scene understanding of autonomous robots.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Power of Image-Tabular Self-Supervised Learning via Breaking Cross-Tabular Barriers</title>
<link>https://arxiv.org/abs/2512.14026</link>
<guid>https://arxiv.org/abs/2512.14026</guid>
<content:encoded><![CDATA[
<div> Multi-modal learning, Self-Supervised Learning, Cross-tabular, Prototype-guided mixture-of-linear layer, Alzheimer's disease<br /><br />Summary:<br /><br />1. This paper addresses the challenge of integrating medical images and heterogeneous tabular data through multi-modal learning, which has become crucial for clinical decision-making. <br />2. Self-Supervised Learning (SSL) methods have been effective for pretraining on large unlabeled datasets but typically struggle with heterogeneous tabular data due to rigid tabular modeling, limiting their ability to transfer knowledge across diverse data cohorts. <br />3. The authors propose CITab, a novel SSL framework that enables cross-tabular multi-modal feature representation learning by incorporating semantic awareness through the integration of column headers as semantic cues, enhancing transferability and scalability. <br />4. A novel prototype-guided mixture-of-linear layer (P-MoLin) module is introduced, which specializes tabular feature learning, allowing the model to effectively handle heterogeneity within tabular data and to explore underlying medical concepts. <br />5. Extensive experiments on Alzheimer's disease diagnosis across three public cohorts with 4,461 subjects demonstrate that CITab outperforms existing state-of-the-art methods, highlighting its potential for scalable and effective cross-tabular multi-modal learning in medical applications. <div>
arXiv:2512.14026v1 Announce Type: new 
Abstract: Multi-modal learning integrating medical images and tabular data has significantly advanced clinical decision-making in recent years. Self-Supervised Learning (SSL) has emerged as a powerful paradigm for pretraining these models on large-scale unlabeled image-tabular data, aiming to learn discriminative representations. However, existing SSL methods for image-tabular representation learning are often confined to specific data cohorts, mainly due to their rigid tabular modeling mechanisms when modeling heterogeneous tabular data. This inter-tabular barrier hinders the multi-modal SSL methods from effectively learning transferrable medical knowledge shared across diverse cohorts. In this paper, we propose a novel SSL framework, namely CITab, designed to learn powerful multi-modal feature representations in a cross-tabular manner. We design the tabular modeling mechanism from a semantic-awareness perspective by integrating column headers as semantic cues, which facilitates transferrable knowledge learning and the scalability in utilizing multiple data sources for pretraining. Additionally, we propose a prototype-guided mixture-of-linear layer (P-MoLin) module for tabular feature specialization, empowering the model to effectively handle the heterogeneity of tabular data and explore the underlying medical concepts. We conduct comprehensive evaluations on Alzheimer's disease diagnosis task across three publicly available data cohorts containing 4,461 subjects. Experimental results demonstrate that CITab outperforms state-of-the-art approaches, paving the way for effective and scalable cross-tabular multi-modal learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Single-shot Structured Light 3D Imaging via Neural Feature Decoding</title>
<link>https://arxiv.org/abs/2512.14028</link>
<guid>https://arxiv.org/abs/2512.14028</guid>
<content:encoded><![CDATA[
<div> Keywords: structured light, 3D imaging, neural feature matching, depth refinement, synthetic data<br /><br />Summary:<br /><br />1. This paper addresses the challenge of active 3D imaging in single-shot structured light systems commonly used in devices like Apple Face ID and Intel RealSense.<br />2. Traditional pixel-domain correspondence matching methods suffer from robustness issues in difficult scenarios such as occlusions, detailed fine structures, and non-Lambertian surfaces.<br />3. The authors propose a novel learning-based framework that performs correspondence matching in feature space using neural features extracted from projected patterns and captured infrared images.<br />4. To leverage geometric priors, the framework constructs cost volumes in feature space, significantly improving performance compared to pixel-domain methods.<br />5. A depth refinement module is introduced, incorporating strong priors from large-scale monocular depth estimation models to enhance fine details and global structural coherence.<br />6. A physically-based rendering pipeline was developed to synthesize nearly one million structured light pattern and image pairs featuring diverse objects and materials in indoor settings to train the model.<br />7. Experiments show the method generalizes well to real indoor scenes, works across multiple pattern types without retraining, and outperforms commercial structured light systems as well as passive stereo RGB depth estimation approaches.<br />8. The project webpage is available for further details and resources. <div>
arXiv:2512.14028v1 Announce Type: new 
Abstract: We consider the problem of active 3D imaging using single-shot structured light systems, which are widely employed in commercial 3D sensing devices such as Apple Face ID and Intel RealSense. Traditional structured light methods typically decode depth correspondences through pixel-domain matching algorithms, resulting in limited robustness under challenging scenarios like occlusions, fine-structured details, and non-Lambertian surfaces. Inspired by recent advances in neural feature matching, we propose a learning-based structured light decoding framework that performs robust correspondence matching within feature space rather than the fragile pixel domain. Our method extracts neural features from the projected patterns and captured infrared (IR) images, explicitly incorporating their geometric priors by building cost volumes in feature space, achieving substantial performance improvements over pixel-domain decoding approaches. To further enhance depth quality, we introduce a depth refinement module that leverages strong priors from large-scale monocular depth estimation models, improving fine detail recovery and global structural coherence. To facilitate effective learning, we develop a physically-based structured light rendering pipeline, generating nearly one million synthetic pattern-image pairs with diverse objects and materials for indoor settings. Experiments demonstrate that our method, trained exclusively on synthetic data with multiple structured light patterns, generalizes well to real-world indoor environments, effectively processes various pattern types without retraining, and consistently outperforms both commercial structured light systems and passive stereo RGB-based depth estimation methods. Project page: https://namisntimpot.github.io/NSLweb/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACE-SLAM: Scene Coordinate Regression for Neural Implicit Real-Time SLAM</title>
<link>https://arxiv.org/abs/2512.14032</link>
<guid>https://arxiv.org/abs/2512.14032</guid>
<content:encoded><![CDATA[
<div> Keywords: neural SLAM, RGB-D, Scene Coordinate Regression, real-time mapping, implicit map representation<br /><br />Summary:<br /><br />1. The paper introduces a novel neural RGB-D SLAM system that learns an implicit map of scenes in real time using Scene Coordinate Regression (SCR) as the core representation. <br /><br />2. SCR is explored for the first time in neural SLAM pipelines, leveraging a lightweight network that maps 2D image features directly to 3D global coordinates, offering advantages in memory efficiency, speed, and privacy preservation.<br /><br />3. The proposed system is the first to achieve strict real-time performance in neural implicit RGB-D SLAM by using a dedicated SCR-based architecture tailored for this task.<br /><br />4. Key design elements for integrating SCR into a live SLAM pipeline are presented, resulting in a flexible framework that supports both sparse and dense features and maintains reliable operation in dynamic environments without special adaptations.<br /><br />5. The approach is evaluated on established synthetic and real-world datasets, demonstrating competitive performance compared to current state-of-the-art methods.<br /><br />The project code and further details are available at the given GitHub repository. <div>
arXiv:2512.14032v1 Announce Type: new 
Abstract: We present a novel neural RGB-D Simultaneous Localization And Mapping (SLAM) system that learns an implicit map of the scene in real time. For the first time, we explore the use of Scene Coordinate Regression (SCR) as the core implicit map representation in a neural SLAM pipeline, a paradigm that trains a lightweight network to directly map 2D image features to 3D global coordinates. SCR networks provide efficient, low-memory 3D map representations, enable extremely fast relocalization, and inherently preserve privacy, making them particularly suitable for neural implicit SLAM.
  Our system is the first one to achieve strict real-time in neural implicit RGB-D SLAM by relying on a SCR-based representation. We introduce a novel SCR architecture specifically tailored for this purpose and detail the critical design choices required to integrate SCR into a live SLAM pipeline. The resulting framework is simple yet flexible, seamlessly supporting both sparse and dense features, and operates reliably in dynamic environments without special adaptation. We evaluate our approach on established synthetic and real-world benchmarks, demonstrating competitive performance against the state of the art. Project Page: https://github.com/ialzugaray/ace-slam
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASAP-Textured Gaussians: Enhancing Textured Gaussians with Adaptive Sampling and Anisotropic Parameterization</title>
<link>https://arxiv.org/abs/2512.14039</link>
<guid>https://arxiv.org/abs/2512.14039</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, texture parameterization, adaptive sampling, anisotropic parameterization, rendering efficiency<br /><br />Summary: This paper addresses key inefficiencies in textured 3D Gaussian Splatting used for appearance modeling and related tasks. It identifies two major issues with current methods: (1) textures are defined in canonical space, causing inefficient sampling and wasting texture capacity on regions that contribute little to the final rendering, and (2) uniform texture parameterization is applied across all Gaussians regardless of their visual complexity, leading to over-parameterization and unnecessary memory usage. To tackle these problems, the authors propose ASAP Textured Gaussians, which incorporates two strategies: adaptive sampling based on the Gaussian density distribution, enabling more efficient use of texture resources by focusing sampling on visually important areas, and an error-driven anisotropic parameterization that adaptively assigns texture parameters according to rendering error, allocating more detail where needed and saving resources elsewhere. This method significantly improves the balance between quality and computational efficiency, allowing for high-fidelity rendering with substantially fewer texture parameters, making it a practical advancement for memory-constrained applications in 3D graphics and rendering. <div>
arXiv:2512.14039v1 Announce Type: new 
Abstract: Recent advances have equipped 3D Gaussian Splatting with texture parameterizations to capture spatially varying attributes, improving the performance of both appearance modeling and downstream tasks. However, the added texture parameters introduce significant memory efficiency challenges. Rather than proposing new texture formulations, we take a step back to examine the characteristics of existing textured Gaussian methods and identify two key limitations in common: (1) Textures are typically defined in canonical space, leading to inefficient sampling that wastes textures' capacity on low-contribution regions; and (2) texture parameterization is uniformly assigned across all Gaussians, regardless of their visual complexity, resulting in over-parameterization. In this work, we address these issues through two simple yet effective strategies: adaptive sampling based on the Gaussian density distribution and error-driven anisotropic parameterization that allocates texture resources according to rendering error. Our proposed ASAP Textured Gaussians, short for Adaptive Sampling and Anisotropic Parameterization, significantly improve the quality efficiency tradeoff, achieving high-fidelity rendering with far fewer texture parameters.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChartAgent: A Chart Understanding Framework with Tool Integrated Reasoning</title>
<link>https://arxiv.org/abs/2512.14040</link>
<guid>https://arxiv.org/abs/2512.14040</guid>
<content:encoded><![CDATA[
<div> Keywords: ChartAgent, Tool-Integrated Reasoning, chart understanding, multimodal large language models, visual parsing<br /><br />Summary:<br /><br />1. Charts are widely used for data analysis and communication due to their high information density and intuitive readability, yet current multimodal large language models (MLLMs) struggle with chart understanding when key numerical annotations are missing.<br /><br />2. To overcome these limitations, the paper introduces ChartAgent, a novel chart understanding framework based on Tool-Integrated Reasoning (TIR), which mimics human cognition by breaking down complex chart analysis into a series of observable and replayable steps.<br /><br />3. ChartAgent incorporates an extensible and modular tool library consisting of over a dozen specialized tools including key element detection, instance segmentation, and optical character recognition (OCR), allowing dynamic orchestration to systematically parse diverse chart types.<br /><br />4. The framework provides transparency and verifiability by consolidating intermediate outputs into a structured Evidence Package, enabling traceable and reproducible support for its final analytical conclusions.<br /><br />5. Experimental results demonstrate that ChartAgent significantly enhances robustness and reliability in settings with sparse or missing annotations, offering a practical approach toward trustworthy and extensible chart understanding systems. <div>
arXiv:2512.14040v1 Announce Type: new 
Abstract: With their high information density and intuitive readability, charts have become the de facto medium for data analysis and communication across disciplines. Recent multimodal large language models (MLLMs) have made notable progress in automated chart understanding, yet they remain heavily dependent on explicit textual annotations and the performance degrades markedly when key numerals are absent. To address this limitation, we introduce ChartAgent, a chart understanding framework grounded in Tool-Integrated Reasoning (TIR). Inspired by human cognition, ChartAgent decomposes complex chart analysis into a sequence of observable, replayable steps. Supporting this architecture is an extensible, modular tool library comprising more than a dozen core tools, such as keyelement detection, instance segmentation, and optical character recognition (OCR), which the agent dynamically orchestrates to achieve systematic visual parsing across diverse chart types. Leveraging TIRs transparency and verifiability, ChartAgent moves beyond the black box paradigm by standardizing and consolidating intermediate outputs into a structured Evidence Package, providing traceable and reproducible support for final conclusions. Experiments show that ChartAgent substantially improves robustness under sparse annotation settings, offering a practical path toward trustworthy and extensible systems for chart understanding.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.14044</link>
<guid>https://arxiv.org/abs/2512.14044</guid>
<content:encoded><![CDATA[
<div> Vision-Language Models, autonomous driving, object hallucination, reinforcement learning, visual grounding<br /><br />Summary:<br /><br />The paper addresses the challenge of deploying Vision-Language Models (VLMs) in safety-critical autonomous driving (AD) environments, focusing on the prevalent issue of object hallucination caused by ungrounded, text-based Chain-of-Thought (CoT) reasoning. It identifies two key limitations in existing multi-modal CoT approaches: the separation of perception and reasoning phases that hinders end-to-end optimization, and the dependence on costly, dense localization annotations. To overcome these, the authors propose OmniDrive-R1, a unified end-to-end VLM framework that integrates perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) process. A central innovation is the reinforcement-driven visual grounding mechanism, allowing the model to autonomously focus and zoom on important image regions for detailed analysis. This is realized via a novel two-stage reinforcement learning training pipeline and a new Clip-GRPO algorithm. Clip-GRPO introduces an annotation-free grounding reward based on process consistency between visual attention and textual reasoning, removing the need for dense labels and avoiding reliance on external tools. Experiments on the DriveLMM-o1 dataset demonstrate substantial performance gains, with OmniDrive-R1 elevating reasoning scores from 51.77% to 80.35% and answer accuracy from 37.81% to 73.62% compared to the Qwen2.5VL-7B baseline. <div>
arXiv:2512.14044v1 Announce Type: new 
Abstract: The deployment of Vision-Language Models (VLMs) in safety-critical domains like autonomous driving (AD) is critically hindered by reliability failures, most notably object hallucination. This failure stems from their reliance on ungrounded, text-based Chain-of-Thought (CoT) reasoning.While existing multi-modal CoT approaches attempt mitigation, they suffer from two fundamental flaws: (1) decoupled perception and reasoning stages that prevent end-to-end joint optimization, and (2) reliance on expensive, dense localization labels.Thus we introduce OmniDrive-R1, an end-to-end VLM framework designed for autonomous driving, which unifies perception and reasoning through an interleaved Multi-modal Chain-of-Thought (iMCoT) mechanism. Our core innovation is an Reinforcement-driven visual grounding capability, enabling the model to autonomously direct its attention and "zoom in" on critical regions for fine-grained analysis. This capability is enabled by our pure two-stage reinforcement learning training pipeline and Clip-GRPO algorithm. Crucially, Clip-GRPO introduces an annotation-free, process-based grounding reward. This reward not only eliminates the need for dense labels but also circumvents the instability of external tool calls by enforcing real-time cross-modal consistency between the visual focus and the textual reasoning. Extensive experiments on DriveLMM-o1 demonstrate our model's significant improvements. Compared to the baseline Qwen2.5VL-7B, OmniDrive-R1 improves the overall reasoning score from 51.77% to 80.35%, and the final answer accuracy from 37.81% to 73.62%.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SELECT: Detecting Label Errors in Real-world Scene Text Data</title>
<link>https://arxiv.org/abs/2512.14050</link>
<guid>https://arxiv.org/abs/2512.14050</guid>
<content:encoded><![CDATA[
<div> Label error detection, scene text recognition, multi-modal training, sequence label misalignment, data corruption<br /><br />Summary:<br /><br />1. The paper introduces SELECT (Scene tExt Label Errors deteCTion), a novel method designed to detect label errors in real-world scene text datasets by leveraging multi-modal training techniques combining image and text data. 2. SELECT utilizes an image-text encoder alongside a character-level tokenizer to handle challenges such as variable-length sequence labels, sequence misalignment, and character-level errors, leading to improved accuracy over existing approaches. 3. The authors propose a new error introduction method called Similarity-based Sequence Label Corruption (SSLC), which simulates realistic label errors during training by corrupting labels considering both sequence length changes and the visual similarity between characters. 4. SSLC aids in training SELECT to better generalize to various real-world label error scenarios, providing a more robust detection mechanism. 5. Experiments demonstrate that SELECT effectively identifies label errors in diverse scene text recognition datasets and enhances the accuracy of scene text recognition (STR) models, illustrating its practical value and contribution to improving data quality in the field. <div>
arXiv:2512.14050v1 Announce Type: new 
Abstract: We introduce SELECT (Scene tExt Label Errors deteCTion), a novel approach that leverages multi-modal training to detect label errors in real-world scene text datasets. Utilizing an image-text encoder and a character-level tokenizer, SELECT addresses the issues of variable-length sequence labels, label sequence misalignment, and character-level errors, outperforming existing methods in accuracy and practical utility. In addition, we introduce Similarity-based Sequence Label Corruption (SSLC), a process that intentionally introduces errors into the training labels to mimic real-world error scenarios during training. SSLC not only can cause a change in the sequence length but also takes into account the visual similarity between characters during corruption. Our method is the first to detect label errors in real-world scene text datasets successfully accounting for variable-length labels. Experimental results demonstrate the effectiveness of SELECT in detecting label errors and improving STR accuracy on real-world text datasets, showcasing its practical utility.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperVL: An Efficient and Dynamic Multimodal Large Language Model for Edge Devices</title>
<link>https://arxiv.org/abs/2512.14052</link>
<guid>https://arxiv.org/abs/2512.14052</guid>
<content:encoded><![CDATA[
<div> Keywords: HyperVL, multimodal large language model, Visual Resolution Compressor, Dual Consistency Learning, on-device inference  

<br /><br />Summary:  
The article presents HyperVL, an efficient multimodal large language model designed specifically for on-device inference, addressing the challenges of high computational and memory demands commonly seen in current models. HyperVL introduces an image-tiling strategy to limit peak memory usage during processing of high-resolution inputs. It employs two novel techniques: first, a Visual Resolution Compressor (VRC) that adaptively determines the optimal encoding resolution to avoid unnecessary computation; second, Dual Consistency Learning (DCL), which aligns multi-scale Vision Transformer (ViT) encoders within a common framework, allowing dynamic switching between visual branches while sharing a unified large language model (LLM). Extensive experiments show that HyperVL achieves state-of-the-art performance relative to other models of similar size across various benchmarks. Additionally, HyperVL significantly reduces latency and power consumption on real mobile devices, proving its suitability and practicality for on-device multimodal applications. The study highlights that despite the advances of small-parameter models, traditional ViT encoders still pose bottlenecks in latency and memory when processing high-resolution images, which HyperVL effectively overcomes through its innovations. Overall, HyperVL represents a key step forward in making powerful multimodal LLMs more accessible and efficient for edge-computing environments. <div>
arXiv:2512.14052v1 Announce Type: new 
Abstract: Current multimodal large lanauge models possess strong perceptual and reasoning capabilities, however high computational and memory requirements make them difficult to deploy directly on on-device environments. While small-parameter models are progressively endowed with strong general capabilities, standard Vision Transformer (ViT) encoders remain a critical bottleneck, suffering from excessive latency and memory consumption when processing high-resolution inputs.To address these challenges, we introduce HyperVL, an efficient multimodal large language model tailored for on-device inference. HyperVL adopts an image-tiling strategy to cap peak memory usage and incorporates two novel techniques: (1) a Visual Resolution Compressor (VRC) that adaptively predicts optimal encoding resolutions to eliminate redundant computation, and (2) Dual Consistency Learning (DCL), which aligns multi-scale ViT encoders within a unified framework, enabling dynamic switching between visual branches under a shared LLM. Extensive experiments demonstrate that HyperVL achieves state-of-the-art performance among models of comparable size across multiple benchmarks. Furthermore, it significantly significantly reduces latency and power consumption on real mobile devices, demonstrating its practicality for on-device multimodal inference.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FacEDiT: Unified Talking Face Editing and Generation via Facial Motion Infilling</title>
<link>https://arxiv.org/abs/2512.14056</link>
<guid>https://arxiv.org/abs/2512.14056</guid>
<content:encoded><![CDATA[
<div> Keywords: talking face editing, speech-conditional facial motion infilling, FacEDiT, diffusion transformer, FacEDiTBench<br /><br />Summary:<br />1. This paper proposes a unified view of talking face editing and face generation as subtasks of speech-conditional facial motion infilling, rather than treating them separately.<br />2. The authors introduce FacEDiT, a novel speech-conditional Diffusion Transformer model trained with flow matching that synthesizes masked facial motions conditioned on both surrounding motions and speech input.<br />3. FacEDiT’s design is inspired by masked autoencoders and supports localized editing operations such as substitution, insertion, and deletion, while ensuring smooth transitions and lip synchronization with the unedited regions.<br />4. To improve boundary continuity and lip-sync quality, the model incorporates biased attention mechanisms and temporal smoothness constraints.<br />5. The paper also presents FacEDiTBench, the first dedicated dataset and benchmark for talking face editing, featuring varied edit types, lengths, and new evaluation metrics.<br />6. Extensive experiments demonstrate that talking face editing and generation naturally emerge from the proposed speech-conditional motion infilling framework, with FacEDiT achieving accurate, speech-aligned, identity-preserving, and visually smooth edits while generalizing well to full talking face generation tasks. <div>
arXiv:2512.14056v1 Announce Type: new 
Abstract: Talking face editing and face generation have often been studied as distinct problems. In this work, we propose viewing both not as separate tasks but as subtasks of a unifying formulation, speech-conditional facial motion infilling. We explore facial motion infilling as a self-supervised pretext task that also serves as a unifying formulation of dynamic talking face synthesis. To instantiate this idea, we propose FacEDiT, a speech-conditional Diffusion Transformer trained with flow matching. Inspired by masked autoencoders, FacEDiT learns to synthesize masked facial motions conditioned on surrounding motions and speech. This formulation enables both localized generation and edits, such as substitution, insertion, and deletion, while ensuring seamless transitions with unedited regions. In addition, biased attention and temporal smoothness constraints enhance boundary continuity and lip synchronization. To address the lack of a standard editing benchmark, we introduce FacEDiTBench, the first dataset for talking face editing, featuring diverse edit types and lengths, along with new evaluation metrics. Extensive experiments validate that talking face editing and generation emerge as subtasks of speech-conditional motion infilling; FacEDiT produces accurate, speech-aligned facial edits with strong identity preservation and smooth visual continuity while generalizing effectively to talking face generation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time prediction of workplane illuminance distribution for daylight-linked controls using non-intrusive multimodal deep learning</title>
<link>https://arxiv.org/abs/2512.14058</link>
<guid>https://arxiv.org/abs/2512.14058</guid>
<content:encoded><![CDATA[
<div> Daylight-linked controls, indoor workplane illuminance, multimodal deep learning, temporal-spatial features, dynamic indoor scenes  

<br /><br />Summary:  
This study focuses on improving indoor daylight prediction to enhance daylight-linked controls (DLCs), which can significantly reduce energy consumption in buildings. Unlike previous methods that primarily target static scenes, the proposed approach uses a multimodal deep learning framework capable of real-time prediction of indoor workplane illuminance distributions. The model uniquely extracts image features exclusively from the side-lit window areas instead of the entire interior, allowing its application in dynamically occupied indoor spaces without intrusive monitoring. A comprehensive field experiment was conducted in a test room located in Guangzhou, China, where 17,344 image samples were gathered for training and validating the model. The results demonstrate outstanding prediction accuracy, with an R² greater than 0.98 and RMSE below 0.14 on test data drawn from the same distribution as the training set. The model also showed robust temporal generalization by achieving an R² above 0.82 and RMSE less than 0.17 on data from previously unseen days. These findings highlight the model’s potential for real-time indoor daylight monitoring and control in real-world, dynamically changing environments, advancing the practical deployment of energy-saving DLC systems. <div>
arXiv:2512.14058v1 Announce Type: new 
Abstract: Daylight-linked controls (DLCs) have significant potential for energy savings in buildings, especially when abundant daylight is available and indoor workplane illuminance can be accurately predicted in real time. Most existing studies on indoor daylight predictions were developed and tested for static scenes. This study proposes a multimodal deep learning framework that predicts indoor workplane illuminance distributions in real time from non-intrusive images with temporal-spatial features. By extracting image features only from the side-lit window areas rather than interior pixels, the approach remains applicable in dynamically occupied indoor spaces. A field experiment was conducted in a test room in Guangzhou (China), where 17,344 samples were collected for model training and validation. The model achieved R2 > 0.98 with RMSE < 0.14 on the same-distribution test set and R2 > 0.82 with RMSE < 0.17 on an unseen-day test set, indicating high accuracy and acceptable temporal generalization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Fidelity-Reality with Controllable One-Step Diffusion for Image Super-Resolution</title>
<link>https://arxiv.org/abs/2512.14061</link>
<guid>https://arxiv.org/abs/2512.14061</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion-based super-resolution, feature modulation, generative prior activation, text-prompt alignment, one-step inference

<br /><br />Summary: This paper addresses three major limitations in recent diffusion-based one-step image super-resolution methods: loss of fidelity due to compression encoding of low-quality inputs, weak region-discriminative activation of generative priors, and poor alignment between text prompts and semantic regions. To overcome these challenges, the authors introduce CODSR, a controllable one-step diffusion network designed to improve image super-resolution. First, the proposed LQ-guided feature modulation module incorporates information from the original uncompressed low-quality inputs to provide high-fidelity conditioning during the diffusion process, mitigating fidelity loss. Second, a region-adaptive generative prior activation method is developed to improve perceptual richness while preserving local structural details, ensuring better visual quality without sacrificing accuracy. Third, a text-matching guidance strategy is employed to fully exploit text prompts, aligning them more effectively with their corresponding semantic regions in the output image. Extensive experiments demonstrate that CODSR not only achieves superior perceptual quality but also maintains competitive fidelity compared to state-of-the-art methods, all while enabling efficient one-step inference for practical use. <div>
arXiv:2512.14061v1 Announce Type: new 
Abstract: Recent diffusion-based one-step methods have shown remarkable progress in the field of image super-resolution, yet they remain constrained by three critical limitations: (1) inferior fidelity performance caused by the information loss from compression encoding of low-quality (LQ) inputs; (2) insufficient region-discriminative activation of generative priors; (3) misalignment between text prompts and their corresponding semantic regions. To address these limitations, we propose CODSR, a controllable one-step diffusion network for image super-resolution. First, we propose an LQ-guided feature modulation module that leverages original uncompressed information from LQ inputs to provide high-fidelity conditioning for the diffusion process. We then develop a region-adaptive generative prior activation method to effectively enhance perceptual richness without sacrificing local structural fidelity. Finally, we employ a text-matching guidance strategy to fully harness the conditioning potential of text prompts. Extensive experiments demonstrate that CODSR achieves superior perceptual quality and competitive fidelity compared with state-of-the-art methods with efficient one-step inference.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2512.14068</link>
<guid>https://arxiv.org/abs/2512.14068</guid>
<content:encoded><![CDATA[
<div> block-wise discrete diffusion, vision-language understanding, training efficiency, stability, noise scheduling<br /><br />Summary:  
The paper introduces SDAR-VL, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU). Block-wise discrete diffusion provides a balance between parallel generation and causal dependency modeling, but prior approaches suffered from high training costs, slow convergence, and instability, limiting their practical use compared to autoregressive (AR) baselines. To address these challenges, the authors propose an integrated framework composed of three key components: (1) Asynchronous Block-wise Noise Scheduling, which diversifies supervision within each training batch; (2) Effective Mask Ratio Scaling, ensuring unbiased loss normalization when stochastic masking is applied; and (3) a Progressive Beta Noise Curriculum that progressively increases mask coverage while preserving data corruption diversity. Evaluations on 21 diverse benchmarks, including single-image, multi-image, and video tasks, demonstrate that SDAR-VL significantly improves training efficiency, convergence stability, and overall task performance over conventional block diffusion methods. Furthermore, SDAR-VL establishes a new state of the art among diffusion-based vision-language models, and matches or surpasses strong AR baselines such as LLaVA-OneVision and the global diffusion baseline LLaDA-V under comparable experimental settings. This work thus confirms block-wise diffusion as a practical and effective backbone for vision-language understanding. <div>
arXiv:2512.14068v1 Announce Type: new 
Abstract: Block-wise discrete diffusion offers an attractive balance between parallel generation and causal dependency modeling, making it a promising backbone for vision-language modeling. However, its practical adoption has been limited by high training cost, slow convergence, and instability, which have so far kept it behind strong autoregressive (AR) baselines. We present \textbf{SDAR-VL}, the first systematic application of block-wise discrete diffusion to large-scale vision-language understanding (VLU), together with an \emph{integrated framework for efficient and stable training}. This framework unifies three components: (1) \textbf{Asynchronous Block-wise Noise Scheduling} to diversify supervision within each batch; (2) \textbf{Effective Mask Ratio Scaling} for unbiased loss normalization under stochastic masking; and (3) a \textbf{Progressive Beta Noise Curriculum} that increases effective mask coverage while preserving corruption diversity. Experiments on 21 single-image, multi-image, and video benchmarks show that SDAR-VL consistently improves \emph{training efficiency}, \emph{convergence stability}, and \emph{task performance} over conventional block diffusion. On this evaluation suite, SDAR-VL sets a new state of the art among diffusion-based vision-language models and, under matched settings, matches or surpasses strong AR baselines such as LLaVA-OneVision as well as the global diffusion baseline LLaDA-V, establishing block-wise diffusion as a practical backbone for VLU.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaussianPlant: Structure-aligned Gaussian Splatting for 3D Reconstruction of Plants</title>
<link>https://arxiv.org/abs/2512.14087</link>
<guid>https://arxiv.org/abs/2512.14087</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, plant reconstruction, hierarchical representation, structure and appearance disentanglement, multi-view images  

<br /><br />Summary:  
1. The paper proposes GaussianPlant, a novel method that jointly recovers both the appearance and internal structure of botanical plants from multi-view images using 3D Gaussian Splatting (3DGS).  
2. Traditional 3DGS is effective for reconstructing appearance but lacks explicit structural representation like branching patterns, which limits its use in applications such as plant phenotyping.  
3. GaussianPlant introduces a hierarchical 3DGS representation that disentangles structure and appearance by utilizing structure primitives (StPs) and appearance primitives (ApPs).  
4. StPs explicitly model plant geometry, representing branches as cylinders and leaves as disks, with attribute optimization that distinguishes between branches and leaves in a self-organized manner.  
5. ApPs represent the visual appearance bound to each StP, allowing the method to reconstruct both the plant’s geometry and its appearance simultaneously.  
6. StPs and ApPs are jointly optimized leveraging re-rendering losses and gradient flow between appearance and structure components, ensuring accurate reconstruction fidelity.  
7. Experimental results demonstrate that GaussianPlant achieves high-fidelity appearance reconstruction and precise structural recovery, enabling effective extraction of branch structures and leaf instances for practical plant phenotyping tasks. <div>
arXiv:2512.14087v1 Announce Type: new 
Abstract: We present a method for jointly recovering the appearance and internal structure of botanical plants from multi-view images based on 3D Gaussian Splatting (3DGS). While 3DGS exhibits robust reconstruction of scene appearance for novel-view synthesis, it lacks structural representations underlying those appearances (e.g., branching patterns of plants), which limits its applicability to tasks such as plant phenotyping. To achieve both high-fidelity appearance and structural reconstruction, we introduce GaussianPlant, a hierarchical 3DGS representation, which disentangles structure and appearance. Specifically, we employ structure primitives (StPs) to explicitly represent branch and leaf geometry, and appearance primitives (ApPs) to the plants' appearance using 3D Gaussians. StPs represent a simplified structure of the plant, i.e., modeling branches as cylinders and leaves as disks. To accurately distinguish the branches and leaves, StP's attributes (i.e., branches or leaves) are optimized in a self-organized manner. ApPs are bound to each StP to represent the appearance of branches or leaves as in conventional 3DGS. StPs and ApPs are jointly optimized using a re-rendering loss on the input multi-view images, as well as the gradient flow from ApP to StP using the binding correspondence information. We conduct experiments to qualitatively evaluate the reconstruction accuracy of both appearance and structure, as well as real-world experiments to qualitatively validate the practical performance. Experiments show that the GaussianPlant achieves both high-fidelity appearance reconstruction via ApPs and accurate structural reconstruction via StPs, enabling the extraction of branch structure and leaf instances.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoFlow: Interpretable and Robust Surgical Workflow Modeling with Learned Dynamic Scene Graph Prototypes</title>
<link>https://arxiv.org/abs/2512.14092</link>
<guid>https://arxiv.org/abs/2512.14092</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical workflow, scene graphs, prototype learning, graph neural networks, explainability<br /><br />Summary:<br /><br />1. Purpose: The paper addresses the challenge of detailed surgical recognition in AI, hindered by annotation costs, limited data, and lack of interpretability. It proposes ProtoFlow, a framework that employs dynamic scene graph prototypes to model complex surgical workflows in an explainable and robust way.<br /><br />2. Methods: ProtoFlow uses a graph neural network encoder-decoder combined with self-supervised pretraining to learn rich representations, followed by prototype-based fine-tuning. This approach identifies and refines core prototypes representing clinically significant, recurring surgical interaction patterns, enabling explainable workflow analysis.<br /><br />3. Results: Evaluated on the CAT-SG dataset, ProtoFlow surpasses conventional GNN baselines in accuracy and exhibits remarkable robustness in data-scarce, few-shot learning conditions, maintaining strong performance even when trained on a single surgical video. Qualitative analysis confirms that learned prototypes correspond to distinct surgical sub-techniques and provide interpretable insights into workflow deviations and rare complications.<br /><br />4. Conclusion: ProtoFlow combines robust learning with built-in explainability, advancing transparent, reliable, and data-efficient AI models for surgery. This progress could facilitate clinical AI adoption in surgical training, real-time support, and workflow optimization. <div>
arXiv:2512.14092v1 Announce Type: new 
Abstract: Purpose: Detailed surgical recognition is critical for advancing AI-assisted surgery, yet progress is hampered by high annotation costs, data scarcity, and a lack of interpretable models. While scene graphs offer a structured abstraction of surgical events, their full potential remains untapped. In this work, we introduce ProtoFlow, a novel framework that learns dynamic scene graph prototypes to model complex surgical workflows in an interpretable and robust manner.
  Methods: ProtoFlow leverages a graph neural network (GNN) encoder-decoder architecture that combines self-supervised pretraining for rich representation learning with a prototype-based fine-tuning stage. This process discovers and refines core prototypes that encapsulate recurring, clinically meaningful patterns of surgical interaction, forming an explainable foundation for workflow analysis.
  Results: We evaluate our approach on the fine-grained CAT-SG dataset. ProtoFlow not only outperforms standard GNN baselines in overall accuracy but also demonstrates exceptional robustness in limited-data, few-shot scenarios, maintaining strong performance when trained on as few as one surgical video. Our qualitative analyses further show that the learned prototypes successfully identify distinct surgical sub-techniques and provide clear, interpretable insights into workflow deviations and rare complications.
  Conclusion: By uniting robust representation learning with inherent explainability, ProtoFlow represents a significant step toward developing more transparent, reliable, and data-efficient AI systems, accelerating their potential for clinical adoption in surgical training, real-time decision support, and workflow optimization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quality-Aware Framework for Video-Derived Respiratory Signals</title>
<link>https://arxiv.org/abs/2512.14093</link>
<guid>https://arxiv.org/abs/2512.14093</guid>
<content:encoded><![CDATA[
<div> Keywords: respiratory rate estimation, remote photoplethysmography, signal quality, machine learning, adaptive fusion<br /><br />Summary:<br /><br />1. The article addresses the challenge of unreliable video-based respiratory rate (RR) estimation caused by inconsistent signal quality across different extraction methods.<br />2. The authors propose a quality-aware predictive framework that integrates multiple heterogeneous signal sources derived from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines.<br />3. Ten signals are extracted and analyzed using four spectral estimation techniques: Welch’s method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection.<br />4. Segment-level quality indices are computed and fed into machine learning models to predict the estimation accuracy or select the most reliable signals dynamically.<br />5. This approach facilitates adaptive signal fusion and quality-based segment filtering to improve RR estimation.<br />6. The framework was evaluated on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI), demonstrating reduced RR estimation errors compared to individual methods.<br />7. Performance improvements varied depending on dataset characteristics, indicating adaptability of the method.<br />8. Overall, the study highlights the effectiveness of quality-driven predictive modeling in achieving scalable and generalizable video-based respiratory monitoring solutions. <div>
arXiv:2512.14093v1 Announce Type: new 
Abstract: Video-based respiratory rate (RR) estimation is often unreliable due to inconsistent signal quality across extraction methods. We present a predictive, quality-aware framework that integrates heterogeneous signal sources with dynamic assessment of reliability. Ten signals are extracted from facial remote photoplethysmography (rPPG), upper-body motion, and deep learning pipelines, and analyzed using four spectral estimators: Welch's method, Multiple Signal Classification (MUSIC), Fast Fourier Transform (FFT), and peak detection. Segment-level quality indices are then used to train machine learning models that predict accuracy or select the most reliable signal. This enables adaptive signal fusion and quality-based segment filtering. Experiments on three public datasets (OMuSense-23, COHFACE, MAHNOB-HCI) show that the proposed framework achieves lower RR estimation errors than individual methods in most cases, with performance gains depending on dataset characteristics. These findings highlight the potential of quality-driven predictive modeling to deliver scalable and generalizable video-based respiratory monitoring solutions.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorHOI: Zero-shot Generation of 4D Human-Object Interaction via Anchor-based Prior Distillation</title>
<link>https://arxiv.org/abs/2512.14095</link>
<guid>https://arxiv.org/abs/2512.14095</guid>
<content:encoded><![CDATA[
<div> Keywords: 4D human-object interaction, zero-shot generation, video diffusion models, anchor-based prior distillation, Neural Radiance Fields

<br /><br />Summary:  
This paper addresses the challenge of text-driven 4D human-object interaction (HOI) generation, which is traditionally limited by the lack of large-scale datasets. To improve scalability, previous methods have leveraged zero-shot 4D HOI generation using pre-trained image diffusion models but faced limitations in capturing detailed interaction cues across diverse scenarios. The proposed framework, AnchorHOI, advances 4D HOI generation by integrating hybrid priors, specifically introducing video diffusion models in addition to image diffusion models. A key difficulty lies in directly optimizing the high-dimensional 4D HOI data, particularly for accurate human pose and motion representation. AnchorHOI overcomes this challenge through an anchor-based prior distillation strategy, employing a two-step generation process guided by interaction-aware anchors. Two specially designed anchors are introduced: anchor Neural Radiance Fields (NeRFs) for detailed and expressive interaction composition, and anchor keypoints to ensure realistic motion synthesis. Experimental results indicate that AnchorHOI significantly outperforms previous approaches, offering enhanced diversity and better generalization in generated 4D human-object interactions. This framework thus presents a novel and effective solution to the zero-shot 4D HOI generation problem by leveraging advanced priors and structured guidance mechanisms. <div>
arXiv:2512.14095v1 Announce Type: new 
Abstract: Despite significant progress in text-driven 4D human-object interaction (HOI) generation with supervised methods, the scalability remains limited by the scarcity of large-scale 4D HOI datasets. To overcome this, recent approaches attempt zero-shot 4D HOI generation with pre-trained image diffusion models. However, interaction cues are minimally distilled during the generation process, restricting their applicability across diverse scenarios. In this paper, we propose AnchorHOI, a novel framework that thoroughly exploits hybrid priors by incorporating video diffusion models beyond image diffusion models, advancing 4D HOI generation. Nevertheless, directly optimizing high-dimensional 4D HOI with such priors remains challenging, particularly for human pose and compositional motion. To address this challenge, AnchorHOI introduces an anchor-based prior distillation strategy, which constructs interaction-aware anchors and then leverages them to guide generation in a tractable two-step process. Specifically, two tailored anchors are designed for 4D HOI generation: anchor Neural Radiance Fields (NeRFs) for expressive interaction composition, and anchor keypoints for realistic motion synthesis. Extensive experiments demonstrate that AnchorHOI outperforms previous methods with superior diversity and generalization.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration</title>
<link>https://arxiv.org/abs/2512.14096</link>
<guid>https://arxiv.org/abs/2512.14096</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Classifier-Free Guidance, Optimization, Adaptive caching, Evolutionary algorithms  

<br /><br />Summary: Diffusion models are currently the leading technique for generating high-quality images but are computationally intensive due to their iterative denoising process. One common enhancement, Classifier-Free Guidance (CFG), improves generation quality and control but doubles computation by requiring both conditional and unconditional forward passes at each timestep. The paper introduces OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a two-stage framework designed to accelerate diffusion transformers (DiT) by optimizing CFG usage. The first stage uses evolutionary algorithms to strategically select which timesteps to skip and determine appropriate guidance scales, successfully eliminating up to 82% of unconditional passes. The second stage implements adaptive rank allocation that adjusts calibration efforts for each transformer block, thereby preserving caching efficiency despite variable guidance scales. This method addresses the challenge that variable guidance introduces denoising deviations, which degrade traditional caching methods assuming constant CFG scales. Experimental results show that OUSAC achieves significant computational savings—53% on DiT-XL/2 with a 15% quality improvement, 60% savings and 16.1% quality gain on PixArt-alpha, and a 5x speedup on FLUX models—while also improving metrics like the CLIP Score compared to standard 50-step baselines. Overall, OUSAC effectively reduces computational costs while enhancing image generation quality in diffusion models. <div>
arXiv:2512.14096v1 Announce Type: new 
Abstract: Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViewMask-1-to-3: Multi-View Consistent Image Generation via Multimodal Diffusion Models</title>
<link>https://arxiv.org/abs/2512.14099</link>
<guid>https://arxiv.org/abs/2512.14099</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view image generation, discrete diffusion, visual tokens, cross-view consistency, masked token prediction<br /><br />Summary:<br /><br />1. This paper addresses the challenge of generating consistent multi-view images from a single input image and text description, which is difficult due to the need to maintain geometric consistency across viewpoints.<br /><br />2. Unlike prior methods that depend on 3D-aware architectures or specialized diffusion models requiring extensive multi-view data and geometric priors, the authors propose ViewMask-1-to-3, which leverages discrete diffusion for multi-view image synthesis.<br /><br />3. ViewMask-1-to-3 treats multi-view synthesis as a discrete sequence modeling problem, representing each viewpoint via visual tokens obtained through MAGVIT-v2 tokenization.<br /><br />4. The model achieves progressive generation of multiple viewpoints by masked token prediction using text input and iterative token unmasking, employing simple random masking and self-attention to maintain cross-view consistency.<br /><br />5. The method eliminates complex geometric constraints and specialized attention mechanisms, showing state-of-the-art performance on the GSO and 3D-FUTURE datasets according to PSNR, SSIM, and LPIPS metrics, while maintaining architectural simplicity. <div>
arXiv:2512.14099v1 Announce Type: new 
Abstract: Multi-view image generation from a single image and text description remains challenging due to the difficulty of maintaining geometric consistency across different viewpoints. Existing approaches typically rely on 3D-aware architectures or specialized diffusion models that require extensive multi-view training data and complex geometric priors. In this work, we introduce ViewMask-1-to-3, a pioneering approach to apply discrete diffusion models to multi-view image generation. Unlike continuous diffusion methods that operate in latent spaces, ViewMask-1-to-3 formulates multi-view synthesis as a discrete sequence modeling problem, where each viewpoint is represented as visual tokens obtained through MAGVIT-v2 tokenization. By unifying language and vision through masked token prediction, our approach enables progressive generation of multiple viewpoints through iterative token unmasking with text input. ViewMask-1-to-3 achieves cross-view consistency through simple random masking combined with self-attention, eliminating the requirement for complex 3D geometric constraints or specialized attention architectures. Our approach demonstrates that discrete diffusion provides a viable and simple alternative to existing multi-view generation methods, ranking first on average across GSO and 3D-FUTURE datasets in terms of PSNR, SSIM, and LPIPS, while maintaining architectural simplicity.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries</title>
<link>https://arxiv.org/abs/2512.14102</link>
<guid>https://arxiv.org/abs/2512.14102</guid>
<content:encoded><![CDATA[
<div> Keywords: Remote Sensing, Text-to-Image Retrieval, Neurosymbolic AI, Large Language Models, First-Order Logic<br /><br />Summary:<br /><br />1. This paper addresses challenges in text-to-image retrieval in remote sensing (RS), focusing on explainability and handling complex spatial relationships, which remain problematic in current large vision-language models (RS-LVLMS).<br />2. The authors propose RUNE (Reasoning Using Neurosymbolic Entities), a novel approach that integrates Large Language Models (LLMs) with neurosymbolic AI to reason explicitly over detected entities and First-Order Logic (FOL) expressions derived from textual queries.<br />3. Unlike RS-LVLMS that use implicit joint embeddings, RUNE enhances interpretability and performance by performing explicit logic-based reasoning, improving retrieval accuracy especially on complex queries.<br />4. For scalability, the method decomposes logic to operate on conditioned subsets of entities, reducing execution time compared to purely neural methods.<br />5. Foundation models are employed only for generating FOL expressions; the main reasoning process is handled by a dedicated neurosymbolic inference module.<br />6. The authors repurpose the DOTA object detection dataset by augmenting it with more complex queries to evaluate their approach.<br />7. Two new metrics are introduced—Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU)—to assess performance relative to query intricacy and image quality.<br />8. Experimental results demonstrate that RUNE surpasses state-of-the-art RS-LVLMS in performance, robustness, and explainability.<br />9. A practical use case on post-flood satellite image retrieval illustrates RUNE’s potential for real-world remote sensing applications. <div>
arXiv:2512.14102v1 Announce Type: new 
Abstract: Text-to-image retrieval in remote sensing (RS) has advanced rapidly with the rise of large vision-language models (LVLMs) tailored for aerial and satellite imagery, culminating in remote sensing large vision-language models (RS-LVLMS). However, limited explainability and poor handling of complex spatial relations remain key challenges for real-world use. To address these issues, we introduce RUNE (Reasoning Using Neurosymbolic Entities), an approach that combines Large Language Models (LLMs) with neurosymbolic AI to retrieve images by reasoning over the compatibility between detected entities and First-Order Logic (FOL) expressions derived from text queries. Unlike RS-LVLMs that rely on implicit joint embeddings, RUNE performs explicit reasoning, enhancing performance and interpretability. For scalability, we propose a logic decomposition strategy that operates on conditioned subsets of detected entities, guaranteeing shorter execution time compared to neural approaches. Rather than using foundation models for end-to-end retrieval, we leverage them only to generate FOL expressions, delegating reasoning to a neurosymbolic inference module. For evaluation we repurpose the DOTA dataset, originally designed for object detection, by augmenting it with more complex queries than in existing benchmarks. We show the LLM's effectiveness in text-to-logic translation and compare RUNE with state-of-the-art RS-LVLMs, demonstrating superior performance. We introduce two metrics, Retrieval Robustness to Query Complexity (RRQC) and Retrieval Robustness to Image Uncertainty (RRIU), which evaluate performance relative to query complexity and image uncertainty. RUNE outperforms joint-embedding models in complex RS retrieval tasks, offering gains in performance, robustness, and explainability. We show RUNE's potential for real-world RS applications through a use case on post-flood satellite image retrieval.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective, Controlled and Domain-Agnostic Unlearning in Pretrained CLIP: A Training- and Data-Free Approach</title>
<link>https://arxiv.org/abs/2512.14113</link>
<guid>https://arxiv.org/abs/2512.14113</guid>
<content:encoded><![CDATA[
<div> unlearning, CLIP, zero-shot classification, domain-specific forgetting, multimodal nullspace  

<br /><br />Summary:  
This paper addresses the challenge of unlearning specific object classes in pretrained models like CLIP without additional data, retraining, or negative impact on unrelated tasks. The authors introduce a novel, training- and data-free unlearning framework that supports three key forgetting paradigms: (1) global unlearning, which removes selected objects across all visual domains, (2) domain-specific knowledge removal that targets certain domains (e.g., removing sketches while keeping photo recognition intact), and (3) selective complete unlearning within chosen domains only. Their approach exploits a multimodal nullspace by integrating text prompts with synthesized visual prototypes generated from CLIP’s joint embedding space, allowing precise removal of undesired class information while preserving other knowledge. This methodology overcomes the constraints of traditional retraining-based unlearning techniques, delivering a flexible and computationally efficient solution. The proposed framework enhances control over model forgetting, making it applicable in real-world scenarios where selective knowledge removal is critical without compromising overall model performance or requiring costly retraining efforts. <div>
arXiv:2512.14113v1 Announce Type: new 
Abstract: Pretrained models like CLIP have demonstrated impressive zero-shot classification capabilities across diverse visual domains, spanning natural images, artistic renderings, and abstract representations. However, real-world applications often demand the removal (or "unlearning") of specific object classes without requiring additional data or retraining, or affecting the model's performance on unrelated tasks. In this paper, we propose a novel training- and data-free unlearning framework that enables three distinct forgetting paradigms: (1) global unlearning of selected objects across all domains, (2) domain-specific knowledge removal (e.g., eliminating sketch representations while preserving photo recognition), and (3) complete unlearning in selective domains. By leveraging a multimodal nullspace through synergistic integration of text prompts and synthesized visual prototypes derived from CLIP's joint embedding space, our method efficiently removes undesired class information while preserving the remaining knowledge. This approach overcomes the limitations of existing retraining-based methods and offers a flexible and computationally efficient solution for controlled model forgetting.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MFE-GAN: Efficient GAN-based Framework for Document Image Enhancement and Binarization with Multi-scale Feature Extraction</title>
<link>https://arxiv.org/abs/2512.14114</link>
<guid>https://arxiv.org/abs/2512.14114</guid>
<content:encoded><![CDATA[
<div> Keywords: document image enhancement, binarization, GAN, multi-scale feature extraction, Haar wavelet transformation<br /><br />Summary: Document image enhancement and binarization are crucial preprocessing steps in improving the accuracy and efficiency of optical character recognition (OCR) systems, especially for degraded color documents. Traditional approaches use separate generative adversarial networks (GANs) for different color channels to eliminate shadows and noise, but this leads to increased training and inference times. To overcome this, the authors propose MFE-GAN, an efficient GAN-based framework that integrates multi-scale feature extraction (MFE) using Haar wavelet transformation (HWT) and normalization to preprocess images before feeding them into the GAN. This approach reduces both training and inference times without compromising performance. The paper introduces novel generator and discriminator architectures along with specialized loss functions that further enhance the model's effectiveness. Comprehensive ablation studies validate the contributions of each component. Experimental evaluations on Benchmark, Nabuco, and CMATERdb datasets show that MFE-GAN significantly lowers computational costs while achieving performance comparable to state-of-the-art methods. The implementation is publicly accessible at https://ruiyangju.github.io/MFE-GAN, providing a practical solution for efficient document image preprocessing in real-world OCR systems. <div>
arXiv:2512.14114v1 Announce Type: new 
Abstract: Document image enhancement and binarization are commonly performed prior to document analysis and recognition tasks for improving the efficiency and accuracy of optical character recognition (OCR) systems. This is because directly recognizing text in degraded documents, particularly in color images, often results in unsatisfactory recognition performance. To address these issues, existing methods train independent generative adversarial networks (GANs) for different color channels to remove shadows and noise, which, in turn, facilitates efficient text information extraction. However, deploying multiple GANs results in long training and inference times. To reduce both training and inference times of document image enhancement and binarization models, we propose MFE-GAN, an efficient GAN-based framework with multi-scale feature extraction (MFE), which incorporates Haar wavelet transformation (HWT) and normalization to process document images before feeding them into GANs for training. In addition, we present novel generators, discriminators, and loss functions to improve the model's performance, and we conduct ablation studies to demonstrate their effectiveness. Experimental results on the Benchmark, Nabuco, and CMATERdb datasets demonstrate that the proposed MFE-GAN significantly reduces the total training and inference times while maintaining comparable performance with respect to state-of-the-art (SOTA) methods. The implementation of this work is available at https://ruiyangju.github.io/MFE-GAN.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SportsGPT: An LLM-driven Framework for Interpretable Sports Motion Assessment and Training Guidance</title>
<link>https://arxiv.org/abs/2512.14121</link>
<guid>https://arxiv.org/abs/2512.14121</guid>
<content:encoded><![CDATA[
<div> Keywords: SportsGPT, MotionDTW, KISMAM, SportsRAG, interpretable sports motion assessment<br /><br />Summary: This paper introduces SportsGPT, a novel framework that leverages Large Language Models (LLMs) combined with motion analysis techniques to provide interpretable sports motion assessment and professional training guidance. First, the authors present MotionDTW, a two-stage time series alignment algorithm designed to accurately extract keyframes from skeleton-based motion sequences by comparing athlete movements with high-quality target models. Next, they develop the Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM), which produces interpretable metrics such as "insufficient extension" by contrasting extracted keyframes against target models, enabling precise diagnostic insights. Finally, the framework includes SportsRAG, a retrieval-augmented generation (RAG) model based on Qwen3, which utilizes a 6 billion token domain-specific knowledge base containing QA pairs to prompt the LLM to deliver expert training guidance tailored to the athlete's performance. Experimental results show that MotionDTW notably outperforms traditional alignment methods by achieving lower temporal error and higher Intersection over Union (IoU) scores. Ablation studies further validate the effectiveness of KISMAM and SportsRAG, demonstrating that SportsGPT exceeds the diagnostic accuracy and professionalism of general large language models. Overall, SportsGPT establishes a closed-loop system from raw motion time-series input to actionable, interpretable sports training advice, addressing current gaps in intelligent sports analysis solutions. <div>
arXiv:2512.14121v1 Announce Type: new 
Abstract: Existing intelligent sports analysis systems mainly focus on "scoring and visualization," often lacking automatic performance diagnosis and interpretable training guidance. Recent advances of Large Language Models (LMMs) and motion analysis techniques provide new opportunities to address the above limitations. In this paper, we propose SportsGPT, an LLM-driven framework for interpretable sports motion assessment and training guidance, which establishes a closed loop from motion time-series input to professional training guidance. First, given a set of high-quality target models, we introduce MotionDTW, a two-stage time series alignment algorithm designed for accurate keyframe extraction from skeleton-based motion sequences. Subsequently, we design a Knowledge-based Interpretable Sports Motion Assessment Model (KISMAM) to obtain a set of interpretable assessment metrics (e.g., insufficient extension) by constrasting the keyframes with the targe models. Finally, we propose SportsRAG, a RAG-based training guidance model based on Qwen3. Leveraging a 6B-token knowledge base, it prompts the LLM to generate professional training guidance by retrieving domain-specific QA pairs. Experimental results demonstrate that MotionDTW significantly outperforms traditional methods with lower temporal error and higher IoU scores. Furthermore, ablation studies validate the KISMAM and SportsRAG, confirming that SportsGPT surpasses general LLMs in diagnostic accuracy and professionalism.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistent Instance Field for Dynamic Scene Understanding</title>
<link>https://arxiv.org/abs/2512.14126</link>
<guid>https://arxiv.org/abs/2512.14126</guid>
<content:encoded><![CDATA[
<div> Keywords: Consistent Instance Field, deformable 3D Gaussians, spatio-temporal representation, novel-view panoptic segmentation, open-vocabulary 4D querying<br /><br />Summary:<br /><br />1. The paper introduces Consistent Instance Field, a novel continuous and probabilistic spatio-temporal representation designed for dynamic scene understanding, aiming to address limitations of previous discrete tracking or view-dependent feature-based methods.<br /><br />2. The approach disentangles object visibility from persistent object identity by modeling each space-time coordinate with two components: an occupancy probability and a conditional instance distribution.<br /><br />3. A key innovation is the use of deformable 3D Gaussians embedded with instance information, which jointly encode radiance and semantic features. These are learned directly from input RGB images and instance masks using differentiable rasterization techniques.<br /><br />4. The method includes new mechanisms to calibrate identities per Gaussian and to resample Gaussians towards semantically important regions, enabling consistent instance representations across both space and time.<br /><br />5. Experimental validation on the HyperNeRF and Neu3D datasets shows that the proposed method significantly outperforms current state-of-the-art approaches in tasks such as novel-view panoptic segmentation and open-vocabulary 4D querying, demonstrating its robustness and practical effectiveness in dynamic scene analysis. <div>
arXiv:2512.14126v1 Announce Type: new 
Abstract: We introduce Consistent Instance Field, a continuous and probabilistic spatio-temporal representation for dynamic scene understanding. Unlike prior methods that rely on discrete tracking or view-dependent features, our approach disentangles visibility from persistent object identity by modeling each space-time point with an occupancy probability and a conditional instance distribution. To realize this, we introduce a novel instance-embedded representation based on deformable 3D Gaussians, which jointly encode radiance and semantic information and are learned directly from input RGB images and instance masks through differentiable rasterization. Furthermore, we introduce new mechanisms to calibrate per-Gaussian identities and resample Gaussians toward semantically active regions, ensuring consistent instance representations across space and time. Experiments on HyperNeRF and Neu3D datasets demonstrate that our method significantly outperforms state-of-the-art methods on novel-view panoptic segmentation and open-vocabulary 4D querying tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Erasing CLIP Memories: Non-Destructive, Data-Free Zero-Shot class Unlearning in CLIP Models</title>
<link>https://arxiv.org/abs/2512.14137</link>
<guid>https://arxiv.org/abs/2512.14137</guid>
<content:encoded><![CDATA[
<div> Selective Unlearning, Multimodal Models, Nullspace Projection, CLIP, Privacy Preservation  

<br /><br />Summary:  
This paper presents a novel closed-form method for selective unlearning within multimodal models, focusing on pretrained models like CLIP. The proposed technique uses nullspace projection to remove specific target class information embedded in the final projection layer, eliminating the need for retraining or accessing images from the forget set. By calculating an orthonormal basis for the subspace spanned by the target text embeddings and projecting these directions out, the method significantly reduces the alignment between image features and undesired classes. Unlike conventional unlearning methods that depend on iterative fine-tuning and extensive data curation, this approach is computationally efficient and precise. It achieves a marked decline in zero-shot classification performance on undesired target classes while maintaining the model’s overall multimodal understanding. Additionally, experiments show that partial projection allows balancing complete unlearning and retention of useful information, overcoming challenges related to model decontamination and privacy preservation. This approach provides an effective, efficient alternative for managing unwanted knowledge in large pretrained multimodal models without compromising general capabilities. <div>
arXiv:2512.14137v1 Announce Type: new 
Abstract: We introduce a novel, closed-form approach for selective unlearning in multimodal models, specifically targeting pretrained models such as CLIP. Our method leverages nullspace projection to erase the target class information embedded in the final projection layer, without requiring any retraining or the use of images from the forget set. By computing an orthonormal basis for the subspace spanned by target text embeddings and projecting these directions, we dramatically reduce the alignment between image features and undesired classes. Unlike traditional unlearning techniques that rely on iterative fine-tuning and extensive data curation, our approach is both computationally efficient and surgically precise. This leads to a pronounced drop in zero-shot performance for the target classes while preserving the overall multimodal knowledge of the model. Our experiments demonstrate that even a partial projection can balance between complete unlearning and retaining useful information, addressing key challenges in model decontamination and privacy preservation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SketchAssist: A Practical Assistant for Semantic Edits and Precise Local Redrawing</title>
<link>https://arxiv.org/abs/2512.14140</link>
<guid>https://arxiv.org/abs/2512.14140</guid>
<content:encoded><![CDATA[
<div> Keywords: sketch editing, image editing, instruction-guided edits, style preservation, mixture-of-experts<br /><br />Summary:  
This paper introduces SketchAssist, an interactive assistant designed to enhance digital sketch editing by combining high-level semantic instruction-guided edits with precise, local line-guided redrawing, while maintaining the integrity of unrelated sketch regions and overall composition. To support scalable training, the authors develop a controllable data generation pipeline that creates realistic multi-step edit sequences by starting from attribute-free base sketches, adding attributes in sequences, and further augmenting style diversity using a style-preserving attribute-removal model applied across diverse sketches. SketchAssist builds on this dataset through a unified editing framework derived from DiT-based editors, innovatively repurposing RGB image channels to encode multi-modal inputs. This design enables seamless switching between instruction-based global edits and line-based local redraw tasks within a single interface. A novel task-guided mixture-of-experts mechanism is integrated into LoRA layers, which uses textual and visual cues to route information and specialize network behavior per editing mode. Extensive experiments demonstrate that SketchAssist achieves state-of-the-art performance in both instruction adherence and style/structure preservation, outperforming recent baseline methods. Overall, this work delivers an effective, controllable toolset and dataset for practical sketch creation and revision, addressing key challenges in preserving style-sensitive line art structures during complex edits. <div>
arXiv:2512.14140v1 Announce Type: new 
Abstract: Sketch editing is central to digital illustration, yet existing image editing systems struggle to preserve the sparse, style-sensitive structure of line art while supporting both high-level semantic changes and precise local redrawing. We present SketchAssist, an interactive sketch drawing assistant that accelerates creation by unifying instruction-guided global edits with line-guided region redrawing, while keeping unrelated regions and overall composition intact. To enable this assistant at scale, we introduce a controllable data generation pipeline that (i) constructs attribute-addition sequences from attribute-free base sketches, (ii) forms multi-step edit chains via cross-sequence sampling, and (iii) expands stylistic coverage with a style-preserving attribute-removal model applied to diverse sketches. Building on this data, SketchAssist employs a unified sketch editing framework with minimal changes to DiT-based editors. We repurpose the RGB channels to encode the inputs, enabling seamless switching between instruction-guided edits and line-guided redrawing within a single input interface. To further specialize behavior across modes, we integrate a task-guided mixture-of-experts into LoRA layers, routing by text and visual cues to improve semantic controllability, structural fidelity, and style preservation. Extensive experiments show state-of-the-art results on both tasks, with superior instruction adherence and style/structure preservation compared to recent baselines. Together, our dataset and SketchAssist provide a practical, controllable assistant for sketch creation and revision.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TorchTraceAP: A New Benchmark Dataset for Detecting Performance Anti-Patterns in Computer Vision Models</title>
<link>https://arxiv.org/abs/2512.14141</link>
<guid>https://arxiv.org/abs/2512.14141</guid>
<content:encoded><![CDATA[
<div> Keywords: performance anti-patterns, machine learning models, PyTorch traces, large language models, computer vision  

<br /><br />Summary:  
This paper addresses the challenge of identifying performance anti-patterns in machine learning (ML) models, which is vital for efficient training and inference but usually requires specialized expertise. Current approaches used by large tech companies depend on dedicated ML infrastructure engineers and resource-intensive workflows that are inaccessible to many computer vision researchers. The authors highlight the difficulty of pinpointing problematic segments within lengthy execution traces, a task that remains laborious and is not effectively automated by existing ML models, including large language models (LLMs). To overcome this, they introduce the first benchmark dataset designed specifically to evaluate and enhance ML models' capabilities in detecting anti-patterns in traces. This dataset comprises over 600 PyTorch traces collected from a variety of computer vision tasks—classification, detection, segmentation, and generation—across multiple hardware platforms. The paper proposes a novel iterative method involving a lightweight ML model for initial detection of anti-pattern segments, followed by a large language model for detailed classification and targeted feedback. Experimental results demonstrate that this combined approach outperforms unsupervised clustering and rule-based statistical methods, while effectively mitigating LLMs’ limitations in context length and reasoning efficiency. <div>
arXiv:2512.14141v1 Announce Type: new 
Abstract: Identifying and addressing performance anti-patterns in machine learning (ML) models is critical for efficient training and inference, but it typically demands deep expertise spanning system infrastructure, ML models and kernel development. While large tech companies rely on dedicated ML infrastructure engineers to analyze torch traces and benchmarks, such resource-intensive workflows are largely inaccessible to computer vision researchers in general. Among the challenges, pinpointing problematic trace segments within lengthy execution traces remains the most time-consuming task, and is difficult to automate with current ML models, including LLMs. In this work, we present the first benchmark dataset specifically designed to evaluate and improve ML models' ability to detect anti patterns in traces. Our dataset contains over 600 PyTorch traces from diverse computer vision models classification, detection, segmentation, and generation collected across multiple hardware platforms. We also propose a novel iterative approach: a lightweight ML model first detects trace segments with anti patterns, followed by a large language model (LLM) for fine grained classification and targeted feedback. Experimental results demonstrate that our method significantly outperforms unsupervised clustering and rule based statistical techniques for detecting anti pattern regions. Our method also effectively compensates LLM's limited context length and reasoning inefficiencies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIS-BA: Continuous Interaction Space Based Backdoor Attack for Object Detection in the Real-World</title>
<link>https://arxiv.org/abs/2512.14158</link>
<guid>https://arxiv.org/abs/2512.14158</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor attacks, object detection, multi-trigger, inter-object interaction, CIS-BA  

<br /><br />Summary:  
1. The paper addresses the vulnerability of object detection models, especially in autonomous driving, to backdoor attacks that exploit static, single-trigger, single-object mappings.  
2. A novel attack paradigm called CIS-BA is proposed, which shifts the trigger design from static object features to continuous inter-object interaction patterns, capturing how objects co-occur and interact within a scene.  
3. CIS-BA introduces space triggers modeled in a continuous interaction space, enabling multi-trigger, multi-object attacks that maintain robustness through invariant geometric relationships.  
4. To realize this framework, the authors design CIS-Frame, which analyzes interactions to construct space triggers, formalizes them as class-geometry constraints for training data poisoning, and integrates the backdoor into the detector during training.  
5. CIS-Frame supports both single-object attacks (such as misclassification and disappearance) and simultaneous multi-object attacks, creating complex coordinated effects adaptable across varying interaction states.  
6. Experimental results on MS-COCO and real-world videos demonstrate that CIS-BA achieves over 97% attack success in complex environments and maintains more than 95% effectiveness under dynamic multi-trigger scenarios while bypassing three state-of-the-art defense methods.  
7. This work expands the scope of backdoor attacks into interaction-intensive scenarios and offers new insights into securing object detection systems against sophisticated coordinated backdoor threats. <div>
arXiv:2512.14158v1 Announce Type: new 
Abstract: Object detection models deployed in real-world applications such as autonomous driving face serious threats from backdoor attacks. Despite their practical effectiveness,existing methods are inherently limited in both capability and robustness due to their dependence on single-trigger-single-object mappings and fragile pixel-level cues. We propose CIS-BA, a novel backdoor attack paradigm that redefines trigger design by shifting from static object features to continuous inter-object interaction patterns that describe how objects co-occur and interact in a scene. By modeling these patterns as a continuous interaction space, CIS-BA introduces space triggers that, for the first time, enable a multi-trigger-multi-object attack mechanism while achieving robustness through invariant geometric relations. To implement this paradigm, we design CIS-Frame, which constructs space triggers via interaction analysis, formalizes them as class-geometry constraints for sample poisoning, and embeds the backdoor during detector training. CIS-Frame supports both single-object attacks (object misclassification and disappearance) and multi-object simultaneous attacks, enabling complex and coordinated effects across diverse interaction states. Experiments on MS-COCO and real-world videos show that CIS-BA achieves over 97% attack success under complex environments and maintains over 95% effectiveness under dynamic multi-trigger conditions, while evading three state-of-the-art defenses. In summary, CIS-BA extends the landscape of backdoor attacks in interaction-intensive scenarios and provides new insights into the security of object detection systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastDDHPose: Towards Unified, Efficient, and Disentangled 3D Human Pose Estimation</title>
<link>https://arxiv.org/abs/2512.14162</link>
<guid>https://arxiv.org/abs/2512.14162</guid>
<content:encoded><![CDATA[
arXiv:2512.14162v1 Announce Type: new 
Abstract: Recent approaches for monocular 3D human pose estimation (3D HPE) have achieved leading performance by directly regressing 3D poses from 2D keypoint sequences. Despite the rapid progress in 3D HPE, existing methods are typically trained and evaluated under disparate frameworks, lacking a unified framework for fair comparison. To address these limitations, we propose Fast3DHPE, a modular framework that facilitates rapid reproduction and flexible development of new methods. By standardizing training and evaluation protocols, Fast3DHPE enables fair comparison across 3D human pose estimation methods while significantly improving training efficiency. Within this framework, we introduce FastDDHPose, a Disentangled Diffusion-based 3D Human Pose Estimation method which leverages the strong latent distribution modeling capability of diffusion models to explicitly model the distributions of bone length and bone direction while avoiding further amplification of hierarchical error accumulation. Moreover, we design an efficient Kinematic-Hierarchical Spatial and Temporal Denoiser that encourages the model to focus on kinematic joint hierarchies while avoiding unnecessary modeling of overly complex joint topologies. Extensive experiments on Human3.6M and MPI-INF-3DHP show that the Fast3DHPE framework enables fair comparison of all methods while significantly improving training efficiency. Within this unified framework, FastDDHPose achieves state-of-the-art performance with strong generalization and robustness in in-the-wild scenarios. The framework and models will be released at: https://github.com/Andyen512/Fast3DHPE
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes</title>
<link>https://arxiv.org/abs/2512.14177</link>
<guid>https://arxiv.org/abs/2512.14177</guid>
<content:encoded><![CDATA[
arXiv:2512.14177v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) often produce plausible but unreliable outputs, making robust uncertainty estimation essential. Recent work on semantic uncertainty estimates relies on external models to cluster multiple sampled responses and measure their semantic consistency. However, these clustering methods are often fragile, highly sensitive to minor phrasing variations, and can incorrectly group or separate semantically similar answers, leading to unreliable uncertainty estimates. We propose Semantic Gaussian Process Uncertainty (SGPU), a Bayesian framework that quantifies semantic uncertainty by analyzing the geometric structure of answer embeddings, avoiding brittle clustering. SGPU maps generated answers into a dense semantic space, computes the Gram matrix of their embeddings, and summarizes their semantic configuration via the eigenspectrum. This spectral representation is then fed into a Gaussian Process Classifier that learns to map patterns of semantic consistency to predictive uncertainty, and that can be applied in both black-box and white-box settings. Across six LLMs and LVLMs on eight datasets spanning VQA, image classification, and textual QA, SGPU consistently achieves state-of-the-art calibration (ECE) and discriminative (AUROC, AUARC) performance. We further show that SGPU transfers across models and modalities, indicating that its spectral representation captures general patterns of semantic uncertainty.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spherical Voronoi: Directional Appearance as a Differentiable Partition of the Sphere</title>
<link>https://arxiv.org/abs/2512.14180</link>
<guid>https://arxiv.org/abs/2512.14180</guid>
<content:encoded><![CDATA[
arXiv:2512.14180v1 Announce Type: new 
Abstract: Radiance field methods (e.g. 3D Gaussian Splatting) have emerged as a powerful paradigm for novel view synthesis, yet their appearance modeling often relies on Spherical Harmonics (SH), which impose fundamental limitations. SH struggle with high-frequency signals, exhibit Gibbs ringing artifacts, and fail to capture specular reflections - a key component of realistic rendering. Although alternatives like spherical Gaussians offer improvements, they add significant optimization complexity. We propose Spherical Voronoi (SV) as a unified framework for appearance representation in 3D Gaussian Splatting. SV partitions the directional domain into learnable regions with smooth boundaries, providing an intuitive and stable parameterization for view-dependent effects. For diffuse appearance, SV achieves competitive results while keeping optimization simpler than existing alternatives. For reflections - where SH fail - we leverage SV as learnable reflection probes, taking reflected directions as input following principles from classical graphics. This formulation attains state-of-the-art results on synthetic and real-world datasets, demonstrating that SV offers a principled, efficient, and general solution for appearance modeling in explicit 3D representations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fracture Morphology Classification: Local Multiclass Modeling for Multilabel Complexity</title>
<link>https://arxiv.org/abs/2512.14196</link>
<guid>https://arxiv.org/abs/2512.14196</guid>
<content:encoded><![CDATA[
arXiv:2512.14196v1 Announce Type: new 
Abstract: Between $15\,\%$ and $45\,\%$ of children experience a fracture during their growth years, making accurate diagnosis essential. Fracture morphology, alongside location and fragment angle, is a key diagnostic feature. In this work, we propose a method to extract fracture morphology by assigning automatically global AO codes to corresponding fracture bounding boxes. This approach enables the use of public datasets and reformulates the global multilabel task into a local multiclass one, improving the average F1 score by $7.89\,\%$. However, performance declines when using imperfect fracture detectors, highlighting challenges for real-world deployment. Our code is available on GitHub.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond a Single Light: A Large-Scale Aerial Dataset for Urban Scene Reconstruction Under Varying Illumination</title>
<link>https://arxiv.org/abs/2512.14200</link>
<guid>https://arxiv.org/abs/2512.14200</guid>
<content:encoded><![CDATA[
arXiv:2512.14200v1 Announce Type: new 
Abstract: Recent advances in Neural Radiance Fields and 3D Gaussian Splatting have demonstrated strong potential for large-scale UAV-based 3D reconstruction tasks by fitting the appearance of images. However, real-world large-scale captures are often based on multi-temporal data capture, where illumination inconsistencies across different times of day can significantly lead to color artifacts, geometric inaccuracies, and inconsistent appearance. Due to the lack of UAV datasets that systematically capture the same areas under varying illumination conditions, this challenge remains largely underexplored. To fill this gap, we introduceSkyLume, a large-scale, real-world UAV dataset specifically designed for studying illumination robust 3D reconstruction in urban scene modeling: (1) We collect data from 10 urban regions data comprising more than 100k high resolution UAV images (four oblique views and nadir), where each region is captured at three periods of the day to systematically isolate illumination changes. (2) To support precise evaluation of geometry and appearance, we provide per-scene LiDAR scans and accurate 3D ground-truth for assessing depth, surface normals, and reconstruction quality under varying illumination. (3) For the inverse rendering task, we introduce the Temporal Consistency Coefficient (TCC), a metric that measuress cross-time albedo stability and directly evaluates the robustness of the disentanglement of light and material. We aim for this resource to serve as a foundation that advances research and real-world evaluation in large-scale inverse rendering, geometry reconstruction, and novel view synthesis.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos</title>
<link>https://arxiv.org/abs/2512.14217</link>
<guid>https://arxiv.org/abs/2512.14217</guid>
<content:encoded><![CDATA[
arXiv:2512.14217v1 Announce Type: new 
Abstract: Video diffusion models provide powerful real-world simulators for embodied AI but remain limited in controllability for robotic manipulation. Recent works on trajectory-conditioned video generation address this gap but often rely on 2D trajectories or single modality conditioning, which restricts their ability to produce controllable and consistent robotic demonstrations. We present DRAW2ACT, a depth-aware trajectory-conditioned video generation framework that extracts multiple orthogonal representations from the input trajectory, capturing depth, semantics, shape and motion, and injects them into the diffusion model. Moreover, we propose to jointly generate spatially aligned RGB and depth videos, leveraging cross-modality attention mechanisms and depth supervision to enhance the spatio-temporal consistency. Finally, we introduce a multimodal policy model conditioned on the generated RGB and depth sequences to regress the robot's joint angles. Experiments on Bridge V2, Berkeley Autolab, and simulation benchmarks show that DRAW2ACT achieves superior visual fidelity and consistency while yielding higher manipulation success rates compared to existing baselines.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History-Enhanced Two-Stage Transformer for Aerial Vision-and-Language Navigation</title>
<link>https://arxiv.org/abs/2512.14222</link>
<guid>https://arxiv.org/abs/2512.14222</guid>
<content:encoded><![CDATA[
arXiv:2512.14222v1 Announce Type: new 
Abstract: Aerial Vision-and-Language Navigation (AVLN) requires Unmanned Aerial Vehicle (UAV) agents to localize targets in large-scale urban environments based on linguistic instructions. While successful navigation demands both global environmental reasoning and local scene comprehension, existing UAV agents typically adopt mono-granularity frameworks that struggle to balance these two aspects. To address this limitation, this work proposes a History-Enhanced Two-Stage Transformer (HETT) framework, which integrates the two aspects through a coarse-to-fine navigation pipeline. Specifically, HETT first predicts coarse-grained target positions by fusing spatial landmarks and historical context, then refines actions via fine-grained visual analysis. In addition, a historical grid map is designed to dynamically aggregate visual features into a structured spatial memory, enhancing comprehensive scene awareness. Additionally, the CityNav dataset annotations are manually refined to enhance data quality. Experiments on the refined CityNav dataset show that HETT delivers significant performance gains, while extensive ablation studies further verify the effectiveness of each component.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniGen: Unified Multimodal Sensor Generation for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.14225</link>
<guid>https://arxiv.org/abs/2512.14225</guid>
<content:encoded><![CDATA[
arXiv:2512.14225v1 Announce Type: new 
Abstract: Autonomous driving has seen remarkable advancements, largely driven by extensive real-world data collection. However, acquiring diverse and corner-case data remains costly and inefficient. Generative models have emerged as a promising solution by synthesizing realistic sensor data. However, existing approaches primarily focus on single-modality generation, leading to inefficiencies and misalignment in multimodal sensor data. To address these challenges, we propose OminiGen, which generates aligned multimodal sensor data in a unified framework. Our approach leverages a shared Bird\u2019s Eye View (BEV) space to unify multimodal features and designs a novel generalizable multimodal reconstruction method, UAE, to jointly decode LiDAR and multi-view camera data. UAE achieves multimodal sensor decoding through volume rendering, enabling accurate and flexible reconstruction. Furthermore, we incorporate a Diffusion Transformer (DiT) with a ControlNet branch to enable controllable multimodal sensor generation. Our comprehensive experiments demonstrate that OminiGen achieves desired performances in unified multimodal sensor data generation with multimodal consistency and flexible sensor adjustments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-View MRI Approach for Classification of MGMT Methylation in Glioblastoma Patients</title>
<link>https://arxiv.org/abs/2512.14232</link>
<guid>https://arxiv.org/abs/2512.14232</guid>
<content:encoded><![CDATA[
arXiv:2512.14232v1 Announce Type: new 
Abstract: The presence of MGMT promoter methylation significantly affects how well chemotherapy works for patients with Glioblastoma Multiforme (GBM). Currently, confirmation of MGMT promoter methylation relies on invasive brain tumor tissue biopsies. In this study, we explore radiogenomics techniques, a promising approach in precision medicine, to identify genetic markers from medical images. Using MRI scans and deep learning models, we propose a new multi-view approach that considers spatial relationships between MRI views to detect MGMT methylation status. Importantly, our method extracts information from all three views without using a complicated 3D deep learning model, avoiding issues associated with high parameter count, slow convergence, and substantial memory demands. We also introduce a new technique for tumor slice extraction and show its superiority over existing methods based on multiple evaluation metrics. By comparing our approach to state-of-the-art models, we demonstrate the efficacy of our method. Furthermore, we share a reproducible pipeline of published models, encouraging transparency and the development of robust diagnostic tools. Our study highlights the potential of non-invasive methods for identifying MGMT promoter methylation and contributes to advancing precision medicine in GBM treatment.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViBES: A Conversational Agent with Behaviorally-Intelligent 3D Virtual Body</title>
<link>https://arxiv.org/abs/2512.14234</link>
<guid>https://arxiv.org/abs/2512.14234</guid>
<content:encoded><![CDATA[
arXiv:2512.14234v1 Announce Type: new 
Abstract: Human communication is inherently multimodal and social: words, prosody, and body language jointly carry intent. Yet most prior systems model human behavior as a translation task co-speech gesture or text-to-motion that maps a fixed utterance to motion clips-without requiring agentic decision-making about when to move, what to do, or how to adapt across multi-turn dialogue. This leads to brittle timing, weak social grounding, and fragmented stacks where speech, text, and motion are trained or inferred in isolation. We introduce ViBES (Voice in Behavioral Expression and Synchrony), a conversational 3D agent that jointly plans language and movement and executes dialogue-conditioned body actions. Concretely, ViBES is a speech-language-behavior (SLB) model with a mixture-of-modality-experts (MoME) backbone: modality-partitioned transformer experts for speech, facial expression, and body motion. The model processes interleaved multimodal token streams with hard routing by modality (parameters are split per expert), while sharing information through cross-expert attention. By leveraging strong pretrained speech-language models, the agent supports mixed-initiative interaction: users can speak, type, or issue body-action directives mid-conversation, and the system exposes controllable behavior hooks for streaming responses. We further benchmark on multi-turn conversation with automatic metrics of dialogue-motion alignment and behavior quality, and observe consistent gains over strong co-speech and text-to-motion baselines. ViBES goes beyond "speech-conditioned motion generation" toward agentic virtual bodies where language, prosody, and movement are jointly generated, enabling controllable, socially competent 3D interaction. Code and data will be made available at: ai.stanford.edu/~juze/ViBES/
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4D-RaDiff: Latent Diffusion for 4D Radar Point Cloud Generation</title>
<link>https://arxiv.org/abs/2512.14235</link>
<guid>https://arxiv.org/abs/2512.14235</guid>
<content:encoded><![CDATA[
arXiv:2512.14235v1 Announce Type: new 
Abstract: Automotive radar has shown promising developments in environment perception due to its cost-effectiveness and robustness in adverse weather conditions. However, the limited availability of annotated radar data poses a significant challenge for advancing radar-based perception systems. To address this limitation, we propose a novel framework to generate 4D radar point clouds for training and evaluating object detectors. Unlike image-based diffusion, our method is designed to consider the sparsity and unique characteristics of radar point clouds by applying diffusion to a latent point cloud representation. Within this latent space, generation is controlled via conditioning at either the object or scene level. The proposed 4D-RaDiff converts unlabeled bounding boxes into high-quality radar annotations and transforms existing LiDAR point cloud data into realistic radar scenes. Experiments demonstrate that incorporating synthetic radar data of 4D-RaDiff as data augmentation method during training consistently improves object detection performance compared to training on real data only. In addition, pre-training on our synthetic data reduces the amount of required annotated radar data by up to 90% while achieving comparable object detection performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elastic3D: Controllable Stereo Video Conversion with Guided Latent Decoding</title>
<link>https://arxiv.org/abs/2512.14236</link>
<guid>https://arxiv.org/abs/2512.14236</guid>
<content:encoded><![CDATA[
arXiv:2512.14236v1 Announce Type: new 
Abstract: The growing demand for immersive 3D content calls for automated monocular-to-stereo video conversion. We present Elastic3D, a controllable, direct end-to-end method for upgrading a conventional video to a binocular one. Our approach, based on (conditional) latent diffusion, avoids artifacts due to explicit depth estimation and warping. The key to its high-quality stereo video output is a novel, guided VAE decoder that ensures sharp and epipolar-consistent stereo video output. Moreover, our method gives the user control over the strength of the stereo effect (more precisely, the disparity range) at inference time, via an intuitive, scalar tuning knob. Experiments on three different datasets of real-world stereo videos show that our method outperforms both traditional warping-based and recent warping-free baselines and sets a new standard for reliable, controllable stereo video conversion. Please check the project page for the video samples https://elastic3d.github.io.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Visual Programming for Visual Reasoning via Probabilistic Graphs</title>
<link>https://arxiv.org/abs/2512.14257</link>
<guid>https://arxiv.org/abs/2512.14257</guid>
<content:encoded><![CDATA[
arXiv:2512.14257v1 Announce Type: new 
Abstract: Recently, Visual Programming (VP) based on large language models (LLMs) has rapidly developed and demonstrated significant potential in complex Visual Reasoning (VR) tasks. Previous works to enhance VP have primarily focused on improving the quality of LLM-generated visual programs. However, they have neglected to optimize the VP-invoked pre-trained models, which serve as modules for the visual sub-tasks decomposed from the targeted tasks by VP. The difficulty is that there are only final labels of targeted VR tasks rather than labels of sub-tasks. Besides, the non-differentiable nature of VP impedes the direct use of efficient gradient-based optimization methods to leverage final labels for end-to-end learning of the entire VP framework. To overcome these issues, we propose EVPG, a method to Enhance Visual Programming for visual reasoning via Probabilistic Graphs. Specifically, we creatively build a directed probabilistic graph according to the variable dependency relationships during the VP executing process, which reconstructs the non-differentiable VP executing process into a differentiable exact probability inference process on this directed probabilistic graph. As a result, this enables the VP framework to utilize the final labels for efficient, gradient-based optimization in end-to-end supervised learning on targeted VR tasks. Extensive and comprehensive experiments demonstrate the effectiveness and advantages of our EVPG, showing significant performance improvements for VP on three classical complex VR tasks: GQA, NLVRv2, and Open Images.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriverGaze360: OmniDirectional Driver Attention with Object-Level Guidance</title>
<link>https://arxiv.org/abs/2512.14266</link>
<guid>https://arxiv.org/abs/2512.14266</guid>
<content:encoded><![CDATA[
arXiv:2512.14266v1 Announce Type: new 
Abstract: Predicting driver attention is a critical problem for developing explainable autonomous driving systems and understanding driver behavior in mixed human-autonomous vehicle traffic scenarios. Although significant progress has been made through large-scale driver attention datasets and deep learning architectures, existing works are constrained by narrow frontal field-of-view and limited driving diversity. Consequently, they fail to capture the full spatial context of driving environments, especially during lane changes, turns, and interactions involving peripheral objects such as pedestrians or cyclists. In this paper, we introduce DriverGaze360, a large-scale 360$^\circ$ field of view driver attention dataset, containing $\sim$1 million gaze-labeled frames collected from 19 human drivers, enabling comprehensive omnidirectional modeling of driver gaze behavior. Moreover, our panoramic attention prediction approach, DriverGaze360-Net, jointly learns attention maps and attended objects by employing an auxiliary semantic segmentation head. This improves spatial awareness and attention prediction across wide panoramic inputs. Extensive experiments demonstrate that DriverGaze360-Net achieves state-of-the-art attention prediction performance on multiple metrics on panoramic driving images. Dataset and method available at https://av.dfki.de/drivergaze360.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoom-Zero: Reinforced Coarse-to-Fine Video Understanding via Temporal Zoom-in</title>
<link>https://arxiv.org/abs/2512.14273</link>
<guid>https://arxiv.org/abs/2512.14273</guid>
<content:encoded><![CDATA[
arXiv:2512.14273v1 Announce Type: new 
Abstract: Grounded video question answering (GVQA) aims to localize relevant temporal segments in videos and generate accurate answers to a given question; however, large video-language models (LVLMs) exhibit limited temporal awareness. Although existing approaches based on Group Relative Policy Optimization (GRPO) attempt to improve temporal grounding, they still struggle to faithfully ground their answers in the relevant video evidence, leading to temporal mislocalization and hallucinations. In this work, we present Zoom-Zero, a coarse-to-fine framework that first localizes query-relevant segments and then temporally zooms into the most salient frames for finer-grained visual verification. Our method addresses the limits of GRPO for the GVQA task with two key innovations: (i) a zoom-in accuracy reward that validates the fidelity of temporal grounding prediction and facilitates fine-grained visual verification on grounded frames; (ii) token-selective credit assignment, which attributes rewards to the tokens responsible for temporal localization or answer generation, mitigating GRPO's issue in handling multi-faceted reward signals. Our proposed method advances grounded video question answering, improving temporal grounding by 5.2\% on NExT-GQA and 4.6\% on ReXTime, while also enhancing average answer accuracy by 2.4\%. Additionally, the coarse-to-fine zoom-in during inference further benefits long-form video understanding by preserving critical visual details without compromising global context, yielding an average improvement of 6.4\% on long-video benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TUN: Detecting Significant Points in Persistence Diagrams with Deep Learning</title>
<link>https://arxiv.org/abs/2512.14274</link>
<guid>https://arxiv.org/abs/2512.14274</guid>
<content:encoded><![CDATA[
arXiv:2512.14274v1 Announce Type: new 
Abstract: Persistence diagrams (PDs) provide a powerful tool for understanding the topology of the underlying shape of a point cloud. However, identifying which points in PDs encode genuine signals remains challenging. This challenge directly hinders the practical adoption of topological data analysis in many applications, where automated and reliable interpretation of persistence diagrams is essential for downstream decision-making. In this paper, we study automatic significance detection for one-dimensional persistence diagrams. Specifically, we propose Topology Understanding Net (TUN), a multi-modal network that combines enhanced PD descriptors with self-attention, a PointNet-style point cloud encoder, learned fusion, and per-point classification, alongside stable preprocessing and imbalance-aware training. It provides an automated and effective solution for identifying significant points in PDs, which are critical for downstream applications. Experiments show that TUN outperforms classic methods in detecting significant points in PDs, illustrating its effectiveness in real-world applications.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SS4D: Native 4D Generative Model via Structured Spacetime Latents</title>
<link>https://arxiv.org/abs/2512.14284</link>
<guid>https://arxiv.org/abs/2512.14284</guid>
<content:encoded><![CDATA[
arXiv:2512.14284v1 Announce Type: new 
Abstract: We present SS4D, a native 4D generative model that synthesizes dynamic 3D objects directly from monocular video. Unlike prior approaches that construct 4D representations by optimizing over 3D or video generative models, we train a generator directly on 4D data, achieving high fidelity, temporal coherence, and structural consistency. At the core of our method is a compressed set of structured spacetime latents. Specifically, (1) To address the scarcity of 4D training data, we build on a pre-trained single-image-to-3D model, preserving strong spatial consistency. (2) Temporal consistency is enforced by introducing dedicated temporal layers that reason across frames. (3) To support efficient training and inference over long video sequences, we compress the latent sequence along the temporal axis using factorized 4D convolutions and temporal downsampling blocks. In addition, we employ a carefully designed training strategy to enhance robustness against occlusion
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSMamba: Progressive Self-supervised Vision Mamba for Plant Disease Recognition</title>
<link>https://arxiv.org/abs/2512.14309</link>
<guid>https://arxiv.org/abs/2512.14309</guid>
<content:encoded><![CDATA[
arXiv:2512.14309v1 Announce Type: new 
Abstract: Self-supervised Learning (SSL) has become a powerful paradigm for representation learning without manual annotations. However, most existing frameworks focus on global alignment and struggle to capture the hierarchical, multi-scale lesion patterns characteristic of plant disease imagery. To address this gap, we propose PSMamba, a progressive self-supervised framework that integrates the efficient sequence modelling of Vision Mamba (VM) with a dual-student hierarchical distillation strategy. Unlike conventional single teacher-student designs, PSMamba employs a shared global teacher and two specialised students: one processes mid-scale views to capture lesion distributions and vein structures, while the other focuses on local views to capture fine-grained cues such as texture irregularities and early-stage lesions. This multi-granular supervision facilitates the joint learning of contextual and detailed representations, with consistency losses ensuring coherent cross-scale alignment. Experiments on three benchmark datasets show that PSMamba consistently outperforms state-of-the-art SSL methods, delivering superior accuracy and robustness in both domain-shifted and fine-grained scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region</title>
<link>https://arxiv.org/abs/2512.14312</link>
<guid>https://arxiv.org/abs/2512.14312</guid>
<content:encoded><![CDATA[
arXiv:2512.14312v1 Announce Type: new 
Abstract: In regions of the Middle East and North Africa (MENA), there is a high demand for wastewater treatment plants (WWTPs), crucial for sustainable water management. Precise identification of WWTPs from satellite images enables environmental monitoring. Traditional methods like YOLOv8 segmentation require extensive manual labeling. But studies indicate that vision-language models (VLMs) are an efficient alternative to achieving equivalent or superior results through inherent reasoning and annotation. This study presents a structured methodology for VLM comparison, divided into zero-shot and few-shot streams specifically to identify WWTPs. The YOLOv8 was trained on a governmental dataset of 83,566 high-resolution satellite images from Egypt, Saudi Arabia, and UAE: ~85% WWTPs (positives), 15% non-WWTPs (negatives). Evaluated VLMs include LLaMA 3.2 Vision, Qwen 2.5 VL, DeepSeek-VL2, Gemma 3, Gemini, and Pixtral 12B (Mistral), used to identify WWTP components such as circular/rectangular tanks, aeration basins and distinguish confounders via expert prompts producing JSON outputs with confidence and descriptions. The dataset comprises 1,207 validated WWTP locations (198 UAE, 354 KSA, 655 Egypt) and equal non-WWTP sites from field/AI data, as 600mx600m Geo-TIFF images (Zoom 18, EPSG:4326). Zero-shot evaluations on WWTP images showed several VLMs out-performing YOLOv8's true positive rate, with Gemma-3 highest. Results confirm that VLMs, particularly with zero-shot, can replace YOLOv8 for efficient, annotation-free WWTP classification, enabling scalable remote sensing.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Mismatch and Perceptual Degradation: A New Perspective on Image Editing Immunity</title>
<link>https://arxiv.org/abs/2512.14320</link>
<guid>https://arxiv.org/abs/2512.14320</guid>
<content:encoded><![CDATA[
arXiv:2512.14320v1 Announce Type: new 
Abstract: Text-guided image editing via diffusion models, while powerful, raises significant concerns about misuse, motivating efforts to immunize images against unauthorized edits using imperceptible perturbations. Prevailing metrics for evaluating immunization success typically rely on measuring the visual dissimilarity between the output generated from a protected image and a reference output generated from the unprotected original. This approach fundamentally overlooks the core requirement of image immunization, which is to disrupt semantic alignment with attacker intent, regardless of deviation from any specific output. We argue that immunization success should instead be defined by the edited output either semantically mismatching the prompt or suffering substantial perceptual degradations, both of which thwart malicious intent. To operationalize this principle, we propose Synergistic Intermediate Feature Manipulation (SIFM), a method that strategically perturbs intermediate diffusion features through dual synergistic objectives: (1) maximizing feature divergence from the original edit trajectory to disrupt semantic alignment with the expected edit, and (2) minimizing feature norms to induce perceptual degradations. Furthermore, we introduce the Immunization Success Rate (ISR), a novel metric designed to rigorously quantify true immunization efficacy for the first time. ISR quantifies the proportion of edits where immunization induces either semantic failure relative to the prompt or significant perceptual degradations, assessed via Multimodal Large Language Models (MLLMs). Extensive experiments show our SIFM achieves the state-of-the-art performance for safeguarding visual content against malicious diffusion-based manipulation.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Attention Guided Defense Against Malicious Edits</title>
<link>https://arxiv.org/abs/2512.14333</link>
<guid>https://arxiv.org/abs/2512.14333</guid>
<content:encoded><![CDATA[
arXiv:2512.14333v1 Announce Type: new 
Abstract: Recent progress in text-to-image diffusion models has transformed image editing via text prompts, yet this also introduces significant ethical challenges from potential misuse in creating deceptive or harmful content. While current defenses seek to mitigate this risk by embedding imperceptible perturbations, their effectiveness is limited against malicious tampering. To address this issue, we propose a Dual Attention-Guided Noise Perturbation (DANP) immunization method that adds imperceptible perturbations to disrupt the model's semantic understanding and generation process. DANP functions over multiple timesteps to manipulate both cross-attention maps and the noise prediction process, using a dynamic threshold to generate masks that identify text-relevant and irrelevant regions. It then reduces attention in relevant areas while increasing it in irrelevant ones, thereby misguides the edit towards incorrect regions and preserves the intended targets. Additionally, our method maximizes the discrepancy between the injected noise and the model's predicted noise to further interfere with the generation. By targeting both attention and noise prediction mechanisms, DANP exhibits impressive immunity against malicious edits, and extensive experiments confirm that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure</title>
<link>https://arxiv.org/abs/2512.14336</link>
<guid>https://arxiv.org/abs/2512.14336</guid>
<content:encoded><![CDATA[
arXiv:2512.14336v1 Announce Type: new 
Abstract: Scalable Vector Graphics (SVG) are central to modern web design, and the demand to animate them continues to grow as web environments become increasingly dynamic. Yet automating the animation of vector graphics remains challenging for vision-language models (VLMs) despite recent progress in code generation and motion planning. VLMs routinely mis-handle SVGs, since visually coherent parts are often fragmented into low-level shapes that offer little guidance of which elements should move together. In this paper, we introduce a framework that recovers the semantic structure required for reliable SVG animation and reveals the missing layer that current VLM systems overlook. This is achieved through a statistical aggregation of multiple weak part predictions, allowing the system to stably infer semantics from noisy predictions. By reorganizing SVGs into semantic groups, our approach enables VLMs to produce animations with far greater coherence. Our experiments demonstrate substantial gains over existing approaches, suggesting that semantic recovery is the key step that unlocks robust SVG animation and supports more interpretable interactions between VLMs and vector graphics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Transferable Defense Against Malicious Image Edits</title>
<link>https://arxiv.org/abs/2512.14341</link>
<guid>https://arxiv.org/abs/2512.14341</guid>
<content:encoded><![CDATA[
arXiv:2512.14341v1 Announce Type: new 
Abstract: Recent approaches employing imperceptible perturbations in input images have demonstrated promising potential to counter malicious manipulations in diffusion-based image editing systems. However, existing methods suffer from limited transferability in cross-model evaluations. To address this, we propose Transferable Defense Against Malicious Image Edits (TDAE), a novel bimodal framework that enhances image immunity against malicious edits through coordinated image-text optimization. Specifically, at the visual defense level, we introduce FlatGrad Defense Mechanism (FDM), which incorporates gradient regularization into the adversarial objective. By explicitly steering the perturbations toward flat minima, FDM amplifies immune robustness against unseen editing models. For textual enhancement protection, we propose an adversarial optimization paradigm named Dynamic Prompt Defense (DPD), which periodically refines text embeddings to align the editing outcomes of immunized images with those of the original images, then updates the images under optimized embeddings. Through iterative adversarial updates to diverse embeddings, DPD enforces the generation of immunized images that seek a broader set of immunity-enhancing features, thereby achieving cross-model transferability. Extensive experimental results demonstrate that our TDAE achieves state-of-the-art performance in mitigating malicious edits under both intra- and cross-model evaluations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HGS: Hybrid Gaussian Splatting with Static-Dynamic Decomposition for Compact Dynamic View Synthesis</title>
<link>https://arxiv.org/abs/2512.14352</link>
<guid>https://arxiv.org/abs/2512.14352</guid>
<content:encoded><![CDATA[
arXiv:2512.14352v1 Announce Type: new 
Abstract: Dynamic novel view synthesis (NVS) is essential for creating immersive experiences. Existing approaches have advanced dynamic NVS by introducing 3D Gaussian Splatting (3DGS) with implicit deformation fields or indiscriminately assigned time-varying parameters, surpassing NeRF-based methods. However, due to excessive model complexity and parameter redundancy, they incur large model sizes and slow rendering speeds, making them inefficient for real-time applications, particularly on resource-constrained devices. To obtain a more efficient model with fewer redundant parameters, in this paper, we propose Hybrid Gaussian Splatting (HGS), a compact and efficient framework explicitly designed to disentangle static and dynamic regions of a scene within a unified representation. The core innovation of HGS lies in our Static-Dynamic Decomposition (SDD) strategy, which leverages Radial Basis Function (RBF) modeling for Gaussian primitives. Specifically, for dynamic regions, we employ time-dependent RBFs to effectively capture temporal variations and handle abrupt scene changes, while for static regions, we reduce redundancy by sharing temporally invariant parameters. Additionally, we introduce a two-stage training strategy tailored for explicit models to enhance temporal coherence at static-dynamic boundaries. Experimental results demonstrate that our method reduces model size by up to 98% and achieves real-time rendering at up to 125 FPS at 4K resolution on a single RTX 3090 GPU. It further sustains 160 FPS at 1352 * 1014 on an RTX 3050 and has been integrated into the VR system. Moreover, HGS achieves comparable rendering quality to state-of-the-art methods while providing significantly improved visual fidelity for high-frequency details and abrupt scene changes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability for Vision Models via Shapley Value Optimization</title>
<link>https://arxiv.org/abs/2512.14354</link>
<guid>https://arxiv.org/abs/2512.14354</guid>
<content:encoded><![CDATA[
arXiv:2512.14354v1 Announce Type: new 
Abstract: Deep neural networks have demonstrated remarkable performance across various domains, yet their decision-making processes remain opaque. Although many explanation methods are dedicated to bringing the obscurity of DNNs to light, they exhibit significant limitations: post-hoc explanation methods often struggle to faithfully reflect model behaviors, while self-explaining neural networks sacrifice performance and compatibility due to their specialized architectural designs. To address these challenges, we propose a novel self-explaining framework that integrates Shapley value estimation as an auxiliary task during training, which achieves two key advancements: 1) a fair allocation of the model prediction scores to image patches, ensuring explanations inherently align with the model's decision logic, and 2) enhanced interpretability with minor structural modifications, preserving model performance and compatibility. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art interpretability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mimicking Human Visual Development for Learning Robust Image Representations</title>
<link>https://arxiv.org/abs/2512.14360</link>
<guid>https://arxiv.org/abs/2512.14360</guid>
<content:encoded><![CDATA[
arXiv:2512.14360v1 Announce Type: new 
Abstract: The human visual system is remarkably adept at adapting to changes in the input distribution; a capability modern convolutional neural networks (CNNs) still struggle to match. Drawing inspiration from the developmental trajectory of human vision, we propose a progressive blurring curriculum to improve the generalization and robustness of CNNs. Human infants are born with poor visual acuity, gradually refining their ability to perceive fine details. Mimicking this process, we begin training CNNs on highly blurred images during the initial epochs and progressively reduce the blur as training advances. This approach encourages the network to prioritize global structures over high-frequency artifacts, improving robustness against distribution shifts and noisy inputs. Challenging prior claims that blurring in the initial training epochs imposes a stimulus deficit and irreversibly harms model performance, we reveal that early-stage blurring enhances generalization with minimal impact on in-domain accuracy. Our experiments demonstrate that the proposed curriculum reduces mean corruption error (mCE) by up to 8.30% on CIFAR-10-C and 4.43% on ImageNet-100-C datasets, compared to standard training without blurring. Unlike static blur-based augmentation, which applies blurred images randomly throughout training, our method follows a structured progression, yielding consistent gains across various datasets. Furthermore, our approach complements other augmentation techniques, such as CutMix and MixUp, and enhances both natural and adversarial robustness against common attack methods. Code is available at https://github.com/rajankita/Visual_Acuity_Curriculum.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Semantic Transformer for 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.14364</link>
<guid>https://arxiv.org/abs/2512.14364</guid>
<content:encoded><![CDATA[
arXiv:2512.14364v1 Announce Type: new 
Abstract: Holistic 3D scene understanding involves capturing and parsing unstructured 3D environments. Due to the inherent complexity of the real world, existing models have predominantly been developed and limited to be task-specific. We introduce UNITE, a Unified Semantic Transformer for 3D scene understanding, a novel feed-forward neural network that unifies a diverse set of 3D semantic tasks within a single model. Our model operates on unseen scenes in a fully end-to-end manner and only takes a few seconds to infer the full 3D semantic geometry. Our approach is capable of directly predicting multiple semantic attributes, including 3D scene segmentation, instance embeddings, open-vocabulary features, as well as affordance and articulations, solely from RGB images. The method is trained using a combination of 2D distillation, heavily relying on self-supervision and leverages novel multi-view losses designed to ensure 3D view consistency. We demonstrate that UNITE achieves state-of-the-art performance on several different semantic tasks and even outperforms task-specific models, in many cases, surpassing methods that operate on ground truth 3D geometry. See the project website at unite-page.github.io
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Rank for High-Fidelity Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2512.14366</link>
<guid>https://arxiv.org/abs/2512.14366</guid>
<content:encoded><![CDATA[
arXiv:2512.14366v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) based on vanilla Multi-Layer Perceptrons (MLPs) are widely believed to be incapable of representing high-frequency content. This has directed research efforts towards architectural interventions, such as coordinate embeddings or specialized activation functions, to represent high-frequency signals. In this paper, we challenge the notion that the low-frequency bias of vanilla MLPs is an intrinsic, architectural limitation to learn high-frequency content, but instead a symptom of stable rank degradation during training. We empirically demonstrate that regulating the network's rank during training substantially improves the fidelity of the learned signal, rendering even simple MLP architectures expressive. Extensive experiments show that using optimizers like Muon, with high-rank, near-orthogonal updates, consistently enhances INR architectures even beyond simple ReLU MLPs. These substantial improvements hold across a diverse range of domains, including natural and medical images, and novel view synthesis, with up to 9 dB PSNR improvements over the previous state-of-the-art. Our project page, which includes code and experimental results, is available at: (https://muon-inrs.github.io).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EcoScapes: LLM-Powered Advice for Crafting Sustainable Cities</title>
<link>https://arxiv.org/abs/2512.14373</link>
<guid>https://arxiv.org/abs/2512.14373</guid>
<content:encoded><![CDATA[
arXiv:2512.14373v1 Announce Type: new 
Abstract: Climate adaptation is vital for the sustainability and sometimes the mere survival of our urban areas. However, small cities often struggle with limited personnel resources and integrating vast amounts of data from multiple sources for a comprehensive analysis. To overcome these challenges, this paper proposes a multi-layered system combining specialized LLMs, satellite imagery analysis and a knowledge base to aid in developing effective climate adaptation strategies. The corresponding code can be found at https://github.com/Photon-GitHub/EcoScapes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Broadening View Synthesis of Dynamic Scenes from Constrained Monocular Videos</title>
<link>https://arxiv.org/abs/2512.14406</link>
<guid>https://arxiv.org/abs/2512.14406</guid>
<content:encoded><![CDATA[
arXiv:2512.14406v1 Announce Type: new 
Abstract: In dynamic Neural Radiance Fields (NeRF) systems, state-of-the-art novel view synthesis methods often fail under significant viewpoint deviations, producing unstable and unrealistic renderings. To address this, we introduce Expanded Dynamic NeRF (ExpanDyNeRF), a monocular NeRF framework that leverages Gaussian splatting priors and a pseudo-ground-truth generation strategy to enable realistic synthesis under large-angle rotations. ExpanDyNeRF optimizes density and color features to improve scene reconstruction from challenging perspectives. We also present the Synthetic Dynamic Multiview (SynDM) dataset, the first synthetic multiview dataset for dynamic scenes with explicit side-view supervision-created using a custom GTA V-based rendering pipeline. Quantitative and qualitative results on SynDM and real-world datasets demonstrate that ExpanDyNeRF significantly outperforms existing dynamic NeRF methods in rendering fidelity under extreme viewpoint shifts. Further details are provided in the supplementary materials.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning</title>
<link>https://arxiv.org/abs/2512.14420</link>
<guid>https://arxiv.org/abs/2512.14420</guid>
<content:encoded><![CDATA[
arXiv:2512.14420v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) have shown impressive performance across a broad range of multimodal tasks. However, robust image caption evaluation using LVLMs remains challenging, particularly under domain-shift scenarios. To address this issue, we introduce the Distribution-Aware Score Decoder (DISCODE), a novel finetuning-free method that generates robust evaluation scores better aligned with human judgments across diverse domains. The core idea behind DISCODE lies in its test-time adaptive evaluation approach, which introduces the Adaptive Test-Time (ATT) loss, leveraging a Gaussian prior distribution to improve robustness in evaluation score estimation. This loss is efficiently minimized at test time using an analytical solution that we derive. Furthermore, we introduce the Multi-domain Caption Evaluation (MCEval) benchmark, a new image captioning evaluation benchmark covering six distinct domains, designed to assess the robustness of evaluation metrics. In our experiments, we demonstrate that DISCODE achieves state-of-the-art performance as a reference-free evaluation metric across MCEval and four representative existing benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LCMem: A Universal Model for Robust Image Memorization Detection</title>
<link>https://arxiv.org/abs/2512.14421</link>
<guid>https://arxiv.org/abs/2512.14421</guid>
<content:encoded><![CDATA[
arXiv:2512.14421v1 Announce Type: new 
Abstract: Recent advances in generative image modeling have achieved visual realism sufficient to deceive human experts, yet their potential for privacy preserving data sharing remains insufficiently understood. A central obstacle is the absence of reliable memorization detection mechanisms, limited quantitative evaluation, and poor generalization of existing privacy auditing methods across domains. To address this, we propose to view memorization detection as a unified problem at the intersection of re-identification and copy detection, whose complementary goals cover both identity consistency and augmentation-robust duplication, and introduce Latent Contrastive Memorization Network (LCMem), a cross-domain model evaluated jointly on both tasks. LCMem achieves this through a two-stage training strategy that first learns identity consistency before incorporating augmentation-robust copy detection. Across six benchmark datasets, LCMem achieves improvements of up to 16 percentage points on re-identification and 30 percentage points on copy detection, enabling substantially more reliable memorization detection at scale. Our results show that existing privacy filters provide limited performance and robustness, highlighting the need for stronger protection mechanisms. We show that LCMem sets a new standard for cross-domain privacy auditing, offering reliable and scalable memorization detection. Code and model is publicly available at https://github.com/MischaD/LCMem.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Devil is in Attention Sharing: Improving Complex Non-rigid Image Editing Faithfulness via Attention Synergy</title>
<link>https://arxiv.org/abs/2512.14423</link>
<guid>https://arxiv.org/abs/2512.14423</guid>
<content:encoded><![CDATA[
arXiv:2512.14423v1 Announce Type: new 
Abstract: Training-free image editing with large diffusion models has become practical, yet faithfully performing complex non-rigid edits (e.g., pose or shape changes) remains highly challenging. We identify a key underlying cause: attention collapse in existing attention sharing mechanisms, where either positional embeddings or semantic features dominate visual content retrieval, leading to over-editing or under-editing.To address this issue, we introduce SynPS, a method that Synergistically leverages Positional embeddings and Semantic information for faithful non-rigid image editing. We first propose an editing measurement that quantifies the required editing magnitude at each denoising step. Based on this measurement, we design an attention synergy pipeline that dynamically modulates the influence of positional embeddings, enabling SynPS to balance semantic modifications and fidelity preservation.By adaptively integrating positional and semantic cues, SynPS effectively avoids both over- and under-editing. Extensive experiments on public and newly curated benchmarks demonstrate the superior performance and faithfulness of our approach.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score-Based Turbo Message Passing for Plug-and-Play Compressive Imaging</title>
<link>https://arxiv.org/abs/2512.14435</link>
<guid>https://arxiv.org/abs/2512.14435</guid>
<content:encoded><![CDATA[
arXiv:2512.14435v1 Announce Type: new 
Abstract: Message-passing algorithms have been adapted for compressive imaging by incorporating various off-the-shelf image denoisers. However, these denoisers rely largely on generic or hand-crafted priors and often fall short in accurately capturing the complex statistical structure of natural images. As a result, traditional plug-and-play (PnP) methods often lead to suboptimal reconstruction, especially in highly underdetermined regimes. Recently, score-based generative models have emerged as a powerful framework for accurately characterizing sophisticated image distribution. Yet, their direct use for posterior sampling typically incurs prohibitive computational complexity. In this paper, by exploiting the close connection between score-based generative modeling and empirical Bayes denoising, we devise a message-passing framework that integrates a score-based minimum mean-squared error (MMSE) denoiser for compressive image recovery. The resulting algorithm, named score-based turbo message passing (STMP), combines the fast convergence of message passing with the expressive power of score-based generative priors. For practical systems with quantized measurements, we further propose quantized STMP (Q-STMP), which augments STMP with a component-wise MMSE dequantization module. We demonstrate that the asymptotic performance of STMP and Q-STMP can be accurately predicted by a set of state-evolution (SE) equations. Experiments on the FFHQ dataset demonstrate that STMP strikes a significantly better performance-complexity tradeoff compared with competing baselines, and that Q-STMP remains robust even under 1-bit quantization. Remarkably, both STMP and Q-STMP typically converge within 10 iterations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2D: Sparse-To-Dense Keymask Distillation for Unsupervised Video Instance Segmentation</title>
<link>https://arxiv.org/abs/2512.14440</link>
<guid>https://arxiv.org/abs/2512.14440</guid>
<content:encoded><![CDATA[
arXiv:2512.14440v1 Announce Type: new 
Abstract: In recent years, the state-of-the-art in unsupervised video instance segmentation has heavily relied on synthetic video data, generated from object-centric image datasets such as ImageNet. However, video synthesis by artificially shifting and scaling image instance masks fails to accurately model realistic motion in videos, such as perspective changes, movement by parts of one or multiple instances, or camera motion. To tackle this issue, we propose an unsupervised video instance segmentation model trained exclusively on real video data. We start from unsupervised instance segmentation masks on individual video frames. However, these single-frame segmentations exhibit temporal noise and their quality varies through the video. Therefore, we establish temporal coherence by identifying high-quality keymasks in the video by leveraging deep motion priors. The sparse keymask pseudo-annotations are then used to train a segmentation model for implicit mask propagation, for which we propose a Sparse-To-Dense Distillation approach aided by a Temporal DropLoss. After training the final model on the resulting dense labelset, our approach outperforms the current state-of-the-art across various benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning</title>
<link>https://arxiv.org/abs/2512.14442</link>
<guid>https://arxiv.org/abs/2512.14442</guid>
<content:encoded><![CDATA[
arXiv:2512.14442v1 Announce Type: new 
Abstract: Affordance prediction, which identifies interaction regions on objects based on language instructions, is critical for embodied AI. Prevailing end-to-end models couple high-level reasoning and low-level grounding into a single monolithic pipeline and rely on training over annotated datasets, which leads to poor generalization on novel objects and unseen environments. In this paper, we move beyond this paradigm by proposing A4-Agent, a training-free agentic framework that decouples affordance prediction into a three-stage pipeline. Our framework coordinates specialized foundation models at test time: (1) a $\textbf{Dreamer}$ that employs generative models to visualize $\textit{how}$ an interaction would look; (2) a $\textbf{Thinker}$ that utilizes large vision-language models to decide $\textit{what}$ object part to interact with; and (3) a $\textbf{Spotter}$ that orchestrates vision foundation models to precisely locate $\textit{where}$ the interaction area is. By leveraging the complementary strengths of pre-trained models without any task-specific fine-tuning, our zero-shot framework significantly outperforms state-of-the-art supervised methods across multiple benchmarks and demonstrates robust generalization to real-world settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TACK Tunnel Data (TTD): A Benchmark Dataset for Deep Learning-Based Defect Detection in Tunnels</title>
<link>https://arxiv.org/abs/2512.14477</link>
<guid>https://arxiv.org/abs/2512.14477</guid>
<content:encoded><![CDATA[
arXiv:2512.14477v1 Announce Type: new 
Abstract: Tunnels are essential elements of transportation infrastructure, but are increasingly affected by ageing and deterioration mechanisms such as cracking. Regular inspections are required to ensure their safety, yet traditional manual procedures are time-consuming, subjective, and costly. Recent advances in mobile mapping systems and Deep Learning (DL) enable automated visual inspections. However, their effectiveness is limited by the scarcity of tunnel datasets. This paper introduces a new publicly available dataset containing annotated images of three different tunnel linings, capturing typical defects: cracks, leaching, and water infiltration. The dataset is designed to support supervised, semi-supervised, and unsupervised DL methods for defect detection and segmentation. Its diversity in texture and construction techniques also enables investigation of model generalization and transferability across tunnel types. By addressing the critical lack of domain-specific data, this dataset contributes to advancing automated tunnel inspection and promoting safer, more efficient infrastructure maintenance strategies.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperCLIP: CLIP with Simple Classification Supervision</title>
<link>https://arxiv.org/abs/2512.14480</link>
<guid>https://arxiv.org/abs/2512.14480</guid>
<content:encoded><![CDATA[
arXiv:2512.14480v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining (CLIP) achieves strong generalization in vision-language tasks by aligning images and texts in a shared embedding space. However, recent findings show that CLIP-like models still underutilize fine-grained semantic signals in text, and this issue becomes even more pronounced when dealing with long and detailed captions. This stems from CLIP's training objective, which optimizes only global image-text similarity and overlooks token-level supervision - limiting its ability to achieve fine-grained visual-text alignment. To address this, we propose SuperCLIP, a simple yet effective framework that augments contrastive learning with classification-based supervision. By adding only a lightweight linear layer to the vision encoder, SuperCLIP leverages token-level cues to enhance visual-textual alignment - with just a 0.077% increase in total FLOPs, and no need for additional annotated data. Experiments show that SuperCLIP consistently improves zero-shot classification, image-text retrieval, and purely visual tasks. These gains hold regardless of whether the model is trained on original web data or rich re-captioned data, demonstrating SuperCLIP's ability to recover textual supervision in both cases. Furthermore, SuperCLIP alleviates CLIP's small-batch performance drop through classification-based supervision that avoids reliance on large batch sizes. Code and models will be made open source.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignIT: A Comprehensive Dataset and Multimodal Analysis for Italian Sign Language Recognition</title>
<link>https://arxiv.org/abs/2512.14489</link>
<guid>https://arxiv.org/abs/2512.14489</guid>
<content:encoded><![CDATA[
arXiv:2512.14489v1 Announce Type: new 
Abstract: In this work we present SignIT, a new dataset to study the task of Italian Sign Language (LIS) recognition. The dataset is composed of 644 videos covering 3.33 hours. We manually annotated videos considering a taxonomy of 94 distinct sign classes belonging to 5 macro-categories: Animals, Food, Colors, Emotions and Family. We also extracted 2D keypoints related to the hands, face and body of the users. With the dataset, we propose a benchmark for the sign recognition task, adopting several state-of-the-art models showing how temporal information, 2D keypoints and RGB frames can be influence the performance of these models. Results show the limitations of these models on this challenging LIS dataset. We release data and annotations at the following link: https://fpv-iplab.github.io/SignIT/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native Intelligence Emerges from Large-Scale Clinical Practice: A Retinal Foundation Model with Deployment Efficiency</title>
<link>https://arxiv.org/abs/2512.14499</link>
<guid>https://arxiv.org/abs/2512.14499</guid>
<content:encoded><![CDATA[
arXiv:2512.14499v1 Announce Type: new 
Abstract: Current retinal foundation models remain constrained by curated research datasets that lack authentic clinical context, and require extensive task-specific optimization for each application, limiting their deployment efficiency in low-resource settings. Here, we show that these barriers can be overcome by building clinical native intelligence directly from real-world medical practice. Our key insight is that large-scale telemedicine programs, where expert centers provide remote consultations across distributed facilities, represent a natural reservoir for learning clinical image interpretation. We present ReVision, a retinal foundation model that learns from the natural alignment between 485,980 color fundus photographs and their corresponding diagnostic reports, accumulated through a decade-long telemedicine program spanning 162 medical institutions across China. Through extensive evaluation across 27 ophthalmic benchmarks, we demonstrate that ReVison enables deployment efficiency with minimal local resources. Without any task-specific training, ReVision achieves zero-shot disease detection with an average AUROC of 0.946 across 12 public benchmarks and 0.952 on 3 independent clinical cohorts. When minimal adaptation is feasible, ReVision matches extensively fine-tuned alternatives while requiring orders of magnitude fewer trainable parameters and labeled examples. The learned representations also transfer effectively to new clinical sites, imaging domains, imaging modalities, and systemic health prediction tasks. In a prospective reader study with 33 ophthalmologists, ReVision's zero-shot assistance improved diagnostic accuracy by 14.8% across all experience levels. These results demonstrate that clinical native intelligence can be directly extracted from clinical archives without any further annotation to build medical AI systems suited to various low-resource settings.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DASP: Self-supervised Nighttime Monocular Depth Estimation with Domain Adaptation of Spatiotemporal Priors</title>
<link>https://arxiv.org/abs/2512.14536</link>
<guid>https://arxiv.org/abs/2512.14536</guid>
<content:encoded><![CDATA[
arXiv:2512.14536v1 Announce Type: new 
Abstract: Self-supervised monocular depth estimation has achieved notable success under daytime conditions. However, its performance deteriorates markedly at night due to low visibility and varying illumination, e.g., insufficient light causes textureless areas, and moving objects bring blurry regions. To this end, we propose a self-supervised framework named DASP that leverages spatiotemporal priors for nighttime depth estimation. Specifically, DASP consists of an adversarial branch for extracting spatiotemporal priors and a self-supervised branch for learning. In the adversarial branch, we first design an adversarial network where the discriminator is composed of four devised spatiotemporal priors learning blocks (SPLB) to exploit the daytime priors. In particular, the SPLB contains a spatial-based temporal learning module (STLM) that uses orthogonal differencing to extract motion-related variations along the time axis and an axial spatial learning module (ASLM) that adopts local asymmetric convolutions with global axial attention to capture the multiscale structural information. By combining STLM and ASLM, our model can acquire sufficient spatiotemporal features to restore textureless areas and estimate the blurry regions caused by dynamic objects. In the self-supervised branch, we propose a 3D consistency projection loss to bilaterally project the target frame and source frame into a shared 3D space, and calculate the 3D discrepancy between the two projected frames as a loss to optimize the 3D structural consistency and daytime priors. Extensive experiments on the Oxford RobotCar and nuScenes datasets demonstrate that our approach achieves state-of-the-art performance for nighttime depth estimation. Ablation studies further validate the effectiveness of each component.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPRMIL: Context-Aware Patch Representations for Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2512.14540</link>
<guid>https://arxiv.org/abs/2512.14540</guid>
<content:encoded><![CDATA[
arXiv:2512.14540v1 Announce Type: new 
Abstract: In computational pathology, weak supervision has become the standard for deep learning due to the gigapixel scale of WSIs and the scarcity of pixel-level annotations, with Multiple Instance Learning (MIL) established as the principal framework for slide-level model training. In this paper, we introduce a novel setting for MIL methods, inspired by proceedings in Neural Partial Differential Equation (PDE) Solvers. Instead of relying on complex attention-based aggregation, we propose an efficient, aggregator-agnostic framework that removes the complexity of correlation learning from the MIL aggregator. CAPRMIL produces rich context-aware patch embeddings that promote effective correlation learning on downstream tasks. By projecting patch features -- extracted using a frozen patch encoder -- into a small set of global context/morphology-aware tokens and utilizing multi-head self-attention, CAPRMIL injects global context with linear computational complexity with respect to the bag size. Paired with a simple Mean MIL aggregator, CAPRMIL matches state-of-the-art slide-level performance across multiple public pathology benchmarks, while reducing the total number of trainable parameters by 48%-92.8% versus SOTA MILs, lowering FLOPs during inference by 52%-99%, and ranking among the best models on GPU memory efficiency and training time. Our results indicate that learning rich, context-aware instance representations before aggregation is an effective and scalable alternative to complex pooling for whole-slide analysis. Our code is available at https://github.com/mandlos/CAPRMIL
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion</title>
<link>https://arxiv.org/abs/2512.14542</link>
<guid>https://arxiv.org/abs/2512.14542</guid>
<content:encoded><![CDATA[
arXiv:2512.14542v1 Announce Type: new 
Abstract: Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAT: Task-Adaptive Transformer for All-in-One Medical Image Restoration</title>
<link>https://arxiv.org/abs/2512.14550</link>
<guid>https://arxiv.org/abs/2512.14550</guid>
<content:encoded><![CDATA[
arXiv:2512.14550v1 Announce Type: new 
Abstract: Medical image restoration (MedIR) aims to recover high-quality medical images from their low-quality counterparts. Recent advancements in MedIR have focused on All-in-One models capable of simultaneously addressing multiple different MedIR tasks. However, due to significant differences in both modality and degradation types, using a shared model for these diverse tasks requires careful consideration of two critical inter-task relationships: task interference, which occurs when conflicting gradient update directions arise across tasks on the same parameter, and task imbalance, which refers to uneven optimization caused by varying learning difficulties inherent to each task. To address these challenges, we propose a task-adaptive Transformer (TAT), a novel framework that dynamically adapts to different tasks through two key innovations. First, a task-adaptive weight generation strategy is introduced to mitigate task interference by generating task-specific weight parameters for each task, thereby eliminating potential gradient conflicts on shared weight parameters. Second, a task-adaptive loss balancing strategy is introduced to dynamically adjust loss weights based on task-specific learning difficulties, preventing task domination or undertraining. Extensive experiments demonstrate that our proposed TAT achieves state-of-the-art performance in three MedIR tasks--PET synthesis, CT denoising, and MRI super-resolution--both in task-specific and All-in-One settings. Code is available at https://github.com/Yaziwel/TAT.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLNet: Cross-View Correspondence Makes a Stronger Geo-Localizationer</title>
<link>https://arxiv.org/abs/2512.14560</link>
<guid>https://arxiv.org/abs/2512.14560</guid>
<content:encoded><![CDATA[
arXiv:2512.14560v1 Announce Type: new 
Abstract: Image retrieval-based cross-view geo-localization (IRCVGL) aims to match images captured from significantly different viewpoints, such as satellite and street-level images. Existing methods predominantly rely on learning robust global representations or implicit feature alignment, which often fail to model explicit spatial correspondences crucial for accurate localization. In this work, we propose a novel correspondence-aware feature refinement framework, termed CLNet, that explicitly bridges the semantic and geometric gaps between different views. CLNet decomposes the view alignment process into three learnable and complementary modules: a Neural Correspondence Map (NCM) that spatially aligns cross-view features via latent correspondence fields; a Nonlinear Embedding Converter (NEC) that remaps features across perspectives using an MLP-based transformation; and a Global Feature Recalibration (GFR) module that reweights informative feature channels guided by learned spatial cues. The proposed CLNet can jointly capture both high-level semantics and fine-grained alignments. Extensive experiments on four public benchmarks, CVUSA, CVACT, VIGOR, and University-1652, demonstrate that our proposed CLNet achieves state-of-the-art performance while offering better interpretability and generalizability.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FoodLogAthl-218: Constructing a Real-World Food Image Dataset Using Dietary Management Applications</title>
<link>https://arxiv.org/abs/2512.14574</link>
<guid>https://arxiv.org/abs/2512.14574</guid>
<content:encoded><![CDATA[
arXiv:2512.14574v1 Announce Type: new 
Abstract: Food image classification models are crucial for dietary management applications because they reduce the burden of manual meal logging. However, most publicly available datasets for training such models rely on web-crawled images, which often differ from users' real-world meal photos. In this work, we present FoodLogAthl-218, a food image dataset constructed from real-world meal records collected through the dietary management application FoodLog Athl. The dataset contains 6,925 images across 218 food categories, with a total of 14,349 bounding boxes. Rich metadata, including meal date and time, anonymized user IDs, and meal-level context, accompany each image. Unlike conventional datasets-where a predefined class set guides web-based image collection-our data begins with user-submitted photos, and labels are applied afterward. This yields greater intra-class diversity, a natural frequency distribution of meal types, and casual, unfiltered images intended for personal use rather than public sharing. In addition to (1) a standard classification benchmark, we introduce two FoodLog-specific tasks: (2) an incremental fine-tuning protocol that follows the temporal stream of users' logs, and (3) a context-aware classification task where each image contains multiple dishes, and the model must classify each dish by leveraging the overall meal context. We evaluate these tasks using large multimodal models (LMMs). The dataset is publicly available at https://huggingface.co/datasets/FoodLog/FoodLogAthl-218.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-driven Knowledge Enhancement for Multimodal Cancer Survival Prediction</title>
<link>https://arxiv.org/abs/2512.14594</link>
<guid>https://arxiv.org/abs/2512.14594</guid>
<content:encoded><![CDATA[
arXiv:2512.14594v1 Announce Type: new 
Abstract: Current multimodal survival prediction methods typically rely on pathology images (WSIs) and genomic data, both of which are high-dimensional and redundant, making it difficult to extract discriminative features from them and align different modalities. Moreover, using a simple survival follow-up label is insufficient to supervise such a complex task. To address these challenges, we propose KEMM, an LLM-driven Knowledge-Enhanced Multimodal Model for cancer survival prediction, which integrates expert reports and prognostic background knowledge. 1) Expert reports, provided by pathologists on a case-by-case basis and refined by large language model (LLM), offer succinct and clinically focused diagnostic statements. This information may typically suggest different survival outcomes. 2) Prognostic background knowledge (PBK), generated concisely by LLM, provides valuable prognostic background knowledge on different cancer types, which also enhances survival prediction. To leverage these knowledge, we introduce the knowledge-enhanced cross-modal (KECM) attention module. KECM can effectively guide the network to focus on discriminative and survival-relevant features from highly redundant modalities. Extensive experiments on five datasets demonstrate that KEMM achieves state-of-the-art performance. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TUMTraf EMOT: Event-Based Multi-Object Tracking Dataset and Baseline for Traffic Scenarios</title>
<link>https://arxiv.org/abs/2512.14595</link>
<guid>https://arxiv.org/abs/2512.14595</guid>
<content:encoded><![CDATA[
arXiv:2512.14595v1 Announce Type: new 
Abstract: In Intelligent Transportation Systems (ITS), multi-object tracking is primarily based on frame-based cameras. However, these cameras tend to perform poorly under dim lighting and high-speed motion conditions. Event cameras, characterized by low latency, high dynamic range and high temporal resolution, have considerable potential to mitigate these issues. Compared to frame-based vision, there are far fewer studies on event-based vision. To address this research gap, we introduce an initial pilot dataset tailored for event-based ITS, covering vehicle and pedestrian detection and tracking. We establish a tracking-by-detection benchmark with a specialized feature extractor based on this dataset, achieving excellent performance.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FakeRadar: Probing Forgery Outliers to Detect Unknown Deepfake Videos</title>
<link>https://arxiv.org/abs/2512.14601</link>
<guid>https://arxiv.org/abs/2512.14601</guid>
<content:encoded><![CDATA[
arXiv:2512.14601v1 Announce Type: new 
Abstract: In this paper, we propose FakeRadar, a novel deepfake video detection framework designed to address the challenges of cross-domain generalization in real-world scenarios. Existing detection methods typically rely on manipulation-specific cues, performing well on known forgery types but exhibiting severe limitations against emerging manipulation techniques. This poor generalization stems from their inability to adapt effectively to unseen forgery patterns. To overcome this, we leverage large-scale pretrained models (e.g. CLIP) to proactively probe the feature space, explicitly highlighting distributional gaps between real videos, known forgeries, and unseen manipulations. Specifically, FakeRadar introduces Forgery Outlier Probing, which employs dynamic subcluster modeling and cluster-conditional outlier generation to synthesize outlier samples near boundaries of estimated subclusters, simulating novel forgery artifacts beyond known manipulation types. Additionally, we design Outlier-Guided Tri-Training, which optimizes the detector to distinguish real, fake, and outlier samples using proposed outlier-driven contrastive learning and outlier-conditioned cross-entropy losses. Experiments show that FakeRadar outperforms existing methods across various benchmark datasets for deepfake video detection, particularly in cross-domain evaluations, by handling the variety of emerging manipulation techniques.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldPlay: Towards Long-Term Geometric Consistency for Real-Time Interactive World Modeling</title>
<link>https://arxiv.org/abs/2512.14614</link>
<guid>https://arxiv.org/abs/2512.14614</guid>
<content:encoded><![CDATA[
arXiv:2512.14614v1 Announce Type: new 
Abstract: This paper presents WorldPlay, a streaming video diffusion model that enables real-time, interactive world modeling with long-term geometric consistency, resolving the trade-off between speed and memory that limits current methods. WorldPlay draws power from three key innovations. 1) We use a Dual Action Representation to enable robust action control in response to the user's keyboard and mouse inputs. 2) To enforce long-term consistency, our Reconstituted Context Memory dynamically rebuilds context from past frames and uses temporal reframing to keep geometrically important but long-past frames accessible, effectively alleviating memory attenuation. 3) We also propose Context Forcing, a novel distillation method designed for memory-aware model. Aligning memory context between the teacher and student preserves the student's capacity to use long-range information, enabling real-time speeds while preventing error drift. Taken together, WorldPlay generates long-horizon streaming 720p video at 24 FPS with superior consistency, comparing favorably with existing techniques and showing strong generalization across diverse scenes. Project page and online demo can be found: https://3d-models.hunyuan.tencent.com/world/ and https://3d.hunyuan.tencent.com/sceneTo3D.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distill Video Datasets into Images</title>
<link>https://arxiv.org/abs/2512.14621</link>
<guid>https://arxiv.org/abs/2512.14621</guid>
<content:encoded><![CDATA[
arXiv:2512.14621v1 Announce Type: new 
Abstract: Dataset distillation aims to synthesize compact yet informative datasets that allow models trained on them to achieve performance comparable to training on the full dataset. While this approach has shown promising results for image data, extending dataset distillation methods to video data has proven challenging and often leads to suboptimal performance. In this work, we first identify the core challenge in video set distillation as the substantial increase in learnable parameters introduced by the temporal dimension of video, which complicates optimization and hinders convergence. To address this issue, we observe that a single frame is often sufficient to capture the discriminative semantics of a video. Leveraging this insight, we propose Single-Frame Video set Distillation (SFVD), a framework that distills videos into highly informative frames for each class. Using differentiable interpolation, these frames are transformed into video sequences and matched with the original dataset, while updates are restricted to the frames themselves for improved optimization efficiency. To further incorporate temporal information, the distilled frames are combined with sampled real videos from real videos during the matching process through a channel reshaping layer. Extensive experiments on multiple benchmarks demonstrate that SFVD substantially outperforms prior methods, achieving improvements of up to 5.3% on MiniUCF, thereby offering a more effective solution.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMD-HookNet++: Evolution of AMD-HookNet with Hybrid CNN-Transformer Feature Enhancement for Glacier Calving Front Segmentation</title>
<link>https://arxiv.org/abs/2512.14639</link>
<guid>https://arxiv.org/abs/2512.14639</guid>
<content:encoded><![CDATA[
arXiv:2512.14639v1 Announce Type: new 
Abstract: The dynamics of glaciers and ice shelf fronts significantly impact the mass balance of ice sheets and coastal sea levels. To effectively monitor glacier conditions, it is crucial to consistently estimate positional shifts of glacier calving fronts. AMD-HookNet firstly introduces a pure two-branch convolutional neural network (CNN) for glacier segmentation. Yet, the local nature and translational invariance of convolution operations, while beneficial for capturing low-level details, restricts the model ability to maintain long-range dependencies. In this study, we propose AMD-HookNet++, a novel advanced hybrid CNN-Transformer feature enhancement method for segmenting glaciers and delineating calving fronts in synthetic aperture radar images. Our hybrid structure consists of two branches: a Transformer-based context branch to capture long-range dependencies, which provides global contextual information in a larger view, and a CNN-based target branch to preserve local details. To strengthen the representation of the connected hybrid features, we devise an enhanced spatial-channel attention module to foster interactions between the hybrid CNN-Transformer branches through dynamically adjusting the token relationships from both spatial and channel perspectives. Additionally, we develop a pixel-to-pixel contrastive deep supervision to optimize our hybrid model by integrating pixelwise metric learning into glacier segmentation. Through extensive experiments and comprehensive quantitative and qualitative analyses on the challenging glacier segmentation benchmark dataset CaFFe, we show that AMD-HookNet++ sets a new state of the art with an IoU of 78.2 and a HD95 of 1,318 m, while maintaining a competitive MDE of 367 m. More importantly, our hybrid model produces smoother delineations of calving fronts, resolving the issue of jagged edges typically seen in pure Transformer-based approaches.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multicenter Benchmark of Multiple Instance Learning Models for Lymphoma Subtyping from HE-stained Whole Slide Images</title>
<link>https://arxiv.org/abs/2512.14640</link>
<guid>https://arxiv.org/abs/2512.14640</guid>
<content:encoded><![CDATA[
arXiv:2512.14640v1 Announce Type: new 
Abstract: Timely and accurate lymphoma diagnosis is essential for guiding cancer treatment. Standard diagnostic practice combines hematoxylin and eosin (HE)-stained whole slide images with immunohistochemistry, flow cytometry, and molecular genetic tests to determine lymphoma subtypes, a process requiring costly equipment, skilled personnel, and causing treatment delays. Deep learning methods could assist pathologists by extracting diagnostic information from routinely available HE-stained slides, yet comprehensive benchmarks for lymphoma subtyping on multicenter data are lacking. In this work, we present the first multicenter lymphoma benchmarking dataset covering four common lymphoma subtypes and healthy control tissue. We systematically evaluate five publicly available pathology foundation models (H-optimus-1, H0-mini, Virchow2, UNI2, Titan) combined with attention-based (AB-MIL) and transformer-based (TransMIL) multiple instance learning aggregators across three magnifications (10x, 20x, 40x). On in-distribution test sets, models achieve multiclass balanced accuracies exceeding 80% across all magnifications, with all foundation models performing similarly and both aggregation methods showing comparable results. The magnification study reveals that 40x resolution is sufficient, with no performance gains from higher resolutions or cross-magnification aggregation. However, on out-of-distribution test sets, performance drops substantially to around 60%, highlighting significant generalization challenges. To advance the field, larger multicenter studies covering additional rare lymphoma subtypes are needed. We provide an automated benchmarking pipeline to facilitate such future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble</title>
<link>https://arxiv.org/abs/2512.14648</link>
<guid>https://arxiv.org/abs/2512.14648</guid>
<content:encoded><![CDATA[
arXiv:2512.14648v1 Announce Type: new 
Abstract: Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking</title>
<link>https://arxiv.org/abs/2512.14654</link>
<guid>https://arxiv.org/abs/2512.14654</guid>
<content:encoded><![CDATA[
arXiv:2512.14654v1 Announce Type: new 
Abstract: CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller's Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Visual Sentiment Analysis via Semiotic Isotopy-Guided Dataset Construction</title>
<link>https://arxiv.org/abs/2512.14665</link>
<guid>https://arxiv.org/abs/2512.14665</guid>
<content:encoded><![CDATA[
arXiv:2512.14665v1 Announce Type: new 
Abstract: Visual Sentiment Analysis (VSA) is a challenging task due to the vast diversity of emotionally salient images and the inherent difficulty of acquiring sufficient data to capture this variability comprehensively. Key obstacles include building large-scale VSA datasets and developing effective methodologies that enable algorithms to identify emotionally significant elements within an image. These challenges are reflected in the limited generalization performance of VSA algorithms and models when trained and tested across different datasets. Starting from a pool of existing data collections, our approach enables the creation of a new larger dataset that not only contains a wider variety of images than the original ones, but also permits training new models with improved capability to focus on emotionally relevant combinations of image elements. This is achieved through the integration of the semiotic isotopy concept within the dataset creation process, providing deeper insights into the emotional content of images. Empirical evaluations show that models trained on a dataset generated with our method consistently outperform those trained on the original data collections, achieving superior generalization across major VSA benchmarks
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ART: Articulated Reconstruction Transformer</title>
<link>https://arxiv.org/abs/2512.14671</link>
<guid>https://arxiv.org/abs/2512.14671</guid>
<content:encoded><![CDATA[
arXiv:2512.14671v1 Announce Type: new 
Abstract: We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</title>
<link>https://arxiv.org/abs/2512.14677</link>
<guid>https://arxiv.org/abs/2512.14677</guid>
<content:encoded><![CDATA[
arXiv:2512.14677v1 Announce Type: new 
Abstract: We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Native and Compact Structured Latents for 3D Generation</title>
<link>https://arxiv.org/abs/2512.14692</link>
<guid>https://arxiv.org/abs/2512.14692</guid>
<content:encoded><![CDATA[
arXiv:2512.14692v1 Announce Type: new 
Abstract: Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</title>
<link>https://arxiv.org/abs/2512.14696</link>
<guid>https://arxiv.org/abs/2512.14696</guid>
<content:encoded><![CDATA[
arXiv:2512.14696v1 Announce Type: new 
Abstract: We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP's ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spherical Leech Quantization for Visual Tokenization and Generation</title>
<link>https://arxiv.org/abs/2512.14697</link>
<guid>https://arxiv.org/abs/2512.14697</guid>
<content:encoded><![CDATA[
arXiv:2512.14697v1 Announce Type: new 
Abstract: Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($\Lambda_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2512.14698</link>
<guid>https://arxiv.org/abs/2512.14698</guid>
<content:encoded><![CDATA[
arXiv:2512.14698v1 Announce Type: new 
Abstract: This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</title>
<link>https://arxiv.org/abs/2512.14699</link>
<guid>https://arxiv.org/abs/2512.14699</guid>
<content:encoded><![CDATA[
arXiv:2512.14699v1 Announce Type: new 
Abstract: The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06112</link>
<guid>https://arxiv.org/abs/2512.06112</guid>
<content:encoded><![CDATA[
arXiv:2512.06112v2 Announce Type: cross 
Abstract: We introduce WAM-Flow, a vision-language-action (VLA) model that casts ego-trajectory planning as discrete flow matching over a structured token space. In contrast to autoregressive decoders, WAM-Flow performs fully parallel, bidirectional denoising, enabling coarse-to-fine refinement with a tunable compute-accuracy trade-off. Specifically, the approach combines a metric-aligned numerical tokenizer that preserves scalar geometry via triplet-margin learning, a geometry-aware flow objective and a simulator-guided GRPO alignment that integrates safety, ego progress, and comfort rewards while retaining parallel generation. A multi-stage adaptation converts a pre-trained auto-regressive backbone (Janus-1.5B) from causal decoding to non-causal flow model and strengthens road-scene competence through continued multimodal pretraining. Thanks to the inherent nature of consistency model training and parallel decoding inference, WAM-Flow achieves superior closed-loop performance against autoregressive and diffusion-based VLA baselines, with 1-step inference attaining 89.1 PDMS and 5-step inference reaching 90.3 PDMS on NAVSIM v1 benchmark. These results establish discrete flow matching as a new promising paradigm for end-to-end autonomous driving. The code will be publicly available soon.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.11872</link>
<guid>https://arxiv.org/abs/2512.11872</guid>
<content:encoded><![CDATA[
arXiv:2512.11872v1 Announce Type: cross 
Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Deep Learning for Heat Pump Stress Detection: A Comprehensive Analysis on When2Heat Dataset</title>
<link>https://arxiv.org/abs/2512.13696</link>
<guid>https://arxiv.org/abs/2512.13696</guid>
<content:encoded><![CDATA[
arXiv:2512.13696v1 Announce Type: cross 
Abstract: Heat pump systems are critical components in modern energy-efficient buildings, yet their operational stress detection remains challenging due to complex thermodynamic interactions and limited real-world data. This paper presents a novel Physics-Guided Deep Neural Network (PG-DNN) approach for heat pump stress classification using the When2Heat dataset, containing 131,483 samples with 656 features across 26 European countries. The methodology integrates physics-guided feature selection and class definition with a deep neural network architecture featuring 5 hidden layers and dual regularization strategies. The model achieves 78.1\% test accuracy and 78.5% validation accuracy, demonstrating significant improvements over baseline approaches: +5.0% over shallow networks, +4.0% over limited feature sets, and +2.0% over single regularization strategies. Comprehensive ablation studies validate the effectiveness of physics-guided feature selection, variable thresholding for realistic class distribution, and cross-country energy pattern analysis. The proposed system provides a production-ready solution for heat pump stress detection with 181,348 parameters and 720 seconds training time on AMD Ryzen 9 7950X with RTX 4080 hardware.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution</title>
<link>https://arxiv.org/abs/2512.13729</link>
<guid>https://arxiv.org/abs/2512.13729</guid>
<content:encoded><![CDATA[
arXiv:2512.13729v1 Announce Type: cross 
Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Plausibility of Pressure Distributions Synthesized from Depth through Generative Modeling</title>
<link>https://arxiv.org/abs/2512.13757</link>
<guid>https://arxiv.org/abs/2512.13757</guid>
<content:encoded><![CDATA[
arXiv:2512.13757v1 Announce Type: cross 
Abstract: Monitoring contact pressure in hospital beds is essential for preventing pressure ulcers and enabling real-time patient assessment. Current methods can predict pressure maps but often lack physical plausibility, limiting clinical reliability. This work proposes a framework that enhances plausibility via Informed Latent Space (ILS) and Weight Optimization Loss (WOL) with generative modeling to produce high-fidelity, physically consistent pressure estimates. This study also applies diffusion based conditional Brownian Bridge Diffusion Model (BBDM) and proposes training strategy for its latent counterpart Latent Brownian Bridge Diffusion Model (LBBDM) tailored for pressure synthesis in lying postures. Experiment results shows proposed method improves physical plausibility and performance over baselines: BBDM with ILS delivers highly detailed maps at higher computational cost and large inference time, whereas LBBDM provides faster inference with competitive performance. Overall, the approach supports non-invasive, vision-based, real-time patient monitoring in clinical environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training</title>
<link>https://arxiv.org/abs/2512.13770</link>
<guid>https://arxiv.org/abs/2512.13770</guid>
<content:encoded><![CDATA[
arXiv:2512.13770v1 Announce Type: cross 
Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-D3: A Solution to the Hidden Overfitting Problem of Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.13806</link>
<guid>https://arxiv.org/abs/2512.13806</guid>
<content:encoded><![CDATA[
arXiv:2512.13806v1 Announce Type: cross 
Abstract: Deep learning for decoding EEG signals has gained traction, with many claims to state-of-the-art accuracy. However, despite the convincing benchmark performance, successful translation to real applications is limited. The frequent disconnect between performance on controlled BCI benchmarks and its lack of generalisation to practical settings indicates hidden overfitting problems. We introduce Disentangled Decoding Decomposition (D3), a weakly supervised method for training deep learning models across EEG datasets. By predicting the place in the respective trial sequence from which the input window was sampled, EEG-D3 separates latent components of brain activity, akin to non-linear ICA. We utilise a novel model architecture with fully independent sub-networks for strict interpretability. We outline a feature interpretation paradigm to contrast the component activation profiles on different datasets and inspect the associated temporal and spatial filters. The proposed method reliably separates latent components of brain activity on motor imagery data. Training downstream classifiers on an appropriate subset of these components prevents hidden overfitting caused by task-correlated artefacts, which severely affects end-to-end classifiers. We further exploit the linearly separable latent space for effective few-shot learning on sleep stage classification. The ability to distinguish genuine components of brain activity from spurious features results in models that avoid the hidden overfitting problem and generalise well to real-world applications, while requiring only minimal labelled data. With interest to the neuroscience community, the proposed method gives researchers a tool to separate individual brain processes and potentially even uncover heretofore unknown dynamics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI for Video Translation: A Scalable Architecture for Multilingual Video Conferencing</title>
<link>https://arxiv.org/abs/2512.13904</link>
<guid>https://arxiv.org/abs/2512.13904</guid>
<content:encoded><![CDATA[
arXiv:2512.13904v1 Announce Type: cross 
Abstract: The real-time deployment of cascaded generative AI pipelines for applications like video translation is constrained by significant system-level challenges. These include the cumulative latency of sequential model inference and the quadratic ($\mathcal{O}(N^2)$) computational complexity that renders multi-user video conferencing applications unscalable. This paper proposes and evaluates a practical system-level framework designed to mitigate these critical bottlenecks. The proposed architecture incorporates a turn-taking mechanism to reduce computational complexity from quadratic to linear in multi-user scenarios, and a segmented processing protocol to manage inference latency for a perceptually real-time experience. We implement a proof-of-concept pipeline and conduct a rigorous performance analysis across a multi-tiered hardware setup, including commodity (NVIDIA RTX 4060), cloud (NVIDIA T4), and enterprise (NVIDIA A100) GPUs. Our objective evaluation demonstrates that the system achieves real-time throughput ($\tau < 1.0$) on modern hardware. A subjective user study further validates the approach, showing that a predictable, initial processing delay is highly acceptable to users in exchange for a smooth, uninterrupted playback experience. The work presents a validated, end-to-end system design that offers a practical roadmap for deploying scalable, real-time generative AI applications in multilingual communication platforms.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAIM: Camera-LiDAR Alignment with Intensity and Monodepth</title>
<link>https://arxiv.org/abs/2512.14001</link>
<guid>https://arxiv.org/abs/2512.14001</guid>
<content:encoded><![CDATA[
arXiv:2512.14001v1 Announce Type: cross 
Abstract: In this paper, we unleash the potential of the powerful monodepth model in camera-LiDAR calibration and propose CLAIM, a novel method of aligning data from the camera and LiDAR. Given the initial guess and pairs of images and LiDAR point clouds, CLAIM utilizes a coarse-to-fine searching method to find the optimal transformation minimizing a patched Pearson correlation-based structure loss and a mutual information-based texture loss. These two losses serve as good metrics for camera-LiDAR alignment results and require no complicated steps of data processing, feature extraction, or feature matching like most methods, rendering our method simple and adaptive to most scenes. We validate CLAIM on public KITTI, Waymo, and MIAS-LCEC datasets, and the experimental results demonstrate its superior performance compared with the state-of-the-art methods. The code is available at https://github.com/Tompson11/claim.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expert Switching for Robust AAV Landing: A Dual-Detector Framework in Simulation</title>
<link>https://arxiv.org/abs/2512.14054</link>
<guid>https://arxiv.org/abs/2512.14054</guid>
<content:encoded><![CDATA[
arXiv:2512.14054v1 Announce Type: cross 
Abstract: Reliable helipad detection is essential for Autonomous Aerial Vehicle (AAV) landing, especially under GPS-denied or visually degraded conditions. While modern detectors such as YOLOv8 offer strong baseline performance, single-model pipelines struggle to remain robust across the extreme scale transitions that occur during descent, where helipads appear small at high altitude and large near touchdown. To address this limitation, we propose a scale-adaptive dual-expert perception framework that decomposes the detection task into far-range and close-range regimes. Two YOLOv8 experts are trained on scale-specialized versions of the HelipadCat dataset, enabling one model to excel at detecting small, low-resolution helipads and the other to provide high-precision localization when the target dominates the field of view. During inference, both experts operate in parallel, and a geometric gating mechanism selects the expert whose prediction is most consistent with the AAV's viewpoint. This adaptive routing prevents the degradation commonly observed in single-detector systems when operating across wide altitude ranges. The dual-expert perception module is evaluated in a closed-loop landing environment that integrates CARLA's photorealistic rendering with NASA's GUAM flight-dynamics engine. Results show substantial improvements in alignment stability, landing accuracy, and overall robustness compared to single-detector baselines. By introducing a scale-aware expert routing strategy tailored to the landing problem, this work advances resilient vision-based perception for autonomous descent and provides a foundation for future multi-expert AAV frameworks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis</title>
<link>https://arxiv.org/abs/2512.14157</link>
<guid>https://arxiv.org/abs/2512.14157</guid>
<content:encoded><![CDATA[
arXiv:2512.14157v1 Announce Type: cross 
Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Stochastic Object Models from Noisy Data via Ambient Measurement-Integrated Diffusion</title>
<link>https://arxiv.org/abs/2512.14187</link>
<guid>https://arxiv.org/abs/2512.14187</guid>
<content:encoded><![CDATA[
arXiv:2512.14187v1 Announce Type: cross 
Abstract: Task-based measures of image quality (IQ) are critical for evaluating medical imaging systems, which must account for randomness including anatomical variability. Stochastic object models (SOMs) provide a statistical description of such variability, but conventional mathematical SOMs fail to capture realistic anatomy, while data-driven approaches typically require clean data rarely available in clinical tasks. To address this challenge, we propose AMID, an unsupervised Ambient Measurement-Integrated Diffusion with noise decoupling, which establishes clean SOMs directly from noisy measurements. AMID introduces a measurement-integrated strategy aligning measurement noise with the diffusion trajectory, and explicitly models coupling between measurement and diffusion noise across steps, an ambient loss is thus designed base on it to learn clean SOMs. Experiments on real CT and mammography datasets show that AMID outperforms existing methods in generation fidelity and yields more reliable task-based IQ evaluation, demonstrating its potential for unsupervised medical imaging analysis.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Safety Metric to Evaluate Perception in Autonomous Systems</title>
<link>https://arxiv.org/abs/2512.14367</link>
<guid>https://arxiv.org/abs/2512.14367</guid>
<content:encoded><![CDATA[
arXiv:2512.14367v1 Announce Type: cross 
Abstract: Complete perception of the environment and its correct interpretation is crucial for autonomous vehicles. Object perception is the main component of automotive surround sensing. Various metrics already exist for the evaluation of object perception. However, objects can be of different importance depending on their velocity, orientation, distance, size, or the potential damage that could be caused by a collision due to a missed detection. Thus, these additional parameters have to be considered for safety evaluation. We propose a new safety metric that incorporates all these parameters and returns a single easily interpretable safety assessment score for object perception. This new metric is evaluated with both real world and virtual data sets and compared to state of the art metrics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VICTOR: Dataset Copyright Auditing in Video Recognition Systems</title>
<link>https://arxiv.org/abs/2512.14439</link>
<guid>https://arxiv.org/abs/2512.14439</guid>
<content:encoded><![CDATA[
arXiv:2512.14439v1 Announce Type: cross 
Abstract: Video recognition systems are increasingly being deployed in daily life, such as content recommendation and security monitoring. To enhance video recognition development, many institutions have released high-quality public datasets with open-source licenses for training advanced models. At the same time, these datasets are also susceptible to misuse and infringement. Dataset copyright auditing is an effective solution to identify such unauthorized use. However, existing dataset copyright solutions primarily focus on the image domain; the complex nature of video data leaves dataset copyright auditing in the video domain unexplored. Specifically, video data introduces an additional temporal dimension, which poses significant challenges to the effectiveness and stealthiness of existing methods.
  In this paper, we propose VICTOR, the first dataset copyright auditing approach for video recognition systems. We develop a general and stealthy sample modification strategy that enhances the output discrepancy of the target model. By modifying only a small proportion of samples (e.g., 1%), VICTOR amplifies the impact of published modified samples on the prediction behavior of the target models. Then, the difference in the model's behavior for published modified and unpublished original samples can serve as a key basis for dataset auditing. Extensive experiments on multiple models and datasets highlight the superiority of VICTOR. Finally, we show that VICTOR is robust in the presence of several perturbation mechanisms to the training videos or the target models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test Time Optimized Generalized AI-based Medical Image Registration Method</title>
<link>https://arxiv.org/abs/2512.14556</link>
<guid>https://arxiv.org/abs/2512.14556</guid>
<content:encoded><![CDATA[
arXiv:2512.14556v1 Announce Type: cross 
Abstract: Medical image registration is critical for aligning anatomical structures across imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound. Among existing techniques, non-rigid registration (NRR) is particularly challenging due to the need to capture complex anatomical deformations caused by physiological processes like respiration or contrast-induced signal variations. Traditional NRR methods, while theoretically robust, often require extensive parameter tuning and incur high computational costs, limiting their use in real-time clinical workflows. Recent deep learning (DL)-based approaches have shown promise; however, their dependence on task-specific retraining restricts scalability and adaptability in practice. These limitations underscore the need for efficient, generalizable registration frameworks capable of handling heterogeneous imaging contexts. In this work, we introduce a novel AI-driven framework for 3D non-rigid registration that generalizes across multiple imaging modalities and anatomical regions. Unlike conventional methods that rely on application-specific models, our approach eliminates anatomy- or modality-specific customization, enabling streamlined integration into diverse clinical environments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction</title>
<link>https://arxiv.org/abs/2512.14620</link>
<guid>https://arxiv.org/abs/2512.14620</guid>
<content:encoded><![CDATA[
arXiv:2512.14620v1 Announce Type: cross 
Abstract: This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields</title>
<link>https://arxiv.org/abs/2512.14656</link>
<guid>https://arxiv.org/abs/2512.14656</guid>
<content:encoded><![CDATA[
arXiv:2512.14656v1 Announce Type: cross 
Abstract: We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2512.14666</link>
<guid>https://arxiv.org/abs/2512.14666</guid>
<content:encoded><![CDATA[
arXiv:2512.14666v1 Announce Type: cross 
Abstract: Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame'' this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMGR: Multi-Modal Generative Reasoning</title>
<link>https://arxiv.org/abs/2512.14691</link>
<guid>https://arxiv.org/abs/2512.14691</guid>
<content:encoded><![CDATA[
arXiv:2512.14691v1 Announce Type: cross 
Abstract: Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness</title>
<link>https://arxiv.org/abs/2312.04960</link>
<guid>https://arxiv.org/abs/2312.04960</guid>
<content:encoded><![CDATA[
arXiv:2312.04960v5 Announce Type: replace 
Abstract: Vision Transformers (ViTs) have emerged as a fundamental architecture and serve as the backbone of modern vision-language models. Despite their impressive performance, ViTs exhibit notable vulnerability to evasion attacks, necessitating the development of specialized Adversarial Training (AT) strategies tailored to their unique architecture. While a direct solution might involve applying existing AT methods to ViTs, our analysis reveals significant incompatibilities, particularly with state-of-the-art (SOTA) approaches such as Generalist (CVPR 2023) and DBAT (USENIX Security 2024). This paper presents a systematic investigation of adversarial robustness in ViTs and provides a novel theoretical Mutual Information (MI) analysis in its autoencoder-based self-supervised pre-training. Specifically, we show that MI between the adversarial example and its latent representation in ViT-based autoencoders should be constrained via derived MI bounds. Building on this insight, we propose a self-supervised AT method, MIMIR, that employs an MI penalty to facilitate adversarial pre-training by masked image modeling with autoencoders. Extensive experiments on CIFAR-10, Tiny-ImageNet, and ImageNet-1K show that MIMIR can consistently provide improved natural and robust accuracy, where MIMIR outperforms SOTA AT results on ImageNet-1K. Notably, MIMIR demonstrates superior robustness against unforeseen attacks and common corruption data and can also withstand adaptive attacks where the adversary possesses full knowledge of the defense mechanism. Our code and trained models are publicly available at: https://github.com/xiaoyunxxy/MIMIR.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSM: A Comprehensive Metric for Assessing the Safety of Lane Detection Systems in Autonomous Driving</title>
<link>https://arxiv.org/abs/2407.07740</link>
<guid>https://arxiv.org/abs/2407.07740</guid>
<content:encoded><![CDATA[
arXiv:2407.07740v2 Announce Type: replace 
Abstract: Comprehensive perception of the vehicle's environment and correct interpretation of the environment are crucial for the safe operation of autonomous vehicles. The perception of surrounding objects is the main component for further tasks such as trajectory planning. However, safe trajectory planning requires not only object detection, but also the detection of drivable areas and lane corridors. While first approaches consider an advanced safety evaluation of object detection, the evaluation of lane detection still lacks sufficient safety metrics. Similar to the safety metrics for object detection, additional factors such as the semantics of the scene with road type and road width, the detection range as well as the potential causes of missing detections, incorporated by vehicle speed, should be considered for the evaluation of lane detection. Therefore, we propose the Lane Safety Metric (LSM), which takes these factors into account and allows to evaluate the safety of lane detection systems by determining an easily interpretable safety score. We evaluate our offline safety metric on various virtual scenarios using different lane detection approaches and compare it with state-of-the-art performance metrics.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Framework with Multimodal Fine-tuning for Remote Sensing Semantic Segmentation</title>
<link>https://arxiv.org/abs/2410.11160</link>
<guid>https://arxiv.org/abs/2410.11160</guid>
<content:encoded><![CDATA[
arXiv:2410.11160v2 Announce Type: replace 
Abstract: Multimodal remote sensing data, acquired from diverse sensors, offer a comprehensive and integrated perspective of the Earth's surface. Leveraging multimodal fusion techniques, semantic segmentation enables detailed and accurate analysis of geographic scenes, surpassing single-modality approaches. Building on advancements in vision foundation models, particularly the Segment Anything Model (SAM), this study proposes a unified framework incorporating a novel Multimodal Fine-tuning Network (MFNet) for remote sensing semantic segmentation. The proposed framework is designed to seamlessly integrate with various fine-tuning mechanisms, demonstrated through the inclusion of Adapter and Low-Rank Adaptation (LoRA) as representative examples. This extensibility ensures the framework's adaptability to other emerging fine-tuning strategies, allowing models to retain SAM's general knowledge while effectively leveraging multimodal data. Additionally, a pyramid-based Deep Fusion Module (DFM) is introduced to integrate high-level geographic features across multiple scales, enhancing feature representation prior to decoding. This work also highlights SAM's robust generalization capabilities with Digital Surface Model (DSM) data, a novel application. Extensive experiments on three benchmark multimodal remote sensing datasets, ISPRS Vaihingen, ISPRS Potsdam and MMHunan, demonstrate that the proposed MFNet significantly outperforms existing methods in multimodal semantic segmentation, setting a new standard in the field while offering a versatile foundation for future research and applications. The source code for this work is accessible at https://github.com/sstary/SSRS.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Renal Cell Carcinoma subtyping: learning from multi-resolution localization</title>
<link>https://arxiv.org/abs/2411.09471</link>
<guid>https://arxiv.org/abs/2411.09471</guid>
<content:encoded><![CDATA[
arXiv:2411.09471v2 Announce Type: replace 
Abstract: Renal Cell Carcinoma is typically asymptomatic at the early stages for many patients. This leads to a late diagnosis of the tumor, where the curability likelihood is lower, and makes the mortality rate of Renal Cell Carcinoma high, with respect to its incidence rate. To increase the survival chance, a fast and correct categorization of the tumor subtype is paramount. Nowadays, computerized methods, based on artificial intelligence, represent an interesting opportunity to improve the productivity and the objectivity of the microscopy-based Renal Cell Carcinoma diagnosis. Nonetheless, much of their exploitation is hampered by the paucity of annotated dataset, essential for a proficient training of supervised machine learning technologies. This study sets out to investigate a novel self supervised training strategy for machine learning diagnostic tools, based on the multi-resolution nature of the histological samples. We aim at reducing the need of annotated dataset, without significantly reducing the accuracy of the tool. We demonstrate the classification capability of our tool on a whole slide imaging dataset for Renal Cancer subtyping, and we compare our solution with several state-of-the-art classification counterparts.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Free Procedural 3D Shapes Are Surprisingly Good Teachers</title>
<link>https://arxiv.org/abs/2411.17467</link>
<guid>https://arxiv.org/abs/2411.17467</guid>
<content:encoded><![CDATA[
arXiv:2411.17467v3 Announce Type: replace 
Abstract: Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to scale and raising copyright concerns. To address these challenges, we propose learning 3D representations from procedural 3D programs that automatically generate 3D shapes using simple 3D primitives and augmentations.
  Remarkably, despite lacking semantic content, the 3D representations learned from the procedurally generated 3D shapes perform on par with state-of-the-art representations learned from semantically recognizable 3D models (e.g., airplanes) across various downstream 3D tasks, such as shape classification, part segmentation, masked point cloud completion, and both scene semantic and instance segmentation. We provide a detailed analysis on factors that make a good 3D procedural programs. Extensive experiments further suggest that current 3D self-supervised learning methods on point clouds do not rely on semantics of 3D shapes, shedding light on the nature of 3D representations learned.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds</title>
<link>https://arxiv.org/abs/2501.01728</link>
<guid>https://arxiv.org/abs/2501.01728</guid>
<content:encoded><![CDATA[
arXiv:2501.01728v3 Announce Type: replace 
Abstract: Assessment of forest biodiversity is crucial for ecosystem management and conservation. While traditional field surveys provide high-quality assessments, they are labor-intensive and spatially limited. This study investigates whether deep learning-based fusion of close-range sensing data from 2D orthophotos and 3D airborne laser scanning (ALS) point clouds can reliable assess the biodiversity potential of forests. We introduce the BioVista dataset, comprising 44378 paired samples of orthophotos and ALS point clouds from temperate forests in Denmark, designed to explore multimodal fusion approaches. Using deep neural networks (ResNet for orthophotos and PointVector for ALS point clouds), we investigate each data modality's ability to assess forest biodiversity potential, achieving overall accuracies of 76.7% and 75.8%, respectively. We explore various 2D and 3D fusion approaches: confidence-based ensembling, feature-level concatenation, and end-to-end training, with the latter achieving an overall accuracies of 82.0% when separating low- and high potential forest areas. Our results demonstrate that spectral information from orthophotos and structural information from ALS point clouds effectively complement each other in the assessment of forest biodiversity potential.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From My View to Yours: Ego-to-Exo Transfer in VLMs for Understanding Activities of Daily Living</title>
<link>https://arxiv.org/abs/2501.05711</link>
<guid>https://arxiv.org/abs/2501.05711</guid>
<content:encoded><![CDATA[
arXiv:2501.05711v3 Announce Type: replace 
Abstract: Vision Language Models (VLMs) have achieved strong performance across diverse video understanding tasks. However, their viewpoint invariant training limits their ability to understand egocentric properties (e.g., human object interactions) from exocentric video observations. This limitation is critical for many applications, such as Activities of Daily Living (ADL) monitoring, where the understanding of egocentric properties is essential, and egocentric cameras are impractical to deploy. To address this limitation, we propose Ego2ExoVLM, a VLM that learns to infer egocentric properties from exocentric videos by leveraging time-synchronized ego-exo videos during training. Ego2ExoVLM accomplishes this through the use of two components: Ego2Exo Sequence Distillation, which transfers knowledge from an egocentric teacher to an exocentric student, and Ego Adaptive Visual Tokens, designed to enhance the effectiveness of this knowledge transfer. To measure this capability, we introduce Ego-in-Exo Perception, a benchmark of 3.9K questions curated to explicitly measure the understanding of egocentric properties from exocentric videos. Ego2ExoVLM is evaluated on 10 tasks across Ego-in-Exo Perception and existing ADL benchmarks, achieving state-of-the-art results on the ADL-X benchmark suite and outperforming strong baselines on our proposed benchmark. All code, models, and data will be released at https://github.com/dominickrei/EgoExo4ADL.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LWGANet: Addressing Spatial and Channel Redundancy in Remote Sensing Visual Tasks with Light-Weight Grouped Attention</title>
<link>https://arxiv.org/abs/2501.10040</link>
<guid>https://arxiv.org/abs/2501.10040</guid>
<content:encoded><![CDATA[
arXiv:2501.10040v3 Announce Type: replace 
Abstract: Light-weight neural networks for remote sensing (RS) visual analysis must overcome two inherent redundancies: spatial redundancy from vast, homogeneous backgrounds, and channel redundancy, where extreme scale variations render a single feature space inefficient. Existing models, often designed for natural images, fail to address this dual challenge in RS scenarios. To bridge this gap, we propose LWGANet, a light-weight backbone engineered for RS-specific properties. LWGANet introduces two core innovations: a Top-K Global Feature Interaction (TGFI) module that mitigates spatial redundancy by focusing computation on salient regions, and a Light-Weight Grouped Attention (LWGA) module that resolves channel redundancy by partitioning channels into specialized, scale-specific pathways. By synergistically resolving these core inefficiencies, LWGANet achieves a superior trade-off between feature representation quality and computational cost. Extensive experiments on twelve diverse datasets across four major RS tasks--scene classification, oriented object detection, semantic segmentation, and change detection--demonstrate that LWGANet consistently outperforms state-of-the-art light-weight backbones in both accuracy and efficiency. Our work establishes a new, robust baseline for efficient visual analysis in RS images.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exo2Ego: Exocentric Knowledge Guided MLLM for Egocentric Video Understanding</title>
<link>https://arxiv.org/abs/2503.09143</link>
<guid>https://arxiv.org/abs/2503.09143</guid>
<content:encoded><![CDATA[
arXiv:2503.09143v2 Announce Type: replace 
Abstract: AI personal assistants, deployed through robots or wearables, require embodied understanding to collaborate effectively with humans. However, current Multimodal Large Language Models (MLLMs) primarily focus on third-person (exocentric) vision, overlooking the unique challenges of first-person (egocentric) videos. Additionally, high acquisition costs limit data size, impairing MLLM performance. To address these challenges, we propose learning the mapping between exocentric and egocentric domains, leveraging the extensive exocentric knowledge within existing MLLMs to enhance egocentric video understanding. To this end, we introduce Ego-ExoClip, a pre-training dataset comprising 1.1M synchronized ego-exo clip-text pairs derived from Ego-Exo4D, together with the instruction-tuning dataset EgoIT, which is collected from multiple sources to enhance the model's instruction-following capabilities. Building upon the datasets, we propose a migration strategy and further design a progressive mapping learning pipeline with three stages: Demonstrator Self-Preparation, Demonstrator-Learner Guidance, and Learner Self-Practice. Extensive experiments across diverse egocentric tasks reveal that existing MLLMs perform inadequately in egocentric video understanding, while our model significantly outperforms these leading models.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GASPACHO: Gaussian Splatting for Controllable Humans and Objects</title>
<link>https://arxiv.org/abs/2503.09342</link>
<guid>https://arxiv.org/abs/2503.09342</guid>
<content:encoded><![CDATA[
arXiv:2503.09342v2 Announce Type: replace 
Abstract: We present GASPACHO, a method for generating photorealistic, controllable renderings of human-object interactions from multi-view RGB video. Unlike prior work that reconstructs only the human and treats objects as background, GASPACHO simultaneously recovers animatable templates for both the human and the interacting object as distinct sets of Gaussians, thereby allowing for controllable renderings of novel human object interactions in different poses from novel-camera viewpoints. We introduce a novel formulation that learns object Gaussians on an underlying 2D surface manifold rather than in 3D volume, yielding sharper, fine-grained object details for dynamic object reconstruction. We further propose a contact constraint in Gaussian space that regularizes human-object relations and enables natural, physically plausible animation. Across three benchmarks - BEHAVE, NeuralDome, and DNA-Rendering - GASPACHO achieves high-quality reconstructions under heavy occlusion and supports controllable synthesis of novel human-object interactions. We also demonstrate that our method allows for composition of humans and objects in 3D scenes and for the first time showcase that neural rendering can be used for the controllable generation of photoreal humans interacting with dynamic objects in diverse scenes. Our results are available at: https://miraymen.github.io/gaspacho/
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Med3DVLM: An Efficient Vision-Language Model for 3D Medical Image Analysis</title>
<link>https://arxiv.org/abs/2503.20047</link>
<guid>https://arxiv.org/abs/2503.20047</guid>
<content:encoded><![CDATA[
arXiv:2503.20047v3 Announce Type: replace 
Abstract: Vision-language models (VLMs) have shown promise in 2D medical image analysis, but extending them to 3D remains challenging due to the high computational demands of volumetric data and the difficulty of aligning 3D spatial features with clinical text. We present Med3DVLM, a 3D VLM designed to address these challenges through three key innovations: (1) DCFormer, an efficient encoder that uses decomposed 3D convolutions to capture fine-grained spatial features at scale; (2) SigLIP, a contrastive learning strategy with pairwise sigmoid loss that improves image-text alignment without relying on large negative batches; and (3) a dual-stream MLP-Mixer projector that fuses low- and high-level image features with text embeddings for richer multi-modal representations. We evaluate our model on the M3D dataset, which includes radiology reports and VQA data for 120,084 3D medical images. Results show that Med3DVLM achieves superior performance across multiple benchmarks. For image-text retrieval, it reaches 61.00% R@1 on 2,000 samples, significantly outperforming the current state-of-the-art M3D model (19.10%). For report generation, it achieves a METEOR score of 36.42% (vs. 14.38%). In open-ended visual question answering (VQA), it scores 36.76% METEOR (vs. 33.58%), and in closed-ended VQA, it achieves 79.95% accuracy (vs. 75.78%). These results highlight Med3DVLM's ability to bridge the gap between 3D imaging and language, enabling scalable, multi-task reasoning across clinical applications. Our code is publicly available at https://github.com/mirthAI/Med3DVLM.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera Color Constancy</title>
<link>https://arxiv.org/abs/2504.07959</link>
<guid>https://arxiv.org/abs/2504.07959</guid>
<content:encoded><![CDATA[
arXiv:2504.07959v2 Announce Type: replace 
Abstract: Computational color constancy, or white balancing, is a key module in a camera's image signal processor (ISP) that corrects color casts from scene lighting. Because this operation occurs in the camera-specific raw color space, white balance algorithms must adapt to different cameras. This paper introduces a learning-based method for cross-camera color constancy that generalizes to new cameras without retraining. Our method leverages pre-calibrated color correction matrices (CCMs) available on ISPs that map the camera's raw color space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to transform predefined illumination colors (i.e., along the Planckian locus) into the test camera's raw space. The mapped illuminants are encoded into a compact camera fingerprint embedding (CFE) that enables the network to adapt to unseen cameras. To prevent overfitting due to limited cameras and CCMs during training, we introduce a data augmentation technique that interpolates between cameras and their CCMs. Experimental results across multiple datasets and backbones show that our method achieves state-of-the-art cross-camera color constancy while remaining lightweight and relying only on data readily available in camera ISPs.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2504.20865</link>
<guid>https://arxiv.org/abs/2504.20865</guid>
<content:encoded><![CDATA[
arXiv:2504.20865v3 Announce Type: replace 
Abstract: The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GT2-GS: Geometry-aware Texture Transfer for Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.15208</link>
<guid>https://arxiv.org/abs/2505.15208</guid>
<content:encoded><![CDATA[
arXiv:2505.15208v2 Announce Type: replace 
Abstract: Transferring 2D textures to 3D modalities is of great significance for improving the efficiency of multimedia content creation. Existing approaches have rarely focused on transferring image textures onto 3D representations. 3D style transfer methods are capable of transferring abstract artistic styles to 3D scenes. However, these methods often overlook the geometric information of the scene, which makes it challenging to achieve high-quality 3D texture transfer results. In this paper, we present GT^2-GS, a geometry-aware texture transfer framework for gaussian splitting. From the perspective of matching texture features with geometric information in rendered views, we identify the issue of insufficient texture features and propose a geometry-aware texture augmentation module to expand the texture feature set. Moreover, a geometry-consistent texture loss is proposed to optimize texture features into the scene representation. This loss function incorporates both camera pose and 3D geometric information of the scene, enabling controllable texture-oriented appearance editing. Finally, a geometry preservation strategy is introduced. By alternating between the texture transfer and geometry correction stages over multiple iterations, this strategy achieves a balance between learning texture features and preserving geometric integrity. Extensive experiments demonstrate the effectiveness and controllability of our method. Through geometric awareness, our approach achieves texture transfer results that better align with human visual perception. Our homepage is available at https://vpx-ecnu.github.io/GT2-GS-website.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIBE: Can a VLM Read the Room?</title>
<link>https://arxiv.org/abs/2506.11162</link>
<guid>https://arxiv.org/abs/2506.11162</guid>
<content:encoded><![CDATA[
arXiv:2506.11162v2 Announce Type: replace 
Abstract: Understanding human social behavior such as recognizing emotions and the social dynamics causing them is an important and challenging problem. While LLMs have made remarkable advances, they are limited to the textual domain and cannot account for the major role that non-verbal cues play in understanding social situations. Vision Language Models (VLMs) can potentially account for this gap, however their ability to make correct inferences over such social cues has received little attention. In this paper, we explore the capabilities of VLMs at social reasoning. We identify a previously overlooked limitation in VLMs: the Visual Social-Pragmatic Inference gap. To target this gap, we propose a new task for VLMs: Visual Social-Pragmatic Inference. We construct a high quality dataset to test the abilities of a VLM for this task and benchmark the performance of several VLMs on it.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wi-CBR: Salient-aware Adaptive WiFi Sensing for Cross-domain Behavior Recognition</title>
<link>https://arxiv.org/abs/2506.11616</link>
<guid>https://arxiv.org/abs/2506.11616</guid>
<content:encoded><![CDATA[
arXiv:2506.11616v4 Announce Type: replace 
Abstract: The challenge in WiFi-based cross-domain Behavior Recognition lies in the significant interference of domain-specific signals on gesture variation. However, previous methods alleviate this interference by mapping the phase from multiple domains into a common feature space. If the Doppler Frequency Shift (DFS) signal is used to dynamically supplement the phase features to achieve better generalization, it enables the model to not only explore a wider feature space but also to avoid potential degradation of gesture semantic information. Specifically, we propose a novel Salient-aware Adaptive WiFi Sensing for Cross-domain Behavior Recognition (Wi-CBR), which constructs a dual-branch self-attention module that captures temporal features from phase information reflecting dynamic path length variations while extracting kinematic features from DFS correlated with motion velocity. Moreover, we design a Saliency Guidance Module that employs group attention mechanisms to mine critical activity features and utilizes gating mechanisms to optimize information entropy, facilitating feature fusion and enabling effective interaction between salient and non-salient behavioral characteristics. Extensive experiments on two large-scale public datasets (Widar3.0 and XRF55) demonstrate the superior performance of our method in both in-domain and cross-domain scenarios.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review</title>
<link>https://arxiv.org/abs/2506.14831</link>
<guid>https://arxiv.org/abs/2506.14831</guid>
<content:encoded><![CDATA[
arXiv:2506.14831v2 Announce Type: replace 
Abstract: With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as social robot navigation, autonomous navigation, and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2025. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TextMesh4D: Text-to-4D Mesh Generation via Jacobian Deformation Field</title>
<link>https://arxiv.org/abs/2506.24121</link>
<guid>https://arxiv.org/abs/2506.24121</guid>
<content:encoded><![CDATA[
arXiv:2506.24121v2 Announce Type: replace 
Abstract: Dynamic 3D (4D) content generation, particularly text-to-4D, remains a challenging and under-explored problem due to its inherent spatiotemporal complexity. Existing text-to-4D methods typically avoid direct mesh generation due to inherent topological constraints, favoring alternative representations like NeRFs or 3DGS. However, these non-mesh approaches, suffer from insufficient geometric fidelity, temporal artifacts, and limited compatibility with modern computer graphics (CG) pipelines. In contrast, directly generating dynamic meshes faces two key challenges: i) deformation inflexibility, as traditional vertex-based optimization is constrained by meshes' explicitly encoded topology, and ii) semantic inconsistency, arising from stochastic noise in distilled priors.
  In this paper, we introduce TextMesh4D, a pioneering framework for text-to-4D mesh generation that directly addresses these challenges. TextMesh4D features two core innovations: 1) the Jacobian Deformation Field (JDF), which shifts the deformation unit from vertices to faces, using per-face Jacobians to model flexible transformations free from topological constraints. 2) the Local-Global Semantic Regularizer (LGSR), which leverages the mesh's innate geometric properties to enforce semantic coherence both locally and globally across frames. Extensive experiments demonstrate that TextMesh4D achieves state-of-the-art performance in temporal consistency, structural fidelity, and visual realism, while requiring only a single 24GB GPU. Our work establishes a new benchmark for efficient and high-quality text-to-4D mesh generation. The code will be released to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inter- and Intra-image Refinement for Few Shot Segmentation</title>
<link>https://arxiv.org/abs/2507.05838</link>
<guid>https://arxiv.org/abs/2507.05838</guid>
<content:encoded><![CDATA[
arXiv:2507.05838v2 Announce Type: replace 
Abstract: Deep neural networks for semantic segmentation rely on large-scale annotated datasets, leading to an annotation bottleneck that motivates few shot semantic segmentation (FSS) which aims to generalize to novel classes with minimal labeled exemplars. Most existing FSS methods adopt a prototype-based paradigm, which generates query prior map by extracting masked-area features from support images and then makes predictions guided by the prior map. However, they suffer from two critical limitations induced by inter- and intra-image discrepancies: 1) The intra-class gap between support and query images, caused by single-prototype representation, results in scattered and noisy prior maps; 2) The inter-class interference from visually similar but semantically distinct regions leads to inconsistent support-query feature matching and erroneous predictions. To address these issues, we propose the Inter- and Intra-image Refinement (IIR) model. The model contains an inter-image class activation mapping based method that generates two prototypes for class-consistent region matching, including core discriminative features and local specific features, and yields an accurate and robust prior map. For intra-image refinement, a directional dropout mechanism is introduced to mask inconsistent support-query feature pairs in cross attention, thereby enhancing decoder performance. Extensive experiments demonstrate that IIR achieves state-of-the-art performance on 9 benchmarks, covering standard FSS, part FSS, and cross-domain FSS. Our source code is available at \href{https://github.com/forypipi/IIR}{https://github.com/forypipi/IIR}.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection</title>
<link>https://arxiv.org/abs/2507.11252</link>
<guid>https://arxiv.org/abs/2507.11252</guid>
<content:encoded><![CDATA[
arXiv:2507.11252v2 Announce Type: replace 
Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of deep learning, image-based smoke detection has become a crucial method for detecting and preventing forest fires. However, the scarcity of smoke image data from forest fires is one of the significant factors hindering the detection of forest fire smoke. Image generation models offer a promising solution for synthesizing realistic smoke images. However, current inpainting models exhibit limitations in generating high-quality smoke representations, particularly manifesting as inconsistencies between synthesized smoke and background contexts. To solve these problems, we proposed a comprehensive framework for generating forest fire smoke images. Firstly, we employed the pre-trained segmentation model and the multimodal model to obtain smoke masks and image captions.Then, to address the insufficient utilization of masks and masked images by inpainting models, we introduced a network architecture guided by mask and masked image features. We also proposed a new loss function, the mask random difference loss, which enhances the consistency of the generated effects around the mask by randomly expanding and eroding the mask edges.Finally, to generate a smoke image dataset using random masks for subsequent detection tasks, we incorporated smoke characteristics and use a multimodal large language model as a filtering tool to select diverse and reasonable smoke images, thereby improving the quality of the synthetic dataset. Experiments showed that our generated smoke images are realistic and diverse, and effectively enhance the performance of forest fire smoke detection models. Code is available at https://github.com/wghr123/MFGDiffusion.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Embedded Swin-UMamba for DeepLesion Segmentation</title>
<link>https://arxiv.org/abs/2508.06453</link>
<guid>https://arxiv.org/abs/2508.06453</guid>
<content:encoded><![CDATA[
arXiv:2508.06453v2 Announce Type: replace 
Abstract: Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow has the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, our method achieved a high Dice score of 82.64, and a low Hausdorff distance of 6.34 pixels was obtained for lesion segmentation. The proposed Text-Swin-U/Mamba model outperformed prior approaches: 37.79% improvement over the LLM-driven LanGuideMedSeg model (p < 0.001), and surpassed the purely image-based XLSTM-UNet and nnUNet models by 2.58% and 1.01%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guideline-Consistent Segmentation via Multi-Agent Refinement</title>
<link>https://arxiv.org/abs/2509.04687</link>
<guid>https://arxiv.org/abs/2509.04687</guid>
<content:encoded><![CDATA[
arXiv:2509.04687v2 Announce Type: replace 
Abstract: Semantic segmentation in real-world applications often requires not only accurate masks but also strict adherence to textual labeling guidelines. These guidelines are typically complex and long, and both human and automated labeling often fail to follow them faithfully. Traditional approaches depend on expensive task-specific retraining that must be repeated as the guidelines evolve. Although recent open-vocabulary segmentation methods excel with simple prompts, they often fail when confronted with sets of paragraph-length guidelines that specify intricate segmentation rules. To address this, we introduce a multi-agent, training-free framework that coordinates general-purpose vision-language models within an iterative Worker-Supervisor refinement architecture. The Worker performs the segmentation, the Supervisor critiques it against the retrieved guidelines, and a lightweight reinforcement learning stop policy decides when to terminate the loop, ensuring guideline-consistent masks while balancing resource use. Evaluated on the Waymo and ReasonSeg datasets, our method notably outperforms state-of-the-art baselines, demonstrating strong generalization and instruction adherence.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation</title>
<link>https://arxiv.org/abs/2509.13907</link>
<guid>https://arxiv.org/abs/2509.13907</guid>
<content:encoded><![CDATA[
arXiv:2509.13907v2 Announce Type: replace 
Abstract: Few-Shot 3D Point Cloud Semantic Segmentation (FS-PCS) aims to predict per-point labels for an unlabeled point cloud, given only a few labeled examples. To extract discriminative representations from the limited labeled set, existing methods have constructed prototypes using algorithms such as farthest point sampling (FPS). However, we point out that this convention has undesirable effects as performance fluctuates depending on sampling, while the prototype generation process remains underexplored in the field. This motivates us to investigate an advanced prototype generation method based on attention mechanism. Despite its potential, we found that vanilla attention module suffers from the distributional gap between prototypical tokens and support features. To overcome this, we propose White Aggregation and Restoration Module (WARM), which resolves the misalignment by sandwiching cross-attention between whitening and coloring transformations. Specifically, whitening aligns the features to tokens before the attention process, and coloring subsequently restores the original distribution to the attended tokens. This simple yet effective design enables robust attention, thereby generating prototypes that capture the semantic relationships in support features. WARM achieves state-of-the-art performance with a significant margin on FS-PCS benchmarks, and demonstrates its effectiveness through extensive experiments.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning neuroimaging models from health system-scale data</title>
<link>https://arxiv.org/abs/2509.18638</link>
<guid>https://arxiv.org/abs/2509.18638</guid>
<content:encoded><![CDATA[
arXiv:2509.18638v2 Announce Type: replace 
Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological diseases. The global demand for magnetic resonance imaging (MRI) studies has risen steadily, placing significant strain on health systems, prolonging turnaround times, and intensifying physician burnout. These challenges disproportionately impact patients in low-resource and rural settings. Here, we utilized a large academic health system as a data engine to develop Prima, the first vision language model (VLM) serving as an AI foundation for neuroimaging that supports real-world, clinical MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a hierarchical vision architecture that provides general and transferable MRI features. Prima was tested in a 1-year health system-wide study that included 30K MRI studies. Across 52 radiologic diagnoses from the major neurologic disorders, including neoplastic, inflammatory, infectious, and developmental lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0, outperforming other state-of-the-art general and medical AI models. Prima offers explainable differential diagnoses, worklist priority for radiologists, and clinical referral recommendations across diverse patient demographics and MRI systems. Prima demonstrates algorithmic fairness across sensitive groups and can help mitigate health system biases, such as prolonged turnaround times for low-resource populations. These findings highlight the transformative potential of health system-scale VLMs and Prima's role in advancing AI-driven healthcare.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Generation with Stable Transparency via Shiftable RGB-A Distribution Learner</title>
<link>https://arxiv.org/abs/2509.24979</link>
<guid>https://arxiv.org/abs/2509.24979</guid>
<content:encoded><![CDATA[
arXiv:2509.24979v3 Announce Type: replace 
Abstract: Generating RGB-A videos, which include alpha channels for transparency, has wide applications. However, current methods often suffer from low quality due to confusion between RGB and alpha. In this paper, we address this problem by learning shiftable RGB-A distributions. We adjust both the latent space and noise space, shifting the alpha distribution outward while preserving the RGB distribution, thereby enabling stable transparency generation without compromising RGB quality. Specifically, for the latent space, we propose a transparency-aware bidirectional diffusion loss during VAE training, which shifts the RGB-A distribution according to likelihood. For the noise space, we propose shifting the mean of diffusion noise sampling and applying a Gaussian ellipse mask to provide transparency guidance and controllability. Additionally, we construct a high-quality RGB-A video dataset. Compared to state-of-the-art methods, our model excels in visual quality, naturalness, transparency rendering, inference convenience, and controllability. The released model is available on our website: https://donghaotian123.github.io/Wan-Alpha/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training</title>
<link>https://arxiv.org/abs/2510.03189</link>
<guid>https://arxiv.org/abs/2510.03189</guid>
<content:encoded><![CDATA[
arXiv:2510.03189v3 Announce Type: replace 
Abstract: Interactive 3D biomedical image segmentation requires efficient models that can iteratively refine predictions based on user prompts. Current foundation models either lack volumetric awareness or suffer from limited interactive capabilities. We propose a training strategy that combines dynamic volumetric prompt generation with content-aware adaptive cropping to optimize the use of the image encoder. Our method simulates realistic user interaction patterns during training while addressing the computational challenges of learning from sequential refinement feedback on a single GPU. For efficient training, we initialize our network using the publicly available weights from the nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models for Interactive 3D Biomedical Image Segmentation} competition demonstrates strong performance with an average final Dice score of 0.6385, normalized surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice) and 2.5671 (NSD).
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking</title>
<link>https://arxiv.org/abs/2510.19981</link>
<guid>https://arxiv.org/abs/2510.19981</guid>
<content:encoded><![CDATA[
arXiv:2510.19981v2 Announce Type: replace 
Abstract: We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework that builds on existing 3D detectors by introducing a transformer-based smoother and a fusion-driven tracker. Inspired by query-based tracking frameworks, FutrTrack employs a multimodal two-stage transformer refinement and tracking pipeline. Our fusion tracker integrates bounding boxes with multimodal bird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without the need for an explicit motion model. The tracker assigns and propagates identities across frames, leveraging both geometric and semantic cues for robust re-identification under occlusion and viewpoint changes. Prior to tracking, we refine sequences of bounding boxes with a temporal smoother over a moving window to refine trajectories, reduce jitter, and improve spatial consistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that query-based transformer tracking methods benefit significantly from multimodal sensor features compared with previous single-sensor approaches. With an aMOTA of 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D MOT benchmarks, reducing identity switches while maintaining competitive accuracy. Our approach provides an efficient framework for improving transformer-based trackers to compete with other neural-network-based methods even with limited data and without pretraining.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations</title>
<link>https://arxiv.org/abs/2511.00456</link>
<guid>https://arxiv.org/abs/2511.00456</guid>
<content:encoded><![CDATA[
arXiv:2511.00456v4 Announce Type: replace 
Abstract: Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia affected regions. Furthermore, we evaluate seven pre-trained deep learning models including a Vision Transformer under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high accuracy (96-98%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V2 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations in this study confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel</title>
<link>https://arxiv.org/abs/2511.00981</link>
<guid>https://arxiv.org/abs/2511.00981</guid>
<content:encoded><![CDATA[
arXiv:2511.00981v2 Announce Type: replace 
Abstract: Accurate vessel segmentation is critical for clinical applications such as disease diagnosis and surgical planning, yet remains challenging due to thin, branching structures and low texture contrast. While foundation models like the Segment Anything Model (SAM) have shown promise in generic segmentation, they perform sub-optimally on vascular structures. In this work, we present VesSAM, a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM integrates (1) a convolutional adapter to enhance local texture features, (2) a multi-prompt encoder that fuses anatomical prompts, including skeletons, bifurcation points, and segment midpoints, via hierarchical cross-attention, and (3) a lightweight mask decoder to reduce jagged artifacts. We also introduce an automated pipeline to generate structured multi-prompt annotations, and curate a diverse benchmark dataset spanning 8 datasets across 5 imaging modalities. Experimental results demonstrate that VesSAM consistently outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13% IoU, and achieves competitive performance compared to fully fine-tuned methods, with significantly fewer parameters. VesSAM also generalizes well to out-of-distribution (OoD) settings, outperforming all baselines in average OoD Dice and IoU.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement</title>
<link>https://arxiv.org/abs/2511.01510</link>
<guid>https://arxiv.org/abs/2511.01510</guid>
<content:encoded><![CDATA[
arXiv:2511.01510v2 Announce Type: replace 
Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing reconstruction fidelity with cross-scenario generalization. While existing methods predominantly focus on deterministic pixel-level mappings between paired low/normal-light images, they often neglect the continuous physical process of luminance transitions in real-world environments, leading to performance drop when normal-light references are unavailable. Inspired by empirical analysis of natural luminance dynamics revealing power-law distributed intensity transitions, this paper introduces Luminance-Aware Statistical Quantification (LASQ), a novel framework that reformulates LLIE as a statistical sampling process over hierarchical luminance distributions. Our LASQ re-conceptualizes luminance transition as a power-law distribution in intensity coordinate space that can be approximated by stratified power functions, therefore, replacing deterministic mappings with probabilistic sampling over continuous luminance layers. A diffusion forward process is designed to autonomously discover optimal transition paths between luminance layers, achieving unsupervised distribution emulation without normal-light references. In this way, it considerably improves the performance in practical situations, enabling more adaptable and versatile light restoration. This framework is also readily applicable to cases with normal-light references, where it achieves superior performance on domain-specific datasets alongside better generalization-ability across non-reference datasets.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control</title>
<link>https://arxiv.org/abs/2511.02483</link>
<guid>https://arxiv.org/abs/2511.02483</guid>
<content:encoded><![CDATA[
arXiv:2511.02483v3 Announce Type: replace 
Abstract: We introduce OLATverse, a large-scale dataset comprising around 9M images of 765 real-world objects, captured from multiple viewpoints under a diverse set of precisely controlled lighting conditions. While recent advances in object-centric inverse rendering, novel view synthesis and relighting have shown promising results, most techniques still heavily rely on the synthetic datasets for training and small-scale real-world datasets for benchmarking, which limits their realism and generalization. To address this gap, OLATverse offers two key advantages over existing datasets: large-scale coverage of real objects and high-fidelity appearance under precisely controlled illuminations. Specifically, OLATverse contains 765 common and uncommon real-world objects, spanning a wide range of material categories. Each object is captured using 35 DSLR cameras and 331 individually controlled light sources, enabling the simulation of diverse illumination conditions. In addition, for each object, we provide well-calibrated camera parameters, accurate object masks, photometric surface normals, and diffuse albedo as auxiliary resources. We also construct an extensive evaluation set, establishing the first comprehensive real-world object-centric benchmark for inverse rendering and normal estimation. We believe that OLATverse represents a pivotal step toward integrating the next generation of inverse rendering and relighting methods with real-world data. The full dataset, along with all post-processing workflows, will be publicly released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes</title>
<link>https://arxiv.org/abs/2511.02503</link>
<guid>https://arxiv.org/abs/2511.02503</guid>
<content:encoded><![CDATA[
arXiv:2511.02503v2 Announce Type: replace 
Abstract: The automation of workflows in advanced microscopy is a key goal where foundation models like Language Models (LLMs) and Vision-Language Models (VLMs) show great potential. However, adapting these general-purpose models for specialized scientific tasks is critical, and the optimal domain adaptation strategy is often unclear. To address this, we introduce PtychoBench, a new multi-modal, multi-task benchmark for ptychographic analysis. Using this benchmark, we systematically compare two specialization strategies: Supervised Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies on a visual artifact detection task with VLMs and a textual parameter recommendation task with LLMs in a data-scarce regime. Our findings reveal that the optimal specialization pathway is task-dependent. For the visual task, SFT and ICL are highly complementary, with a fine-tuned model guided by context-aware examples achieving the highest mean performance (Micro-F1 of 0.728). Conversely, for the textual task, ICL on a large base model is the superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a powerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirm the superiority of context-aware prompting and identify a consistent contextual interference phenomenon in fine-tuned models. These results, benchmarked against strong baselines including GPT-4o and a DINOv3-based classifier, offer key observations for AI in science: the optimal specialization path in our benchmark is dependent on the task modality, offering a clear framework for developing more effective science-based agentic systems.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v3 Announce Type: replace 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases</title>
<link>https://arxiv.org/abs/2511.15656</link>
<guid>https://arxiv.org/abs/2511.15656</guid>
<content:encoded><![CDATA[
arXiv:2511.15656v3 Announce Type: replace 
Abstract: Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransientTrack: Advanced Multi-Object Tracking and Classification of Cancer Cells with Transient Fluorescent Signals</title>
<link>https://arxiv.org/abs/2512.01885</link>
<guid>https://arxiv.org/abs/2512.01885</guid>
<content:encoded><![CDATA[
arXiv:2512.01885v2 Announce Type: replace 
Abstract: Tracking cells in time-lapse videos is an essential technique for monitoring cell population dynamics at a single-cell level. Current methods for cell tracking are developed on videos with mostly single, constant signals and do not detect pivotal events such as cell death. Here, we present TransientTrack, a deep learning-based framework for cell tracking in multi-channel microscopy video data with transient fluorescent signals that fluctuate over time following processes such as the circadian rhythm of cells. By identifying key cellular events - mitosis (cell division) and apoptosis (cell death) our method allows us to build complete trajectories, including cell lineage information. TransientTrack is lightweight and performs matching on cell detection embeddings directly, without the need for quantification of tracking-specific cell features. Furthermore, our approach integrates Transformer Networks, multi-stage matching using all detection boxes, and the interpolation of missing tracklets with the Kalman Filter. This unified framework achieves strong performance across diverse conditions, effectively tracking cells and capturing cell division and death. We demonstrate the use of TransientTrack in an analysis of the efficacy of a chemotherapeutic drug at a single-cell level. The proposed framework could further advance quantitative studies of cancer cell dynamics, enabling detailed characterization of treatment response and resistance mechanisms. The code is available at https://github.com/bozeklab/TransientTrack.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management</title>
<link>https://arxiv.org/abs/2512.04540</link>
<guid>https://arxiv.org/abs/2512.04540</guid>
<content:encoded><![CDATA[
arXiv:2512.04540v2 Announce Type: replace 
Abstract: Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARDis: Time Attenuated Representation Disentanglement for Incomplete Multi-Modal Tumor Segmentation and Classification</title>
<link>https://arxiv.org/abs/2512.04576</link>
<guid>https://arxiv.org/abs/2512.04576</guid>
<content:encoded><![CDATA[
arXiv:2512.04576v2 Announce Type: replace 
Abstract: The accurate diagnosis and segmentation of tumors in contrast-enhanced Computed Tomography (CT) are fundamentally driven by the distinctive hemodynamic profiles of contrast agents over time. However, in real-world clinical practice, complete temporal dynamics are often hard to capture by strict radiation dose limits and inconsistent acquisition protocols across institutions, leading to a prevalent missing modality problem. Existing deep learning approaches typically treat missing phases as absent independent channels, ignoring the inherent temporal continuity of hemodynamics. In this work, we propose Time Attenuated Representation Disentanglement (TARDis), a novel physics-aware framework that redefines missing modalities as missing sample points on a continuous Time-Attenuation Curve. We first hypothesize that the latent feature can be disentangled into a time-invariant static component (anatomy) and a time-dependent dynamic component (perfusion). We achieve this via a dual-path architecture: a quantization-based path using a learnable embedding dictionary to extract consistent anatomical structures, and a probabilistic path using a Hemodynamic Conditional Variational Autoencoder to model dynamic enhancement conditioned on the estimated scan time. This design allows the network to infer missing hemodynamic features by sampling from the learned latent distribution. Extensive experiments on a large-scale multi-modal private abdominal CT dataset (2,282 patients) and two public datasets demonstrate that TARDis significantly outperforms state-of-the-art incomplete modality frameworks. Notably, our method maintains robust diagnostic performance even in extreme data-sparsity scenarios, highlighting its potential for reducing radiation exposure while maintaining diagnostic precision.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM3-I: Segment Anything with Instructions</title>
<link>https://arxiv.org/abs/2512.04585</link>
<guid>https://arxiv.org/abs/2512.04585</guid>
<content:encoded><![CDATA[
arXiv:2512.04585v2 Announce Type: replace 
Abstract: Segment Anything Model 3 (SAM3) has advanced open-vocabulary segmentation through promptable concept segmentation, allowing users to segment all instances corresponding to a given concept, typically specified with short noun-phrase (NP) prompts. While this marks the first integration of language-level concepts within the SAM family, real-world usage typically requires far richer expressions that include attributes, spatial relations, functionalities, actions, states, and even implicit reasoning over instances. Currently, SAM3 relies on external multi-modal agents to convert complex instructions into NPs and then conduct iterative mask filtering. However, these NP-level concepts remain overly coarse, often failing to precisely represent a specific instance. In this work, we present SAM3-I, an enhanced framework that unifies concept-level understanding and instruction-level reasoning within the SAM family. SAM3-I introduces an instruction-aware cascaded adaptation mechanism that progressively aligns expressive instruction semantics with SAM3's existing vision-language representations, enabling direct instruction-following segmentation without sacrificing its original concept-driven capabilities. Furthermore, we design a structured instruction taxonomy spanning concept, simple, and complex levels, and develop a scalable data engine to construct a dataset with diverse instruction-mask pairs. Experiments show that SAM3-I delivers appealing performance, demonstrating that SAM3 can be effectively extended to follow natural-language instructions while preserving its strong concept grounding. We open-source SAM3-I and provide practical fine-tuning workflows, enabling researchers to adapt it to domain-specific applications. The source code is available here.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Order Matters: 3D Shape Generation from Sequential VR Sketches</title>
<link>https://arxiv.org/abs/2512.04761</link>
<guid>https://arxiv.org/abs/2512.04761</guid>
<content:encoded><![CDATA[
arXiv:2512.04761v2 Announce Type: replace 
Abstract: VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HQ-DM: Single Hadamard Transformation-Based Quantization-Aware Training for Low-Bit Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05746</link>
<guid>https://arxiv.org/abs/2512.05746</guid>
<content:encoded><![CDATA[
arXiv:2512.05746v2 Announce Type: replace 
Abstract: Diffusion models have demonstrated significant applications in the field of image generation. However, their high computational and memory costs pose challenges for deployment. Model quantization has emerged as a promising solution to reduce storage overhead and accelerate inference. Nevertheless, existing quantization methods for diffusion models struggle to mitigate outliers in activation matrices during inference, leading to substantial performance degradation under low-bit quantization scenarios. To address this, we propose HQ-DM, a novel Quantization-Aware Training framework that applies Single Hadamard Transformation to activation matrices. This approach effectively reduces activation outliers while preserving model performance under quantization. Compared to traditional Double Hadamard Transformation, our proposed scheme offers distinct advantages by seamlessly supporting INT convolution operations while preventing the amplification of weight outliers. For conditional generation on the ImageNet 256x256 dataset using the LDM-4 model, our W4A4 and W4A3 quantization schemes improve the Inception Score by 12.8% and 467.73%, respectively, over the existing state-of-the-art method.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing Diffusion Priors via 3D Attention for Consistent Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.07345</link>
<guid>https://arxiv.org/abs/2512.07345</guid>
<content:encoded><![CDATA[
arXiv:2512.07345v2 Announce Type: replace 
Abstract: Versatile 3D tasks (e.g., generation or editing) that distill from Text-to-Image (T2I) diffusion models have attracted significant research interest for not relying on extensive 3D training data. However, T2I models exhibit limitations resulting from prior view bias, which produces conflicting appearances between different views of an object. This bias causes subject-words to preferentially activate prior view features during cross-attention (CA) computation, regardless of the target view condition. To overcome this limitation, we conduct a comprehensive mathematical analysis to reveal the root cause of the prior view bias in T2I models. Moreover, we find different UNet layers show different effects of prior view in CA. Therefore, we propose a novel framework, TD-Attn, which addresses multi-view inconsistency via two key components: (1) the 3D-Aware Attention Guidance Module (3D-AAG) constructs a view-consistent 3D attention Gaussian for subject-words to enforce spatial consistency across attention-focused regions, thereby compensating for the limited spatial information in 2D individual view CA maps; (2) the Hierarchical Attention Modulation Module (HAM) utilizes a Semantic Guidance Tree (SGT) to direct the Semantic Response Profiler (SRP) in localizing and modulating CA layers that are highly responsive to view conditions, where the enhanced CA maps further support the construction of more consistent 3D attention Gaussians. Notably, HAM facilitates semantic-specific interventions, enabling controllable and precise 3D editing. Extensive experiments firmly establish that TD-Attn has the potential to serve as a universal plugin, significantly enhancing multi-view consistency across 3D tasks.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UnCageNet: Tracking and Pose Estimation of Caged Animal</title>
<link>https://arxiv.org/abs/2512.07712</link>
<guid>https://arxiv.org/abs/2512.07712</guid>
<content:encoded><![CDATA[
arXiv:2512.07712v2 Announce Type: replace 
Abstract: Animal tracking and pose estimation systems, such as STEP (Simultaneous Tracking and Pose Estimation) and ViTPose, experience substantial performance drops when processing images and videos with cage structures and systematic occlusions. We present a three-stage preprocessing pipeline that addresses this limitation through: (1) cage segmentation using a Gabor-enhanced ResNet-UNet architecture with tunable orientation filters, (2) cage inpainting using CRFill for content-aware reconstruction of occluded regions, and (3) evaluation of pose estimation and tracking on the uncaged frames. Our Gabor-enhanced segmentation model leverages orientation-aware features with 72 directional kernels to accurately identify and segment cage structures that severely impair the performance of existing methods. Experimental validation demonstrates that removing cage occlusions through our pipeline enables pose estimation and tracking performance comparable to that in environments without occlusions. We also observe significant improvements in keypoint detection accuracy and trajectory consistency.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenVE-3M: A Large-Scale High-Quality Dataset for Instruction-Guided Video Editing</title>
<link>https://arxiv.org/abs/2512.07826</link>
<guid>https://arxiv.org/abs/2512.07826</guid>
<content:encoded><![CDATA[
arXiv:2512.07826v2 Announce Type: replace 
Abstract: The quality and diversity of instruction-based image editing datasets are continuously increasing, yet large-scale, high-quality datasets for instruction-based video editing remain scarce. To address this gap, we introduce OpenVE-3M, an open-source, large-scale, and high-quality dataset for instruction-based video editing. It comprises two primary categories: spatially-aligned edits (Global Style, Background Change, Local Change, Local Remove, Local Add, and Subtitles Edit) and non-spatially-aligned edits (Camera Multi-Shot Edit and Creative Edit). All edit types are generated via a meticulously designed data pipeline with rigorous quality filtering. OpenVE-3M surpasses existing open-source datasets in terms of scale, diversity of edit types, instruction length, and overall quality. Furthermore, to address the lack of a unified benchmark in the field, we construct OpenVE-Bench, containing 431 video-edit pairs that cover a diverse range of editing tasks with three key metrics highly aligned with human judgment. We present OpenVE-Edit, a 5B model trained on our dataset that demonstrates remarkable efficiency and effectiveness by setting a new state-of-the-art on OpenVE-Bench, outperforming all prior open-source models including a 14B baseline. Project page is at https://lewandofskee.github.io/projects/OpenVE.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation</title>
<link>https://arxiv.org/abs/2512.07829</link>
<guid>https://arxiv.org/abs/2512.07829</guid>
<content:encoded><![CDATA[
arXiv:2512.07829v2 Announce Type: replace 
Abstract: Visual generative models (e.g., diffusion models) typically operate in compressed latent spaces to balance training efficiency and sample quality. In parallel, there has been growing interest in leveraging high-quality pre-trained visual representations, either by aligning them inside VAEs or directly within the generative model. However, adapting such representations remains challenging due to fundamental mismatches between understanding-oriented features and generation-friendly latent spaces. Representation encoders benefit from high-dimensional latents that capture diverse hypotheses for masked regions, whereas generative models favor low-dimensional latents that must faithfully preserve injected noise. This discrepancy has led prior work to rely on complex objectives and architectures. In this work, we propose FAE (Feature Auto-Encoder), a simple yet effective framework that adapts pre-trained visual representations into low-dimensional latents suitable for generation using as little as a single attention layer, while retaining sufficient information for both reconstruction and understanding. The key is to couple two separate deep decoders: one trained to reconstruct the original feature space, and a second that takes the reconstructed features as input for image generation. FAE is generic; it can be instantiated with a variety of self-supervised encoders (e.g., DINO, SigLIP) and plugged into two distinct generative families: diffusion models and normalizing flows. Across class-conditional and text-to-image benchmarks, FAE achieves strong performance. For example, on ImageNet 256x256, our diffusion model with CFG attains a near state-of-the-art FID of 1.29 (800 epochs) and 1.70 (80 epochs). Without CFG, FAE reaches the state-of-the-art FID of 1.48 (800 epochs) and 2.08 (80 epochs), demonstrating both high quality and fast learning.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Representation Learning from Sparse Transformation Analysis</title>
<link>https://arxiv.org/abs/2410.05564</link>
<guid>https://arxiv.org/abs/2410.05564</guid>
<content:encoded><![CDATA[
arXiv:2410.05564v2 Announce Type: replace-cross 
Abstract: There is a vast literature on representation learning based on principles such as coding efficiency, statistical independence, causality, controllability, or symmetry. In this paper we propose to learn representations from sequence data by factorizing the transformations of the latent variables into sparse components. Input data are first encoded as distributions of latent activations and subsequently transformed using a probability flow model, before being decoded to predict a future input state. The flow model is decomposed into a number of rotational (divergence-free) vector fields and a number of potential flow (curl-free) fields. Our sparsity prior encourages only a small number of these fields to be active at any instant and infers the speed with which the probability flows along these fields. Training this model is completely unsupervised using a standard variational objective and results in a new form of disentangled representations where the input is not only represented by a combination of independent factors, but also by a combination of independent transformation primitives given by the learned flow fields. When viewing the transformations as symmetries one may interpret this as learning approximately equivariant representations. Empirically we demonstrate that this model achieves state of the art in terms of both data likelihood and unsupervised approximate equivariance errors on datasets composed of sequence transformations.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Translating Electrocardiograms to Cardiac Magnetic Resonance Imaging Useful for Cardiac Assessment and Disease Screening: A Multi-Center Study</title>
<link>https://arxiv.org/abs/2411.13602</link>
<guid>https://arxiv.org/abs/2411.13602</guid>
<content:encoded><![CDATA[
arXiv:2411.13602v3 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are the leading cause of global mortality, necessitating accessible and accurate diagnostic tools. While cardiac magnetic resonance imaging (CMR) provides gold-standard insights into cardiac structure and function, its clinical utility is limited by high cost and complexity. In contrast, electrocardiography (ECG) is inexpensive and widely available but lacks the granularity of CMR. We propose CardioNets, a deep learning framework that translates 12-lead ECG signals into CMR-level functional parameters and synthetic images, enabling scalable cardiac assessment. CardioNets integrates cross-modal contrastive learning and generative pretraining, aligning ECG with CMR-derived cardiac phenotypes and synthesizing high-resolution CMR images via a masked autoregressive model. Trained on 159,819 samples from five cohorts, including the UK Biobank (n=42,483) and MIMIC-IV-ECG (n=164,550), and externally validated on independent clinical datasets (n=3,767), CardioNets achieved strong performance across disease screening and phenotype estimation tasks. In the UK Biobank, it improved cardiac phenotype regression R2 by 24.8% and cardiomyopathy AUC by up to 39.3% over baseline models. In MIMIC, it increased AUC for pulmonary hypertension detection by 5.6%. Generated CMR images showed 36.6% higher SSIM and 8.7% higher PSNR than prior approaches. In a reader study, ECG-only CardioNets achieved 13.9% higher accuracy than human physicians using both ECG and real CMR. These results suggest that CardioNets offers a promising, low-cost alternative to CMR for large-scale CVD screening, particularly in resource-limited settings. Future efforts will focus on clinical deployment and regulatory validation of ECG-based synthetic imaging.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Grounded Monocular Depth via Nanophotonic Wavefront Prompting</title>
<link>https://arxiv.org/abs/2503.15770</link>
<guid>https://arxiv.org/abs/2503.15770</guid>
<content:encoded><![CDATA[
arXiv:2503.15770v2 Announce Type: replace-cross 
Abstract: Depth foundation models offer strong learned priors for 3D perception but lack physical depth cues, leading to ambiguities in metric scale. We introduce a birefringent metalens -- a planar nanophotonic lens composed of subwavelength pixels for wavefront shaping with a thickness of 700 nm and a diameter of 3 mm -- to physically prompt depth foundation models. In a single monocular shot, our metalens physically embeds depth information into two polarized optical wavefronts, which we decode through a lightweight prompting and fine-tuning framework that aligns depth foundation models with the optical signals. To scale the training data, we develop a light wave propagation simulator that synthesizes metalens responses from RGB-D datasets, incorporating key physical factors to minimize the sim-to-real gap. Simulated and physical experiments with our fabricated titanium-dioxide metalens demonstrate accurate and consistent metric depth over state-of-the-art monocular depth estimators. The research demonstrates that nanophotonic wavefront formation offers a promising bridge for grounding depth foundation models in physical depth sensing.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data</title>
<link>https://arxiv.org/abs/2505.02677</link>
<guid>https://arxiv.org/abs/2505.02677</guid>
<content:encoded><![CDATA[
arXiv:2505.02677v2 Announce Type: replace-cross 
Abstract: Stroke is a major public health problem, affecting millions worldwide. Deep learning has recently demonstrated promise for enhancing the diagnosis and risk prediction of stroke. However, existing methods rely on costly medical imaging modalities, such as computed tomography. Recent studies suggest that retinal imaging could offer a cost-effective alternative for cerebrovascular health assessment due to the shared clinical pathways between the retina and the brain. Hence, this study explores the impact of leveraging retinal images and clinical data for stroke detection and risk prediction. We propose a multimodal deep neural network that processes Optical Coherence Tomography (OCT) and infrared reflectance retinal scans, combined with clinical data, such as demographics, vital signs, and diagnosis codes. We pretrained our model using a self-supervised learning framework using a real-world dataset consisting of $37$ k scans, and then fine-tuned and evaluated the model using a smaller labeled subset. Our empirical findings establish the predictive ability of the considered modalities in detecting lasting effects in the retina associated with acute stroke and forecasting future risk within a specific time horizon. The experimental results demonstrate the effectiveness of our proposed framework by achieving $5$\% AUROC improvement as compared to the unimodal image-only baseline, and $8$\% improvement compared to an existing state-of-the-art foundation model. In conclusion, our study highlights the potential of retinal imaging in identifying high-risk patients and improving long-term outcomes.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RTR-GS: 3D Gaussian Splatting for Inverse Rendering with Radiance Transfer and Reflection</title>
<link>https://arxiv.org/abs/2507.07733</link>
<guid>https://arxiv.org/abs/2507.07733</guid>
<content:encoded><![CDATA[
arXiv:2507.07733v2 Announce Type: replace-cross 
Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities in novel view synthesis. However, rendering reflective objects remains a significant challenge, particularly in inverse rendering and relighting. We introduce RTR-GS, a novel inverse rendering framework capable of robustly rendering objects with arbitrary reflectance properties, decomposing BRDF and lighting, and delivering credible relighting results. Given a collection of multi-view images, our method effectively recovers geometric structure through a hybrid rendering model that combines forward rendering for radiance transfer with deferred rendering for reflections. This approach successfully separates high-frequency and low-frequency appearances, mitigating floating artifacts caused by spherical harmonic overfitting when handling high-frequency details. We further refine BRDF and lighting decomposition using an additional physically-based deferred rendering branch. Experimental results show that our method enhances novel view synthesis, normal estimation, decomposition, and relighting while maintaining efficient training inference process.
]]></content:encoded>
<pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefBench-PRO: Perceptual and Reasoning Oriented Benchmark for Referring Expression Comprehension</title>
<link>https://arxiv.org/abs/2512.06276</link>
<guid>https://arxiv.org/abs/2512.06276</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Expression Comprehension, Multi-modal Large Language Model, RefBench-PRO, Dynamic IoU-based GRPO, Reinforcement Learning

<br /><br />Summary:  
This paper addresses limitations in existing Referring Expression Comprehension (REC) benchmarks, which mainly test perceptual ability and lack interpretable scoring to assess multi-modal large language models' (MLLMs) grounding capabilities across cognitive domains. To overcome this, the authors introduce RefBench-PRO, a new and comprehensive REC benchmark that decomposes referring expressions into two main dimensions—perception and reasoning—and further divides these into six progressively challenging sub-tasks: attribute recognition, position identification, interaction understanding, commonsense reasoning, relational comprehension, and rejection tasks. They develop a fully automated data generation pipeline that creates diverse referring expressions aligned with these six sub-dimensions, enabling broad and detailed evaluation. Additionally, the paper proposes Ref-R1, a reinforcement learning-based training scheme that implements Dynamic IoU-based GRPO to enhance localization accuracy when handling complex reasoning requirements. This approach establishes a stronger baseline for REC performance. Extensive experiments show that RefBench-PRO facilitates interpretable evaluations of MLLMs on referring expression comprehension, revealing performance gaps in both perceptual and reasoning abilities, and thus presents greater challenges for future research in this area. <div>
arXiv:2512.06276v2 Announce Type: replace 
Abstract: Referring Expression Comprehension (REC) is a vision-language task that localizes a specific image region based on a textual description. Existing REC benchmarks primarily evaluate perceptual capabilities and lack interpretable scoring mechanisms, which cannot reveal the grounding capability of Multi-modal Large Language Model (MLLM) across different cognitive abilities. To address this limitation, we introduce RefBench-PRO, a comprehensive REC benchmark, which decomposes referring expressions into two core dimensions, i.e., perception and reasoning, and further subdivides them into six progressively challenging tasks, such as attribute, position, interaction, commonsense, relation and reject. We also develop a fully automated data-generation pipeline that produces diverse referring expressions across these six sub-dimensions. Furthermore, We propose Ref-R1, an RL-based learning scheme, which incorporates Dynamic IoU-based GRPO to improve localization accuracy under increasingly complex reasoning conditions, establishing a stronger baseline for REC. Extensive experiments demonstrate that our RefBench-PRO enables interpretable evaluation of MLLM on referring expression comprehension, presenting greater challenges in both perception and reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDOT: Efficient Unified Video Creation via Optimal Transport Distillation</title>
<link>https://arxiv.org/abs/2512.06802</link>
<guid>https://arxiv.org/abs/2512.06802</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, video creation, distribution matching distillation, optimal transport, unified benchmark<br /><br />Summary:  
The paper addresses challenges in video creation using generative models, focusing on the inefficiency and limited scope of existing methods. To improve practicality, the authors introduce VDOT, an efficient unified video creation model. VDOT employs a distribution matching distillation (DMD) paradigm enhanced with a novel computational optimal transport (OT) technique rather than relying solely on Kullback-Leibler (KL) divergence minimization. The OT distance introduces geometric constraints that prevent issues like zero-forcing and gradient collapse during few-step generation, boosting distillation efficiency and stability. Additionally, the model incorporates a discriminator to help it better perceive real video data, which leads to higher quality generated videos. To enable training of unified video creation models across multiple tasks, the authors develop a fully automated pipeline for video annotation and filtering. Furthermore, they curate a unified evaluation benchmark, UVCBench, to standardize performance assessment. Experimental results demonstrate that VDOT achieves comparable or better outcomes than methods using 100 denoising steps, despite operating with only 4 steps, highlighting its significant improvements in speed and quality for video generation tasks. <div>
arXiv:2512.06802v2 Announce Type: replace 
Abstract: The rapid development of generative models has significantly advanced image and video applications. Among these, video creation, aimed at generating videos under various conditions, has gained substantial attention. However, existing video creation models either focus solely on a few specific conditions or suffer from excessively long generation times due to complex model inference, making them impractical for real-world applications. To mitigate these issues, we propose an efficient unified video creation model, named VDOT. Concretely, we model the training process with the distribution matching distillation (DMD) paradigm. Instead of using the Kullback-Leibler (KL) minimization, we additionally employ a novel computational optimal transport (OT) technique to optimize the discrepancy between the real and fake score distributions. The OT distance inherently imposes geometric constraints, mitigating potential zero-forcing or gradient collapse issues that may arise during KL-based distillation within the few-step generation scenario, and thus, enhances the efficiency and stability of the distillation process. Further, we integrate a discriminator to enable the model to perceive real video data, thereby enhancing the quality of generated videos. To support training unified video creation models, we propose a fully automated pipeline for video data annotation and filtering that accommodates multiple video creation tasks. Meanwhile, we curate a unified testing benchmark, UVCBench, to standardize evaluation. Experiments demonstrate that our 4-step VDOT outperforms or matches other baselines with 100 denoising steps.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics</title>
<link>https://arxiv.org/abs/2512.07155</link>
<guid>https://arxiv.org/abs/2512.07155</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Image morphing, Adaptive Cache Injection, Semantic Anchor Prompting, Global-Local Consistency Score  

<br /><br />Summary:  
This paper introduces CHIMERA, a novel zero-shot diffusion-based framework designed to achieve smooth and semantically consistent image morphing, addressing challenges faced by existing methods such as abrupt transitions and over-saturation. CHIMERA formulates the morphing process as a cached inversion-guided denoising operation, allowing better control over feature fusion. To handle large semantic and appearance disparities between images, the method employs Adaptive Cache Injection (ACI), which caches and adaptively re-injects features from multiple layers (down, mid, up blocks) during diffusion denoising. This enables spatial and semantic alignment that adapts dynamically with respect to depth and denoising time, facilitating natural and smooth transitions. In addition, CHIMERA uses Semantic Anchor Prompting (SAP), which leverages vision-language models to create a shared semantic anchor prompt that bridges dissimilar inputs and guides the denoising toward coherent intermediate results. The paper further proposes the Global-Local Consistency Score (GLCS), a novel quantitative metric to evaluate both the global harmony of the input images and the smoothness of the local morphing transition. Extensive experiments and user studies demonstrate that CHIMERA outperforms current state-of-the-art methods, producing more natural and semantically aligned morphing sequences. The authors plan to publicly release the code and project resources. <div>
arXiv:2512.07155v3 Announce Type: replace 
Abstract: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Pose Problems: Reasoning-Driven and Solver-Adaptive Data Synthesis for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2511.09907</link>
<guid>https://arxiv.org/abs/2511.09907</guid>
<content:encoded><![CDATA[
<div> Data synthesis, problem generation, reasoning model, adaptive difficulty, co-evolution  

<br /><br />Summary:  
This paper addresses the challenges in data synthesis for training large reasoning models, focusing on improving problem generation quality and relevance. First, it highlights the issues in current methods, such as indiscriminate problem generation ignoring solver ability and lack of reasoning in the creation process, which results in low-value or shallow problems. Second, the authors propose a novel problem generator that explicitly reasons to plan problem directions before synthesis and adapts the difficulty based on the solver’s current ability. Third, the method involves constructing related problem pairs augmented with intermediate problem-design chain-of-thought (CoT) generated by a reasoning model, which helps bootstrap problem-design strategies. Fourth, solver feedback on synthetic problems is used as a reward signal to calibrate difficulty dynamically, producing problems that stay at the edge of the solver’s competence and complement its training. Fifth, extensive experiments on 10 mathematical and general reasoning benchmarks demonstrate an average performance improvement of 2.5% and the method’s ability to generalize across language and vision-language models. Additionally, the approach enables a co-evolutionary training process, where the improved solver provides better rewards for further generator training, yielding an additional 0.7% gain. The authors plan to publicly release their code to support further research. <div>
arXiv:2511.09907v3 Announce Type: replace-cross 
Abstract: Data synthesis for training large reasoning models offers a scalable alternative to limited, human-curated datasets, enabling the creation of high-quality data. However, existing approaches face several challenges: (i) indiscriminate generation that ignores the solver's ability and yields low-value problems, or reliance on complex data pipelines to balance problem difficulty; and (ii) a lack of reasoning in problem generation, leading to shallow problem variants. In this paper, we develop a problem generator that reasons explicitly to plan problem directions before synthesis and adapts difficulty to the solver's ability. Specifically, we construct related problem pairs and augment them with intermediate problem-design CoT produced by a reasoning model. These data bootstrap problem-design strategies from the generator. Then, we treat the solver's feedback on synthetic problems as a reward signal, enabling the generator to calibrate difficulty and produce complementary problems near the edge of the solver's competence. Extensive experiments on 10 mathematical and general reasoning benchmarks show that our method achieves an average improvement of 2.5% and generalizes to both language and vision-language models. Moreover, a solver trained on the synthesized data provides improved rewards for continued generator training, enabling co-evolution and yielding a further 0.7% performance gain. Our code will be made publicly available here.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.11865</link>
<guid>https://arxiv.org/abs/2512.11865</guid>
<content:encoded><![CDATA[
<div> Smart farming, RGB cameras, photometric perturbations, adversarial attacks, Vision-Language-Action model<br /><br />Summary:<br /><br />Smart farming utilizes automation and intelligent control to enhance modern agriculture. However, current systems that depend on RGB cameras for perception and robotic manipulators for control are vulnerable to photometric perturbations like hue shifts, illumination changes, and noise, which may lead to malfunctions, especially under adversarial attacks. To mitigate these vulnerabilities, the paper proposes an explainable adversarial-robust Vision-Language-Action model built on the OpenVLA-OFT framework. A key innovation is the integration of the Evidence-3 module, designed to detect photometric perturbations and provide natural language explanations outlining their causes and effects. Experimental results demonstrate the effectiveness of the proposed model, showing a reduction in Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to a baseline model. This indicates that the approach not only enhances the accuracy of action prediction in smart farming systems but also improves the explainability of the model's decisions under challenging adversarial conditions, contributing to more reliable and transparent automation in agriculture. <div>
arXiv:2512.11865v1 Announce Type: new 
Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion</title>
<link>https://arxiv.org/abs/2512.11869</link>
<guid>https://arxiv.org/abs/2512.11869</guid>
<content:encoded><![CDATA[
<div> Monocular 3D lane detection, Temporal LSTM Fusion, Anchor3DLane, multi-task loss, temporal consistency<br /><br />Summary:<br /><br />1) The paper addresses significant challenges in monocular 3D lane detection, specifically depth ambiguity, occlusion, and temporal instability across video frames.<br />2) It builds upon the Anchor3DLane model, which performs regression of continuous 3D lane curves from multi-camera surround views but has limitations such as sensitivity to regression outliers, insufficient global curve supervision, difficulty balancing multiple loss terms, and underutilization of temporal continuity.<br />3) The authors propose Temporal-Anchor3DLane, which introduces several key improvements: enhanced multi-task loss functions including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, combined with focal and Dice losses for classification and visibility tasks.<br />4) A lightweight Temporal LSTM Fusion module is developed to aggregate per-anchor features across frames, replacing heavier Transformer-based temporal fusion to improve efficiency.<br />5) The training scheme is refined using ESCOP-style techniques that enforce curve-level supervision alongside temporal consistency, yielding improved robustness.<br />6) Experimental results on the OpenLane dataset show a significant F1 score improvement of +6.2 and smoother lane trajectory predictions.<br />7) These enhancements demonstrate that relatively small architectural and loss refinements can substantially boost 3D lane detection robustness without requiring additional sensors or larger model scaling. <div>
arXiv:2512.11869v1 Announce Type: new 
Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops</title>
<link>https://arxiv.org/abs/2512.11871</link>
<guid>https://arxiv.org/abs/2512.11871</guid>
<content:encoded><![CDATA[
<div> Keywords: agriculture, crop disease detection, cactus-fig, mobile-efficient architectures, offline inference<br /><br />Summary:<br /><br />1. The study focuses on the Tigray region of Ethiopia, where over 80% of the population depends on agriculture but faces challenges in accessing expert crop disease diagnosis due to infrastructural disruptions.  
2. Researchers created a new indigenous cactus-fig (Opuntia ficus-indica) dataset with 3,587 field images categorized into three symptom classes to support disease detection.  
3. Considering deployment constraints in resource-limited, post-conflict edge environments, three mobile-efficient models were evaluated: a custom lightweight CNN, EfficientNet-Lite1, and the hybrid CNN-Transformer MobileViT-XS.  
4. The research isolated cactus-fig model performance from other crops (potato, apple, corn) to specifically study attention sensitivity and inductive bias transfer relevant to the unique morphology of indigenous plants.  
5. Results revealed a Pareto trade-off between accuracy and deployment efficiency: EfficientNet-Lite1 achieved 90.7% test accuracy; the lightweight CNN offered 89.5% accuracy with the best latency (42 ms) and smallest model size (4.8 MB); MobileViT-XS attained the highest mean cross-validation accuracy of 97.3% by leveraging MHSA-based global reasoning, effectively differentiating pest clusters and fungal lesions.  
6. The trained ARM-compatible models are integrated into a Flutter application localized in Tigrigna and Amharic, capable of fully offline inference on Cortex-A53 devices, promoting inclusive, food security critical diagnostics in the region. <div>
arXiv:2512.11871v1 Announce Type: new 
Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training</title>
<link>https://arxiv.org/abs/2512.11874</link>
<guid>https://arxiv.org/abs/2512.11874</guid>
<content:encoded><![CDATA[
<div> Keywords: self-training, semantic segmentation, SegFormer, teacher-student loop, data augmentation  

<br /><br />Summary:  
This extended abstract presents a solution developed for the Global Wheat Full Semantic Segmentation Competition. The authors designed a systematic self-training framework aimed at enhancing model performance through effective data utilization. Central to their approach is a two-stage hybrid training strategy combined with extensive data augmentation techniques to improve generalization. The core model employed is SegFormer, which uses a Mix Transformer (MiT-B4) backbone known for its strong feature representation abilities. To further refine accuracy, the team implements an iterative teacher-student loop, allowing the model to progressively improve by leveraging its own predictions as pseudo-labels for retraining. This iterative process maximizes the use of available data and helps mitigate overfitting issues. The proposed method demonstrated competitive performance metrics on both the Development and Testing Phase datasets of the competition. Overall, the framework effectively integrates advanced semantic segmentation techniques with iterative self-training to address the challenge of wheat segmentation in diverse visual conditions. <div>
arXiv:2512.11874v1 Announce Type: new 
Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors</title>
<link>https://arxiv.org/abs/2512.11884</link>
<guid>https://arxiv.org/abs/2512.11884</guid>
<content:encoded><![CDATA[
<div> Keywords: instance segmentation, SAM3, YOLO11, MinneApple dataset, zero-shot learning<br /><br />Summary:<br /><br />This paper presents a detailed comparison between the Segment Anything Model version 3 (SAM3) operating in zero-shot mode and three fine-tuned variants of Ultralytics YOLO11 (nano, medium, and large) for instance segmentation. The evaluation uses the MinneApple dataset featuring 670 orchard images with 28,179 apple instances, emphasizing high object density and occlusion challenges. The study reveals that Intersection over Union (IoU) threshold choices significantly influence performance gaps—up to 30% variation. At a low IoU threshold of 0.15, YOLO models outperform SAM3 in F1 scores, achieving 68.9%, 72.2%, and 71.9% versus SAM3’s 59.8% in zero-shot mode, highlighting YOLO’s superiority in detection completeness when fine-tuned. However, YOLO models experience steep performance degradation (48-50 points) over varying IoU thresholds, while SAM3’s performance decreases by only 4 points, demonstrating 12 times greater boundary stability and mask precision. This suggests a trade-off between SAM3’s mask accuracy and YOLO’s detection coverage. The authors provide open-source code, evaluation pipelines, and methodological recommendations for selecting between specialized fine-tuned or generalist foundation models in dense instance segmentation tasks, contributing valuable insights for practical applications. The project's repository is accessible on GitHub. <div>
arXiv:2512.11884v1 Announce Type: new 
Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description</title>
<link>https://arxiv.org/abs/2512.11894</link>
<guid>https://arxiv.org/abs/2512.11894</guid>
<content:encoded><![CDATA[
<div> mmWave radar, Implicit Neural Representations, hypernetworks, activity recognition, pose estimation<br /><br />Summary:<br /><br />This paper introduces mmWeaver, a novel framework designed to synthesize realistic, environment-specific mmWave radar signals efficiently. It addresses challenges in mmWave signal generation, which is traditionally complex, sparse, and computationally expensive to simulate physically. mmWeaver uses Implicit Neural Representations (INRs) to model signals as continuous functions, achieving up to 49-fold data compression. The framework employs hypernetworks that dynamically generate INR parameters conditioned on environmental context from RGB-D images and human motion features derived via text-to-pose generation through MotionGPT. By incorporating semantic and geometric priors, mmWeaver produces diverse in-phase/quadrature (I/Q) signals at multiple resolutions, retaining critical phase information essential for tasks like point cloud estimation and activity classification. Experimentally, mmWeaver attains a complex Structural Similarity Index Measure (SSIM) of 0.88 and a Peak Signal-to-Noise Ratio (PSNR) of 35 dB, surpassing prior methods in signal realism. Additionally, it enhances activity recognition accuracy by up to 7% and decreases human pose estimation error by up to 15%. Notably, mmWeaver operates significantly faster—between 6 and 35 times—compared to traditional simulation approaches, enabling efficient and adaptive mmWave signal synthesis tailored to real-world environments. <div>
arXiv:2512.11894v1 Announce Type: new 
Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hot H\'em: S\`ai G\`on Gi\~ua C\'ai N\'ong H\^ong C\`ong B\`ang -- Saigon in Unequal Heat</title>
<link>https://arxiv.org/abs/2512.11896</link>
<guid>https://arxiv.org/abs/2512.11896</guid>
<content:encoded><![CDATA[
<div> Pedestrian heat exposure, GeoAI, land surface temperature, semantic image segmentation, heat-aware routing<br /><br />Summary:<br /><br />1. The article addresses the critical health risk posed by pedestrian heat exposure in dense tropical cities, specifically focusing on Hô Chí Minh City (HCMC), Vietnam.  
2. It highlights the inadequacy of standard routing algorithms, which often ignore micro-scale thermal variations experienced at a pedestrian level.  
3. The authors introduce Hot Hém, a GeoAI workflow designed to estimate and operationalize pedestrian heat exposure by integrating multiple spatial data science techniques.  
4. The workflow combines Google Street View imagery, semantic image segmentation, and remote sensing data to generate detailed thermal information at a fine spatial resolution.  
5. Two XGBoost machine learning models are trained using a GSV-derived dataset from selected administrative wards (phường) to predict land surface temperature (LST).  
6. These models are applied in a patchwork manner across the entire pedestrian network derived from OSMnx, allowing the creation of a heat-aware pedestrian routing system.  
7. The developed model can help city planners and stakeholders identify corridors with disproportionately high temperatures and understand the underlying infrastructural factors contributing to heat exposure, enabling better urban climate resilience and public health mitigation strategies. <div>
arXiv:2512.11896v1 Announce Type: new 
Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot H\'em is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in H\^o Ch\'i Minh City (HCMC), Vi\d{e}t Nam, colloquially known as S\`ai G\`on. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as ph\u{o}ng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic</title>
<link>https://arxiv.org/abs/2512.11898</link>
<guid>https://arxiv.org/abs/2512.11898</guid>
<content:encoded><![CDATA[
<div> Microscopic vehicle trajectories, UAV data, heterogeneous traffic, urban traffic, Data from Sky<br /><br />Summary: This paper introduces openly accessible microscopic vehicle trajectory (MVT) datasets gathered via unmanned aerial vehicles (UAVs) in heterogeneous and area-based urban traffic environments. Conventional roadside video capture often struggles with occlusions, limited viewpoints, and erratic vehicle movements in dense mixed traffic scenarios. In contrast, UAV-based recordings provide a top-down perspective that alleviates these limitations, enabling detailed capture of spatial and temporal traffic dynamics. The datasets were created using the Data from Sky (DFS) platform and rigorously validated against manual vehicle counts, space mean speeds, and probe trajectories documented in previous studies. Each dataset includes timestamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a high resolution of 30 frames per second. Data collection occurred at six mid-block urban locations in the national capital region of India, capturing a broad spectrum of traffic compositions and densities. Exploratory analyses of the datasets reveal key behavioral patterns such as lane-keeping tendencies, speed distributions, and typical lateral maneuvers inherent to heterogeneous traffic. These datasets aim to serve the global research community in traffic simulation modeling, safety analysis, and behavioral studies within complex urban traffic settings. By providing these empirical data openly, the study enables improved development, testing, and validation of models that better reflect real-world urban traffic complexities. <div>
arXiv:2512.11898v1 Announce Type: new 
Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.11899</link>
<guid>https://arxiv.org/abs/2512.11899</guid>
<content:encoded><![CDATA[
<div> typographic attacks, vision-language models, selective text use, visual question answering, RIO-Bench  

<br /><br />Summary:  
This paper addresses the vulnerability of large vision-language models (LVLMs) to typographic attacks, where misleading text in images can override correct visual understanding. Existing evaluation protocols and defenses focus mainly on object recognition and often encourage ignoring text entirely for robustness. However, many real-world scenarios require models to reason jointly about both visual objects and embedded text, such as reading traffic signs while recognizing pedestrians. To tackle this gap, the authors introduce a new task named Read-or-Ignore VQA (RIO-VQA), which challenges models to selectively decide when to read text or ignore it based on context during visual question answering (VQA). For evaluation, the paper proposes the Read-or-Ignore Benchmark (RIO-Bench), a dataset containing real images paired with same-scene counterfactuals differing only in textual content and question types to test both reading and ignoring capabilities. Experimental results on RIO-Bench show that current strong LVLMs and defenses fail to effectively balance robustness to typographic attacks with the ability to read relevant text. To improve performance, the authors develop a novel data-driven defense that adaptively selects text use, moving beyond previous methods that simply ignore text non-adaptively. Overall, this work highlights a fundamental misalignment between existing evaluations and real-world requirements and offers a principled direction for developing more reliable LVLMs. Further details and resources are available on their project page. <div>
arXiv:2512.11899v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities</title>
<link>https://arxiv.org/abs/2512.11901</link>
<guid>https://arxiv.org/abs/2512.11901</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal fusion, Graph Attention Network, representation learning, multimodal robustness, contrastive loss<br /><br />Summary:<br /><br />1. CLARGA is a versatile multimodal fusion architecture designed to handle any number and type of input modalities without altering its framework.<br /><br />2. It constructs sample-specific attention-weighted graphs over modality features, leveraging a multi-head Graph Attention Network to enable adaptive and efficient cross-modal interaction.<br /><br />3. The model achieves computational efficiency with sub-quadratic complexity as the number of modalities increases and incorporates a learnable mask to manage missing modality inputs.<br /><br />4. Training employs a hybrid loss combining supervised task objectives with contrastive InfoNCE loss, which enhances cross-modal consistency and resilience to noisy or incomplete data.<br /><br />5. Extensive experiments across 7 diverse datasets—including finance, human-computer interaction, multimedia classification, and affective computing—show that CLARGA outperforms existing baselines and state-of-the-art models, demonstrating robustness and strong performance on niche tasks.<br /><br />Overall, CLARGA offers a plug-and-play, efficient, and adaptive solution for multimodal representation learning across a wide range of machine learning applications. <div>
arXiv:2512.11901v1 Announce Type: new 
Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life</title>
<link>https://arxiv.org/abs/2512.11905</link>
<guid>https://arxiv.org/abs/2512.11905</guid>
<content:encoded><![CDATA[
<div> Keywords: subjective well-being, smile intensity, smartphone sensing, positive affect, deep learning<br /><br />Summary:<br /><br />
1. The study addresses the challenge of measuring subjective well-being, traditionally reliant on self-report methods that are vulnerable to recall bias and participant burden. <br />2. Researchers hypothesized that candid smiles, captured unobtrusively during natural smartphone interactions, could serve as an objective and scalable indicator of positive affect.<br />3. Analysis was performed on 405,448 video clips collected passively from 233 participants over one week, with a deep learning model quantifying smile intensity.<br />4. Results revealed distinct diurnal and daily patterns of smiling, with daily smile intensity correlating strongly with national happiness survey data (r=0.92) and diurnal rhythms aligning with day reconstruction method results (r=0.80).<br />5. Higher daily mean smile intensity was significantly linked to increased physical activity and light exposure, while no significant relationship was found with smartphone usage duration.<br />6. The findings indicate that passive smartphone sensing of smiles can provide a robust, ecologically valid approach to studying real-world affective behaviors, potentially enabling population-scale research on well-being dynamics. <div>
arXiv:2512.11905v1 Announce Type: new 
Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MPath: Multimodal Pathology Report Generation from Whole Slide Images</title>
<link>https://arxiv.org/abs/2512.11906</link>
<guid>https://arxiv.org/abs/2512.11906</guid>
<content:encoded><![CDATA[
<div> Keywords: pathology report generation, whole slide images, multimodal framework, BioBART, visual-prefix prompting<br /><br />Summary: Automated diagnostic pathology report generation from whole slide images (WSIs) is a challenging task due to high morphological variability and complex narrative structures in pathology. The paper introduces MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on visual embeddings derived from WSIs using a visual-prefix prompting mechanism. Unlike traditional end-to-end vision-language pretraining, MPath employs foundation-model WSI features from CONCH and Titan and integrates them into BioBART through a compact projection module, allowing the language model to remain frozen, which enhances stability and data efficiency. The approach was evaluated on the RED 2025 Grand Challenge dataset, where MPath secured 4th place in Test Phase 2 despite having limited chances to submit. The results demonstrate that prompt-based multimodal conditioning is a promising scalable and interpretable method for generating pathology reports directly from WSIs. This method leverages foundation models effectively while reducing training complexity and maintaining high performance in clinical text generation for computational pathology. <div>
arXiv:2512.11906v1 Announce Type: new 
Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications</title>
<link>https://arxiv.org/abs/2512.11925</link>
<guid>https://arxiv.org/abs/2512.11925</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D plant models, procedural modeling, large language models, parametric geometry, computational phenotyping<br /><br />Summary:<br /><br />1. The paper introduces FloraForge, a novel framework that integrates large language models (LLMs) to assist domain experts in creating accurate, fully parametric 3D plant models using natural language refinements, reducing the need for programming expertise.<br /><br />2. FloraForge leverages LLM-enabled co-design to iteratively refine Python scripts that generate hierarchical B-spline surface representations of plants, ensuring botanical accuracy through explicit control points and parametric deformation functions.<br /><br />3. The resulting 3D models are mathematically continuous and can be tessellated into polygonal meshes with arbitrary precision, making them compatible with advanced plant functional structural analyses such as light simulation, CFD, and finite element analysis.<br /><br />4. The framework was demonstrated on multiple plant species including maize, soybean, and mung bean, fitting procedural models to empirical point cloud data via manual refinement of human-readable Plant Descriptor files.<br /><br />5. FloraForge outputs two types of meshes: one optimized for visualization and another augmented with parametric metadata for quantitative analysis, combining LLM-assistance, parametric control, and biological rigor to democratize sophisticated geometric modeling for plant science.<br /><br /> <div>
arXiv:2512.11925v1 Announce Type: new 
Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder</title>
<link>https://arxiv.org/abs/2512.11926</link>
<guid>https://arxiv.org/abs/2512.11926</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D object detection, LiDAR, transformer, point cloud completion, autonomous driving  

<br /><br />Summary:  
This paper addresses the challenge of detecting objects in distant regions with sparse LiDAR points, critical for autonomous driving. It proposes a joint completion and detection framework that enhances detection features in sparse areas without increasing computational cost. The core innovation is TransBridge, a transformer-based up-sampling block that fuses features from both detection and completion networks, allowing the detection network to leverage implicit completion features. Additionally, the Dynamic-Static Reconstruction (DSRecon) module is introduced to generate dense LiDAR data as ground truth for the completion network. The use of transformer mechanisms facilitates the connection of channel-wise and spatial relations, leading to a high-resolution feature map beneficial for completion tasks. Extensive evaluation on the nuScenes and Waymo datasets validates the framework's effectiveness, showing consistent improvements in end-to-end 3D object detection. Performance gains in mean average precision (mAP) range between 0.7 to 1.5 points across various methods, demonstrating strong generalization ability. Notably, for two-stage detection frameworks, the approach yields an mAP improvement of up to 5.78 points, highlighting its substantial impact on detection accuracy in sparse LiDAR scenarios. <div>
arXiv:2512.11926v1 Announce Type: new 
Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion</title>
<link>https://arxiv.org/abs/2512.11928</link>
<guid>https://arxiv.org/abs/2512.11928</guid>
<content:encoded><![CDATA[
<div> Cell Painting, Diffusion Model, Brightfield Imaging, Time-Lapse Videos, In-Context Learning  

<br /><br />Summary: This paper addresses two major limitations of traditional cell painting techniques: the labor-intensive process and the requirement for chemical fixation, which prevents studying cell dynamics. To overcome these challenges, the authors develop MONET (Morphological Observation Neural Enhancement Tool), a diffusion model trained on a large dataset to predict cell paint channels directly from brightfield images. They demonstrate that increasing the scale of the model leads to improved prediction quality. MONET incorporates a consistency architecture that enables the generation of time-lapse videos, despite the lack of actual cell paint video training data. Furthermore, this architecture supports a form of in-context learning, allowing the model to partially generalize to out-of-distribution cell lines and diverse imaging protocols. The work suggests that virtual cell painting via MONET is not intended to fully replace physical cell painting but rather to serve as a complementary tool. This virtual approach can facilitate novel biological research workflows by providing high-contrast, interpretable morphological images without the constraints of chemical fixation and laborious imaging setups. <div>
arXiv:2512.11928v1 Announce Type: new 
Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains</title>
<link>https://arxiv.org/abs/2512.11939</link>
<guid>https://arxiv.org/abs/2512.11939</guid>
<content:encoded><![CDATA[
<div> Peano scan, hidden Markov chains, Bayesian segmentation, contextual PS, evidential Markov chains<br /><br />Summary:<br /><br />This article focuses on the transformation of two-dimensional image pixel data into one-dimensional sequences using a Peano scan (PS), facilitating the application of hidden Markov chains (HMCs) for unsupervised image segmentation. It highlights that Bayesian segmentation methods based on HMCs can rival those based on hidden Markov fields (HMFs) while being computationally more efficient. The study introduces an extension called the contextual Peano scan (CPS) and presents the associated HMC model, HMC-CPS, which has shown promising initial results in image segmentation tasks. Furthermore, it discusses the development of hidden evidential Markov chains (HEMCs), which enhance Bayesian segmentation performance beyond standard HMCs. The core contribution is the new HEMC-CPS model that integrates contextual PS with evidential HMC, demonstrating its effectiveness in maximum posterior mode (MPM) Bayesian segmentation on both synthetic and real images. The segmentation process is unsupervised, with parameters estimated through a stochastic expectation-maximization (SEM) algorithm. The paper suggests the HEMC-CPS model holds significant potential for handling complex imaging data, including three-dimensional and multi-sensor, multi-resolution images. Lastly, it notes that these models extend beyond image segmentation and may be applied to any spatially correlated data analysis. <div>
arXiv:2512.11939v1 Announce Type: new 
Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.11941</link>
<guid>https://arxiv.org/abs/2512.11941</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot learning, skeleton-based action recognition, visual-semantic alignment, dynamic refinement, large language model<br /><br />Summary:  
This paper addresses the limitations of zero-shot skeleton-based action recognition (ZS-SAR) methods that rely on static class-level semantic alignment, which struggle to generalize to unseen classes due to coarse visual-semantic correspondences. The proposed framework, DynaPURLS, improves this by creating multi-scale visual-semantic mappings and dynamically refining them during inference to adapt to domain shifts. DynaPURLS utilizes a large language model to generate hierarchical textual descriptions capturing both overall movements and fine-grained body-part dynamics, enriching semantic information. On the visual side, an adaptive partitioning module groups skeleton joints semantically to produce fine-grained visual representations that better correlate with the detailed textual descriptions. To further address distribution shifts between seen and unseen classes, a dynamic refinement module is introduced that adapts textual features through a lightweight learnable projection during inference. This process is stabilized using a confidence-aware, class-balanced memory bank which reduces error accumulation caused by noisy pseudo-labels. Extensive experiments on major benchmarks NTU RGB+D 60/120 and PKU-MMD show that DynaPURLS outperforms previous methods and sets new state-of-the-art results. The source code is publicly available, facilitating reproducibility and further research. <div>
arXiv:2512.11941v1 Announce Type: new 
Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer</title>
<link>https://arxiv.org/abs/2512.11977</link>
<guid>https://arxiv.org/abs/2512.11977</guid>
<content:encoded><![CDATA[
<div> Keywords: Predictive maintenance, Semiconductor wafer defects, Data-Efficient Image Transformer (DeiT), Convolutional Neural Networks, Imbalanced data<br /><br />Summary:<br /><br />This study focuses on improving defect detection in semiconductor wafer maintenance, a critical area requiring accurate fault prediction to reduce costs. Traditional convolutional neural networks (CNNs) like VGG-19, Xception, and SqueezeNet have been widely used for wafer defect classification but struggle with limited and imbalanced datasets common in this domain. To address this challenge, the research evaluates the Data-Efficient Image Transformer (DeiT) model's performance in these data-constrained scenarios. Experimental analysis demonstrates that DeiT outperforms the CNN baselines, achieving a classification accuracy of 90.83%, compared to VGG-19 (65%), SqueezeNet (82%), Xception (66%), and a Hybrid model (67%). DeiT also excels in the F1-score metric, scoring 90.78%, indicating balanced precision and recall, with particular strength in identifying minority defect classes, which is crucial for effective fault detection. Moreover, DeiT exhibits faster training convergence, improving computational efficiency. These results underline the potential of transformer-based architectures like DeiT in enhancing semiconductor wafer defect detection, supporting more reliable predictive maintenance strategies, and ultimately contributing to better operational efficiency and reduced downtime in semiconductor fabrication processes. <div>
arXiv:2512.11977v1 Announce Type: new 
Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction</title>
<link>https://arxiv.org/abs/2512.11988</link>
<guid>https://arxiv.org/abs/2512.11988</guid>
<content:encoded><![CDATA[
<div> Keywords: human-object interaction, monocular RGB video, 4D reconstruction, pose hypothesis selection, category-agnostic<br /><br />Summary:<br /><br />This paper addresses the challenge of reconstructing accurate 4D human-object interactions from single monocular RGB videos, an important task for applications such as human understanding, gaming, and robot learning. The problem is difficult due to unknown object and human information, depth ambiguity, occlusion, and complex motion that hinder consistent spatial and temporal 3D reconstructions. Unlike prior methods that rely on ground truth object templates or focus on limited object categories, the authors introduce CARI4D, the first category-agnostic method capable of metric-scale 4D reconstruction of human-object interactions from monocular RGB input. The method involves a pose hypothesis selection algorithm that integrates predictions from foundation models and further refines these via a learned render-and-compare approach to ensure spatial, temporal, and pixel-level consistency. Additionally, it reasons about detailed physical contacts to refine the interaction while enforcing physical constraints. Experimental results demonstrate that CARI4D outperforms state-of-the-art methods by 38% on known datasets and 36% on unseen datasets in terms of reconstruction error. Remarkably, the model generalizes well beyond training categories, enabling zero-shot application to in-the-wild internet videos. The authors plan to release their code and pretrained models publicly. <div>
arXiv:2512.11988v1 Announce Type: new 
Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions</title>
<link>https://arxiv.org/abs/2512.11995</link>
<guid>https://arxiv.org/abs/2512.11995</guid>
<content:encoded><![CDATA[
<div> Visual Reasoning, Multi-step Exploration, Vision-language Models, Chain-of-Questions, Benchmark

<br /><br />Summary: This paper addresses the limitations of current vision-language models (VLMs) that perform well on simple, well-defined questions but struggle with complex, open-ended visual reasoning tasks requiring multiple exploration steps. The authors introduce V-REX (Visual Reasoning with multi-step EXploration), a comprehensive evaluation suite consisting of a benchmark and protocol designed to test VLMs’ ability to engage in native multi-step exploratory reasoning across diverse real-world scenarios. V-REX reformulates multi-step reasoning as a Chain-of-Questions (CoQ), separating the process into two distinct capabilities: (1) Planning, where the model breaks down a complex task by selecting a sequence of exploratory questions, and (2) Following, where it sequentially answers these questions to gather necessary information for the final solution. By limiting the options for questions and answers at each step, V-REX enables fine-grained, quantitative evaluation of intermediate reasoning stages rather than only final outcomes. Experiments on state-of-the-art proprietary and open-source VLMs reveal consistent trends with model scaling and highlight notable gaps between planning and following skills. These findings demonstrate significant opportunities to advance multi-step exploratory visual reasoning in VLMs. <div>
arXiv:2512.11995v1 Announce Type: new 
Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus</title>
<link>https://arxiv.org/abs/2512.12012</link>
<guid>https://arxiv.org/abs/2512.12012</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous Vehicles, Long-Tail Data, Neuro-symbolic Framework, Open-Vocabulary Detector, Privacy-Preserving  

<br /><br />Summary:  
The paper introduces Semantic-Drive, a novel local-first neuro-symbolic framework designed to address the scarcity of rare, safety-critical "Long-Tail" training data in Autonomous Vehicles (AVs). The approach separates perception into two key stages: (1) Symbolic Grounding, which uses a real-time open-vocabulary detector called YOLOE to focus attention on relevant objects, and (2) Cognitive Analysis, which employs a Reasoning Vision-Language Model (VLM) for detailed forensic scene evaluation. To counter hallucination and improve inference accuracy, Semantic-Drive implements a "System 2" alignment strategy featuring a multi-model "Judge-Scout" consensus mechanism. Evaluated on the nuScenes dataset with reference to the Waymo Open Dataset taxonomy, the framework achieves a high Recall of 0.966, significantly outperforming the 0.475 Recall of CLIP, and reduces Risk Assessment Error by 40% relative to single-model baselines. Importantly, Semantic-Drive runs fully on consumer-grade hardware (NVIDIA RTX 3090), thereby providing a privacy-preserving, cost-effective alternative to cloud-based video large models (VLMs). This system supports more precise and scalable semantic data mining without compromising user data privacy or incurring cloud operational costs. <div>
arXiv:2512.12012v1 Announce Type: new 
Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2512.12013</link>
<guid>https://arxiv.org/abs/2512.12013</guid>
<content:encoded><![CDATA[
<div> mmWave radar, human activity recognition, dynamic graph neural network, spatial-temporal features, resource-constrained platforms  

<br /><br />Summary:  
This paper addresses the challenges of human activity recognition (HAR) using mmWave radar point clouds, which are inherently sparse and variable in size, unlike dense vision-based data. The authors propose a novel graph-based approach utilizing a discrete dynamic graph neural network (DDGNN) to capture spatial-temporal features of human movements effectively. A star graph structure is designed, incorporating a static center point connected to dynamic radar points within the same and consecutive frames, to model high-dimensional relative relationships. The DDGNN is specifically tailored to handle variable-sized input data from the mmWave radar. Experimental evaluations on real-world HAR datasets demonstrate that the proposed method achieves a high overall classification accuracy of 94.27%, which is close to the 97.25% accuracy attained by vision-based skeleton data. Additionally, the authors perform inference tests on a resource-constrained platform (Raspberry Pi 4), showcasing the method’s practical applicability in low-power environments. A comprehensive ablation study validates the design choices behind variable DDGNN structures. Importantly, the system outperforms three recent radar-specific HAR methods without needing data resampling or frame aggregation steps, highlighting its efficiency and effectiveness in handling mmWave radar data for HAR tasks. <div>
arXiv:2512.12013v1 Announce Type: new 
Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive federated learning for ship detection across diverse satellite imagery sources</title>
<link>https://arxiv.org/abs/2512.12053</link>
<guid>https://arxiv.org/abs/2512.12053</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Ship Detection, Satellite Imagery, YOLOv8, Privacy-Preserving<br /><br />Summary:  
This study explores the use of Federated Learning (FL) for ship detection across multiple heterogeneous satellite datasets, enabling privacy-preserving collaboration without the need to share raw data or centralize collections. The approach is particularly useful when dealing with commercial satellite images or sensitive ship annotations, preserving data confidentiality. Four FL algorithms—FedAvg, FedProx, FedOpt, and FedMedian—are implemented and benchmarked against a local training baseline in which the YOLOv8 detection model is trained independently on each dataset without parameter sharing. Results demonstrate that FL models significantly enhance detection accuracy compared to local-only training on smaller datasets. Furthermore, FL performance approaches that of global training, which requires aggregating all datasets into a single training set. The investigation highlights the critical role of careful hyperparameter tuning, such as the number of communication rounds and the number of local training epochs per round, to balance detection performance with computational efficiency. The findings suggest that FL is a promising approach for improving ship detection accuracy across diverse satellite data sources while safeguarding sensitive information. <div>
arXiv:2512.12053v1 Announce Type: new 
Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management</title>
<link>https://arxiv.org/abs/2512.12056</link>
<guid>https://arxiv.org/abs/2512.12056</guid>
<content:encoded><![CDATA[
<div> Burned areas, semantic segmentation, SPOT-6/7 imagery, U-Net, Test-Time Augmentation  

<br /><br />Summary: After wildfires, accurately mapping burned areas (BAs) is essential for damage assessment and ecosystem recovery. This study presents a supervised semantic segmentation workflow designed to improve both the accuracy and efficiency of BA delineation, focusing on high-resolution SPOT-6/7 satellite imagery known for its on-demand availability. Performance is measured using Dice score, Intersection over Union, and inference time. The experimentation reveals that U-Net and SegFormer models achieve comparable results when trained on limited data; however, SegFormer's greater computational resource needs limit its suitability for urgent emergency response situations. The integration of land cover data as an auxiliary task strengthens model robustness without increasing inference time. Additionally, the application of Test-Time Augmentation (TTA) enhances the quality of BA delineation but at the cost of longer inference times, which can be addressed through optimization strategies such as Mixed Precision computing. Overall, this workflow balances model performance with operational efficiency, making it pragmatic for time-sensitive wildfire management. <div>
arXiv:2512.12056v1 Announce Type: new 
Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos</title>
<link>https://arxiv.org/abs/2512.12060</link>
<guid>https://arxiv.org/abs/2512.12060</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-video, diffusion models, video restoration, structural artifacts, temporal coherence

<br /><br />Summary:  
Modern text-to-video diffusion models generate compelling clips but often suffer from structural issues like distorted faces, warped backgrounds, and inconsistent motion, similar to artifacts seen in very low-quality real-world videos. Traditional video restoration and super-resolution methods mainly handle synthetic degradations like blur and downsampling but tend to stabilize rather than fix such structural defects. Diffusion-prior restorers, typically trained on photometric noise, lack fine control between perceptual quality and fidelity. To address these challenges, the authors introduce CreativeVR, a diffusion-prior-guided video restoration framework designed for AI-generated and real videos exhibiting severe structural and temporal artifacts. CreativeVR uses a deep-adapter-based approach featuring a single precision control knob that balances between faithful restoration of standard degradations and stronger corrective action on complex artifacts. A key innovation is a temporally coherent degradation module during training, which introduces realistic structural failures to help the model learn effective correction. For evaluation, the authors propose the AIGC54 benchmark incorporating FIQA, semantic, and perceptual metrics with multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts, performs competitively on standard benchmarks, and operates efficiently at about 13 frames per second at 720p on a single 80-GB A100 GPU. The project page is available at https://daveishan.github.io/creativevr-webpage/. <div>
arXiv:2512.12060v1 Announce Type: new 
Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models</title>
<link>https://arxiv.org/abs/2512.12080</link>
<guid>https://arxiv.org/abs/2512.12080</guid>
<content:encoded><![CDATA[
<div> Keywords: autoregressive video models, exposure bias, Backwards Aggregation, causal diffusion transformers, long-horizon motion stability  

<br /><br />Summary:  
This paper addresses the challenge of exposure bias in autoregressive video models, which arises due to the discrepancy between training on clean context frames and inference on self-generated frames, leading to progressive error accumulation and quality degradation over time. To mitigate this, the authors propose Backwards Aggregation (BAgger), a novel self-supervised method that generates corrective trajectories from the model's own rollouts, allowing it to learn recovery from its mistakes. Unlike traditional techniques that depend on few-step distillation or distribution-matching losses—which often compromise video quality and diversity—BAgger employs standard score or flow matching objectives, thereby eliminating the need for large teacher models and extensive backpropagation through time. The method is exemplified by integrating BAgger with causal diffusion transformers and tested across several tasks including text-to-video generation, video extension, and multi-prompt video creation. Experimental results demonstrate that BAgger enhances long-horizon motion stability and maintains superior visual consistency by reducing frame drift, thus advancing the performance and robustness of autoregressive video modeling approaches. <div>
arXiv:2512.12080v1 Announce Type: new 
Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer</title>
<link>https://arxiv.org/abs/2512.12083</link>
<guid>https://arxiv.org/abs/2512.12083</guid>
<content:encoded><![CDATA[
<div> VFM representations, latent diffusion models, RePack, Diffusion Transformers, dimensionality reduction  

<br /><br />Summary: This paper addresses the challenge of incorporating high-dimensional Vision Foundation Model (VFM) representations into latent diffusion models (LDMs) for image generation. While injecting VFM features such as those from DINOv3 improves semantic understanding and generation quality, their large size can cause information overload and complicate decoding. To resolve this, the authors propose RePack (Representation Packing), a framework designed to compress VFM representations into compact, low-dimensional manifolds that are easier for Diffusion Transformers (DiTs) to decode. RePack effectively filters out non-semantic noise and retains the essential structural information necessary for high-fidelity image reconstruction. Experimental comparisons demonstrate that RePack accelerates the convergence of DiTs significantly, outperforming methods that use raw VFM features. Specifically, on the DiT-XL/2 architecture, RePack achieves a Fréchet Inception Distance (FID) score of 3.66 in only 64 training epochs—35% faster than the previous state-of-the-art. This shows that RePack not only preserves the semantic richness of VFM representations but also mitigates the drawbacks of their high dimensionality, leading to more efficient and effective diffusion model training and improved image generation outcomes. <div>
arXiv:2512.12083v1 Announce Type: new 
Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering</title>
<link>https://arxiv.org/abs/2512.12089</link>
<guid>https://arxiv.org/abs/2512.12089</guid>
<content:encoded><![CDATA[
<div> Keywords: Large vision-language models, hallucinations, visual attention, attention maps, VEGAS<br /><br />Summary:<br /><br />1. Large vision-language models (LVLMs) demonstrate strong capabilities in reasoning over combined visual and textual data but tend to generate hallucinated outputs—linguistically plausible yet visually inconsistent information. 2. The paper identifies that hallucinations occur when the LVLM's final visual-attention maps do not focus adequately on key objects within images, while attention maps from the vision encoder are more focused and reduce hallucinations. 3. Analysis reveals that vision-text conflicts, which contribute to hallucinations, peak in the middle layers of the language model during decoding. 4. By injecting the vision encoder’s attention maps into these middle layers, hallucinations can be effectively suppressed. 5. Based on these insights, the authors propose VEGAS, a straightforward inference-time approach that incorporates the vision encoder’s attention maps into the language model’s mid-layers, adaptively guiding the token generation process away from hallucinated content. 6. Extensive benchmark evaluations demonstrate that VEGAS achieves state-of-the-art results in reducing hallucinations in LVLMs, emphasizing the importance of vision encoder attention for reliable multimodal reasoning. <div>
arXiv:2512.12089v1 Announce Type: new 
Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPDMark: Selective Parameter Displacement for Robust Video Watermarking</title>
<link>https://arxiv.org/abs/2512.12090</link>
<guid>https://arxiv.org/abs/2512.12090</guid>
<content:encoded><![CDATA[
<div> Keywords: video watermarking, diffusion model, low-rank adaptation, temporal consistency, robustness<br /><br />Summary:<br />1. SPDMark is a novel in-generation watermarking framework designed specifically for video diffusion models to embed watermarks into generated videos by selectively displacing model parameters.<br />2. The watermark embedding is performed by additive compositions of layer-wise basis shifts indexed by a watermarking key, which enables robustness and tractability.<br />3. Low-rank adaptation (LoRA) is used to efficiently implement these basis shifts, optimizing parameter usage.<br />4. During training, basis shifts and watermark extractors are jointly optimized by minimizing losses related to message recovery accuracy, perceptual similarity, and temporal consistency.<br />5. To handle temporal attacks, a cryptographic hashing function generates frame-specific watermark messages, while maximum bipartite matching during extraction helps recover correct frame order.<br />6. SPDMark is evaluated on both text-to-video and image-to-video generation, demonstrating imperceptible watermarks that can be accurately recovered.<br />7. Extensive testing confirms the method’s robustness against common video modifications, and its computational efficiency surpasses existing post-hoc and in-generation watermarking approaches.<br />Overall, SPDMark effectively balances imperceptibility, robustness, and computational practicality in video watermarking for diffusion-based generative models. <div>
arXiv:2512.12090v1 Announce Type: new 
Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging</title>
<link>https://arxiv.org/abs/2512.12101</link>
<guid>https://arxiv.org/abs/2512.12101</guid>
<content:encoded><![CDATA[
<div> Keywords: pollen recognition, digital in-line holographic microscopy, YOLOv8, MobileNetV3L, GAN augmentation<br /><br />Summary:<br /><br />1. This study focuses on fully automated pollen recognition using both conventional optical microscopy and digital in-line holographic microscopy (DIHM) images. <br />2. Recognizing pollen in unreconstructed holographic images is challenging due to speckle noise, twin-image artifacts, and significant visual differences compared to bright-field images.<br />3. The baseline performance was established by training YOLOv8s for object detection and MobileNetV3L for pollen classification on a dual-modality dataset comprising automatically annotated optical and affinely aligned DIHM images.<br />4. On optical images, detection achieved a mean average precision at IoU 0.5 (mAP50) of 91.3% and classification accuracy of 97%, while on DIHM images, detection mAP50 was only 8.15% and classification accuracy 50%.<br />5. Expanding DIHM bounding boxes based on aligned optical image annotations slightly improved detection to 13.3% mAP50 and classification accuracy to 54%.<br />6. To enhance DIHM detection, a Wasserstein GAN with spectral normalization (WGAN-SN) was used to generate synthetic DIHM images, producing an FID score of 58.246.<br />7. Mixing real DIHM images with synthetic GAN-generated images at a ratio of 1.0 : 1.5 improved detection mAP50 to 15.4%.<br />8. The results demonstrate that GAN-based data augmentation can modestly bridge the performance gap, moving automated DIHM-based veterinary imaging closer to practical application. <div>
arXiv:2512.12101v1 Announce Type: new 
Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography</title>
<link>https://arxiv.org/abs/2512.12107</link>
<guid>https://arxiv.org/abs/2512.12107</guid>
<content:encoded><![CDATA[
<div> Keywords: echocardiography, vision-language models, multimodal dataset, measurement-grounded, EchoVLM  

<br /><br />Summary:  
1. Echocardiography is widely used in cardiology but its interpretation is complex, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning.  
2. Existing vision-language models (VLMs) have limited applicability to echocardiography due to the lack of large-scale, clinically grounded image-text datasets and measurement-based reasoning mechanisms.  
3. The authors introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, containing 19,065 image-text pairs from 1,572 patients, standardized views, structured measurements, measurement-grounded captions, and guideline-based disease labels.  
4. They propose EchoVLM, a vision-language model pretrained with two novel objectives: (i) a view-informed contrastive loss to encode the view-dependent structure of echo imaging, and (ii) a negation-aware contrastive loss to distinguish critical negative from positive clinical findings.  
5. EchoVLM shows state-of-the-art performance across 36 clinical tasks in five categories, including multimodal disease classification (86.5% AUC in zero-shot), image-text retrieval, view classification (95.1% accuracy), chamber segmentation, and landmark detection.  
6. The study demonstrates that clinically grounded multimodal pretraining creates transferable visual representations, positioning EchoVLM as a foundational model for end-to-end echocardiography interpretation.  
7. The authors will release EchoGround-MIMIC and data curation code to support reproducibility and future research in multimodal echocardiography analysis. <div>
arXiv:2512.12107v1 Announce Type: new 
Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Patch-Based TDA Approach for Computed Tomography</title>
<link>https://arxiv.org/abs/2512.12108</link>
<guid>https://arxiv.org/abs/2512.12108</guid>
<content:encoded><![CDATA[
<div> Topological Data Analysis, Persistent Homology, Computed Tomography, Patch-based Approach, Medical Imaging<br /><br />Summary:<br /><br />This article presents a novel patch-based method for constructing persistent homology (PH) from volumetric computed tomography (CT) images, a key technique in topological data analysis (TDA). Traditional approaches typically use a 3D cubical complex filtration suited for grid-structured data but often face challenges related to computational complexity and suboptimal performance with high-resolution CT images. The proposed patch-based PH method divides volumetric images into smaller patches for analysis, improving both efficiency and effectiveness in extracting topological features such as connected components, cycles, and voids. Extensive experiments on multiple 3D CT datasets demonstrate that the patch-based method consistently outperforms the standard cubical complex algorithm across various metrics, including accuracy, area under the curve (AUC), sensitivity, specificity, and F1 score, with improvements averaging 10.38%, 6.94%, 2.06%, 11.58%, and 8.51%, respectively. This approach offers a significant advancement in ML feature engineering for medical imaging by harnessing deeper structural insights while reducing computational costs. To promote accessibility and adoption, the authors provide Patch-TDA, a Python package implementing this patch-based PH method, facilitating its integration into medical imaging and machine learning workflows. <div>
arXiv:2512.12108v1 Announce Type: new 
Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery</title>
<link>https://arxiv.org/abs/2512.12128</link>
<guid>https://arxiv.org/abs/2512.12128</guid>
<content:encoded><![CDATA[
<div> Keywords: road damage assessment, post-disaster imagery, machine learning, spatial alignment, uncrewed aerial systems (sUAS)<br /><br />Summary:<br /><br />1. This paper introduces the largest benchmark dataset for road damage assessment and road alignment, named CRASAR-U-DRIODs, based on post-disaster sUAS imagery collected from 10 federally declared disasters.  
2. The dataset addresses three key limitations in previous datasets, which were either small-scale or used low-resolution imagery inadequate for detecting road damages critical to emergency management.  
3. The authors annotated 657.25 km of roads using a 10-class labeling schema to comprehensively categorize damage and conditions.  
4. They trained and tested 18 baseline machine learning models on this dataset and deployed these models operationally during the 2024 responses to Hurricanes Debby and Helene.  
5. Observing significant issues with misaligned road lines in practice, the dataset includes 9,184 spatial adjustments for road line alignment to improve model accuracy.  
6. Deployment experiments show that ignoring spatial alignment reduces model performance by an average of 5.596% in Macro IoU, with around 8% of adverse road conditions mislabeled and 9% of road lines misaligned from the actual roads.  
7. The study highlights essential gaps in machine learning, computer vision, and robotics fields that must be addressed to enhance disaster response through more accurate road damage assessment and alignment. <div>
arXiv:2512.12128v1 Announce Type: new 
Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater</title>
<link>https://arxiv.org/abs/2512.12142</link>
<guid>https://arxiv.org/abs/2512.12142</guid>
<content:encoded><![CDATA[
<div> Keywords: Greenland ice sheet, surface meltwater, deep learning, remote sensing, downscaling<br /><br />Summary:<br /><br />1. The Greenland ice sheet is melting faster due to complex, not fully understood processes that are difficult to measure directly. Surface meltwater distribution is critical for understanding these processes and can be observed through remote sensing.  
2. Existing surface meltwater maps face a resolution trade-off: they can be high resolution in time or space, but not both simultaneously.  
3. The authors develop a deep learning model that fuses data from remote sensing (synthetic aperture radar and passive microwave) and physics-based regional climate models along with digital elevation models to produce daily 100m resolution meltwater maps over Helheim Glacier, Eastern Greenland, covering 2017-2023.  
4. Using SAR-derived meltwater maps as ground truth, the deep learning approach significantly improves accuracy (95%) compared to non-deep learning methods relying solely on RCM outputs (83%) or passive microwave data (72%). A simpler SAR running window method also achieves good accuracy (90%) but underestimates extreme melt events and does not require deep learning.  
5. The study evaluates common deep learning architectures (UNet and DeepLabv3+), publishes the aligned dataset as MeltwaterBench for benchmarking, and provides code and data openly at github.com/blutjens/hrmelt to support further research in spatiotemporal downscaling of meltwater observations. <div>
arXiv:2512.12142v1 Announce Type: new 
Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Horizons: Evaluating Deep Models in the Wild</title>
<link>https://arxiv.org/abs/2512.12146</link>
<guid>https://arxiv.org/abs/2512.12146</guid>
<content:encoded><![CDATA[
<div> Open-set recognition, few-shot class-incremental learning, CIFAR-10, pretrained visual encoders, prototype-based methods  

<br /><br />Summary:  
This paper investigates open-world deployment challenges by studying open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on the CIFAR-10 dataset. For OSR, the authors evaluate three pretrained frozen visual encoders—ResNet-50, ConvNeXt-Tiny, and CLIP ViT-B/16—using a linear probe combined with four post-hoc scoring functions (MSP, Energy, Mahalanobis, and kNN). The results show that CLIP excels in distinguishing known from unknown categories across various metrics, including AUROC, AUPR, FPR@95, and OSCR, with the Energy score offering the most consistent performance. In FSCIL, three methods—modified SPPR, OrCo, and ConCM—are compared, all using partially frozen ResNet-50 backbones, under 1-, 5-, and 10-shot conditions. ConCM achieves the highest accuracy (84.7%) in the 10-shot setting and produces the cleanest confusion matrix. However, all approaches experience performance saturation beyond five shots. The study highlights the significant impact of backbone choice and scoring strategies on unknown detection and demonstrates that prototype-based approaches like ConCM effectively mitigate catastrophic forgetting while adapting incrementally to novel classes. This controlled evaluation provides valuable insights for building reliable open-world recognition systems. <div>
arXiv:2512.12146v1 Announce Type: new 
Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video</title>
<link>https://arxiv.org/abs/2512.12165</link>
<guid>https://arxiv.org/abs/2512.12165</guid>
<content:encoded><![CDATA[
<div> camera motion, audio-visual integration, relative camera pose estimation, direction-of-arrival spectra, robustness<br /><br />Summary:<br />1. This paper addresses the fundamental challenge of understanding camera motion, which is crucial in embodied perception and 3D scene understanding.<br />2. Traditional visual methods for camera pose estimation often fail in degraded visual conditions such as motion blur or occlusions.<br />3. The authors propose leveraging passive scene sounds as complementary cues for estimating relative camera pose in real-world, in-the-wild videos.<br />4. They introduce a novel audio-visual framework that incorporates direction-of-arrival (DOA) spectra and binauralized audio embeddings into a state-of-the-art vision-only pose estimation model.<br />5. Experiments on two large datasets demonstrate consistent improvements over strong visual-only baselines and increased robustness when visual information is corrupted.<br />6. This work marks the first successful use of audio signals for relative camera pose estimation in real-world videos.<br />7. The findings establish incidental, everyday audio as an unexpected yet promising signal for addressing spatial understanding challenges related to camera motion.<br />8. The project page provides additional resources and code for further research and application: http://vision.cs.utexas.edu/projects/av_camera_pose. <div>
arXiv:2512.12165v1 Announce Type: new 
Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation</title>
<link>https://arxiv.org/abs/2512.12193</link>
<guid>https://arxiv.org/abs/2512.12193</guid>
<content:encoded><![CDATA[
<div> Keywords: customized video generation, subject appearance, motion consistency, self-supervised encoder, LoRA fine-tuning<br /><br />Summary:<br /><br />Customized video generation aims to create videos that retain the subject's appearance from reference images while preserving motion consistency from reference videos. Existing methods face challenges balancing subject appearance similarity and motion pattern consistency due to insufficient object-level guidance. To overcome this, the authors propose SMRABooth, which integrates a self-supervised encoder and an optical flow encoder to extract object-level subject and motion representations. These representations are aligned with the generation model through LoRA (Low-Rank Adaptation) fine-tuning. The approach consists of three key stages: first, subject representations generated via a self-supervised encoder guide subject alignment, improving the capture of subject structure and enhancing semantic consistency. Second, motion representations derived from an optical flow encoder capture object-level motion trajectories independently of appearance, ensuring motion coherence. Third, a subject-motion association decoupling strategy employs sparse LoRA injections across spatial locations and time steps, reducing interference between subject and motion modulations. Extensive experiments demonstrate that SMRABooth excels at both subject and motion customization, effectively maintaining consistent subject appearance and motion patterns, thus proving its effectiveness in controllable text-to-video generation tasks. <div>
arXiv:2512.12193v1 Announce Type: new 
Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms</title>
<link>https://arxiv.org/abs/2512.12199</link>
<guid>https://arxiv.org/abs/2512.12199</guid>
<content:encoded><![CDATA[
<div> Keywords: micro UAV, wildfire tracking, thermal imaging, edge detection, inertial feedback<br /><br />Summary:<br /><br />This study proposes a lightweight perimeter tracking method tailored for micro UAV teams operating in wildfire environments where bandwidth is severely limited. It utilizes thermal image frames to generate initial coarse hot region masks via adaptive thresholding and morphological refinement, while RGB frames provide edge cues and help suppress false detections due to texture by applying gradient-based filtering. A rule-level merging strategy consolidates boundary candidates and refines them using the Ramer Douglas Peucker algorithm to simplify the shapes. The system incorporates periodic beacons and an inertial feedback loop to maintain stable UAV trajectories even when GPS signals degrade. Designed to run efficiently on embedded System on Chip (SoC) platforms, the guidance loop targets less than 50 ms latency by limiting per-frame pixel operations and precomputing gradient tables. Small-scale simulations show that this method reduces average path length and boundary jitter compared to pure edge tracking baselines, while effectively covering the environment as measured by intersection merge analysis. Additionally, battery consumption and computational utilization metrics indicate feasibility for achieving forward motion speeds of 10 to 15 m/s on standard micro UAV platforms. Overall, this approach enables rapid field deployment with robust sensing and minimal communication, suitable for emergency wildfire reconnaissance applications. <div>
arXiv:2512.12199v1 Announce Type: new 
Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection</title>
<link>https://arxiv.org/abs/2512.12205</link>
<guid>https://arxiv.org/abs/2512.12205</guid>
<content:encoded><![CDATA[
<div> Keywords: urban streetlights, visual drift, anomaly detection, CNN-VAE, smart city  

<br /><br />Summary:  
1. This work introduces a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras across Bristol, U.K., spanning from 2021 to 2025.  
2. The dataset includes over 526,000 images taken hourly under various lighting, weather, and seasonal conditions, with comprehensive metadata such as timestamps, GPS coordinates, and device identifiers.  
3. The dataset facilitates detailed studies on visual drift, anomaly detection, and machine learning operations (MLOps) for smart city applications.  
4. To assist secondary analysis, the authors provide a self-supervised modeling framework using convolutional variational autoencoders (CNN-VAEs), trained separately for each camera and for day/night image sets.  
5. Two per-sample drift metrics are defined: relative centroid drift, which tracks latent space deviations from a baseline quarter, and relative reconstruction error, which quantifies normalized image-domain degradation.  
6. The dataset serves as a fine-grained, realistic benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems in urban environments.  
7. The images and structured metadata are publicly available in JPEG and CSV formats to ensure reproducibility and enable downstream applications such as streetlight monitoring, weather inference, and urban scene understanding.  
8. Dataset access is provided via the Zenodo repository at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120. <div>
arXiv:2512.12205v1 Announce Type: new 
Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB</title>
<link>https://arxiv.org/abs/2512.12206</link>
<guid>https://arxiv.org/abs/2512.12206</guid>
<content:encoded><![CDATA[
<div> Keywords: distracted driving, impulse radio ultra-wideband radar, Vision Transformer, driver activity recognition, dataset

<br /><br />Summary: This paper tackles two major challenges in driver activity recognition (DAR) using impulse radio ultra-wideband (IR-UWB) radar: the scarcity of large-scale real-world datasets for diverse distracted driving behaviors and the difficulty of adapting fixed-input Vision Transformers (ViTs) to radar data with non-standard dimensions. First, the authors introduce the ALERT dataset, comprising 10,220 radar samples gathered under real driving conditions that cover seven different distracted driving activities. Second, they propose the input-size-agnostic Vision Transformer (ISA-ViT), a novel framework specifically designed for UWB radar-based DAR. ISA-ViT effectively resizes radar data while preserving important radar-domain features such as Doppler shifts and phase information, overcoming the limitations of simple resizing methods. The approach adjusts patch configurations and uses pre-trained positional embedding vectors (PEVs), enabling flexible input dimensions for ViTs. Moreover, a domain fusion strategy combining range- and frequency-domain features is implemented to boost classification accuracy further. Extensive experiments demonstrate that ISA-ViT achieves a 22.68% improvement in accuracy compared to existing ViT-based methods for UWB radar DAR. By releasing the ALERT dataset and detailing the input-size-agnostic method, this work advances scalable, privacy-preserving distracted driving detection for real-world applications. <div>
arXiv:2512.12206v1 Announce Type: new 
Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction</title>
<link>https://arxiv.org/abs/2512.12208</link>
<guid>https://arxiv.org/abs/2512.12208</guid>
<content:encoded><![CDATA[
<div> Keywords: Autism Spectrum Disorder, emotion recognition, humanoid robot, deep learning, MediaPipe FaceMesh<br /><br />Summary:<br /><br />This study addresses the challenge of understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interactions, focusing on reactions to name-calling by a humanoid robot (NAO) in controlled settings. A substantial dataset of around 50,000 facial frames from 15 ASD children was collected for analysis. The proposed approach combines a fine-tuned ResNet-50 convolutional neural network (CNN) with a three-layer Graph Convolutional Network (GCN) that processes both visual data and geometric features extracted from MediaPipe FaceMesh landmarks. To annotate emotions, a weighted ensemble method using DeepFace and FER models generated probabilistic soft labels across seven emotion categories. The final classification utilized a fused embedding optimized by minimizing the Kullback-Leibler divergence, enhancing the recognition of subtle affective cues. This hybrid model demonstrated robust performance in capturing micro-emotions in neurodivergent children, addressing a significant gap in autism-specific human-robot interaction (HRI) research. Moreover, this work represents the first large-scale real-world dataset and deep learning pipeline from India dedicated to emotion analysis in ASD using social robotics. Consequently, it lays an essential foundation for future personalized assistive technologies and clinical applications involving affective profiling in ASD. <div>
arXiv:2512.12208v1 Announce Type: new 
Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CineLOG: A Training Free Approach for Cinematic Long Video Generation</title>
<link>https://arxiv.org/abs/2512.12209</link>
<guid>https://arxiv.org/abs/2512.12209</guid>
<content:encoded><![CDATA[
<div> Keywords: controllable video synthesis, camera trajectory, cinematic genres, CineLOG dataset, Trajectory Guided Transition Module<br /><br />Summary:<br /><br />1. The paper addresses the challenge of controllable video synthesis in computer vision, focusing on fine-grained control beyond textual prompts, specifically targeting cinematic attributes such as camera trajectory and film genre. <br />2. Existing datasets suffer from problems like data imbalance, noisy labels, and a gap between simulated and real videos, limiting progress in this area. <br />3. To overcome these issues, the authors introduce CineLOG, a novel dataset comprising 5,000 high-quality, balanced, and uncut video clips. Each video is accompanied by detailed scene descriptions, explicit camera instructions following a standardized cinematic taxonomy, and genre labels, covering 17 camera movements and 15 film genres evenly. <br />4. The paper presents a novel multi-stage pipeline that breaks down the complex text-to-video generation task into four simpler stages using more mature technology, improving generation quality and control. <br />5. A key innovation is the Trajectory Guided Transition Module, which produces smooth spatio-temporal interpolations enabling coherent multi-shot video sequences. Human evaluations demonstrate that this pipeline substantially outperforms state-of-the-art end-to-end text-to-video models in faithfully following camera and screenplay instructions while maintaining professional visual quality. <br />6. The dataset and code have been made publicly available to facilitate further research and development in controllable cinematic video synthesis. <div>
arXiv:2512.12209v1 Announce Type: new 
Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking</title>
<link>https://arxiv.org/abs/2512.12218</link>
<guid>https://arxiv.org/abs/2512.12218</guid>
<content:encoded><![CDATA[
<div> Reasoning-augmented vision language models, visual faithfulness, reasoning chains, perception steps, self-reflection procedure  

<br /><br />Summary:  
This paper addresses new failure modes in reasoning-augmented vision language models (VLMs), where models may produce correct answers through visually unfaithful intermediate steps or reason faithfully but still err in the final prediction. It highlights that traditional evaluation metrics focusing solely on final-answer accuracy fail to capture these nuanced behaviors. To overcome this, the authors introduce the concept of visual faithfulness for reasoning chains, emphasizing the importance of perception steps being grounded in the image itself. They propose a novel, training- and reference-free evaluation framework that separates reasoning chains into perception and reasoning components, utilizing off-the-shelf VLM judges to assess step-level faithfulness. This method is further validated by a human meta-evaluation to ensure reliability. Building on this evaluation metric, the paper presents a lightweight self-reflection procedure that detects and selectively regenerates unfaithful perception steps without requiring additional training. Experimental results across multiple reasoning-trained VLMs and perception-intensive benchmarks demonstrate that this approach successfully reduces the rate of unfaithful perception steps while maintaining final-answer accuracy. Overall, the work enhances the reliability and interpretability of multimodal reasoning in VLMs by ensuring perception steps remain visually faithful. <div>
arXiv:2512.12218v1 Announce Type: new 
Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Grained Zero-Shot Learning with Attribute-Centric Representations</title>
<link>https://arxiv.org/abs/2512.12219</link>
<guid>https://arxiv.org/abs/2512.12219</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-shot learning, attribute disentanglement, mixture-of-experts, transformer, fine-grained recognition

<br /><br />Summary:  
This paper addresses the challenge of recognizing unseen fine-grained categories by tackling attribute entanglement, where conventional models merge distinct visual attributes such as color, shape, and texture into a single embedding, causing interference and loss of critical distinctions. The authors propose a zero-shot learning framework called Attribute-Centric Representations (ACR) that explicitly enforces attribute disentanglement during representation learning. ACR employs two mixture-of-experts components: Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). MoPE is integrated into the transformer model via a dual-level routing mechanism that routes image patches to specialized experts, ensuring that coherent attribute families are processed separately. MoAE then projects these refined features into sparse, part-aware attribute maps, enabling more robust zero-shot classification. The method is evaluated on standard zero-shot learning benchmarks CUB, AwA2, and SUN, demonstrating consistent state-of-the-art performance. This approach surpasses prior post-hoc disentanglement methods since it prevents attributes from mixing during early representation learning rather than correcting mixed features afterward. Overall, ACR advances fine-grained zero-shot recognition through a novel transformer-based disentanglement framework using expert mixtures for improved interpretability and accuracy. <div>
arXiv:2512.12219v1 Announce Type: new 
Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation</title>
<link>https://arxiv.org/abs/2512.12220</link>
<guid>https://arxiv.org/abs/2512.12220</guid>
<content:encoded><![CDATA[
<div> Keywords: professional image generation, ProImage-Bench, scientific diagrams, rubric evaluation, iterative refinement<br /><br />Summary:<br /><br />1. This paper addresses professional image generation, focusing on creating scientifically precise and information-dense illustrations from technical descriptions rather than just visually plausible images.<br />2. The authors introduce ProImage-Bench, a detailed rubric-based benchmark aimed at biology schematics, engineering and patent drawings, and general scientific diagrams.<br />3. ProImage-Bench is built using 654 figures sourced from real textbooks and technical reports, with detailed image instructions and a hierarchical rubric system that breaks down correctness into 6,076 criteria and 44,131 binary checks.<br />4. The rubrics are developed by leveraging large multimodal models to extract criteria from surrounding texts and reference figures, and are evaluated using an automated large multimodal model (LMM)-based judge with a penalty scheme to generate interpretable scores.<br />5. Benchmarking existing text-to-image models on ProImage-Bench shows a significant performance gap, with the best model achieving only 0.791 rubric accuracy and 0.553 criterion score, indicating challenges in achieving fine-grained scientific fidelity.<br />6. The rubrics also function as actionable supervision; by feeding failed checks back into an editing model, an iterative refinement process substantially improved generation quality from 0.653 to 0.865 rubric accuracy and from 0.388 to 0.697 criterion score.<br />7. Overall, ProImage-Bench provides both a rigorous diagnostic tool and a scalable signal to improve the fidelity of specification-faithful scientific illustrations through professional image generation. <div>
arXiv:2512.12220v1 Announce Type: new 
Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs</title>
<link>https://arxiv.org/abs/2512.12222</link>
<guid>https://arxiv.org/abs/2512.12222</guid>
<content:encoded><![CDATA[
<div> infant brain MRI, segmentation accuracy, SynthSeg, SamSeg, fractal dimension<br /><br />Summary:<br /><br />1. Accurate segmentation of infant brain MRI is critical for quantifying developmental structural changes but is challenging due to ongoing myelination and reduced tissue contrast.<br />2. This study assessed segmentation performance on the Baby Open Brains (BOB) dataset comprising 71 scans from infants aged 1 to 9 months.<br />3. Two methods, SynthSeg and SamSeg, were compared against expert manual annotations using metrics including Dice coefficient, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information.<br />4. SynthSeg consistently outperformed SamSeg across all quality metrics, achieving a mean Dice score over 0.8 for major brain regions and volumetric estimates closely aligned with the manual references (+4% mean deviation).<br />5. SamSeg showed significant overestimation of ventricular and whole-brain volumes, with a mean bias of +76%.<br />6. Segmentation accuracy improved with increasing infant age, correlating with enhanced tissue contrast during myelination.<br />7. Fractal dimension (FD) analyses revealed notable regional differences between SynthSeg and expert segmentations, and variability related to segmentation error exceeded most group differences observed in developmental studies.<br />8. Volumetric and FD deviations were positively correlated, indicating that segmentation bias directly impacts FD estimation.<br />9. Overall, SynthSeg was identified as the most reliable method for volumetric and FD assessment in pediatric MRI, though small morphological variations should be interpreted cautiously due to inherent segmentation uncertainties. <div>
arXiv:2512.12222v1 Announce Type: new 
Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder</title>
<link>https://arxiv.org/abs/2512.12229</link>
<guid>https://arxiv.org/abs/2512.12229</guid>
<content:encoded><![CDATA[
<div> Keywords: ultra-low bitrate, image compression, shallow encoder, diffusion decoder, feature distillation  

<br /><br />Summary:  
This paper addresses ultra-low bitrate image compression (below 0.05 bits per pixel), a critical need for bandwidth-limited and computation-constrained devices like edge hardware. Current methods rely heavily on large pretrained encoders, such as VAEs or tokenizer-based models, which restrict deployment on weak senders due to their computational demand. The authors propose a novel Asymmetric Extreme Image Compression (AEIC) framework that balances encoding simplicity and decoding quality by employing moderate or shallow encoder networks paired with a one-step diffusion decoder. This approach enables high-fidelity, realistic image reconstructions even at extremely low bitrates. To improve shallow encoder performance, a dual-side feature distillation technique is introduced, transferring knowledge from moderate encoder AEIC models to their shallow counterparts. Experimental results demonstrate that AEIC surpasses existing methods in rate-distortion-perception metrics at ultra-low bitrates. Moreover, the framework achieves exceptional encoding efficiency, reaching 35.8 frames per second on 1080p inputs, while maintaining competitive decoding speeds relative to existing approaches. Overall, AEIC offers a compelling solution for encoding-constrained scenarios, combining fast encoding, quality reconstruction, and resource efficiency. <div>
arXiv:2512.12229v1 Announce Type: new 
Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moment and Highlight Detection via MLLM Frame Segmentation</title>
<link>https://arxiv.org/abs/2512.12246</link>
<guid>https://arxiv.org/abs/2512.12246</guid>
<content:encoded><![CDATA[
<div> Detecting video moments, highlights, natural-language queries, segmentation losses, multimodal LLM  

<br /><br />Summary:  
This paper presents a novel approach for detecting video moments and highlights from natural-language queries by leveraging generative multimodal large language models (MLLMs). Unlike previous transformer-based methods or RL approaches, the method treats the output tokens of the LLM as binary segmentation sequences, where each token corresponds to a frame labeled "0" (background) or "1" (foreground). Input to the LLM is limited to a fixed set of frames along with a prompt guiding this binary output format. This allows direct application of segmentation losses as training objectives alongside the standard causal language modeling loss, enabling frame-level supervision and stable complementary signals for learning. At inference, beam search is used to generate the sequence and corresponding logits, which serve as predicted moments and highlight saliency scores. The approach is computationally efficient, sampling only 25 frames per video—less than half what comparable methods require. Despite this, it achieves strong highlight detection performance with 56.74 HIT@1 on the QVHighlights dataset, and surpasses baseline scores (35.28 MAP) in moment retrieval tasks. Overall, the segmentation-driven training approach effectively exploits the LLM’s reasoning and language capabilities to improve video understanding tasks based on text queries. <div>
arXiv:2512.12246v1 Announce Type: new 
Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.12268</link>
<guid>https://arxiv.org/abs/2512.12268</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, test-time prompt tuning, meta-learning, domain shifts, self-supervised auxiliary task<br /><br />Summary:<br /><br />This paper addresses the issue of domain shift sensitivity in Vision-Language Models (VLMs) like CLIP, which, despite strong zero-shot generalization, struggle to maintain performance when the data distribution changes at test time. To overcome this, the authors propose Meta Test-Time Prompt Tuning (MetaTPT), a novel meta-learning framework designed to improve test-time adaptation. MetaTPT introduces a self-supervised auxiliary task that dynamically learns parameterized augmentations tailored to each sample, allowing the model to generate more expressive and informative transformations specific to the target domain. The framework uses a dual-loop optimization strategy: the inner loop focuses on learning the self-supervised task that produces consistent and meaningful views of the input, while the outer loop performs prompt tuning by enforcing consistency between these augmented views. By integrating augmentation learning directly with prompt tuning, MetaTPT enhances the model’s ability to generalize under challenging domain shifts. Extensive experimental results demonstrate that MetaTPT outperforms existing methods on multiple domain generalization and cross-dataset benchmarks, establishing new state-of-the-art performance in these areas and providing a robust approach for test-time adaptation in vision-language tasks. <div>
arXiv:2512.12268v1 Announce Type: new 
Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions</title>
<link>https://arxiv.org/abs/2512.12277</link>
<guid>https://arxiv.org/abs/2512.12277</guid>
<content:encoded><![CDATA[
<div> Keywords: Facial Expression Recognition, Continual Learning, Catastrophic Forgetting, Bayesian Gaussian Mixture Models, Facial Action Units  

<br /><br />Summary:  
This paper addresses the challenge of recognizing human emotions through facial expressions in AI systems that continuously learn over time. The authors propose a hybrid framework combining deep convolutional neural network features with Facial Action Units (AUs) derived from the Facial Action Coding System (FACS) to capture complementary information. To model the combined features, Bayesian Gaussian Mixture Models (BGMMs) are used, offering a probabilistic and lightweight approach that avoids the need for retraining while maintaining strong discriminative ability. The framework is tested on the Compound Facial Expression of Emotion (CFEE) dataset, demonstrating that the model can initially learn basic expressions and then progressively incorporate recognition of compound expressions. Experimental results show that this method improves overall accuracy, enhances knowledge retention, and significantly reduces catastrophic forgetting compared to traditional continual learning methods. The approach supports the development of emotionally intelligent AI applications, with potential uses in education, healthcare, and adaptive user interfaces where ongoing understanding of human emotions is critical. <div>
arXiv:2512.12277v1 Announce Type: new 
Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection</title>
<link>https://arxiv.org/abs/2512.12281</link>
<guid>https://arxiv.org/abs/2512.12281</guid>
<content:encoded><![CDATA[
arXiv:2512.12281v1 Announce Type: new 
Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealDrag: The First Dragging Benchmark with Real Target Image</title>
<link>https://arxiv.org/abs/2512.12287</link>
<guid>https://arxiv.org/abs/2512.12287</guid>
<content:encoded><![CDATA[
arXiv:2512.12287v1 Announce Type: new 
Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search</title>
<link>https://arxiv.org/abs/2512.12296</link>
<guid>https://arxiv.org/abs/2512.12296</guid>
<content:encoded><![CDATA[
arXiv:2512.12296v1 Announce Type: new 
Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.12302</link>
<guid>https://arxiv.org/abs/2512.12302</guid>
<content:encoded><![CDATA[
arXiv:2512.12302v1 Announce Type: new 
Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.12303</link>
<guid>https://arxiv.org/abs/2512.12303</guid>
<content:encoded><![CDATA[
arXiv:2512.12303v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2512.12307</link>
<guid>https://arxiv.org/abs/2512.12307</guid>
<content:encoded><![CDATA[
arXiv:2512.12307v1 Announce Type: new 
Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeDetect: Fast Open-Vocabulary Object Detection as Retrieval</title>
<link>https://arxiv.org/abs/2512.12309</link>
<guid>https://arxiv.org/abs/2512.12309</guid>
<content:encoded><![CDATA[
arXiv:2512.12309v1 Announce Type: new 
Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Control for Inference-Time Guidance of Denoising Diffusion Models</title>
<link>https://arxiv.org/abs/2512.12339</link>
<guid>https://arxiv.org/abs/2512.12339</guid>
<content:encoded><![CDATA[
arXiv:2512.12339v1 Announce Type: new 
Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection</title>
<link>https://arxiv.org/abs/2512.12357</link>
<guid>https://arxiv.org/abs/2512.12357</guid>
<content:encoded><![CDATA[
arXiv:2512.12357v1 Announce Type: new 
Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2512.12360</link>
<guid>https://arxiv.org/abs/2512.12360</guid>
<content:encoded><![CDATA[
arXiv:2512.12360v1 Announce Type: new 
Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative</title>
<link>https://arxiv.org/abs/2512.12372</link>
<guid>https://arxiv.org/abs/2512.12372</guid>
<content:encoded><![CDATA[
arXiv:2512.12372v1 Announce Type: new 
Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping</title>
<link>https://arxiv.org/abs/2512.12375</link>
<guid>https://arxiv.org/abs/2512.12375</guid>
<content:encoded><![CDATA[
arXiv:2512.12375v1 Announce Type: new 
Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2512.12378</link>
<guid>https://arxiv.org/abs/2512.12378</guid>
<content:encoded><![CDATA[
arXiv:2512.12378v1 Announce Type: new 
Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speedrunning ImageNet Diffusion</title>
<link>https://arxiv.org/abs/2512.12386</link>
<guid>https://arxiv.org/abs/2512.12386</guid>
<content:encoded><![CDATA[
arXiv:2512.12386v1 Announce Type: new 
Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States</title>
<link>https://arxiv.org/abs/2512.12395</link>
<guid>https://arxiv.org/abs/2512.12395</guid>
<content:encoded><![CDATA[
arXiv:2512.12395v1 Announce Type: new 
Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams</title>
<link>https://arxiv.org/abs/2512.12410</link>
<guid>https://arxiv.org/abs/2512.12410</guid>
<content:encoded><![CDATA[
arXiv:2512.12410v1 Announce Type: new 
Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics</title>
<link>https://arxiv.org/abs/2512.12424</link>
<guid>https://arxiv.org/abs/2512.12424</guid>
<content:encoded><![CDATA[
arXiv:2512.12424v1 Announce Type: new 
Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation</title>
<link>https://arxiv.org/abs/2512.12425</link>
<guid>https://arxiv.org/abs/2512.12425</guid>
<content:encoded><![CDATA[
arXiv:2512.12425v1 Announce Type: new 
Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endless World: Real-Time 3D-Aware Long Video Generation</title>
<link>https://arxiv.org/abs/2512.12430</link>
<guid>https://arxiv.org/abs/2512.12430</guid>
<content:encoded><![CDATA[
arXiv:2512.12430v1 Announce Type: new 
Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields</title>
<link>https://arxiv.org/abs/2512.12459</link>
<guid>https://arxiv.org/abs/2512.12459</guid>
<content:encoded><![CDATA[
arXiv:2512.12459v1 Announce Type: new 
Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.12487</link>
<guid>https://arxiv.org/abs/2512.12487</guid>
<content:encoded><![CDATA[
arXiv:2512.12487v1 Announce Type: new 
Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings</title>
<link>https://arxiv.org/abs/2512.12492</link>
<guid>https://arxiv.org/abs/2512.12492</guid>
<content:encoded><![CDATA[
arXiv:2512.12492v1 Announce Type: new 
Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention</title>
<link>https://arxiv.org/abs/2512.12498</link>
<guid>https://arxiv.org/abs/2512.12498</guid>
<content:encoded><![CDATA[
arXiv:2512.12498v1 Announce Type: new 
Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Spatiotemporal Data Augmentation</title>
<link>https://arxiv.org/abs/2512.12508</link>
<guid>https://arxiv.org/abs/2512.12508</guid>
<content:encoded><![CDATA[
arXiv:2512.12508v1 Announce Type: new 
Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Animus3D: Text-driven 3D Animation via Motion Score Distillation</title>
<link>https://arxiv.org/abs/2512.12534</link>
<guid>https://arxiv.org/abs/2512.12534</guid>
<content:encoded><![CDATA[
arXiv:2512.12534v1 Announce Type: new 
Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling</title>
<link>https://arxiv.org/abs/2512.12539</link>
<guid>https://arxiv.org/abs/2512.12539</guid>
<content:encoded><![CDATA[
arXiv:2512.12539v1 Announce Type: new 
Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised Contrastive Frame Aggregation for Video Representation Learning</title>
<link>https://arxiv.org/abs/2512.12549</link>
<guid>https://arxiv.org/abs/2512.12549</guid>
<content:encoded><![CDATA[
arXiv:2512.12549v1 Announce Type: new 
Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding</title>
<link>https://arxiv.org/abs/2512.12560</link>
<guid>https://arxiv.org/abs/2512.12560</guid>
<content:encoded><![CDATA[
arXiv:2512.12560v1 Announce Type: new 
Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models</title>
<link>https://arxiv.org/abs/2512.12571</link>
<guid>https://arxiv.org/abs/2512.12571</guid>
<content:encoded><![CDATA[
arXiv:2512.12571v1 Announce Type: new 
Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis</title>
<link>https://arxiv.org/abs/2512.12586</link>
<guid>https://arxiv.org/abs/2512.12586</guid>
<content:encoded><![CDATA[
arXiv:2512.12586v1 Announce Type: new 
Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Wire-Harness Color Sequence Detector</title>
<link>https://arxiv.org/abs/2512.12590</link>
<guid>https://arxiv.org/abs/2512.12590</guid>
<content:encoded><![CDATA[
arXiv:2512.12590v1 Announce Type: new 
Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation</title>
<link>https://arxiv.org/abs/2512.12595</link>
<guid>https://arxiv.org/abs/2512.12595</guid>
<content:encoded><![CDATA[
arXiv:2512.12595v1 Announce Type: new 
Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models</title>
<link>https://arxiv.org/abs/2512.12596</link>
<guid>https://arxiv.org/abs/2512.12596</guid>
<content:encoded><![CDATA[
arXiv:2512.12596v1 Announce Type: new 
Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Aware Scene-Consistent Image Generation</title>
<link>https://arxiv.org/abs/2512.12598</link>
<guid>https://arxiv.org/abs/2512.12598</guid>
<content:encoded><![CDATA[
arXiv:2512.12598v1 Announce Type: new 
Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching</title>
<link>https://arxiv.org/abs/2512.12604</link>
<guid>https://arxiv.org/abs/2512.12604</guid>
<content:encoded><![CDATA[
arXiv:2512.12604v1 Announce Type: new 
Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching</title>
<link>https://arxiv.org/abs/2512.12610</link>
<guid>https://arxiv.org/abs/2512.12610</guid>
<content:encoded><![CDATA[
arXiv:2512.12610v1 Announce Type: new 
Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation</title>
<link>https://arxiv.org/abs/2512.12622</link>
<guid>https://arxiv.org/abs/2512.12622</guid>
<content:encoded><![CDATA[
arXiv:2512.12622v1 Announce Type: new 
Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space</title>
<link>https://arxiv.org/abs/2512.12623</link>
<guid>https://arxiv.org/abs/2512.12623</guid>
<content:encoded><![CDATA[
arXiv:2512.12623v1 Announce Type: new 
Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2512.12633</link>
<guid>https://arxiv.org/abs/2512.12633</guid>
<content:encoded><![CDATA[
arXiv:2512.12633v1 Announce Type: new 
Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Fundus Image Registration under Large FoV Disparity</title>
<link>https://arxiv.org/abs/2512.12657</link>
<guid>https://arxiv.org/abs/2512.12657</guid>
<content:encoded><![CDATA[
arXiv:2512.12657v1 Announce Type: new 
Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogDoc: Towards Unified thinking in Documents</title>
<link>https://arxiv.org/abs/2512.12658</link>
<guid>https://arxiv.org/abs/2512.12658</guid>
<content:encoded><![CDATA[
arXiv:2512.12658v1 Announce Type: new 
Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images</title>
<link>https://arxiv.org/abs/2512.12662</link>
<guid>https://arxiv.org/abs/2512.12662</guid>
<content:encoded><![CDATA[
arXiv:2512.12662v1 Announce Type: new 
Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation</title>
<link>https://arxiv.org/abs/2512.12664</link>
<guid>https://arxiv.org/abs/2512.12664</guid>
<content:encoded><![CDATA[
arXiv:2512.12664v1 Announce Type: new 
Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning</title>
<link>https://arxiv.org/abs/2512.12667</link>
<guid>https://arxiv.org/abs/2512.12667</guid>
<content:encoded><![CDATA[
arXiv:2512.12667v1 Announce Type: new 
Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation</title>
<link>https://arxiv.org/abs/2512.12673</link>
<guid>https://arxiv.org/abs/2512.12673</guid>
<content:encoded><![CDATA[
arXiv:2512.12673v1 Announce Type: new 
Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling</title>
<link>https://arxiv.org/abs/2512.12675</link>
<guid>https://arxiv.org/abs/2512.12675</guid>
<content:encoded><![CDATA[
arXiv:2512.12675v1 Announce Type: new 
Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\beta$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment</title>
<link>https://arxiv.org/abs/2512.12678</link>
<guid>https://arxiv.org/abs/2512.12678</guid>
<content:encoded><![CDATA[
arXiv:2512.12678v1 Announce Type: new 
Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $\beta$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $\beta$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $\beta$-Contextualized Contrastive Alignment Loss ($\beta$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $\beta$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $\beta$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Vision-Language Reasoning via Adaptive Token Pruning</title>
<link>https://arxiv.org/abs/2512.12701</link>
<guid>https://arxiv.org/abs/2512.12701</guid>
<content:encoded><![CDATA[
arXiv:2512.12701v1 Announce Type: new 
Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Motion Generation using Part-level Reliable Data from Videos</title>
<link>https://arxiv.org/abs/2512.12703</link>
<guid>https://arxiv.org/abs/2512.12703</guid>
<content:encoded><![CDATA[
arXiv:2512.12703v1 Announce Type: new 
Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images</title>
<link>https://arxiv.org/abs/2512.12718</link>
<guid>https://arxiv.org/abs/2512.12718</guid>
<content:encoded><![CDATA[
arXiv:2512.12718v1 Announce Type: new 
Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation</title>
<link>https://arxiv.org/abs/2512.12751</link>
<guid>https://arxiv.org/abs/2512.12751</guid>
<content:encoded><![CDATA[
arXiv:2512.12751v1 Announce Type: new 
Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning</title>
<link>https://arxiv.org/abs/2512.12756</link>
<guid>https://arxiv.org/abs/2512.12756</guid>
<content:encoded><![CDATA[
arXiv:2512.12756v1 Announce Type: new 
Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence</title>
<link>https://arxiv.org/abs/2512.12768</link>
<guid>https://arxiv.org/abs/2512.12768</guid>
<content:encoded><![CDATA[
arXiv:2512.12768v1 Announce Type: new 
Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior</title>
<link>https://arxiv.org/abs/2512.12774</link>
<guid>https://arxiv.org/abs/2512.12774</guid>
<content:encoded><![CDATA[
arXiv:2512.12774v1 Announce Type: new 
Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context</title>
<link>https://arxiv.org/abs/2512.12790</link>
<guid>https://arxiv.org/abs/2512.12790</guid>
<content:encoded><![CDATA[
arXiv:2512.12790v1 Announce Type: new 
Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning</title>
<link>https://arxiv.org/abs/2512.12799</link>
<guid>https://arxiv.org/abs/2512.12799</guid>
<content:encoded><![CDATA[
arXiv:2512.12799v1 Announce Type: new 
Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Common and Salient Generative Factors Between Two Image Datasets</title>
<link>https://arxiv.org/abs/2512.12800</link>
<guid>https://arxiv.org/abs/2512.12800</guid>
<content:encoded><![CDATA[
arXiv:2512.12800v1 Announce Type: new 
Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding</title>
<link>https://arxiv.org/abs/2512.12822</link>
<guid>https://arxiv.org/abs/2512.12822</guid>
<content:encoded><![CDATA[
arXiv:2512.12822v1 Announce Type: new 
Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners</title>
<link>https://arxiv.org/abs/2512.12824</link>
<guid>https://arxiv.org/abs/2512.12824</guid>
<content:encoded><![CDATA[
arXiv:2512.12824v1 Announce Type: new 
Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal</title>
<link>https://arxiv.org/abs/2512.12875</link>
<guid>https://arxiv.org/abs/2512.12875</guid>
<content:encoded><![CDATA[
arXiv:2512.12875v1 Announce Type: new 
Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection</title>
<link>https://arxiv.org/abs/2512.12884</link>
<guid>https://arxiv.org/abs/2512.12884</guid>
<content:encoded><![CDATA[
arXiv:2512.12884v1 Announce Type: new 
Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition</title>
<link>https://arxiv.org/abs/2512.12885</link>
<guid>https://arxiv.org/abs/2512.12885</guid>
<content:encoded><![CDATA[
arXiv:2512.12885v1 Announce Type: new 
Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification</title>
<link>https://arxiv.org/abs/2512.12887</link>
<guid>https://arxiv.org/abs/2512.12887</guid>
<content:encoded><![CDATA[
arXiv:2512.12887v1 Announce Type: new 
Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution</title>
<link>https://arxiv.org/abs/2512.12898</link>
<guid>https://arxiv.org/abs/2512.12898</guid>
<content:encoded><![CDATA[
arXiv:2512.12898v1 Announce Type: new 
Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2512.12906</link>
<guid>https://arxiv.org/abs/2512.12906</guid>
<content:encoded><![CDATA[
arXiv:2512.12906v1 Announce Type: new 
Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2512.12925</link>
<guid>https://arxiv.org/abs/2512.12925</guid>
<content:encoded><![CDATA[
arXiv:2512.12925v1 Announce Type: new 
Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation</title>
<link>https://arxiv.org/abs/2512.12929</link>
<guid>https://arxiv.org/abs/2512.12929</guid>
<content:encoded><![CDATA[
arXiv:2512.12929v1 Announce Type: new 
Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion</title>
<link>https://arxiv.org/abs/2512.12935</link>
<guid>https://arxiv.org/abs/2512.12935</guid>
<content:encoded><![CDATA[
arXiv:2512.12935v1 Announce Type: new 
Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content Adaptive based Motion Alignment Framework for Learned Video Compression</title>
<link>https://arxiv.org/abs/2512.12936</link>
<guid>https://arxiv.org/abs/2512.12936</guid>
<content:encoded><![CDATA[
arXiv:2512.12936v1 Announce Type: new 
Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction</title>
<link>https://arxiv.org/abs/2512.12941</link>
<guid>https://arxiv.org/abs/2512.12941</guid>
<content:encoded><![CDATA[
arXiv:2512.12941v1 Announce Type: new 
Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer</title>
<link>https://arxiv.org/abs/2512.12963</link>
<guid>https://arxiv.org/abs/2512.12963</guid>
<content:encoded><![CDATA[
arXiv:2512.12963v1 Announce Type: new 
Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference</title>
<link>https://arxiv.org/abs/2512.12977</link>
<guid>https://arxiv.org/abs/2512.12977</guid>
<content:encoded><![CDATA[
arXiv:2512.12977v1 Announce Type: new 
Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes</title>
<link>https://arxiv.org/abs/2512.12982</link>
<guid>https://arxiv.org/abs/2512.12982</guid>
<content:encoded><![CDATA[
arXiv:2512.12982v1 Announce Type: new 
Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Uncertainty for Zero-Shot Adversarial CLIP</title>
<link>https://arxiv.org/abs/2512.12997</link>
<guid>https://arxiv.org/abs/2512.12997</guid>
<content:encoded><![CDATA[
arXiv:2512.12997v1 Announce Type: new 
Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Step Distillation for Text-to-Image Generation: A Practical Guide</title>
<link>https://arxiv.org/abs/2512.13006</link>
<guid>https://arxiv.org/abs/2512.13006</guid>
<content:encoded><![CDATA[
arXiv:2512.13006v1 Announce Type: new 
Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light Field Based 6DoF Tracking of Previously Unobserved Objects</title>
<link>https://arxiv.org/abs/2512.13007</link>
<guid>https://arxiv.org/abs/2512.13007</guid>
<content:encoded><![CDATA[
arXiv:2512.13007v1 Announce Type: new 
Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading</title>
<link>https://arxiv.org/abs/2512.13008</link>
<guid>https://arxiv.org/abs/2512.13008</guid>
<content:encoded><![CDATA[
arXiv:2512.13008v1 Announce Type: new 
Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion</title>
<link>https://arxiv.org/abs/2512.13014</link>
<guid>https://arxiv.org/abs/2512.13014</guid>
<content:encoded><![CDATA[
arXiv:2512.13014v1 Announce Type: new 
Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Happens Next? Next Scene Prediction with a Unified Video Model</title>
<link>https://arxiv.org/abs/2512.13015</link>
<guid>https://arxiv.org/abs/2512.13015</guid>
<content:encoded><![CDATA[
arXiv:2512.13015v1 Announce Type: new 
Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing</title>
<link>https://arxiv.org/abs/2512.13018</link>
<guid>https://arxiv.org/abs/2512.13018</guid>
<content:encoded><![CDATA[
arXiv:2512.13018v1 Announce Type: new 
Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SneakPeek: Future-Guided Instructional Streaming Video Generation</title>
<link>https://arxiv.org/abs/2512.13019</link>
<guid>https://arxiv.org/abs/2512.13019</guid>
<content:encoded><![CDATA[
arXiv:2512.13019v1 Announce Type: new 
Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motus: A Unified Latent Action World Model</title>
<link>https://arxiv.org/abs/2512.13030</link>
<guid>https://arxiv.org/abs/2512.13030</guid>
<content:encoded><![CDATA[
arXiv:2512.13030v1 Announce Type: new 
Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs</title>
<link>https://arxiv.org/abs/2512.13031</link>
<guid>https://arxiv.org/abs/2512.13031</guid>
<content:encoded><![CDATA[
arXiv:2512.13031v1 Announce Type: new 
Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13039</link>
<guid>https://arxiv.org/abs/2512.13039</guid>
<content:encoded><![CDATA[
arXiv:2512.13039v1 Announce Type: new 
Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</title>
<link>https://arxiv.org/abs/2512.13043</link>
<guid>https://arxiv.org/abs/2512.13043</guid>
<content:encoded><![CDATA[
arXiv:2512.13043v1 Announce Type: new 
Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing</title>
<link>https://arxiv.org/abs/2512.13055</link>
<guid>https://arxiv.org/abs/2512.13055</guid>
<content:encoded><![CDATA[
arXiv:2512.13055v1 Announce Type: new 
Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models</title>
<link>https://arxiv.org/abs/2512.13072</link>
<guid>https://arxiv.org/abs/2512.13072</guid>
<content:encoded><![CDATA[
arXiv:2512.13072v1 Announce Type: new 
Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heart Disease Prediction using Case Based Reasoning (CBR)</title>
<link>https://arxiv.org/abs/2512.13078</link>
<guid>https://arxiv.org/abs/2512.13078</guid>
<content:encoded><![CDATA[
arXiv:2512.13078v1 Announce Type: new 
Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiRe: Diversity-promoting Regularization for Dataset Condensation</title>
<link>https://arxiv.org/abs/2512.13083</link>
<guid>https://arxiv.org/abs/2512.13083</guid>
<content:encoded><![CDATA[
arXiv:2512.13083v1 Announce Type: new 
Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era</title>
<link>https://arxiv.org/abs/2512.13089</link>
<guid>https://arxiv.org/abs/2512.13089</guid>
<content:encoded><![CDATA[
arXiv:2512.13089v1 Announce Type: new 
Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13095</link>
<guid>https://arxiv.org/abs/2512.13095</guid>
<content:encoded><![CDATA[
arXiv:2512.13095v1 Announce Type: new 
Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.13101</link>
<guid>https://arxiv.org/abs/2512.13101</guid>
<content:encoded><![CDATA[
arXiv:2512.13101v1 Announce Type: new 
Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection</title>
<link>https://arxiv.org/abs/2512.13104</link>
<guid>https://arxiv.org/abs/2512.13104</guid>
<content:encoded><![CDATA[
arXiv:2512.13104v1 Announce Type: new 
Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather</title>
<link>https://arxiv.org/abs/2512.13107</link>
<guid>https://arxiv.org/abs/2512.13107</guid>
<content:encoded><![CDATA[
arXiv:2512.13107v1 Announce Type: new 
Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass</title>
<link>https://arxiv.org/abs/2512.13122</link>
<guid>https://arxiv.org/abs/2512.13122</guid>
<content:encoded><![CDATA[
arXiv:2512.13122v1 Announce Type: new 
Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping</title>
<link>https://arxiv.org/abs/2512.13130</link>
<guid>https://arxiv.org/abs/2512.13130</guid>
<content:encoded><![CDATA[
arXiv:2512.13130v1 Announce Type: new 
Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models</title>
<link>https://arxiv.org/abs/2512.13144</link>
<guid>https://arxiv.org/abs/2512.13144</guid>
<content:encoded><![CDATA[
arXiv:2512.13144v1 Announce Type: new 
Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion</title>
<link>https://arxiv.org/abs/2512.13147</link>
<guid>https://arxiv.org/abs/2512.13147</guid>
<content:encoded><![CDATA[
arXiv:2512.13147v1 Announce Type: new 
Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Image Fusion for Multi-View 3D Material Reconstruction</title>
<link>https://arxiv.org/abs/2512.13157</link>
<guid>https://arxiv.org/abs/2512.13157</guid>
<content:encoded><![CDATA[
arXiv:2512.13157v1 Announce Type: new 
Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis</title>
<link>https://arxiv.org/abs/2512.13164</link>
<guid>https://arxiv.org/abs/2512.13164</guid>
<content:encoded><![CDATA[
arXiv:2512.13164v1 Announce Type: new 
Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.13175</link>
<guid>https://arxiv.org/abs/2512.13175</guid>
<content:encoded><![CDATA[
arXiv:2512.13175v1 Announce Type: new 
Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion</title>
<link>https://arxiv.org/abs/2512.13177</link>
<guid>https://arxiv.org/abs/2512.13177</guid>
<content:encoded><![CDATA[
arXiv:2512.13177v1 Announce Type: new 
Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception</title>
<link>https://arxiv.org/abs/2512.13191</link>
<guid>https://arxiv.org/abs/2512.13191</guid>
<content:encoded><![CDATA[
arXiv:2512.13191v1 Announce Type: new 
Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling</title>
<link>https://arxiv.org/abs/2512.13192</link>
<guid>https://arxiv.org/abs/2512.13192</guid>
<content:encoded><![CDATA[
arXiv:2512.13192v1 Announce Type: new 
Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance</title>
<link>https://arxiv.org/abs/2512.13238</link>
<guid>https://arxiv.org/abs/2512.13238</guid>
<content:encoded><![CDATA[
arXiv:2512.13238v1 Announce Type: new 
Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits</title>
<link>https://arxiv.org/abs/2512.13247</link>
<guid>https://arxiv.org/abs/2512.13247</guid>
<content:encoded><![CDATA[
arXiv:2512.13247v1 Announce Type: new 
Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection</title>
<link>https://arxiv.org/abs/2512.13250</link>
<guid>https://arxiv.org/abs/2512.13250</guid>
<content:encoded><![CDATA[
arXiv:2512.13250v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing</title>
<link>https://arxiv.org/abs/2512.13276</link>
<guid>https://arxiv.org/abs/2512.13276</guid>
<content:encoded><![CDATA[
arXiv:2512.13276v1 Announce Type: new 
Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?</title>
<link>https://arxiv.org/abs/2512.13281</link>
<guid>https://arxiv.org/abs/2512.13281</guid>
<content:encoded><![CDATA[
arXiv:2512.13281v1 Announce Type: new 
Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images</title>
<link>https://arxiv.org/abs/2512.13285</link>
<guid>https://arxiv.org/abs/2512.13285</guid>
<content:encoded><![CDATA[
arXiv:2512.13285v1 Announce Type: new 
Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.13290</link>
<guid>https://arxiv.org/abs/2512.13290</guid>
<content:encoded><![CDATA[
arXiv:2512.13290v1 Announce Type: new 
Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement</title>
<link>https://arxiv.org/abs/2512.13303</link>
<guid>https://arxiv.org/abs/2512.13303</guid>
<content:encoded><![CDATA[
arXiv:2512.13303v1 Announce Type: new 
Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KlingAvatar 2.0 Technical Report</title>
<link>https://arxiv.org/abs/2512.13313</link>
<guid>https://arxiv.org/abs/2512.13313</guid>
<content:encoded><![CDATA[
arXiv:2512.13313v1 Announce Type: new 
Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face Identity Unlearning for Retrieval via Embedding Dispersion</title>
<link>https://arxiv.org/abs/2512.13317</link>
<guid>https://arxiv.org/abs/2512.13317</guid>
<content:encoded><![CDATA[
arXiv:2512.13317v1 Announce Type: new 
Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated User Identification from Facial Thermograms with Siamese Networks</title>
<link>https://arxiv.org/abs/2512.13361</link>
<guid>https://arxiv.org/abs/2512.13361</guid>
<content:encoded><![CDATA[
arXiv:2512.13361v1 Announce Type: new 
Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"</title>
<link>https://arxiv.org/abs/2512.13376</link>
<guid>https://arxiv.org/abs/2512.13376</guid>
<content:encoded><![CDATA[
arXiv:2512.13376v1 Announce Type: new 
Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs</title>
<link>https://arxiv.org/abs/2512.13392</link>
<guid>https://arxiv.org/abs/2512.13392</guid>
<content:encoded><![CDATA[
arXiv:2512.13392v1 Announce Type: new 
Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>rNCA: Self-Repairing Segmentation Masks</title>
<link>https://arxiv.org/abs/2512.13397</link>
<guid>https://arxiv.org/abs/2512.13397</guid>
<content:encoded><![CDATA[
arXiv:2512.13397v1 Announce Type: new 
Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $\beta_0$ errors by 60% and $\beta_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery</title>
<link>https://arxiv.org/abs/2512.13402</link>
<guid>https://arxiv.org/abs/2512.13402</guid>
<content:encoded><![CDATA[
arXiv:2512.13402v1 Announce Type: new 
Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computer vision training dataset generation for robotic environments using Gaussian splatting</title>
<link>https://arxiv.org/abs/2512.13411</link>
<guid>https://arxiv.org/abs/2512.13411</guid>
<content:encoded><![CDATA[
arXiv:2512.13411v1 Announce Type: new 
Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2512.13415</link>
<guid>https://arxiv.org/abs/2512.13415</guid>
<content:encoded><![CDATA[
arXiv:2512.13415v1 Announce Type: new 
Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Generate Cross-Task Unexploitable Examples</title>
<link>https://arxiv.org/abs/2512.13416</link>
<guid>https://arxiv.org/abs/2512.13416</guid>
<content:encoded><![CDATA[
arXiv:2512.13416v1 Announce Type: new 
Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RecTok: Reconstruction Distillation along Rectified Flow</title>
<link>https://arxiv.org/abs/2512.13421</link>
<guid>https://arxiv.org/abs/2512.13421</guid>
<content:encoded><![CDATA[
arXiv:2512.13421v1 Announce Type: new 
Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MineTheGap: Automatic Mining of Biases in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.13427</link>
<guid>https://arxiv.org/abs/2512.13427</guid>
<content:encoded><![CDATA[
arXiv:2512.13427v1 Announce Type: new 
Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification</title>
<link>https://arxiv.org/abs/2512.13428</link>
<guid>https://arxiv.org/abs/2512.13428</guid>
<content:encoded><![CDATA[
arXiv:2512.13428v1 Announce Type: new 
Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&amp;E whole slide images</title>
<link>https://arxiv.org/abs/2512.13440</link>
<guid>https://arxiv.org/abs/2512.13440</guid>
<content:encoded><![CDATA[
arXiv:2512.13440v1 Announce Type: new 
Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&amp;E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Modification: Inverse Domain Transformation for Robust Perception</title>
<link>https://arxiv.org/abs/2512.13454</link>
<guid>https://arxiv.org/abs/2512.13454</guid>
<content:encoded><![CDATA[
arXiv:2512.13454v1 Announce Type: new 
Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence</title>
<link>https://arxiv.org/abs/2512.13465</link>
<guid>https://arxiv.org/abs/2512.13465</guid>
<content:encoded><![CDATA[
arXiv:2512.13465v1 Announce Type: new 
Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$</title>
<link>https://arxiv.org/abs/2512.13492</link>
<guid>https://arxiv.org/abs/2512.13492</guid>
<content:encoded><![CDATA[
arXiv:2512.13492v1 Announce Type: new 
Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation</title>
<link>https://arxiv.org/abs/2512.13495</link>
<guid>https://arxiv.org/abs/2512.13495</guid>
<content:encoded><![CDATA[
arXiv:2512.13495v1 Announce Type: new 
Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model</title>
<link>https://arxiv.org/abs/2512.13507</link>
<guid>https://arxiv.org/abs/2512.13507</guid>
<content:encoded><![CDATA[
arXiv:2512.13507v1 Announce Type: new 
Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding</title>
<link>https://arxiv.org/abs/2512.13511</link>
<guid>https://arxiv.org/abs/2512.13511</guid>
<content:encoded><![CDATA[
arXiv:2512.13511v1 Announce Type: new 
Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains</title>
<link>https://arxiv.org/abs/2512.13534</link>
<guid>https://arxiv.org/abs/2512.13534</guid>
<content:encoded><![CDATA[
arXiv:2512.13534v1 Announce Type: new 
Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Human-Human Interaction Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.13560</link>
<guid>https://arxiv.org/abs/2512.13560</guid>
<content:encoded><![CDATA[
arXiv:2512.13560v1 Announce Type: new 
Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMhops-R1: Multimodal Multi-hop Reasoning</title>
<link>https://arxiv.org/abs/2512.13573</link>
<guid>https://arxiv.org/abs/2512.13573</guid>
<content:encoded><![CDATA[
arXiv:2512.13573v1 Announce Type: new 
Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lighting in Motion: Spatiotemporal HDR Lighting Estimation</title>
<link>https://arxiv.org/abs/2512.13597</link>
<guid>https://arxiv.org/abs/2512.13597</guid>
<content:encoded><![CDATA[
arXiv:2512.13597v1 Announce Type: new 
Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides</title>
<link>https://arxiv.org/abs/2512.13600</link>
<guid>https://arxiv.org/abs/2512.13600</guid>
<content:encoded><![CDATA[
arXiv:2512.13600v1 Announce Type: new 
Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongVie 2: Multimodal Controllable Ultra-Long Video World Model</title>
<link>https://arxiv.org/abs/2512.13604</link>
<guid>https://arxiv.org/abs/2512.13604</guid>
<content:encoded><![CDATA[
arXiv:2512.13604v1 Announce Type: new 
Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis</title>
<link>https://arxiv.org/abs/2512.13608</link>
<guid>https://arxiv.org/abs/2512.13608</guid>
<content:encoded><![CDATA[
arXiv:2512.13608v1 Announce Type: new 
Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.13609</link>
<guid>https://arxiv.org/abs/2512.13609</guid>
<content:encoded><![CDATA[
arXiv:2512.13609v1 Announce Type: new 
Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13635</link>
<guid>https://arxiv.org/abs/2512.13635</guid>
<content:encoded><![CDATA[
arXiv:2512.13635v1 Announce Type: new 
Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.13636</link>
<guid>https://arxiv.org/abs/2512.13636</guid>
<content:encoded><![CDATA[
arXiv:2512.13636v1 Announce Type: new 
Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All</title>
<link>https://arxiv.org/abs/2512.13639</link>
<guid>https://arxiv.org/abs/2512.13639</guid>
<content:encoded><![CDATA[
arXiv:2512.13639v1 Announce Type: new 
Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</title>
<link>https://arxiv.org/abs/2512.13665</link>
<guid>https://arxiv.org/abs/2512.13665</guid>
<content:encoded><![CDATA[
arXiv:2512.13665v1 Announce Type: new 
Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.13671</link>
<guid>https://arxiv.org/abs/2512.13671</guid>
<content:encoded><![CDATA[
arXiv:2512.13671v1 Announce Type: new 
Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Interactive Intelligence for Digital Humans</title>
<link>https://arxiv.org/abs/2512.13674</link>
<guid>https://arxiv.org/abs/2512.13674</guid>
<content:encoded><![CDATA[
arXiv:2512.13674v1 Announce Type: new 
Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</title>
<link>https://arxiv.org/abs/2512.13677</link>
<guid>https://arxiv.org/abs/2512.13677</guid>
<content:encoded><![CDATA[
arXiv:2512.13677v1 Announce Type: new 
Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedforward 3D Editing via Text-Steerable Image-to-3D</title>
<link>https://arxiv.org/abs/2512.13678</link>
<guid>https://arxiv.org/abs/2512.13678</guid>
<content:encoded><![CDATA[
arXiv:2512.13678v1 Announce Type: new 
Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.13680</link>
<guid>https://arxiv.org/abs/2512.13680</guid>
<content:encoded><![CDATA[
arXiv:2512.13680v1 Announce Type: new 
Abstract: Recent feed-forward reconstruction models like VGGT and $\pi^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</title>
<link>https://arxiv.org/abs/2512.13683</link>
<guid>https://arxiv.org/abs/2512.13683</guid>
<content:encoded><![CDATA[
arXiv:2512.13683v1 Announce Type: new 
Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recurrent Video Masked Autoencoders</title>
<link>https://arxiv.org/abs/2512.13684</link>
<guid>https://arxiv.org/abs/2512.13684</guid>
<content:encoded><![CDATA[
arXiv:2512.13684v1 Announce Type: new 
Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Scalable Pre-training of Visual Tokenizers for Generation</title>
<link>https://arxiv.org/abs/2512.13687</link>
<guid>https://arxiv.org/abs/2512.13687</guid>
<content:encoded><![CDATA[
arXiv:2512.13687v1 Announce Type: new 
Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LitePT: Lighter Yet Stronger Point Transformer</title>
<link>https://arxiv.org/abs/2512.13689</link>
<guid>https://arxiv.org/abs/2512.13689</guid>
<content:encoded><![CDATA[
arXiv:2512.13689v1 Announce Type: new 
Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</title>
<link>https://arxiv.org/abs/2512.13690</link>
<guid>https://arxiv.org/abs/2512.13690</guid>
<content:encoded><![CDATA[
arXiv:2512.13690v1 Announce Type: new 
Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights</title>
<link>https://arxiv.org/abs/2512.11802</link>
<guid>https://arxiv.org/abs/2512.11802</guid>
<content:encoded><![CDATA[
arXiv:2512.11802v1 Announce Type: cross 
Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention</title>
<link>https://arxiv.org/abs/2512.11811</link>
<guid>https://arxiv.org/abs/2512.11811</guid>
<content:encoded><![CDATA[
arXiv:2512.11811v1 Announce Type: cross 
Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images</title>
<link>https://arxiv.org/abs/2512.11817</link>
<guid>https://arxiv.org/abs/2512.11817</guid>
<content:encoded><![CDATA[
arXiv:2512.11817v1 Announce Type: cross 
Abstract: This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision</title>
<link>https://arxiv.org/abs/2512.11824</link>
<guid>https://arxiv.org/abs/2512.11824</guid>
<content:encoded><![CDATA[
arXiv:2512.11824v1 Announce Type: cross 
Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?</title>
<link>https://arxiv.org/abs/2512.11827</link>
<guid>https://arxiv.org/abs/2512.11827</guid>
<content:encoded><![CDATA[
arXiv:2512.11827v1 Announce Type: cross 
Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Design of One-step Diffusion via Shortcutting Flow Paths</title>
<link>https://arxiv.org/abs/2512.11831</link>
<guid>https://arxiv.org/abs/2512.11831</guid>
<content:encoded><![CDATA[
arXiv:2512.11831v1 Announce Type: cross 
Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Decision Tree classifier: explainable and extendable PyTorch implementation</title>
<link>https://arxiv.org/abs/2512.11833</link>
<guid>https://arxiv.org/abs/2512.11833</guid>
<content:encoded><![CDATA[
arXiv:2512.11833v1 Announce Type: cross 
Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Foundry: A System for Training Foundational Vision AI Models</title>
<link>https://arxiv.org/abs/2512.11837</link>
<guid>https://arxiv.org/abs/2512.11837</guid>
<content:encoded><![CDATA[
arXiv:2512.11837v1 Announce Type: cross 
Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document</title>
<link>https://arxiv.org/abs/2512.11849</link>
<guid>https://arxiv.org/abs/2512.11849</guid>
<content:encoded><![CDATA[
arXiv:2512.11849v1 Announce Type: cross 
Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Dangers of Bootstrapping Generation for Continual Learning and Beyond</title>
<link>https://arxiv.org/abs/2512.11867</link>
<guid>https://arxiv.org/abs/2512.11867</guid>
<content:encoded><![CDATA[
arXiv:2512.11867v1 Announce Type: cross 
Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"</title>
<link>https://arxiv.org/abs/2512.11883</link>
<guid>https://arxiv.org/abs/2512.11883</guid>
<content:encoded><![CDATA[
arXiv:2512.11883v1 Announce Type: cross 
Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics</title>
<link>https://arxiv.org/abs/2512.11903</link>
<guid>https://arxiv.org/abs/2512.11903</guid>
<content:encoded><![CDATA[
arXiv:2512.11903v1 Announce Type: cross 
Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-training vision models for the classification of alerts from wide-field time-domain surveys</title>
<link>https://arxiv.org/abs/2512.11957</link>
<guid>https://arxiv.org/abs/2512.11957</guid>
<content:encoded><![CDATA[
arXiv:2512.11957v1 Announce Type: cross 
Abstract: Modern wide-field time-domain surveys facilitate the study of transient, variable and moving phenomena by conducting image differencing and relaying alerts to their communities. Machine learning tools have been used on data from these surveys and their precursors for more than a decade, and convolutional neural networks (CNNs), which make predictions directly from input images, saw particularly broad adoption through the 2010s. Since then, continually rapid advances in computer vision have transformed the standard practices around using such models. It is now commonplace to use standardized architectures pre-trained on large corpora of everyday images (e.g., ImageNet). In contrast, time-domain astronomy studies still typically design custom CNN architectures and train them from scratch. Here, we explore the affects of adopting various pre-training regimens and standardized model architectures on the performance of alert classification. We find that the resulting models match or outperform a custom, specialized CNN like what is typically used for filtering alerts. Moreover, our results show that pre-training on galaxy images from Galaxy Zoo tends to yield better performance than pre-training on ImageNet or training from scratch. We observe that the design of standardized architectures are much better optimized than the custom CNN baseline, requiring significantly less time and memory for inference despite having more trainable parameters. On the eve of the Legacy Survey of Space and Time and other image-differencing surveys, these findings advocate for a paradigm shift in the creation of vision models for alerts, demonstrating that greater performance and efficiency, in time and in data, can be achieved by adopting the latest practices from the computer vision field.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic search for 100M+ galaxy images using AI-generated captions</title>
<link>https://arxiv.org/abs/2512.11982</link>
<guid>https://arxiv.org/abs/2512.11982</guid>
<content:encoded><![CDATA[
arXiv:2512.11982v1 Announce Type: cross 
Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoMV: An Automatic Multi-Agent System for Music Video Generation</title>
<link>https://arxiv.org/abs/2512.12196</link>
<guid>https://arxiv.org/abs/2512.12196</guid>
<content:encoded><![CDATA[
arXiv:2512.12196v1 Announce Type: cross 
Abstract: Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for "story" or "singer" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion</title>
<link>https://arxiv.org/abs/2512.12203</link>
<guid>https://arxiv.org/abs/2512.12203</guid>
<content:encoded><![CDATA[
arXiv:2512.12203v1 Announce Type: cross 
Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT</title>
<link>https://arxiv.org/abs/2512.12236</link>
<guid>https://arxiv.org/abs/2512.12236</guid>
<content:encoded><![CDATA[
arXiv:2512.12236v1 Announce Type: cross 
Abstract: Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.
  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</title>
<link>https://arxiv.org/abs/2512.12284</link>
<guid>https://arxiv.org/abs/2512.12284</guid>
<content:encoded><![CDATA[
arXiv:2512.12284v1 Announce Type: cross 
Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JPEG-Inspired Cloud-Edge Holography</title>
<link>https://arxiv.org/abs/2512.12367</link>
<guid>https://arxiv.org/abs/2512.12367</guid>
<content:encoded><![CDATA[
arXiv:2512.12367v1 Announce Type: cross 
Abstract: Computer-generated holography (CGH) presents a transformative solution for near-eye displays in augmented and virtual reality. Recent advances in deep learning have greatly improved CGH in reconstructed quality and computational efficiency. However, deploying neural CGH pipelines directly on compact, eyeglass-style devices is hindered by stringent constraints on computation and energy consumption, while cloud offloading followed by transmission with natural image codecs often distorts phase information and requires high bandwidth to maintain reconstruction quality. Neural compression methods can reduce bandwidth but impose heavy neural decoders at the edge, increasing inference latency and hardware demand. In this work, we introduce JPEG-Inspired Cloud-Edge Holography, an efficient pipeline designed around a learnable transform codec that retains the block-structured and hardware-friendly nature of JPEG. Our system shifts all heavy neural processing to the cloud, while the edge device performs only lightweight decoding without any neural inference. To further improve throughput, we implement custom CUDA kernels for entropy coding on both cloud and edge. This design achieves a peak signal-to-noise ratio of 32.15 dB at $<$ 2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirm the high reconstruction quality of the holograms. By aligning CGH with a codec that preserves JPEG's structural efficiency while extending it with learnable components, our framework enables low-latency, bandwidth-efficient hologram streaming on resource-constrained wearable devices-using only simple block-based decoding readily supported by modern system-on-chips, without requiring neural decoders or specialized hardware.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.12663</link>
<guid>https://arxiv.org/abs/2512.12663</guid>
<content:encoded><![CDATA[
arXiv:2512.12663v1 Announce Type: cross 
Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis</title>
<link>https://arxiv.org/abs/2512.12683</link>
<guid>https://arxiv.org/abs/2512.12683</guid>
<content:encoded><![CDATA[
arXiv:2512.12683v1 Announce Type: cross 
Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning</title>
<link>https://arxiv.org/abs/2512.12690</link>
<guid>https://arxiv.org/abs/2512.12690</guid>
<content:encoded><![CDATA[
arXiv:2512.12690v1 Announce Type: cross 
Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering</title>
<link>https://arxiv.org/abs/2512.12694</link>
<guid>https://arxiv.org/abs/2512.12694</guid>
<content:encoded><![CDATA[
arXiv:2512.12694v1 Announce Type: cross 
Abstract: Large-scale digitization initiatives have unlocked massive collections of historical newspapers, yet effective computational access remains hindered by OCR corruption, multilingual orthographic variation, and temporal language drift. We develop and evaluate a multilingual Retrieval-Augmented Generation pipeline specifically designed for question answering on noisy historical documents. Our approach integrates: (i) semantic query expansion and multi-query fusion using Reciprocal Rank Fusion to improve retrieval robustness against vocabulary mismatch; (ii) a carefully engineered generation prompt that enforces strict grounding in retrieved evidence and explicit abstention when evidence is insufficient; and (iii) a modular architecture enabling systematic component evaluation. We conduct comprehensive ablation studies on Named Entity Recognition and embedding model selection, demonstrating the importance of syntactic coherence in entity extraction and balanced performance-efficiency trade-offs in dense retrieval. Our end-to-end evaluation framework shows that the pipeline generates faithful answers for well-supported queries while correctly abstaining from unanswerable questions. The hybrid retrieval strategy improves recall stability, particularly benefiting from RRF's ability to smooth performance variance across query formulations. We release our code and configurations at https://anonymous.4open.science/r/RAGs-C5AE/, providing a reproducible foundation for robust historical document question answering.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Feedback Alignment</title>
<link>https://arxiv.org/abs/2512.12762</link>
<guid>https://arxiv.org/abs/2512.12762</guid>
<content:encoded><![CDATA[
arXiv:2512.12762v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation</title>
<link>https://arxiv.org/abs/2512.12772</link>
<guid>https://arxiv.org/abs/2512.12772</guid>
<content:encoded><![CDATA[
arXiv:2512.12772v1 Announce Type: cross 
Abstract: Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients</title>
<link>https://arxiv.org/abs/2512.12827</link>
<guid>https://arxiv.org/abs/2512.12827</guid>
<content:encoded><![CDATA[
arXiv:2512.12827v1 Announce Type: cross 
Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams</title>
<link>https://arxiv.org/abs/2512.12939</link>
<guid>https://arxiv.org/abs/2512.12939</guid>
<content:encoded><![CDATA[
arXiv:2512.12939v1 Announce Type: cross 
Abstract: We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \(\alpha\) (trade-off between temporal misalignment and diagram discrepancy) and \(\beta\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework</title>
<link>https://arxiv.org/abs/2512.12945</link>
<guid>https://arxiv.org/abs/2512.12945</guid>
<content:encoded><![CDATA[
arXiv:2512.12945v1 Announce Type: cross 
Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Compression to Construct Transferable Bitrate Ladders</title>
<link>https://arxiv.org/abs/2512.12952</link>
<guid>https://arxiv.org/abs/2512.12952</guid>
<content:encoded><![CDATA[
arXiv:2512.12952v1 Announce Type: cross 
Abstract: Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs</title>
<link>https://arxiv.org/abs/2512.12984</link>
<guid>https://arxiv.org/abs/2512.12984</guid>
<content:encoded><![CDATA[
arXiv:2512.12984v1 Announce Type: cross 
Abstract: We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.12987</link>
<guid>https://arxiv.org/abs/2512.12987</guid>
<content:encoded><![CDATA[
arXiv:2512.12987v1 Announce Type: cross 
Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning</title>
<link>https://arxiv.org/abs/2512.13131</link>
<guid>https://arxiv.org/abs/2512.13131</guid>
<content:encoded><![CDATA[
arXiv:2512.13131v1 Announce Type: cross 
Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.13262</link>
<guid>https://arxiv.org/abs/2512.13262</guid>
<content:encoded><![CDATA[
arXiv:2512.13262v1 Announce Type: cross 
Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging</title>
<link>https://arxiv.org/abs/2512.13434</link>
<guid>https://arxiv.org/abs/2512.13434</guid>
<content:encoded><![CDATA[
arXiv:2512.13434v1 Announce Type: cross 
Abstract: Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing</title>
<link>https://arxiv.org/abs/2512.13497</link>
<guid>https://arxiv.org/abs/2512.13497</guid>
<content:encoded><![CDATA[
arXiv:2512.13497v1 Announce Type: cross 
Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Diffusion Preview with Consistency Solver</title>
<link>https://arxiv.org/abs/2512.13592</link>
<guid>https://arxiv.org/abs/2512.13592</guid>
<content:encoded><![CDATA[
arXiv:2512.13592v1 Announce Type: cross 
Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</title>
<link>https://arxiv.org/abs/2512.13641</link>
<guid>https://arxiv.org/abs/2512.13641</guid>
<content:encoded><![CDATA[
arXiv:2512.13641v1 Announce Type: cross 
Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Models Can Leverage Human Videos for Dexterous Manipulation</title>
<link>https://arxiv.org/abs/2512.13644</link>
<guid>https://arxiv.org/abs/2512.13644</guid>
<content:encoded><![CDATA[
arXiv:2512.13644v1 Announce Type: cross 
Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</title>
<link>https://arxiv.org/abs/2512.13660</link>
<guid>https://arxiv.org/abs/2512.13660</guid>
<content:encoded><![CDATA[
arXiv:2512.13660v1 Announce Type: cross 
Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Directional Textual Inversion for Personalized Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2512.13672</link>
<guid>https://arxiv.org/abs/2512.13672</guid>
<content:encoded><![CDATA[
arXiv:2512.13672v1 Announce Type: cross 
Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>We Can Always Catch You: Detecting Adversarial Patched Objects WITH or WITHOUT Signature</title>
<link>https://arxiv.org/abs/2106.05261</link>
<guid>https://arxiv.org/abs/2106.05261</guid>
<content:encoded><![CDATA[
arXiv:2106.05261v3 Announce Type: replace 
Abstract: Recently, object detection has proven vulnerable to adversarial patch attacks. The attackers holding a specially crafted patch can hide themselves from state-of-the-art detectors, e.g., YOLO, even in the physical world. This attack can bring serious security threats, such as escaping from surveillance cameras. How to effectively detect this kind of adversarial examples to catch potential attacks has become an important problem. In this paper, we propose two detection methods: the signature-based method and the signature-independent method. First, we identify two signatures of existing adversarial patches that can be utilized to precisely locate patches within adversarial examples. By employing the signatures, a fast signature-based method is developed to detect the adversarial objects. Second, we present a robust signature-independent method based on the \textit{content semantics consistency} of model outputs. Adversarial objects violate this consistency, appearing locally but disappearing globally, while benign ones remain consistently present. The experiments demonstrate that two proposed methods can effectively detect attacks both in the digital and physical world. These methods each offer distinct advantage. Specifically, the signature-based method is capable of real-time detection, while the signature-independent method can detect unknown adversarial patch attacks and makes defense-aware attacks almost impossible to perform.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer</title>
<link>https://arxiv.org/abs/2112.01330</link>
<guid>https://arxiv.org/abs/2112.01330</guid>
<content:encoded><![CDATA[
arXiv:2112.01330v2 Announce Type: replace 
Abstract: Interval and large invasive breast cancers, which are associated with worse prognosis than other cancers, are usually detected at a late stage due to false negative assessments of screening mammograms. The missed screening-time detection is commonly caused by the tumor being obscured by its surrounding breast tissues, a phenomenon called masking. To study and benchmark mammographic masking of cancer, in this work we introduce CSAW-M, the largest public mammographic dataset, collected from over 10,000 individuals and annotated with potential masking. In contrast to the previous approaches which measure breast image density as a proxy, our dataset directly provides annotations of masking potential assessments from five specialists. We also trained deep learning models on CSAW-M to estimate the masking level and showed that the estimated masking is significantly more predictive of screening participants diagnosed with interval and large invasive cancers -- without being explicitly trained for these tasks -- than its breast density counterparts.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExReg: Wide-range Photo Exposure Correction via a Multi-dimensional Regressor with Attention</title>
<link>https://arxiv.org/abs/2212.14801</link>
<guid>https://arxiv.org/abs/2212.14801</guid>
<content:encoded><![CDATA[
arXiv:2212.14801v2 Announce Type: replace 
Abstract: Photo exposure correction is widely investigated, but fewer studies focus on correcting under- and over-exposed images simultaneously. Three issues remain open to handle and correct both under- and over-exposed images in a unified way. First, a locally-adaptive exposure adjustment may be more flexible instead of learning a global mapping. Second, it is an ill-posed problem to determine the suitable exposure values locally. Third, photos with the same content but different exposures may not reach consistent adjustment results. To this end, we proposed a novel exposure correction network, ExReg, to address the challenges by formulating exposure correction as a multi-dimensional regression process. Given an input image, a compact multi-exposure generation network is introduced to generate images with different exposure conditions for multi-dimensional regression and exposure correction in the next stage. An auxiliary module is designed to predict the region-wise exposure values, guiding the proposed Encoder-Decoder ANP (Attentive Neural Processes) to regress the final corrected image. The experimental results show that ExReg can generate well-exposed results and outperform the SOTA method in PSNR for extensive exposure problems. Furthermore, the processing speed, with 0.05 seconds per image on an RTX 3090, is efficient. When tested on the same image under various exposure levels, ExReg also yields results that are visually consistent and physically accurate.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WCCNet: Wavelet-context Cooperative Network for Efficient Multispectral Pedestrian Detection</title>
<link>https://arxiv.org/abs/2308.01042</link>
<guid>https://arxiv.org/abs/2308.01042</guid>
<content:encoded><![CDATA[
arXiv:2308.01042v3 Announce Type: replace 
Abstract: Multispectral pedestrian detection is essential to various tasks especially autonomous driving, for which both the accuracy and computational cost are of paramount importance. Most existing approaches treat RGB and infrared modalities equally. They typically adopt two symmetrical backbones for multimodal feature extraction, which ignore the substantial differences between modalities and bring great difficulty for the reduction of the computational cost as well as effective crossmodal fusion. In this work, we propose a novel and efficient framework named Wavelet-context Cooperative Network (WCCNet), which differentially extracts complementary features across spectra with low computational cost and further fuses these diverse features based on their spatially relevant cross-modal semantics. WCCNet explores an asymmetric but cooperative dual-stream backbone, in which WCCNet utilizes generic neural layers for texture-rich feature extraction from RGB modality, while proposing Mixture of Wavelet Experts (MoWE) to capture complementary frequency patterns of infrared modality. By assessing multispectral environmental context, MoWE generates routing scores to selectively activate specific learnable Adaptive DWT (ADWT) layers, alongside shared static DWT, which are both considerible lightwight and efficient to significantly reduce computational overhead and facilitate subsequent fusion. To further fuse these multispectral features with significant semantic differences, we elaborately design the crossmodal rearranging fusion module (CMRF), which aims to mitigate misalignment and merge semantically complementary features in spatially-related local regions to amplify the crossmodal reciprocal information. Results from comprehensive evaluations on KAIST and FLIR benchmarks indicate that WCCNet outperforms state-of-the-art methods with considerable computational efficiency and competitive accuracy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PADS: Plug-and-Play 3D Human Pose Analysis via Diffusion Generative Modeling</title>
<link>https://arxiv.org/abs/2401.08930</link>
<guid>https://arxiv.org/abs/2401.08930</guid>
<content:encoded><![CDATA[
arXiv:2401.08930v2 Announce Type: replace 
Abstract: Diffusion models have demonstrated impressive capabilities in modeling complex data distributions and are increasingly applied in various generative tasks. In this work, we propose Pose Analysis by Diffusion Synthesis PADS, a unified generative modeling framework for 3D human pose analysis. PADS first learns a task-agnostic 3D pose prior via unconditional diffusion synthesis and then performs training-free adaptation to a wide range of pose analysis tasks, including 3D pose estimation, denoising, completion, etc., through a posterior sampling scheme. By formulating each task as an inverse problem with a known forward operator, PADS injects task-specific constraints during inference while keeping the pose prior fixed. This plug-and-play framework removes the need for task-specific supervision or retraining, offering flexibility and scalability across diverse conditions. Extensive experiments on different benchmarks showcase the superior performance against both learning-based and optimization-based baselines, demonstrating the effectiveness and generalization capability of our method.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foveated Retinotopy Improves Classification and Localization in Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2402.15480</link>
<guid>https://arxiv.org/abs/2402.15480</guid>
<content:encoded><![CDATA[
arXiv:2402.15480v5 Announce Type: replace 
Abstract: From falcons spotting preys to humans recognizing faces, rapid visual abilities depend on a foveated retinal organization which delivers high-acuity central vision while preserving low-resolution periphery. This organization is conserved along early visual pathways but remains underexplored in machine learning. Here we examine how embedding a foveated retinotopic transformation as a preprocessing layer impacts convolutional neural networks (CNNs) for image classification. By applying a log-polar mapping to off-the-shelf models and retraining them, we retain comparable accuracy while improving robustness to scale and rotation. We show that this architecture becomes highly sensitive to fixation-point shifts, and that this sensitivity yields a proxy for defining saliency maps that effectively facilitates object localization. Our results show that foveated retinotopy encodes prior geometric knowledge, offering a solution to visual-search and enhancing both classification and localization. These findings connect biological vision principles with artificial networks, pointing to new, robust and efficient directions for computer-vision systems.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient and Harmonized Framework for Balanced Cross-Domain Feature Integration</title>
<link>https://arxiv.org/abs/2403.18461</link>
<guid>https://arxiv.org/abs/2403.18461</guid>
<content:encoded><![CDATA[
arXiv:2403.18461v3 Announce Type: replace 
Abstract: Despite significant advancements in image generation using advanced generative frameworks, cross-image integration of content and style remains a key challenge. Current generative models, while powerful, frequently depend on vague textual prompts to define styles--creating difficulties in balancing content semantics and style preservation. We propose a novel framework that utilizes customized models to learn style representations. It enhances content preservation through cross-model feature and attention modulation, leveraging the inherent semantic consistency across models. Additionally, we introduce fixed feature and adaptive attention fusion to achieve the desired balance between content and style. We further develop spatial (mask-guided localized) and temporal (multi-style compositional) multi-model combinations, enabling flexible fusion of models and styles. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in balancing content preservation and stylistic coherence.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Wrong-way Cycling Detection in CCTV Videos: Sparse Sampling is All You Need</title>
<link>https://arxiv.org/abs/2405.07293</link>
<guid>https://arxiv.org/abs/2405.07293</guid>
<content:encoded><![CDATA[
arXiv:2405.07293v2 Announce Type: replace 
Abstract: Effective monitoring of unusual transportation behaviors, such as wrong-way cycling (i.e., riding a bicycle or e-bike against designated traffic flow), is crucial for optimizing law enforcement deployment and traffic planning. However, accurately recording all wrong-way cycling events is both unnecessary and infeasible in resource-constrained environments, as it requires high-resolution cameras for evidence collection and event detection. To address this challenge, we propose WWC-Predictor, a novel method for efficiently estimating the wrong-way cycling ratio, defined as the proportion of wrong-way cycling events relative to the total number of cycling movements over a given time period. The core innovation of our method lies in accurately detecting wrong-way cycling events in sparsely sampled frames using a light-weight detector, then estimating the overall ratio using an autoregressive moving average model. To evaluate the effectiveness of our method, we construct a benchmark dataset consisting of 35 minutes of video sequences with minute-level annotations.Our method achieves an average error rate of a mere 1.475\% while consuming only 19.12\% GPU time required by conventional tracking methods, validating its effectiveness in estimating the wrong-way cycling ratio. Our source code is publicly available at: https://github.com/VICA-Lab-HKUST-GZ/WWC-Predictor.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</title>
<link>https://arxiv.org/abs/2405.20336</link>
<guid>https://arxiv.org/abs/2405.20336</guid>
<content:encoded><![CDATA[
arXiv:2405.20336v3 Announce Type: replace 
Abstract: In this work, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs, advancing beyond existing works that typically address these two modalities in isolation. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes. With the RapVerse dataset, we investigate the extent to which scaling autoregressive multimodal transformers across language, audio, and motion can enhance the coherent and realistic generation of vocals and whole-body human motions. For modality unification, a vector-quantized variational autoencoder is employed to encode whole-body motion sequences into discrete motion tokens, while a vocal-to-unit model is leveraged to obtain quantized audio tokens preserving content, prosodic information and singer identity. By jointly performing transformer modeling on these three modalities in a unified way, our framework ensures a seamless and realistic blend of vocals and human motions. Extensive experiments demonstrate that our unified generation framework not only produces coherent and realistic singing vocals alongside human motions directly from textual inputs, but also rivals the performance of specialized single-modality generation systems, establishing new benchmarks for joint vocal-motion generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUOTA: Quantifying Objects with Text-to-Image Models for Any Domain</title>
<link>https://arxiv.org/abs/2411.19534</link>
<guid>https://arxiv.org/abs/2411.19534</guid>
<content:encoded><![CDATA[
arXiv:2411.19534v2 Announce Type: replace 
Abstract: We tackle the problem of quantifying the number of objects by a generative text-to-image model. Rather than retraining such a model for each new image domain of interest, which leads to high computational costs and limited scalability, we are the first to consider this problem from a domain-agnostic perspective. We propose QUOTA, an optimization framework for text-to-image models that enables effective object quantification across unseen domains without retraining. It leverages a dual-loop meta-learning strategy to optimize a domain-invariant prompt. Further, by integrating prompt learning with learnable counting and domain tokens, our method captures stylistic variations and maintains accuracy, even for object classes not encountered during training. For evaluation, we adopt a new benchmark specifically designed for object quantification in domain generalization, enabling rigorous assessment of object quantification accuracy and adaptability across unseen domains in text-to-image generation. Extensive experiments demonstrate that QUOTA outperforms conventional models in both object quantification accuracy and semantic consistency, setting a new benchmark for efficient and scalable text-to-image generation for any domain.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeWalker: Personalized Neural Space for Lifelong Head Avatars</title>
<link>https://arxiv.org/abs/2412.02421</link>
<guid>https://arxiv.org/abs/2412.02421</guid>
<content:encoded><![CDATA[
arXiv:2412.02421v2 Announce Type: replace 
Abstract: We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture identity at the momentary level(e.g., instant photography or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life. At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from parametric model, and move/rotate with the change of expression. Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized 'time traveling' in a breeze.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CleanDIFT: Diffusion Features without Noise</title>
<link>https://arxiv.org/abs/2412.03439</link>
<guid>https://arxiv.org/abs/2412.03439</guid>
<content:encoded><![CDATA[
arXiv:2412.03439v3 Announce Type: replace 
Abstract: Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep priors for satellite image restoration with accurate uncertainties</title>
<link>https://arxiv.org/abs/2412.04130</link>
<guid>https://arxiv.org/abs/2412.04130</guid>
<content:encoded><![CDATA[
arXiv:2412.04130v2 Announce Type: replace 
Abstract: Satellite optical images, upon their on-ground receipt, offer a distorted view of the observed scene. Their restoration, including denoising, deblurring, and sometimes super-resolution, is required before their exploitation. Moreover, quantifying the uncertainties related to this restoration helps to reduce the risks of misinterpreting the image content. Deep learning methods are now state-of-the-art for satellite image restoration. Among them, direct inversion methods train a specific network for each sensor, and generally provide a point estimation of the restored image without the associated uncertainties. Alternatively, deep regularization (DR) methods learn a deep prior on target images before plugging it, as the regularization term, into a model-based optimization scheme. This allows for restoring images from several sensors with a single network and possibly for estimating associated uncertainties. In this paper, we introduce VBLE-xz, a DR method that solves the inverse problem in the latent space of a variational compressive autoencoder (CAE). We adapt the regularization strength by modulating the bitrate of the trained CAE with a training-free approach. Then, VBLE-xz estimates relevant uncertainties jointly in the latent and in the image spaces by sampling an explicit posterior estimated within variational inference. This enables fast posterior sampling, unlike state-of-the-art DR methods that use Markov chains or diffusion-based approaches. We conduct a comprehensive set of experiments on very high-resolution simulated and real Pl\'eiades images, asserting the performance, robustness and scalability of the proposed method. They demonstrate that VBLE-xz represents a compelling alternative to direct inversion methods when uncertainty quantification is required. The code associated to this paper is available in https://github.com/MaudBqrd/VBLExz.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KNN-MMD: Cross Domain Wireless Sensing via Local Distribution Alignment</title>
<link>https://arxiv.org/abs/2412.04783</link>
<guid>https://arxiv.org/abs/2412.04783</guid>
<content:encoded><![CDATA[
arXiv:2412.04783v5 Announce Type: replace 
Abstract: Wireless sensing has recently found widespread applications in diverse environments, including homes, offices, and public spaces. By analyzing patterns in channel state information (CSI), it is possible to infer human actions for tasks such as person identification, gesture recognition, and fall detection. However, CSI is highly sensitive to environmental changes, where even minor alterations can significantly distort the CSI patterns. This sensitivity often leads to performance degradation or outright failure when applying wireless sensing models trained in one environment to another. To address this challenge, Domain Alignment (DAL) has been widely adopted for cross-domain classification tasks, as it focuses on aligning the global distributions of the source and target domains in feature space. Despite its popularity, DAL often neglects inter-category relationships, which can lead to misalignment between categories across domains, even when global alignment is achieved. To overcome these limitations, we propose K-Nearest Neighbors Maximum Mean Discrepancy (KNN-MMD), a novel few-shot method for cross-domain wireless sensing. Our approach begins by constructing a help set using KNN from the target domain, enabling local alignment between the source and target domains within each category using MMD. Additionally, we address a key instability issue commonly observed in cross-domain methods, where model performance fluctuates sharply between epochs. Further, most existing methods struggle to determine an optimal stopping point during training due to the absence of labeled data from the target domain. Our method resolves this by excluding the support set from the target domain during training and employing it as a validation set to determine the stopping criterion. The dataset and code are publicly available at https://github.com/RS2002/KNN-MMD .
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoTexDensifier: Geometry-Texture-Aware Densification for High-Quality Photorealistic 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2412.16809</link>
<guid>https://arxiv.org/abs/2412.16809</guid>
<content:encoded><![CDATA[
arXiv:2412.16809v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) has recently attracted wide attentions in various areas such as 3D navigation, Virtual Reality (VR) and 3D simulation, due to its photorealistic and efficient rendering performance. High-quality reconstrution of 3DGS relies on sufficient splats and a reasonable distribution of these splats to fit real geometric surface and texture details, which turns out to be a challenging problem. We present GeoTexDensifier, a novel geometry-texture-aware densification strategy to reconstruct high-quality Gaussian splats which better comply with the geometric structure and texture richness of the scene. Specifically, our GeoTexDensifier framework carries out an auxiliary texture-aware densification method to produce a denser distribution of splats in fully textured areas, while keeping sparsity in low-texture regions to maintain the quality of Gaussian point cloud. Meanwhile, a geometry-aware splitting strategy takes depth and normal priors to guide the splitting sampling and filter out the noisy splats whose initial positions are far from the actual geometric surfaces they aim to fit, under a Validation of Depth Ratio Change checking. With the help of relative monocular depth prior, such geometry-aware validation can effectively reduce the influence of scattered Gaussians to the final rendering quality, especially in regions with weak textures or without sufficient training views. The texture-aware densification and geometry-aware splitting strategies are fully combined to obtain a set of high-quality Gaussian splats. We experiment our GeoTexDensifier framework on various datasets and compare our Novel View Synthesis results to other state-of-the-art 3DGS approaches, with detailed quantitative and qualitative evaluations to demonstrate the effectiveness of our method in producing more photorealistic 3DGS models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Reality-Virtuality Interconnections in Urban Digital Twins for Superior Intelligent Road Inspection and Simulation</title>
<link>https://arxiv.org/abs/2412.17699</link>
<guid>https://arxiv.org/abs/2412.17699</guid>
<content:encoded><![CDATA[
arXiv:2412.17699v2 Announce Type: replace 
Abstract: Road inspection is crucial for maintaining road serviceability and ensuring traffic safety, as road defects gradually develop and compromise functionality. Traditional inspection methods, which rely on manual evaluations, are labor-intensive, costly, and time-consuming. While data-driven approaches are gaining traction, the scarcity and spatial sparsity of real-world road defects present significant challenges in acquiring high-quality datasets. Existing simulators designed to generate detailed synthetic driving scenes, however, lack models for road defects. Moreover, advanced driving tasks that involve interactions with road surfaces, such as planning and control in defective areas, remain underexplored. To address these limitations, we propose a multi-modal sensor platform integrated with an urban digital twin (UDT) system for intelligent road inspection. First, hierarchical road models are constructed from real-world driving data collected using vehicle-mounted sensors, resulting in highly detailed representations of road defect structures and surface elevations. Next, digital road twins are generated to create simulation environments for comprehensive analysis and evaluation of algorithm performance. These scenarios are then imported into a simulator to facilitate both data acquisition and physical simulation. Experimental results demonstrate that driving tasks, including perception and decision-making, benefit significantly from the high-fidelity road defect scenes generated by our system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPBridge: Latent Diffusion Bridge for Dense Prediction</title>
<link>https://arxiv.org/abs/2412.20506</link>
<guid>https://arxiv.org/abs/2412.20506</guid>
<content:encoded><![CDATA[
arXiv:2412.20506v4 Announce Type: replace 
Abstract: Diffusion models demonstrate remarkable capabilities in capturing complex data distributions and have achieved compelling results in many generative tasks. While they have recently been extended to dense prediction tasks such as depth estimation and surface normal prediction, their full potential in this area remains underexplored. As target signal maps and input images are pixel-wise aligned, the conventional noise-to-data generation paradigm is inefficient, and input images can serve as a more informative prior compared to pure noise. Diffusion bridge models, which support data-to-data generation between two general data distributions, offer a promising alternative, but they typically fail to exploit the rich visual priors embedded in large pretrained foundation models. To address these limitations, we integrate diffusion bridge formulation with structured visual priors and introduce DPBridge, the first latent diffusion bridge framework for dense prediction tasks. To resolve the incompatibility between diffusion bridge models and pretrained diffusion backbones, we propose (1) a tractable reverse transition kernel for the diffusion bridge process, enabling maximum likelihood training scheme; (2) finetuning strategies including distribution-aligned normalization and image consistency loss. Experiments across extensive benchmarks validate that our method consistently achieves superior performance, demonstrating its effectiveness and generalization capability under different scenarios.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Anchored, Class Variance-Optimized Clustering for Robust Semi-Supervised Few-Shot Learning</title>
<link>https://arxiv.org/abs/2501.14401</link>
<guid>https://arxiv.org/abs/2501.14401</guid>
<content:encoded><![CDATA[
arXiv:2501.14401v3 Announce Type: replace 
Abstract: Few-shot learning has been extensively explored to address problems where the amount of labeled samples is very limited for some classes. In the semi-supervised few-shot learning setting, substantial quantities of unlabeled samples are available. Such unlabeled samples are generally cheaper to obtain and can be used to improve the few-shot learning performance of the model. Some of the recent methods for this setting rely on clustering to generate pseudo-labels for the unlabeled samples. Since the effectiveness of clustering heavily influences the labeling of the unlabeled samples, it can significantly affect the few-shot learning performance. In this paper, we focus on improving the representation learned by the model in order to improve the clustering and, consequently, the model performance. We propose an approach for semi-supervised few-shot learning that performs a class-variance optimized clustering coupled with a cluster separation tuner in order to improve the effectiveness of clustering the labeled and unlabeled samples in this setting. It also optimizes the clustering-based pseudo-labeling process using a restricted pseudo-labeling approach and performs semantic information injection in order to improve the semi-supervised few-shot learning performance of the model. We experimentally demonstrate that our proposed approach significantly outperforms recent state-of-the-art methods on the benchmark datasets. To further establish its robustness, we conduct extensive experiments under challenging conditions, showing that the model generalizes well to domain shifts and achieves new state-of-the-art performance in open-set settings with distractor classes, highlighting its effectiveness for real-world applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Calibration of a Multi-Camera System with Limited Overlapping Fields of View for 3D Surgical Scene Reconstruction</title>
<link>https://arxiv.org/abs/2501.16221</link>
<guid>https://arxiv.org/abs/2501.16221</guid>
<content:encoded><![CDATA[
arXiv:2501.16221v3 Announce Type: replace 
Abstract: The purpose of this study is to develop an automated and accurate external camera calibration method for multi-camera systems used in 3D surgical scene reconstruction (3D-SSR), eliminating the need for operator intervention or specialized expertise. The method specifically addresses the problem of limited overlapping fields of view caused by significant variations in optical zoom levels and camera locations. We contribute a novel, fast, and fully automatic calibration method based on the projection of multi-scale markers (MSMs) using a ceiling-mounted projector. MSMs consist of 2D patterns projected at varying scales, ensuring accurate extraction of well distributed point correspondences across significantly different viewpoints and zoom levels. Validation is performed using both synthetic and real data captured in a mock-up OR, with comparisons to traditional manual marker-based methods as well as markerless calibration methods. The method achieves accuracy comparable to manual, operator-dependent calibration methods while exhibiting higher robustness under conditions of significant differences in zoom levels. Additionally, we show that state-of-the-art Structure-from-Motion (SfM) pipelines are ineffective in 3D-SSR settings, even when additional texture is projected onto the OR floor. The use of a ceiling-mounted entry-level projector proves to be an effective alternative to operator-dependent, traditional marker-based methods, paving the way for fully automated 3D-SSR.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Overall Real-Time Mechanism for Classification and Quality Evaluation of Rice</title>
<link>https://arxiv.org/abs/2502.13764</link>
<guid>https://arxiv.org/abs/2502.13764</guid>
<content:encoded><![CDATA[
arXiv:2502.13764v4 Announce Type: replace 
Abstract: Rice is one of the most widely cultivated crops globally and has been developed into numerous varieties. The quality of rice during cultivation is primarily determined by its cultivar and characteristics. Traditionally, rice classification and quality assessment rely on manual visual inspection, a process that is both time-consuming and prone to errors. However, with advancements in machine vision technology, automating rice classification and quality evaluation based on its cultivar and characteristics has become increasingly feasible, enhancing both accuracy and efficiency. This study proposes a real-time evaluation mechanism for comprehensive rice grain assessment, integrating a one-stage object detection approach, a deep convolutional neural network, and traditional machine learning techniques. The proposed framework enables rice variety identification, grain completeness grading, and grain chalkiness evaluation. The rice grain dataset used in this study comprises approximately 20,000 images from six widely cultivated rice varieties in China. Experimental results demonstrate that the proposed mechanism achieves a mean average precision (mAP) of 99.14% in the object detection task and an accuracy of 97.89% in the classification task. Furthermore, the framework attains an average accuracy of 97.56% in grain completeness grading within the same rice variety, contributing to an effective quality evaluation system.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Improved Pure Fully Connected Neural Network for Rice Grain Classification</title>
<link>https://arxiv.org/abs/2503.03111</link>
<guid>https://arxiv.org/abs/2503.03111</guid>
<content:encoded><![CDATA[
arXiv:2503.03111v4 Announce Type: replace 
Abstract: Rice is a staple food for a significant portion of the world's population, providing essential nutrients and serving as a versatile in-gredient in a wide range of culinary traditions. Recently, the use of deep learning has enabled automated classification of rice, im-proving accuracy and efficiency. However, classical models based on first-stage training may face difficulties in distinguishing between rice varieties with similar external characteristics, thus leading to misclassifications. Considering the transparency and feasibility of model, we selected and gradually improved pure fully connected neural network to achieve classification of rice grain. The dataset we used contains both global and domestic rice images obtained from websites and laboratories respectively. First, the training mode was changed from one-stage training to two-stage training, which significantly contributes to distinguishing two similar types of rice. Secondly, the preprocessing method was changed from random tilting to horizontal or vertical position cor-rection. After those two enhancements, the accuracy of our model increased notably from 97% to 99%. In summary, two subtle methods proposed in this study can remarkably enhance the classification ability of deep learning models in terms of the classification of rice grain.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames</title>
<link>https://arxiv.org/abs/2503.03726</link>
<guid>https://arxiv.org/abs/2503.03726</guid>
<content:encoded><![CDATA[
arXiv:2503.03726v2 Announce Type: replace 
Abstract: Estimating the 6D pose of textureless objects from RGB images is an important problem in robotics. Due to appearance ambiguities, rotational symmetries, and severe occlusions, single-view based 6D pose estimators are still unable to handle a wide range of objects, motivating research towards multi-view pose estimation and next-best-view prediction that addresses these limitations. In this work, we propose a comprehensive active perception framework for estimating the 6D poses of textureless objects using only RGB images. Our approach is built upon a key idea: decoupling the 6D pose estimation into a two-step sequential process can greatly improve both accuracy and efficiency. First, we estimate the 3D translation of each object, resolving scale and depth ambiguities inherent to RGB images. These estimates are then used to simplify the subsequent task of determining the 3D orientation, which we achieve through canonical scale template matching. Building on this formulation, we then introduce an active perception strategy that predicts the next best camera viewpoint to capture an RGB image, effectively reducing object pose uncertainty and enhancing pose accuracy. We evaluate our method on the public ROBI and TOD datasets, as well as on our reconstructed transparent object dataset, T-ROBI. Under the same camera viewpoints, our multi-view pose estimation significantly outperforms state-of-the-art approaches. Furthermore, by leveraging our next-best-view strategy, our approach achieves high pose accuracy with fewer viewpoints than heuristic-based policies across all evaluated datasets. The accompanying video and T-ROBI dataset will be released on our project page: https://trailab.github.io/ActiveODPE.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CineBrain: A Large-Scale Multi-Modal Brain Dataset During Naturalistic Audiovisual Narrative Processing</title>
<link>https://arxiv.org/abs/2503.06940</link>
<guid>https://arxiv.org/abs/2503.06940</guid>
<content:encoded><![CDATA[
arXiv:2503.06940v2 Announce Type: replace 
Abstract: Most research decoding brain signals into images, often using them as priors for generative models, has focused only on visual content. This overlooks the brain's natural ability to integrate auditory and visual information, for instance, sound strongly influences how we perceive visual scenes. To investigate this, we propose a new task of reconstructing continuous video stimuli from multimodal brain signals recorded during audiovisual stimulation. To enable this, we introduce CineBrain, the first large-scale dataset that synchronizes fMRI and EEG during audiovisual viewing, featuring six hours of \textit{The Big Bang Theory} episodes for cross-modal alignment. We also conduct the first systematic exploration of combining fMRI and EEG for video reconstruction and present CineSync, a framework for reconstructing dynamic video using a Multi-Modal Fusion Encoder and a Neural Latent Decoder. CineSync achieves state-of-the-art performance in dynamic reconstruction, leveraging the complementary strengths of fMRI and EEG to improve visual fidelity. Our analysis shows that auditory cortical activations enhance decoding accuracy, highlighting the role of auditory input in visual perception. Project Page: https://jianxgao.github.io/CineBrain.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Anatomical Supervision for Accurate 3D Multi-Chamber Cardiac Mesh Reconstruction</title>
<link>https://arxiv.org/abs/2503.07874</link>
<guid>https://arxiv.org/abs/2503.07874</guid>
<content:encoded><![CDATA[
arXiv:2503.07874v2 Announce Type: replace 
Abstract: Accurate reconstruction of multi-chamber cardiac anatomy from medical images is a cornerstone for patient-specific modeling, physiological simulation, and interventional planning. However, current reconstruction pipelines fundamentally rely on surface-wise geometric supervision and model each chamber in isolation, resulting in anatomically implausible inter-chamber violations despite apparently favorable overlap or distance metrics. In this work, we propose a relational anatomical supervision framework for multi-chamber cardiac mesh reconstruction by introducing a Mesh Interrelation Enhancement (MIE) loss. The proposed formulation explicitly encodes spatial relationships between cardiac structures into a differentiable occupancy-based objective, thereby transforming qualitative anatomical rules into quantitative geometric supervision. We further establish violation-aware evaluation metrics to directly quantify inter-chamber structural correctness, revealing systematic limitations of commonly used geometric measures such as Dice and Chamfer distance. Extensive experiments on multi-center CT data, densely sampled MR data, and two independent external cohorts, including a highly heterogeneous congenital heart disease population, demonstrate that the proposed method consistently suppresses clinically critical boundary violations by up to 83\%, while maintaining competitive volumetric accuracy and achieving superior surface fidelity. Notably, the proposed relational supervision generalizes robustly across imaging modalities, centers, and pathological conditions, even under severe anatomical deformation. These results demonstrate that distance-based supervision alone is insufficient to guarantee anatomically faithful reconstruction, and that explicit enforcement of multi-structure anatomical relations provides a principled and robust pathway toward reliable patient-specific cardiac modeling.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpurLens: Automatic Detection of Spurious Cues in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2503.08884</link>
<guid>https://arxiv.org/abs/2503.08884</guid>
<content:encoded><![CDATA[
arXiv:2503.08884v2 Announce Type: replace 
Abstract: Unimodal vision models are known to rely on spurious correlations, but it remains unclear to what extent Multimodal Large Language Models (MLLMs) exhibit similar biases despite language supervision. In this paper, we investigate spurious bias in MLLMs and introduce SpurLens, a pipeline that leverages GPT-4 and open-set object detectors to automatically identify spurious visual cues without human supervision. Our findings reveal that spurious correlations cause two major failure modes in MLLMs: (1) over-reliance on spurious cues for object recognition, where removing these cues reduces accuracy, and (2) object hallucination, where spurious cues amplify the hallucination by over 10x. We validate our findings in various MLLMs and datasets. Beyond diagnosing these failures, we explore potential mitigation strategies, such as prompt ensembling and reasoning-based prompting, and conduct ablation studies to examine the root causes of spurious bias in MLLMs. By exposing the persistence of spurious correlations, our study calls for more rigorous evaluation methods and mitigation strategies to enhance the reliability of MLLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FourierSR: A Fourier Token-based Plugin for Efficient Image Super-Resolution</title>
<link>https://arxiv.org/abs/2503.10043</link>
<guid>https://arxiv.org/abs/2503.10043</guid>
<content:encoded><![CDATA[
arXiv:2503.10043v2 Announce Type: replace 
Abstract: Image super-resolution (SR) aims to recover low-resolution images to high-resolution images, where improving SR efficiency is a high-profile challenge. However, commonly used units in SR, like convolutions and window-based Transformers, have limited receptive fields, making it challenging to apply them to improve SR under extremely limited computational cost. To address this issue, inspired by modeling convolution theorem through token mix, we propose a Fourier token-based plugin called FourierSR to improve SR uniformly, which avoids the instability or inefficiency of existing token mix technologies when applied as plug-ins. Furthermore, compared to convolutions and windows-based Transformers, our FourierSR only utilizes Fourier transform and multiplication operations, greatly reducing complexity while having global receptive fields. Experimental results show that our FourierSR as a plug-and-play unit brings an average PSNR gain of 0.34dB for existing efficient SR methods on Manga109 test set at the scale of x4, while the average increase in the number of Params and FLOPs is only 0.6% and 1.5% of original sizes. We will release our codes upon acceptance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PanDx: AI-assisted Early Detection of Pancreatic Ductal Adenocarcinoma on Contrast-enhanced CT</title>
<link>https://arxiv.org/abs/2503.10068</link>
<guid>https://arxiv.org/abs/2503.10068</guid>
<content:encoded><![CDATA[
arXiv:2503.10068v3 Announce Type: replace 
Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the most aggressive forms of pancreatic cancer and is often diagnosed at an advanced stage due to subtle early imaging signs. To enable earlier detection and improve clinical decision-making, we propose a coarse-to-fine AI-assisted framework named PanDx for identifying PDAC on contrast-enhanced CT scans. Our approach integrates two novel techniques: (1) distribution-aware stratified ensembling to improve generalization across lesion variations, and (2) peak-scaled lesion candidate extraction to enhance lesion localization precision. PanDx is developed and evaluated as part of the PANORAMA challenge, where it ranked 1st place on the official test set with an AUROC of 0.9263 and an AP of 0.7243. Furthermore, we analyzed failure cases with a radiologist to identify the limitations of AI models on this task and discussed potential future directions for model improvement. Our code and models are publicly available at https://github.com/han-liu/PanDx.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastVID: Dynamic Density Pruning for Fast Video Large Language Models</title>
<link>https://arxiv.org/abs/2503.11187</link>
<guid>https://arxiv.org/abs/2503.11187</guid>
<content:encoded><![CDATA[
arXiv:2503.11187v3 Announce Type: replace 
Abstract: Video Large Language Models have demonstrated strong video understanding capabilities, yet their practical deployment is hindered by substantial inference costs caused by redundant video tokens. Existing pruning techniques fail to effectively exploit the spatiotemporal redundancy present in video data. To bridge this gap, we perform a systematic analysis of video redundancy from two perspectives: temporal context and visual context. Leveraging these insights, we propose Dynamic Density Pruning for Fast Video LLMs termed FastVID. Specifically, FastVID dynamically partitions videos into temporally ordered segments to preserve temporal structure and applies a density-based token pruning strategy to maintain essential spatial and temporal information. Our method significantly reduces computational overhead while maintaining temporal and visual integrity. Extensive evaluations show that FastVID achieves state-of-the-art performance across various short- and long-video benchmarks on leading Video LLMs, including LLaVA-OneVision, LLaVA-Video, Qwen2-VL, and Qwen2.5-VL. Notably, on LLaVA-OneVision-7B, FastVID effectively prunes $\textbf{90.3%}$ of video tokens, reduces FLOPs to $\textbf{8.3%}$, and accelerates the LLM prefill stage by $\textbf{7.1}\times$, while maintaining $\textbf{98.0%}$ of the original accuracy. The code is available at https://github.com/LunarShen/FastVID.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
<link>https://arxiv.org/abs/2503.14505</link>
<guid>https://arxiv.org/abs/2503.14505</guid>
<content:encoded><![CDATA[
arXiv:2503.14505v2 Announce Type: replace 
Abstract: We introduce MusicInfuser, an approach that aligns pre-trained text-to-video diffusion models to generate high-quality dance videos synchronized with specified music tracks. Rather than training a multimodal audio-video or audio-motion model from scratch, our method demonstrates how existing video diffusion models can be efficiently adapted to align with musical inputs. We propose a novel layer-wise adaptability criterion based on a guidance-inspired constructive influence function to select adaptable layers, significantly reducing training costs while preserving rich prior knowledge, even with limited, specialized datasets. Experiments show that MusicInfuser effectively bridges the gap between music and video, generating novel and diverse dance movements that respond dynamically to music. Furthermore, our framework generalizes well to unseen music tracks, longer video sequences, and unconventional subjects, outperforming baseline models in consistency and synchronization. All of this is achieved without requiring motion data, with training completed on a single GPU within a day.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset Distillation of 3D Point Clouds via Distribution Matching</title>
<link>https://arxiv.org/abs/2503.22154</link>
<guid>https://arxiv.org/abs/2503.22154</guid>
<content:encoded><![CDATA[
arXiv:2503.22154v3 Announce Type: replace 
Abstract: Large-scale datasets are usually required to train deep neural networks, but it increases the computational complexity hindering the practical applications. Recently, dataset distillation for images and texts has been attracting a lot of attention, that reduces the original dataset to a synthetic dataset to alleviate the computational burden of training while preserving essential task-relevant information. However, the dataset distillation for 3D point clouds remains largely unexplored, as the point clouds exhibit fundamentally different characteristics from that of images, making the dataset distillation more challenging. In this paper, we propose a distribution matching-based distillation framework for 3D point clouds that jointly optimizes the geometric structures as well as the orientations of the synthetic 3D objects. To address the semantic misalignment caused by unordered indexing of points, we introduce a Semantically Aligned Distribution Matching loss computed on the sorted features in each channel. Moreover, to address the rotation variation, we jointly learn the optimal rotation angles while updating the synthetic dataset to better align with the original feature distribution. Extensive experiments on widely used benchmark datasets demonstrate that the proposed method consistently outperforms existing dataset distillation methods, achieving superior accuracy and strong cross-architecture generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning</title>
<link>https://arxiv.org/abs/2504.07960</link>
<guid>https://arxiv.org/abs/2504.07960</guid>
<content:encoded><![CDATA[
arXiv:2504.07960v3 Announce Type: replace 
Abstract: Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose VisualCloze, a universal image generation framework, which supports a wide range of in-domain tasks, generalization to unseen ones, unseen unification of multiple tasks, and reverse generation. Unlike existing methods that rely on language-based task instruction, leading to task ambiguity and weak generalization, we integrate visual in-context learning, allowing models to identify tasks from visual demonstrations. Meanwhile, the inherent sparsity of visual task distributions hampers the learning of transferable knowledge across tasks. To this end, we introduce Graph200K, a graph-structured dataset that establishes various interrelated tasks, enhancing task density and transferable knowledge. Furthermore, we uncover that our unified image generation formulation shared a consistent objective with image infilling, enabling us to leverage the strong generative priors of pre-trained infilling models without modifying the architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relation-R1: Progressively Cognitive Chain-of-Thought Guided Reinforcement Learning for Unified Relation Comprehension</title>
<link>https://arxiv.org/abs/2504.14642</link>
<guid>https://arxiv.org/abs/2504.14642</guid>
<content:encoded><![CDATA[
arXiv:2504.14642v3 Announce Type: replace 
Abstract: Recent advances in multi-modal large language models (MLLMs) have significantly improved object-level grounding and region captioning. However, they remain limited in visual relation understanding, struggling even with binary relation detection, let alone \textit{N}-ary relations involving multiple semantic roles. The core reason is the lack of modeling for \textit{structural semantic dependencies} among multi-entities, leading to unreliable outputs, hallucinations, and over-reliance on language priors (\eg, defaulting to ``person drinks a milk'' if a person is merely holding it). To this end, we propose Relation-R1, the \textit{first unified} relation comprehension framework that explicitly integrates cognitive chain-of-thought (CoT)-guided supervised fine-tuning (SFT) and group relative policy optimization (GRPO) within a reinforcement learning (RL) paradigm. Specifically, we first establish foundational reasoning capabilities via SFT, enforcing structured outputs with thinking processes. Then, GRPO is utilized to refine these outputs via multi-rewards optimization, prioritizing visual-semantic grounding over language-induced biases, thereby improving generalization capability. Furthermore, we investigate the impact of various CoT strategies within this framework, demonstrating that a specific-to-general progressive approach in CoT guidance further improves generalization, especially in capturing synonymous \textit{N}-ary relations. Extensive experiments on widely-used PSG and SWiG datasets demonstrate that Relation-R1 achieves state-of-the-art performance in both binary and \textit{N}-ary relation understanding.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation</title>
<link>https://arxiv.org/abs/2504.15134</link>
<guid>https://arxiv.org/abs/2504.15134</guid>
<content:encoded><![CDATA[
arXiv:2504.15134v4 Announce Type: replace 
Abstract: Category-level object pose estimation aims to predict the 6D pose and size of previously unseen instances from predefined categories, requiring strong generalization across diverse object instances. Although many previous methods attempt to mitigate intra-class variations, they often struggle with instances exhibiting complex geometries or significant deviations from canonical shapes. To address this issue, we propose INKL-Pose, a novel category-level object pose estimation framework that enables INstance-adaptive Keypoint Learning with local-to-global geometric aggregation. Specifically, our method first predicts semantically consistent and geometrically informative keypoints using an Instance-Adaptive Keypoint Detector, then refines them: (1) a Local Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global Keypoint Feature Aggregator using bidirectional Mamba for structural consistency. To enable bidirectional modeling in Mamba, we introduce a simple yet effective Feature Sequence Flipping strategy that preserves spatial coherence while constructing backward feature sequence. Additionally, we design a surface loss and a separation loss to encourage uniform coverage and spatial diversity in keypoint distribution. The resulting keypoints are mapped to a canonical space for 6D pose and size regression. Extensive experiments on CAMERA25, REAL275, and HouseCat6D show that INKL-Pose achieves state-of-the-art performance with 16.7M parameters and runs at 36 FPS on an NVIDIA RTX 4090D GPU.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering</title>
<link>https://arxiv.org/abs/2504.17545</link>
<guid>https://arxiv.org/abs/2504.17545</guid>
<content:encoded><![CDATA[
arXiv:2504.17545v2 Announce Type: replace 
Abstract: We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for radiance field rendering, wherein a set of 2D opaque surfels with view-dependent colors represent the coarse-scale geometry and appearance of scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale appearance details. The rendering with GESs consists of two passes -- surfels are first rasterized through a standard graphics pipeline to produce depth and color maps, and then Gaussians are splatted with depth testing and color accumulation on each pixel order independently. The optimization of GESs from multi-view images is performed through an elaborate coarse-to-fine procedure, faithfully capturing rich scene appearance. The entirely sorting-free rendering of GESs not only achieves very fast rates, but also produces view-consistent images, successfully avoiding popping artifacts under view changes. The basic GES representation can be easily extended to achieve anti-aliasing in rendering (Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage (Compact-GES), and reconstruct better scene geometries by replacing 3D Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs advance the state-of-the-arts as a compelling representation for ultra-fast high-fidelity radiance field rendering.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Potential of Difficulty Prior in RL-based Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2505.13261</link>
<guid>https://arxiv.org/abs/2505.13261</guid>
<content:encoded><![CDATA[
arXiv:2505.13261v2 Announce Type: replace 
Abstract: In this work, we investigate how explicitly modeling problem's difficulty prior information shapes the effectiveness of reinforcement learning based fine-tuning for multimodal reasoning. Our exploration mainly comprises of following three perspective: First, through offline data curation, we analyze the U-shaped difficulty distribution of two given datasets using the base model by multi-round sampling, and then filter out prompts that are either too simple or extremely difficult to provide meaningful gradients and perform subsequent two-stage training. Second, we implement an online advantage differentiation, computing group-wise empirical accuracy as a difficulty proxy to adaptively reweight advantages estimation, providing stronger learning signals for more challenging problems. Finally, we introduce difficulty hints as explicit prompts for more complex samples in the second training stage, encouraging the model to calibrate its reasoning depth and perform reflective validation checks. Our comprehensive approach demonstrates significant performances across various multi-modal mathematical reasoning benchmarks with only 2K+0.6K two-stage training data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes</title>
<link>https://arxiv.org/abs/2506.00227</link>
<guid>https://arxiv.org/abs/2506.00227</guid>
<content:encoded><![CDATA[
arXiv:2506.00227v2 Announce Type: replace 
Abstract: Video diffusion techniques have advanced significantly in recent years; however, they struggle to generate realistic imagery of car crashes due to the scarcity of accident events in most driving datasets. Improving traffic safety requires realistic and controllable accident simulations. To tackle the problem, we propose Ctrl-Crash, a controllable car crash video generation model that conditions on signals such as bounding boxes, crash types, and an initial image frame. Our approach enables counterfactual scenario generation where minor variations in input can lead to dramatically different crash outcomes. To support fine-grained control at inference time, we leverage classifier-free guidance with independently tunable scales for each conditioning signal. Ctrl-Crash achieves state-of-the-art performance across quantitative video quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a human-evaluation of physical realism and video quality compared to prior diffusion-based methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How PARTs assemble into wholes: Learning the relative composition of images</title>
<link>https://arxiv.org/abs/2506.03682</link>
<guid>https://arxiv.org/abs/2506.03682</guid>
<content:encoded><![CDATA[
arXiv:2506.03682v2 Announce Type: replace 
Abstract: The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images-an off-grid structural relative positioning that is less tied to absolute appearance and can remain coherent under variations such as partial visibility or stylistic changes. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms grid-based methods like MAE and DropPos, while maintaining competitive performance on global classification tasks. By breaking free from grid constraints, PART opens up a new trajectory for universal self-supervised pretraining across diverse datatypes-from images to EEG signals-with potential in medical imaging, video, and audio.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normalize Filters! Classical Wisdom for Deep Vision</title>
<link>https://arxiv.org/abs/2506.04401</link>
<guid>https://arxiv.org/abs/2506.04401</guid>
<content:encoded><![CDATA[
arXiv:2506.04401v4 Announce Type: replace 
Abstract: Classical image filters, such as those for averaging or differencing, are carefully normalized to ensure consistency, interpretability, and to avoid artifacts like intensity shifts, halos, or ringing. In contrast, convolutional filters learned end-to-end in deep networks lack such constraints. Although they may resemble wavelets and blob/edge detectors, they are not normalized in the same or any way. Consequently, when images undergo atmospheric transfer, their responses become distorted, leading to incorrect outcomes. We address this limitation by proposing filter normalization, followed by learnable scaling and shifting, akin to batch normalization. This simple yet effective modification ensures that the filters are atmosphere-equivariant, enabling co-domain symmetry. By integrating classical filtering principles into deep learning (applicable to both convolutional neural networks and convolution-dependent vision transformers), our method achieves significant improvements on artificial and natural intensity variation benchmarks. Our ResNet34 could even outperform CLIP by a large margin. Our analysis reveals that unnormalized filters degrade performance, whereas filter normalization regularizes learning, promotes diversity, and improves robustness and generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>U-NetMN and SegNetMN: Modified U-Net and SegNet models for bimodal SAR image segmentation</title>
<link>https://arxiv.org/abs/2506.05444</link>
<guid>https://arxiv.org/abs/2506.05444</guid>
<content:encoded><![CDATA[
arXiv:2506.05444v2 Announce Type: replace 
Abstract: Segmenting Synthetic Aperture Radar (SAR) images is crucial for many remote sensing applications, particularly water body detection. However, deep learning-based segmentation models often face challenges related to convergence speed and stability, mainly due to the complex statistical distribution of this type of data. In this study, we evaluate the impact of mode normalization on two widely used semantic segmentation models, U-Net and SegNet. Specifically, we integrate mode normalization, to reduce convergence time while maintaining the performance of the baseline models. Experimental results demonstrate that mode normalization significantly accelerates convergence. Furthermore, cross-validation results indicate that normalized models exhibit increased stability in different zones. These findings highlight the effectiveness of normalization in improving computational efficiency and generalization in SAR image segmentation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WakeupUrban: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imagery</title>
<link>https://arxiv.org/abs/2506.09476</link>
<guid>https://arxiv.org/abs/2506.09476</guid>
<content:encoded><![CDATA[
arXiv:2506.09476v3 Announce Type: replace 
Abstract: Historical satellite imagery archive, such as Keyhole satellite data, offers rare insights into understanding early urban development and long-term transformation. However, severe quality degradation ($\textit{e.g.}$, distortion, misalignment, and spectral scarcity) and the absence of annotations have long hindered its analysis. To bridge this gap and enhance understanding of urban development, we introduce $\textbf{WakeupUrbanBench}$, an annotated segmentation dataset based on historical satellite imagery with the earliest observation time among all existing remote sensing (RS) datasets, along with a framework for unsupervised segmentation tasks, $\textbf{WakeupUSM}$. First, WakeupUrbanBench serves as a pioneer, expertly annotated dataset built on mid-$20^{\text{th}}$ century RS imagery, involving four key urban classes and spanning 4 cities across 2 continents with nearly 1000 km$^2$ area of diverse urban morphologies, and additionally introducing one present-day city. Second, WakeupUSM is a novel unsupervised semantic segmentation framework for historical RS imagery. It employs a confidence-aware alignment mechanism and focal-confidence loss based on a self-supervised learning architecture, which generates robust pseudo-labels and adaptively prioritizes prediction difficulty and label reliability to improve unsupervised segmentation on noisy historical data without manual supervision. Comprehensive experiments demonstrate WakeupUSM significantly outperforms existing unsupervised segmentation methods $\textbf{both WakeupUrbanBench and public dataset}$, promising to pave the way for quantitative studies of long-term urban change using modern computer vision. Our benchmark and codes will be released at https://github.com/Tianxiang-Hao/WakeupUrban.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmenting Visuals With Querying Words: Language Anchors For Semi-Supervised Image Segmentation</title>
<link>https://arxiv.org/abs/2506.13925</link>
<guid>https://arxiv.org/abs/2506.13925</guid>
<content:encoded><![CDATA[
arXiv:2506.13925v3 Announce Type: replace 
Abstract: Vision Language Models (VLMs) provide rich semantic priors but are underexplored in Semi supervised Semantic Segmentation. Recent attempts to integrate VLMs to inject high level semantics overlook the semantic misalignment between visual and textual representations that arises from using domain invariant text embeddings without adapting them to dataset and image specific contexts. This lack of domain awareness, coupled with limited annotations, weakens the model semantic understanding by preventing effective vision language alignment. As a result, the model struggles with contextual reasoning, shows weak intra class discrimination, and confuses similar classes. To address these challenges, we propose Hierarchical Vision Language transFormer (HVLFormer), which achieves domain aware and domain robust alignment between visual and textual representations within a mask transformer architecture. Firstly, we transform text embeddings from pretrained VLMs into textual object queries, enabling the generation of multi scale, dataset aware queries that capture class semantics from coarse to fine granularity and enhance contextual reasoning. Next, we refine these queries by injecting image specific visual context to align textual semantics with local scene structures and enhance class discrimination. Finally, to achieve domain robustness, we introduce cross view and modal consistency regularization, which enforces prediction consistency within mask-transformer architecture across augmented views. Moreover, it ensures stable vision language alignment during decoding. With less than 1% training data, HVLFormer outperforms state of the art methods on Pascal VOC, COCO, ADE20K, and Cityscapes. Our code and results will be available on GitHub.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual symbolic mechanisms: Emergent symbol processing in vision language models</title>
<link>https://arxiv.org/abs/2506.15871</link>
<guid>https://arxiv.org/abs/2506.15871</guid>
<content:encoded><![CDATA[
arXiv:2506.15871v2 Announce Type: replace 
Abstract: To accurately process a visual scene, observers must bind features together to represent individual objects. This capacity is necessary, for instance, to distinguish an image containing a red square and a blue circle from an image containing a blue square and a red circle. Recent work has found that language models solve this 'binding problem' via a set of symbol-like, content-independent indices, but it is unclear whether similar mechanisms are employed by Vision Language Models (VLMs). This question is especially relevant, given the persistent failures of VLMs on tasks that require binding. Here, we identify a previously unknown set of emergent symbolic mechanisms that support binding specifically in VLMs, via a content-independent, spatial indexing scheme. Moreover, we find that binding errors, when they occur, can be traced directly to failures in these mechanisms. Taken together, these results shed light on the mechanisms that support symbol-like processing in VLMs, and suggest possible avenues for reducing the number of binding failures exhibited by these models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Camera Calibration via Circular Patterns: A Comprehensive Framework with Detection Uncertainty and Unbiased Projection Model</title>
<link>https://arxiv.org/abs/2506.16842</link>
<guid>https://arxiv.org/abs/2506.16842</guid>
<content:encoded><![CDATA[
arXiv:2506.16842v2 Announce Type: replace 
Abstract: Camera calibration using planar targets has been widely favored, and two types of control points have been mainly considered as measurements: the corners of the checkerboard and the centroid of circles. Since a centroid is derived from numerous pixels, the circular pattern provides more precise measurements than the checkerboard. However, the existing projection model of circle centroids is biased under lens distortion, resulting in low performance. To surmount this limitation, we propose an unbiased projection model of the circular pattern and demonstrate its superior accuracy compared to the checkerboard. Complementing this, we introduce uncertainty into circular patterns to enhance calibration robustness and completeness. Defining centroid uncertainty improves the performance of calibration components, including pattern detection, optimization, and evaluation metrics. We also provide guidelines for performing good camera calibration based on the evaluation metric. The core concept of this approach is to model the boundary points of a two-dimensional shape as a Markov random field, considering its connectivity. The shape distribution is propagated to the centroid uncertainty through an appropriate shape representation based on the Green theorem. Consequently, the resulting framework achieves marked gains in calibration accuracy and robustness. The complete source code and demonstration video are available at https://github.com/chaehyeonsong/discocal.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MR-COSMO: Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation</title>
<link>https://arxiv.org/abs/2506.20991</link>
<guid>https://arxiv.org/abs/2506.20991</guid>
<content:encoded><![CDATA[
arXiv:2506.20991v3 Announce Type: replace 
Abstract: The rapid advancement of vision-language models (VLMs) in 3D domains has accelerated research in text-query-guided point cloud processing, though existing methods underperform in point-level segmentation due to inadequate 3D-text alignment that limits local feature-text context linking. To address this limitation, we propose MR-COSMO, a Visual-Text Memory Recall and Direct CrOSs-MOdal Alignment Method for Query-Driven 3D Segmentation, establishing explicit alignment between 3D point clouds and text/2D image data through a dedicated direct cross-modal alignment module while implementing a visual-text memory module with specialized feature banks. This direct alignment mechanism enables precise fusion of geometric and semantic features, while the memory module employs specialized banks storing text features, visual features, and their correspondence mappings to dynamically enhance scene-specific representations via attention-based knowledge recall. Comprehensive experiments across 3D instruction, reference, and semantic segmentation benchmarks confirm state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FACM: Flow-Anchored Consistency Models</title>
<link>https://arxiv.org/abs/2507.03738</link>
<guid>https://arxiv.org/abs/2507.03738</guid>
<content:encoded><![CDATA[
arXiv:2507.03738v2 Announce Type: replace 
Abstract: Continuous-time Consistency Models (CMs) promise efficient few-step generation but face significant challenges with training instability. We argue this instability stems from a fundamental conflict: Training the network exclusively on a shortcut objective leads to the catastrophic forgetting of the instantaneous velocity field that defines the flow. Our solution is to explicitly anchor the model in the underlying flow, ensuring high trajectory fidelity during training. We introduce the Flow-Anchored Consistency Model (FACM), where a Flow Matching (FM) task serves as a dynamic anchor for the primary CM shortcut objective. Key to this Flow-Anchoring approach is a novel expanded time interval strategy that unifies optimization for a single model while decoupling the two tasks to ensure stable, architecturally-agnostic training. By distilling a pre-trained LightningDiT model, our method achieves a state-of-the-art FID of 1.32 with two steps (NFE=2) and 1.70 with just one step (NFE=1) on ImageNet 256x256. To address the challenge of scalability, we develop a memory-efficient Chain-JVP that resolves key incompatibilities with FSDP. This method allows us to scale FACM training on a 14B parameter model (Wan 2.2), accelerating its Text-to-Image inference from 2x40 to 2-8 steps. Our code and pretrained models: https://github.com/ali-vilab/FACM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAST-Phys: Contactless Affective States Through Physiological signals Database</title>
<link>https://arxiv.org/abs/2507.06080</link>
<guid>https://arxiv.org/abs/2507.06080</guid>
<content:encoded><![CDATA[
arXiv:2507.06080v2 Announce Type: replace 
Abstract: In recent years, affective computing and its applications have become a fast-growing research topic. Despite significant advancements, the lack of affective multi-modal datasets remains a major bottleneck in developing accurate emotion recognition systems. Furthermore, the use of contact-based devices during emotion elicitation often unintentionally influences the emotional experience, reducing or altering the genuine spontaneous emotional response. This limitation highlights the need for methods capable of extracting affective cues from multiple modalities without physical contact, such as remote physiological emotion recognition. To address this, we present the Contactless Affective States Through Physiological Signals Database (CAST-Phys), a novel high-quality dataset explicitly designed for multi-modal remote physiological emotion recognition using facial and physiological cues. The dataset includes diverse physiological signals, such as photoplethysmography (PPG), electrodermal activity (EDA), and respiration rate (RR), alongside high-resolution uncompressed facial video recordings, enabling the potential for remote signal recovery. Our analysis highlights the crucial role of physiological signals in realistic scenarios where facial expressions alone may not provide sufficient emotional information. Furthermore, we demonstrate the potential of remote multi-modal emotion recognition by evaluating the impact of individual and fused modalities, showcasing its effectiveness in advancing contactless emotion recognition technologies.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnthroTAP: Learning Point Tracking with Real-World Motion</title>
<link>https://arxiv.org/abs/2507.06233</link>
<guid>https://arxiv.org/abs/2507.06233</guid>
<content:encoded><![CDATA[
arXiv:2507.06233v2 Announce Type: replace 
Abstract: Point tracking models often struggle to generalize to real-world videos because large-scale training data is predominantly synthetic--the only source currently feasible to produce at scale. Collecting real-world annotations, however, is prohibitively expensive, as it requires tracking hundreds of points across frames. We introduce AnthroTAP, an automated pipeline that generates large-scale pseudo-labeled point tracking data from real human motion videos. Leveraging the structured complexity of human movement-non-rigid deformations, articulated motion, and frequent occlusions-AnthroTAP fits Skinned Multi-Person Linear (SMPL) models to detected humans, projects mesh vertices onto image planes, resolves occlusions via ray-casting, and filters unreliable tracks using optical flow consistency. A model trained on the AnthroTAP dataset achieves state-of-the-art performance on TAP-Vid, a challenging general-domain benchmark for tracking any point on diverse rigid and non-rigid objects (e.g., humans, animals, robots, and vehicles). Our approach outperforms recent self-supervised teacher-student models trained on vastly larger real datasets, while requiring only one day of training on 4 GPUs. AnthroTAP shows that structured human motion offers a scalable and effective source of real-world supervision for point tracking. Code and datasets will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Linear Separability Ceiling: Aligning Representations in VLMs</title>
<link>https://arxiv.org/abs/2507.07574</link>
<guid>https://arxiv.org/abs/2507.07574</guid>
<content:encoded><![CDATA[
arXiv:2507.07574v3 Announce Type: replace 
Abstract: A challenge in advancing Visual-Language Models (VLMs) is determining whether their failures on abstract reasoning tasks, such as Bongard problems, stem from flawed perception or faulty top-down reasoning. To disentangle these factors, we introduce a diagnostic framework centered on the Linear Separability Ceiling (LSC), the performance achievable by a linear classifier on a VLM's raw visual embeddings. Applying this framework to state-of-the-art VLMs, we uncover a pervasive ``alignment gap'', where most models fail to generatively outperform the linear separability of their representations. We find that the few models surpassing this ceiling do so via two mechanisms: by further refining visual representations into a more linearly separable format or by executing non-linear decision logic. We demonstrate that this bottleneck is not a fundamental limitation but a solvable visual alignment issue. Our method augments standard next-token prediction with a contrastive objective to restructure the visual manifold into a more one-dimensionally linear geometry, improving image-to-image comparison and enabling models to significantly surpass the LSC on abstract binary classification tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffPose-Animal: A Language-Conditioned Diffusion Framework for Animal Pose Estimation</title>
<link>https://arxiv.org/abs/2508.08783</link>
<guid>https://arxiv.org/abs/2508.08783</guid>
<content:encoded><![CDATA[
arXiv:2508.08783v2 Announce Type: replace 
Abstract: Animal pose estimation is a fundamental task in computer vision, with growing importance in ecological monitoring, behavioral analysis, and intelligent livestock management. Compared to human pose estimation, animal pose estimation is more challenging due to high interspecies morphological diversity, complex body structures, and limited annotated data. In this work, we introduce DiffPose-Animal, a novel diffusion-based framework for top-down animal pose estimation. Unlike traditional heatmap regression methods, DiffPose-Animal reformulates pose estimation as a denoising process under the generative framework of diffusion models. To enhance semantic guidance during keypoint generation, we leverage large language models (LLMs) to extract both global anatomical priors and local keypoint-wise semantics based on species-specific prompts. These textual priors are encoded and fused with image features via cross-attention modules to provide biologically meaningful constraints throughout the denoising process. Additionally, a diffusion-based keypoint decoder is designed to progressively refine pose predictions, improving robustness to occlusion and annotation sparsity. Extensive experiments on public animal pose datasets demonstrate the effectiveness and generalization capability of our method, especially under challenging scenarios with diverse species, cluttered backgrounds, and incomplete keypoints.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</title>
<link>https://arxiv.org/abs/2508.10576</link>
<guid>https://arxiv.org/abs/2508.10576</guid>
<content:encoded><![CDATA[
arXiv:2508.10576v4 Announce Type: replace 
Abstract: While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks.Furthermore, grounded in the observation that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, we posit that reasoning ability serves as the key to unlocking it. We devise a multi-stage, modality-progressive reinforcement learning approach, resulting in HumanSense-Omni-Reasoning, which substantially enhances performance on higher-level understanding and interactive tasks. Additionally, we observe that successful reasoning processes appear to exhibit consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner.Project page: \textcolor{brightpink}{https://digital-avatar.github.io/ai/HumanSense/}
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation</title>
<link>https://arxiv.org/abs/2508.17364</link>
<guid>https://arxiv.org/abs/2508.17364</guid>
<content:encoded><![CDATA[
arXiv:2508.17364v3 Announce Type: replace 
Abstract: The image-to-image generation task aims to produce controllable images by leveraging conditional inputs and prompt instructions. However, existing methods often train separate control branches for each type of condition, leading to redundant model structures and inefficient use of computational resources. To address this, we propose a Unified image-to-image Generation (UniGen) framework that supports diverse conditional inputs while enhancing generation efficiency and expressiveness. Specifically, to tackle the widely existing parameter redundancy and computational inefficiency in controllable conditional generation architectures, we propose the Condition Modulated Expert (CoMoE) module. This module aggregates semantically similar patch features and assigns them to dedicated expert modules for visual representation and conditional modeling. By enabling independent modeling of foreground features under different conditions, CoMoE effectively mitigates feature entanglement and redundant computation in multi-condition scenarios. Furthermore, to bridge the information gap between the backbone and control branches, we propose WeaveNet, a dynamic, snake-like connection mechanism that enables effective interaction between global text-level control from the backbone and fine-grained control from conditional branches. Extensive experiments on the Subjects-200K and MultiGen-20M datasets across various conditional image generation tasks demonstrate that our method consistently achieves state-of-the-art performance, validating its advantages in both versatility and effectiveness. The code has been uploaded to https://github.com/gavin-gqzhang/UniGen.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Color Bind: Exploring Color Perception in Text-to-Image Models</title>
<link>https://arxiv.org/abs/2508.19791</link>
<guid>https://arxiv.org/abs/2508.19791</guid>
<content:encoded><![CDATA[
arXiv:2508.19791v3 Announce Type: replace 
Abstract: Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, such as the cosine similarity between text and image CLIP embeddings, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors -- a fundamental attribute commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle to generate images that faithfully reflect multiple color attributes-far more so than with single-color prompts-and that neither inference-time techniques nor existing editing methods reliably resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a wide range of metrics, considering images generated by various text-to-image diffusion-based techniques.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian-Plus-SDF SLAM: High-fidelity 3D Reconstruction at 150+ fps</title>
<link>https://arxiv.org/abs/2509.11574</link>
<guid>https://arxiv.org/abs/2509.11574</guid>
<content:encoded><![CDATA[
arXiv:2509.11574v2 Announce Type: replace 
Abstract: While recent Gaussian-based SLAM methods achieve photorealistic reconstruction from RGB-D data, their computational performance remains a critical bottleneck. State-of-the-art techniques operate at less than 20 fps, significantly lagging behind geometry-based approaches like KinectFusion (hundreds of fps). This limitation stems from the heavy computational burden: modeling scenes requires numerous Gaussians and complex iterative optimization to fit RGB-D data; insufficient Gaussian counts or optimization iterations cause severe quality degradation. To address this, we propose a Gaussian-SDF hybrid representation, combining a colorized signed distance field (SDF) for smooth geometry and appearance with 3D Gaussians to capture underrepresented details. The SDF is efficiently constructed via RGB-D fusion (as in geometry-based methods), while Gaussians undergo iterative optimization. Our representation enables significant Gaussian reduction (50% fewer) by avoiding full-scene Gaussian modeling, and efficient Gaussian optimization (75% fewer iterations) through targeted appearance refinement. Building upon this representation, we develop GPS-SLAM (Gaussian-plus-SDF SLAM), a real-time 3D reconstruction system achieving over 150 fps on real-world Azure Kinect sequences, faster by an order-of-magnitude than state-of-the-art techniques while maintaining comparable reconstruction quality. The source code and data are available at https://gapszju.github.io/GPS-SLAM.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTMA-Net: Towards Multiplication-Avoiding Neural Networks via Hadamard Transform and In-Memory Computing</title>
<link>https://arxiv.org/abs/2509.23103</link>
<guid>https://arxiv.org/abs/2509.23103</guid>
<content:encoded><![CDATA[
arXiv:2509.23103v2 Announce Type: replace 
Abstract: Reducing the cost of multiplications is critical for efficient deep neural network deployment, especially in energy-constrained edge devices. In this work, we introduce HTMA-Net, a novel framework that integrates the Hadamard Transform (HT) with multiplication-avoiding (MA) SRAM-based in-memory computing to reduce arithmetic complexity while maintaining accuracy. Unlike prior methods that only target multiplications in convolutional layers or focus solely on in-memory acceleration, HTMA-Net selectively replaces intermediate convolutions with Hybrid Hadamard-based transform layers whose internal convolutions are implemented via multiplication-avoiding in-memory operations. We evaluate HTMA-Net on ResNet-18 using CIFAR-10, CIFAR-100, and Tiny ImageNet, and provide a detailed comparison against regular, MF-only, and HT-only variants. Results show that HTMA-Net eliminates up to 52\% of multiplications compared to baseline ResNet-18, ResNet-20, and ResNet-32 models, while achieving comparable accuracy in evaluation and significantly reducing computational complexity and the number of parameters. Our results demonstrate that combining structured Hadamard transform layers with SRAM-based in-memory computing multiplication-avoiding operators is a promising path towards efficient deep learning architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>C3-OWD: A Curriculum Cross-modal Contrastive Learning Framework for Open-World Detection</title>
<link>https://arxiv.org/abs/2509.23316</link>
<guid>https://arxiv.org/abs/2509.23316</guid>
<content:encoded><![CDATA[
arXiv:2509.23316v2 Announce Type: replace 
Abstract: Object detection has advanced significantly in the closed-set setting, but real-world deployment remains limited by two challenges: poor generalization to unseen categories and insufficient robustness under adverse conditions. Prior research has explored these issues separately: visible-infrared detection improves robustness but lacks generalization, while open-world detection leverages vision-language alignment strategy for category diversity but struggles under extreme environments. This trade-off leaves robustness and diversity difficult to achieve simultaneously. To mitigate these issues, we propose \textbf{C3-OWD}, a curriculum cross-modal contrastive learning framework that unifies both strengths. Stage~1 enhances robustness by pretraining with RGBT data, while Stage~2 improves generalization via vision-language alignment. To prevent catastrophic forgetting between two stages, we introduce an Exponential Moving Average (EMA) mechanism that theoretically guarantees preservation of pre-stage performance with bounded parameter lag and function consistency. Experiments on FLIR, OV-COCO, and OV-LVIS demonstrate the effectiveness of our approach: C3-OWD achieves $80.1$ AP$^{50}$ on FLIR, $48.6$ AP$^{50}_{\text{Novel}}$ on OV-COCO, and $35.7$ mAP$_r$ on OV-LVIS, establishing competitive performance across both robustness and diversity evaluations. Code available at: https://github.com/justin-herry/C3-OWD.git.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal Training</title>
<link>https://arxiv.org/abs/2509.23661</link>
<guid>https://arxiv.org/abs/2509.23661</guid>
<content:encoded><![CDATA[
arXiv:2509.23661v3 Announce Type: replace 
Abstract: We present LLaVA-OneVision-1.5, a novel family of Large Multimodal Models (LMMs) that achieve state-of-the-art performance with significantly reduced computational and financial costs. Different from the existing works, LLaVA-OneVision-1.5 provides an open, efficient, and reproducible framework for building high-quality vision-language models entirely from scratch. The LLaVA-OneVision-1.5 release comprises three primary components: (1) Large-Scale Curated Datasets: We construct an 85M concept-balanced pretraining dataset LLaVA-OneVision-1.5-Mid-Traning and a meticulously curated 22M instruction dataset LLaVA-OneVision-1.5-Instruct. (2) Efficient Training Framework: We develop a complete end-to-end efficient training framework leveraging an offline parallel data packing strategy to facilitate the training of LLaVA-OneVision-1.5 within a $16,000 budget. (3) State-of-the-art Performance: Experimental results demonstrate that LLaVA-OneVision-1.5 yields exceptionally competitive performance across a broad range of downstream tasks. Specifically, LLaVA-OneVision-1.5-8B outperforms Qwen2.5-VL-7B on 18 of 27 benchmarks, and LLaVA-OneVision-1.5-4B surpasses Qwen2.5-VL-3B on all 27 benchmarks. (4) RL-based Post-training: We unlock the model's latent potential through a lightweight RL stage, effectively eliciting robust chain-of-thought reasoning to significantly boost performance on complex multimodal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PD-Diag-Net: Clinical-Priors guided Network on Brain MRI for Auxiliary Diagnosis of Parkinson's Disease</title>
<link>https://arxiv.org/abs/2509.23719</link>
<guid>https://arxiv.org/abs/2509.23719</guid>
<content:encoded><![CDATA[
arXiv:2509.23719v3 Announce Type: replace 
Abstract: Parkinson's disease (PD) is a common neurodegenerative disorder that severely diminishes patients' quality of life. Its global prevalence has increased markedly in recent decades. Current diagnostic workflows are complex and heavily reliant on neurologists' expertise, often resulting in delays in early detection and missed opportunities for timely intervention. To address these issues, we propose an end-to-end automated diagnostic method for PD, termed PD-Diag-Net, which performs risk assessment and auxiliary diagnosis directly from raw MRI scans. This framework first introduces an MRI Pre-processing Module (MRI-Processor) to mitigate inter-subject and inter-scanner variability by flexibly integrating established medical imaging preprocessing tools. It then incorporates two forms of clinical prior knowledge: (1) Brain-Region-Relevance-Prior (Relevance-Prior), which specifies brain regions strongly associated with PD; and (2) Brain-Region-Aging-Prior (Aging-Prior), which reflects the accelerated aging typically observed in PD-associated regions. Building on these priors, we design two dedicated modules: the Relevance-Prior Guided Feature Aggregation Module (Aggregator), which guides the model to focus on PD-associated regions at the inter-subject level, and the Age-Prior Guided Diagnosis Module (Diagnoser), which leverages brain age gaps as auxiliary constraints at the intra-subject level to enhance diagnostic accuracy and clinical interpretability. Furthermore, we collected external test data from our collaborating hospital. Experimental results show that PD-Diag-Net achieves 86\% accuracy on external tests and over 96% accuracy in early-stage diagnosis, outperforming existing advanced methods by more than 20%.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FUSAR-KLIP: Towards Multimodal Foundation Models for Remote Sensing</title>
<link>https://arxiv.org/abs/2509.23927</link>
<guid>https://arxiv.org/abs/2509.23927</guid>
<content:encoded><![CDATA[
arXiv:2509.23927v3 Announce Type: replace 
Abstract: Cross-modal artificial intelligence, represented by visual language models, has achieved significant success in general image understanding. However, a fundamental cognitive inconsistency exists between general visual representation and remote sensing image interpretation: remote sensing images couple topography, terrain, and spatial structure, thereby inherently requiring models to possess deep geoscientific understanding. This cognitive difference is further amplified in synthetic aperture radar (SAR) imagery: while SAR possesses irreplaceable all-weather, all-day observation capabilities, it is constrained by coherent imaging mechanisms, exhibiting significant modal heterogeneity with general images. To address this inconsistency, we propose FUSAR-KLIP, the first knowledge-guided general multimodal foundational model for SAR, along with reusable data and evaluation baselines. Specifically: (1) FUSAR-GEOVL-1M (the first large-scale SAR dataset with complete geographic projection attributes) was constructed, covering multiple satellite platforms, 120,000 images, and 135 cities; (2) Aligned structured text was generated through hierarchical cognitive thought chains, accurately encoding more than 1 million multidimensional semantic information from geomorphological environment and regional attributes to spatial relationships; (3) A self-consistent iterative optimization mechanism was designed to guide cross-modal learning with this knowledge information consistent with human cognition and physical laws in a self-supervised closed loop consisting of contrast, matching, and reconstruction; (4) A unified evaluation benchmark was established in 11 typical downstream tasks in the two major categories of vision and language, and compared with 15 mainstream foundation models.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Splitting: Self-supervised learning from incomplete data</title>
<link>https://arxiv.org/abs/2510.00929</link>
<guid>https://arxiv.org/abs/2510.00929</guid>
<content:encoded><![CDATA[
arXiv:2510.00929v4 Announce Type: replace 
Abstract: Self-supervised learning for inverse problems allows to train a reconstruction network from noise and/or incomplete data alone. These methods have the potential of enabling learning-based solutions when obtaining ground-truth references for training is expensive or even impossible. In this paper, we propose a new self-supervised learning strategy devised for the challenging setting where measurements are observed via a single incomplete observation model. We introduce a new definition of equivariance in the context of reconstruction networks, and show that the combination of self-supervised splitting losses and equivariant reconstruction networks results in unbiased estimates of the supervised loss. Through a series of experiments on image inpainting, accelerated magnetic resonance imaging, sparse-view computed tomography, and compressive sensing, we demonstrate that the proposed loss achieves state-of-the-art performance in settings with highly rank-deficient forward models. The code is available at https://github.com/vsechaud/Equivariant-Splitting
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Representation Learning for Customized Tasks</title>
<link>https://arxiv.org/abs/2510.04564</link>
<guid>https://arxiv.org/abs/2510.04564</guid>
<content:encoded><![CDATA[
arXiv:2510.04564v2 Announce Type: replace 
Abstract: Conventional representation learning methods learn a universal representation that primarily captures dominant semantics, which may not always align with customized downstream tasks. For instance, in animal habitat analysis, researchers prioritize scene-related features, whereas universal embeddings emphasize categorical semantics, leading to suboptimal results. As a solution, existing approaches resort to supervised fine-tuning, which however incurs high computational and annotation costs. In this paper, we propose Conditional Representation Learning (CRL), aiming to extract representations tailored to arbitrary user-specified criteria. Specifically, we reveal that the semantics of a space are determined by its basis, thereby enabling a set of descriptive words to approximate the basis for a customized feature space. Building upon this insight, given a user-specified criterion, CRL first employs a large language model (LLM) to generate descriptive texts to construct the semantic basis, then projects the image representation into this conditional feature space leveraging a vision-language model (VLM). The conditional representation better captures semantics for the specific criterion, which could be utilized for multiple customized tasks. Extensive experiments on classification and retrieval tasks demonstrate the superiority and generality of the proposed CRL. The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control</title>
<link>https://arxiv.org/abs/2510.09561</link>
<guid>https://arxiv.org/abs/2510.09561</guid>
<content:encoded><![CDATA[
arXiv:2510.09561v2 Announce Type: replace 
Abstract: Current controllable diffusion models typically rely on fixed architectures that modify intermediate activations to inject guidance conditioned on a new modality. This approach uses a static conditioning strategy for a dynamic, multi-stage denoising process, limiting the model's ability to adapt its response as the generation evolves from coarse structure to fine detail. We introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that enables dynamic, context-aware control by conditioning the model's weights directly. Our framework uses a hypernetwork to generate LoRA adapters on-the-fly, tailoring weight modifications for the frozen backbone at each diffusion step based on time and the user's condition. This mechanism enables the model to learn and execute an explicit, adaptive strategy for applying conditional guidance throughout the entire generation process. Through experiments on various data domains, we demonstrate that this dynamic, parametric control significantly enhances generative fidelity and adherence to spatial conditions compared to static, activation-based methods. TC-LoRA establishes an alternative approach in which the model's conditioning strategy is modified through a deeper functional adaptation of its weights, allowing control to align with the dynamic demands of the task and generative stage.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering</title>
<link>https://arxiv.org/abs/2510.09995</link>
<guid>https://arxiv.org/abs/2510.09995</guid>
<content:encoded><![CDATA[
arXiv:2510.09995v2 Announce Type: replace 
Abstract: Lens flare occurs when shooting towards strong light sources, significantly degrading the visual quality of images. Due to the difficulty in capturing flare-corrupted and flare-free image pairs in the real world, existing datasets are typically synthesized in 2D by overlaying artificial flare templates onto background images. However, the lack of flare diversity in templates and the neglect of physical principles in the synthesis process hinder models trained on these datasets from generalizing well to real-world scenarios. To address these challenges, we propose a new physics-informed method for flare data generation, which consists of three stages: parameterized template creation, the laws of illumination-aware 2D synthesis, and physical engine-based 3D rendering, which finally gives us a mixed flare dataset that incorporates both 2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D scenes. Furthermore, we design a masking approach to obtain real-world flare-free images from their corrupted counterparts to measure the performance of the model on real-world images. Extensive experiments demonstrate the effectiveness of our method and dataset.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution</title>
<link>https://arxiv.org/abs/2510.12793</link>
<guid>https://arxiv.org/abs/2510.12793</guid>
<content:encoded><![CDATA[
arXiv:2510.12793v2 Announce Type: replace 
Abstract: Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counting Hallucinations in Diffusion Models</title>
<link>https://arxiv.org/abs/2510.13080</link>
<guid>https://arxiv.org/abs/2510.13080</guid>
<content:encoded><![CDATA[
arXiv:2510.13080v2 Announce Type: replace 
Abstract: Diffusion probabilistic models (DPMs) have demonstrated remarkable progress in generative tasks, such as image and video synthesis. However, they still often produce hallucinated samples (hallucinations) that conflict with real-world knowledge, such as generating an implausible duplicate cup floating beside another cup. Despite their prevalence, the lack of feasible methodologies for systematically quantifying such hallucinations hinders progress in addressing this challenge and obscures potential pathways for designing next-generation generative models under factual constraints. In this work, we bridge this gap by focusing on a specific form of hallucination, which we term counting hallucination, referring to the generation of an incorrect number of instances or structured objects, such as a hand image with six fingers, despite such patterns being absent from the training data. To this end, we construct a dataset suite CountHalluSet, with well-defined counting criteria, comprising ToyShape, SimObject, and RealHand. Using these datasets, we develop a standardized evaluation protocol for quantifying counting hallucinations, and systematically examine how different sampling conditions in DPMs, including solver type, ODE solver order, sampling steps, and initial noise, affect counting hallucination levels. Furthermore, we analyze their correlation with common evaluation metrics such as FID, revealing that this widely used image quality metric fails to capture counting hallucinations consistently. This work aims to take the first step toward systematically quantifying hallucinations in diffusion models and offer new insights into the investigation of hallucination phenomena in image generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation</title>
<link>https://arxiv.org/abs/2510.14945</link>
<guid>https://arxiv.org/abs/2510.14945</guid>
<content:encoded><![CDATA[
arXiv:2510.14945v2 Announce Type: replace 
Abstract: We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.20268</link>
<guid>https://arxiv.org/abs/2510.20268</guid>
<content:encoded><![CDATA[
arXiv:2510.20268v2 Announce Type: replace 
Abstract: Video anomaly detection (VAD) is a challenging task that detects anomalous frames in continuous surveillance videos. Most previous work utilizes the spatio-temporal correlation of visual features to distinguish whether there are abnormalities in video snippets. Recently, some works attempt to introduce multi-modal information, like text feature, to enhance the results of video anomaly detection. However, these works merely incorporate text features into video snippets in a coarse manner, overlooking the significant amount of redundant information that may exist within the video snippets. Therefore, we propose to leverage the diversity among multi-modal information to further refine the extracted features, reducing the redundancy in visual features, and we propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD). Specifically, we generate more grained multi-modal feature based on the video snippet, which summarizes the main content, and text features based on the captions of original video will be introduced to further enhance the visual features of highlighted portions. Experiments show that the proposed GMFVAD achieves state-of-the-art performance on four mainly datasets. Ablation experiments also validate that the improvement of GMFVAD is due to the reduction of redundant information.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Physically Executable 3D Gaussian for Embodied Navigation</title>
<link>https://arxiv.org/abs/2510.21307</link>
<guid>https://arxiv.org/abs/2510.21307</guid>
<content:encoded><![CDATA[
arXiv:2510.21307v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS), a 3D representation method with photorealistic real-time rendering capabilities, is regarded as an effective tool for narrowing the sim-to-real gap. However, it lacks fine-grained semantics and physical executability for Visual-Language Navigation (VLN). To address this, we propose SAGE-3D (Semantically and Physically Aligned Gaussian Environments for 3D Navigation), a new paradigm that upgrades 3DGS into an executable, semantically and physically aligned environment. It comprises two components: (1) Object-Centric Semantic Grounding, which adds object-level fine-grained annotations to 3DGS; and (2) Physics-Aware Execution Jointing, which embeds collision objects into 3DGS and constructs rich physical interfaces. We release InteriorGS, containing 1K object-annotated 3DGS indoor scene data, and introduce SAGE-Bench, the first 3DGS-based VLN benchmark with 2M VLN data. Experiments show that 3DGS scene data is more difficult to converge, while exhibiting strong generalizability, improving baseline performance by 31% on the VLN-CE Unseen task. Our data and code are available at: https://sage-3d.github.io.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance-Level Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2510.25387</link>
<guid>https://arxiv.org/abs/2510.25387</guid>
<content:encoded><![CDATA[
arXiv:2510.25387v2 Announce Type: replace 
Abstract: The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.
  To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RegionRAG: Region-level Retrieval-Augmented Generation for Visual Document Understanding</title>
<link>https://arxiv.org/abs/2510.27261</link>
<guid>https://arxiv.org/abs/2510.27261</guid>
<content:encoded><![CDATA[
arXiv:2510.27261v2 Announce Type: replace 
Abstract: Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method for empowering LLMs by leveraging candidate visual documents. However, current methods consider the entire document as the basic retrieval unit, introducing substantial irrelevant visual content in two ways: 1) Relevant documents often contain large regions unrelated to the query, diluting the focus on salient information; 2) Retrieving multiple documents to increase recall further introduces redundant and irrelevant documents. These redundant contexts distract the model's attention and further degrade the performance. To address this challenge, we propose RegionRAG, a novel framework that shifts the retrieval paradigm from the document level to the region level. During training, we design a hybrid supervision strategy from both labeled data and unlabeled data to pinpoint relevant patches. During inference, we propose a dynamic pipeline that intelligently groups salient patches into complete semantic regions. By delegating the task of identifying relevant regions to the retriever, RegionRAG enables the generator to focus solely on concise, query-relevant visual content, improving both efficiency and accuracy. Experiments on six benchmarks demonstrate that RegionRAG achieves state-of-the-art performance. It improves retrieval accuracy by 10.02% in R@1 on average, and boosts question answering accuracy by 3.56% while using only 71.42% visual tokens compared with prior methods.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ID-Crafter: VLM-Grounded Online RL for Compositional Multi-Subject Video Generation</title>
<link>https://arxiv.org/abs/2511.00511</link>
<guid>https://arxiv.org/abs/2511.00511</guid>
<content:encoded><![CDATA[
arXiv:2511.00511v4 Announce Type: replace 
Abstract: Significant progress has been achieved in high-fidelity video synthesis, yet current paradigms often fall short in effectively integrating identity information from multiple subjects. This leads to semantic conflicts and suboptimal performance in preserving identities and interactions, limiting controllability and applicability. To tackle this issue, we introduce ID-Crafter, a framework for multi-subject video generation that achieves superior identity preservation and semantic coherence. ID-Crafter integrates three key components: (i) a hierarchical identity-preserving attention mechanism that progressively aggregates features at intra-subject, inter-subject, and cross-modal levels; (ii) a semantic understanding module powered by a pretrained Vision-Language Model (VLM) to provide fine-grained guidance and capture complex inter-subject relationships; and (iii) an online reinforcement learning phase to further refine the model for critical concepts. Furthermore, we construct a new dataset to facilitate robust training and evaluation. Extensive experiments demonstrate that ID-Crafter establishes new state-of-the-art performance on multi-subject video generation benchmarks, excelling in identity preservation, temporal consistency, and overall video quality. Project page: https://angericky.github.io/ID-Crafter
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response</title>
<link>https://arxiv.org/abs/2511.03132</link>
<guid>https://arxiv.org/abs/2511.03132</guid>
<content:encoded><![CDATA[
arXiv:2511.03132v3 Announce Type: replace 
Abstract: This paper presents the first AI/ML system for automating building damage assessment in uncrewed aerial systems (sUAS) imagery to be deployed operationally during federally declared disasters (Hurricanes Debby and Helene). In response to major disasters, sUAS teams are dispatched to collect imagery of the affected areas to assess damage; however, at recent disasters, teams collectively delivered between 47GB and 369GB of imagery per day, representing more imagery than can reasonably be transmitted or interpreted by subject matter experts in the disaster scene, thus delaying response efforts. To alleviate this data avalanche encountered in practice, computer vision and machine learning techniques are necessary. While prior work has been deployed to automatically assess damage in satellite imagery, there is no current state of practice for sUAS-based damage assessment systems, as all known work has been confined to academic settings. This work establishes the state of practice via the development and deployment of models for building damage assessment with sUAS imagery. The model development involved training on the largest known dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage labels, and the operational training of 91 disaster practitioners. The best performing model was deployed during the responses to Hurricanes Debby and Helene, where it assessed a combined 415 buildings in approximately 18 minutes. This work contributes documentation of the actual use of AI/ML for damage assessment during a disaster and lessons learned to the benefit of the AI/ML research and user communities.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffRegCD: Integrated Registration and Change Detection with Diffusion Features</title>
<link>https://arxiv.org/abs/2511.07935</link>
<guid>https://arxiv.org/abs/2511.07935</guid>
<content:encoded><![CDATA[
arXiv:2511.07935v3 Announce Type: replace 
Abstract: Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images</title>
<link>https://arxiv.org/abs/2511.08987</link>
<guid>https://arxiv.org/abs/2511.08987</guid>
<content:encoded><![CDATA[
arXiv:2511.08987v3 Announce Type: replace 
Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $\mu m$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS</title>
<link>https://arxiv.org/abs/2511.09397</link>
<guid>https://arxiv.org/abs/2511.09397</guid>
<content:encoded><![CDATA[
arXiv:2511.09397v2 Announce Type: replace 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PANDA -- Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning</title>
<link>https://arxiv.org/abs/2511.09791</link>
<guid>https://arxiv.org/abs/2511.09791</guid>
<content:encoded><![CDATA[
arXiv:2511.09791v3 Announce Type: replace 
Abstract: Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation</title>
<link>https://arxiv.org/abs/2511.12919</link>
<guid>https://arxiv.org/abs/2511.12919</guid>
<content:encoded><![CDATA[
arXiv:2511.12919v3 Announce Type: replace 
Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CapeNext: Rethinking and Refining Dynamic Support Information for Category-Agnostic Pose Estimation</title>
<link>https://arxiv.org/abs/2511.13102</link>
<guid>https://arxiv.org/abs/2511.13102</guid>
<content:encoded><![CDATA[
arXiv:2511.13102v2 Announce Type: replace 
Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features</title>
<link>https://arxiv.org/abs/2511.13115</link>
<guid>https://arxiv.org/abs/2511.13115</guid>
<content:encoded><![CDATA[
arXiv:2511.13115v2 Announce Type: replace 
Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion</title>
<link>https://arxiv.org/abs/2511.16020</link>
<guid>https://arxiv.org/abs/2511.16020</guid>
<content:encoded><![CDATA[
arXiv:2511.16020v2 Announce Type: replace 
Abstract: Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle</title>
<link>https://arxiv.org/abs/2511.17171</link>
<guid>https://arxiv.org/abs/2511.17171</guid>
<content:encoded><![CDATA[
arXiv:2511.17171v2 Announce Type: replace 
Abstract: Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATAC: Augmentation-Based Test-Time Adversarial Correction for CLIP</title>
<link>https://arxiv.org/abs/2511.17362</link>
<guid>https://arxiv.org/abs/2511.17362</guid>
<content:encoded><![CDATA[
arXiv:2511.17362v2 Announce Type: replace 
Abstract: Despite its remarkable success in zero-shot image-text matching, CLIP remains highly vulnerable to adversarial perturbations on images. As adversarial fine-tuning is prohibitively costly, recent works explore various test-time defense strategies; however, these approaches still exhibit limited robustness. In this work, we revisit this problem and propose a simple yet effective strategy: Augmentation-based Test-time Adversarial Correction (ATAC). Our method operates directly in the embedding space of CLIP, calculating augmentation-induced drift vectors to infer a semantic recovery direction and correcting the embedding based on the angular consistency of these latent drifts. Across a wide range of benchmarks, ATAC consistently achieves remarkably high robustness, surpassing that of previous state-of-the-art methods by nearly 50\% on average, all while requiring minimal computational overhead. Furthermore, ATAC retains state-of-the-art robustness in unconventional and extreme settings and even achieves nontrivial robustness against adaptive attacks. Our results demonstrate that ATAC is an efficient method in a novel paradigm for test-time adversarial defenses in the embedding space of CLIP.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Long-tailed Dataset Distillation: A Uni-Level Framework with Unbiased Recovery and Relabeling</title>
<link>https://arxiv.org/abs/2511.18858</link>
<guid>https://arxiv.org/abs/2511.18858</guid>
<content:encoded><![CDATA[
arXiv:2511.18858v2 Announce Type: replace 
Abstract: Dataset distillation creates a small distilled set that enables efficient training by capturing key information from the full dataset. While existing dataset distillation methods perform well on balanced datasets, they struggle under long-tailed distributions, where imbalanced class frequencies induce biased model representations and corrupt statistical estimates such as Batch Normalization (BN) statistics. In this paper, we rethink long-tailed dataset distillation by revisiting the limitations of trajectory-based methods, and instead adopt the statistical alignment perspective to jointly mitigate model bias and restore fair supervision. To this end, we introduce three dedicated components that enable unbiased recovery of distilled images and soft relabeling: (1) enhancing expert models (an observer model for recovery and a teacher model for relabeling) to enable reliable statistics estimation and soft-label generation; (2) recalibrating BN statistics via a full forward pass with dynamically adjusted momentum to reduce representation skew; (3) initializing synthetic images by incrementally selecting high-confidence and diverse augmentations via a multi-round mechanism that promotes coverage and diversity. Extensive experiments on four long-tailed benchmarks show consistent improvements over state-of-the-art methods across varying degrees of class imbalance. Notably, our approach improves top-1 accuracy by 15.6% on CIFAR-100-LT and 11.8% on Tiny-ImageNet-LT under IPC=10 and IF=10. Codes are available at https://github.com/2018cx/RLDD.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Self-Conditioned Representation Guided Diffusion Model for Realistic Text-to-LiDAR Scene Generation</title>
<link>https://arxiv.org/abs/2511.19004</link>
<guid>https://arxiv.org/abs/2511.19004</guid>
<content:encoded><![CDATA[
arXiv:2511.19004v2 Announce Type: replace 
Abstract: Text-to-LiDAR generation can customize 3D data with rich structures and diverse scenes for downstream tasks. However, the scarcity of Text-LiDAR pairs often causes insufficient training priors, generating overly smooth 3D scenes. Moreover, low-quality text descriptions may degrade generation quality and controllability. In this paper, we propose a Text-to-LiDAR Diffusion Model for scene generation, named T2LDM, with a Self-Conditioned Representation Guidance (SCRG). Specifically, SCRG, by aligning to the real representations, provides the soft supervision with reconstruction details for the Denoising Network (DN) in training, while decoupled in inference. In this way, T2LDM can perceive rich geometric structures from data distribution, generating detailed objects in scenes. Meanwhile, we construct a content-composable Text-LiDAR benchmark, T2nuScenes, along with a controllability metric. Based on this, we analyze the effects of different text prompts for LiDAR generation quality and controllability, providing practical prompt paradigms and insights. Furthermore, a directional position prior is designed to mitigate street distortion, further improving scene fidelity. Additionally, by learning a conditional encoder via frozen DN, T2LDM can support multiple conditional tasks, including Sparse-to-Dense, Dense-to-Sparse, and Semantic-to-LiDAR generation. Extensive experiments in unconditional and conditional generation demonstrate that T2LDM outperforms existing methods, achieving state-of-the-art scene generation.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODEST: Multi-Optics Depth-of-Field Stereo Dataset</title>
<link>https://arxiv.org/abs/2511.20853</link>
<guid>https://arxiv.org/abs/2511.20853</guid>
<content:encoded><![CDATA[
arXiv:2511.20853v2 Announce Type: replace 
Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAPVid-360: Tracking Any Point in 360 from Narrow Field of View Video</title>
<link>https://arxiv.org/abs/2511.21946</link>
<guid>https://arxiv.org/abs/2511.21946</guid>
<content:encoded><![CDATA[
arXiv:2511.21946v2 Announce Type: replace 
Abstract: Humans excel at constructing panoramic mental models of their surroundings, maintaining object permanence and inferring scene structure beyond visible regions. In contrast, current artificial vision systems struggle with persistent, panoramic understanding, often processing scenes egocentrically on a frame-by-frame basis. This limitation is pronounced in the Track Any Point (TAP) task, where existing methods fail to track 2D points outside the field of view. To address this, we introduce TAPVid-360, a novel task that requires predicting the 3D direction to queried scene points across a video sequence, even when far outside the narrow field of view of the observed video. This task fosters learning allocentric scene representations without needing dynamic 4D ground truth scene models for training. Instead, we exploit 360 videos as a source of supervision, resampling them into narrow field-of-view perspectives while computing ground truth directions by tracking points across the full panorama using a 2D pipeline. We introduce a new dataset and benchmark, TAPVid360-10k comprising 10k perspective videos with ground truth directional point tracking. Our baseline adapts CoTracker v3 to predict per-point rotations for direction updates, outperforming existing TAP and TAPVid 3D methods. Project page: https://finlay-hudson.github.io/tapvid360
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking</title>
<link>https://arxiv.org/abs/2512.02648</link>
<guid>https://arxiv.org/abs/2512.02648</guid>
<content:encoded><![CDATA[
arXiv:2512.02648v2 Announce Type: replace 
Abstract: We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.02660</link>
<guid>https://arxiv.org/abs/2512.02660</guid>
<content:encoded><![CDATA[
arXiv:2512.02660v2 Announce Type: replace 
Abstract: Late-interaction multimodal retrieval models like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on area efficiency. We evaluate on BBox-DocVQA with ground-truth bounding boxes. For within-page localization (given correct page retrieval), ColQwen3-4B with percentile-50 thresholding achieves 59.7% hit rate at IoU@0.5 (84.4% at IoU@0.25, 35.8% at IoU@0.7), with mean IoU of 0.569, compared to ~6.7% for random region selection. Our approach reduces context tokens by 28.8% compared to returning all OCR regions and by 52.3% compared to full-page image tokens. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation at https://github.com/athrael-soju/Snappy.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</title>
<link>https://arxiv.org/abs/2512.02700</link>
<guid>https://arxiv.org/abs/2512.02700</guid>
<content:encoded><![CDATA[
arXiv:2512.02700v2 Announce Type: replace 
Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup. The code is available at https://github.com/Casey-bit/VLMPruner.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval</title>
<link>https://arxiv.org/abs/2512.02792</link>
<guid>https://arxiv.org/abs/2512.02792</guid>
<content:encoded><![CDATA[
arXiv:2512.02792v2 Announce Type: replace 
Abstract: Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Difference Decomposition Networks for Infrared Small Target Detection</title>
<link>https://arxiv.org/abs/2512.03470</link>
<guid>https://arxiv.org/abs/2512.03470</guid>
<content:encoded><![CDATA[
arXiv:2512.03470v2 Announce Type: replace 
Abstract: Infrared small target detection (ISTD) faces two major challenges: a lack of discernible target texture and severe background clutter, which results in the background obscuring the target. To enhance targets and suppress backgrounds, we propose the Basis Decomposition Module (BDM) as an extensible and lightweight module based on basis decomposition, which decomposes a complex feature into several basis features and enhances certain information while eliminating redundancy. Extending BDM leads to a series of modules, including the Spatial Difference Decomposition Module (SD$^\mathrm{2}$M), Spatial Difference Decomposition Downsampling Module (SD$^\mathrm{3}$M), and Temporal Difference Decomposition Module (TD$^\mathrm{2}$M). Based on these modules, we develop the Spatial Difference Decomposition Network (SD$^\mathrm{2}$Net) for single-frame ISTD (SISTD) and the Spatiotemporal Difference Decomposition Network (STD$^\mathrm{2}$Net) for multi-frame ISTD (MISTD). SD$^\mathrm{2}$Net integrates SD$^\mathrm{2}$M and SD$^\mathrm{3}$M within an adapted U-shaped architecture. We employ TD$^\mathrm{2}$M to introduce motion information, which transforms SD$^\mathrm{2}$Net into STD$^\mathrm{2}$Net. Extensive experiments on SISTD and MISTD datasets demonstrate state-of-the-art (SOTA) performance. On the SISTD task, SD$^\mathrm{2}$Net performs well compared to most established networks. On the MISTD datasets, STD$^\mathrm{2}$Net achieves a mIoU of 87.68\%, outperforming SD$^\mathrm{2}$Net, which achieves a mIoU of 64.97\%. Our codes are available: https://github.com/greekinRoma/IRSTD_HC_Platform.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlurDM: A Blur Diffusion Model for Image Deblurring</title>
<link>https://arxiv.org/abs/2512.03979</link>
<guid>https://arxiv.org/abs/2512.03979</guid>
<content:encoded><![CDATA[
arXiv:2512.03979v2 Announce Type: replace 
Abstract: Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The project page is available at https://jin-ting-he.github.io/Blur-Diffusion-Model/.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Group Actions In Disentangled Latent Image Representations</title>
<link>https://arxiv.org/abs/2512.04015</link>
<guid>https://arxiv.org/abs/2512.04015</guid>
<content:encoded><![CDATA[
arXiv:2512.04015v2 Announce Type: replace 
Abstract: Modeling group actions on latent representations enables controllable transformations of high-dimensional image data. Prior works applying group-theoretic priors or modeling transformations typically operate in the high-dimensional data space, where group actions apply uniformly across the entire input, making it difficult to disentangle the subspace that varies under transformations. While latent-space methods offer greater flexibility, they still require manual partitioning of latent variables into equivariant and invariant subspaces, limiting the ability to robustly learn and operate group actions within the representation space. To address this, we introduce a novel end-to-end framework that for the first time learns group actions on latent image manifolds, automatically discovering transformation-relevant structures without manual intervention. Our method uses learnable binary masks with straight-through estimation to dynamically partition latent representations into transformation-sensitive and invariant components. We formulate this within a unified optimization framework that jointly learns latent disentanglement and group transformation mappings. The framework can be seamlessly integrated with any standard encoder-decoder architecture. We validate our approach on five 2D/3D image datasets, demonstrating its ability to automatically learn disentangled latent factors for group actions in diverse data, while downstream classification tasks confirm the effectiveness of the learned representations. Our code is publicly available at https://github.com/farhanaswarnali/Learning-Group-Actions-In-Disentangled-Latent-Image-Representations .
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture</title>
<link>https://arxiv.org/abs/2512.04810</link>
<guid>https://arxiv.org/abs/2512.04810</guid>
<content:encoded><![CDATA[
arXiv:2512.04810v5 Announce Type: replace 
Abstract: We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-X: Generative 4D Video Rendering with Camera and Illumination Control</title>
<link>https://arxiv.org/abs/2512.05115</link>
<guid>https://arxiv.org/abs/2512.05115</guid>
<content:encoded><![CDATA[
arXiv:2512.05115v2 Announce Type: replace 
Abstract: Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEAR: Dataset for Evaluating the Aesthetics of Rendering</title>
<link>https://arxiv.org/abs/2512.05209</link>
<guid>https://arxiv.org/abs/2512.05209</guid>
<content:encoded><![CDATA[
arXiv:2512.05209v2 Announce Type: replace 
Abstract: Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tau Anomaly Detection in PET Imaging via Bilateral-Guided Deterministic Diffusion Model</title>
<link>https://arxiv.org/abs/2405.13199</link>
<guid>https://arxiv.org/abs/2405.13199</guid>
<content:encoded><![CDATA[
arXiv:2405.13199v2 Announce Type: replace-cross 
Abstract: The emergence of tau PET imaging over the last decade has enabled Alzheimer's disease (AD) researchers to examine tau pathology in vivo and more effectively characterize the disease trajectories of AD. Current tau PET analysis methods, however, typically perform inferences on large cortical ROIs and are limited in the detection of localized tau pathology that varies across subjects. In this work, we propose a novel bilateral-guided deterministic diffusion sampling method to perform anomaly detection from tau PET imaging data. By including individualized brain structure and cognitively normal (CN) template conditions, our model computes a voxel-level anomaly map based on the deterministically sampled pseudo-healthy reconstruction. We train our model on ADNI CN subjects (n=380) and evaluate anomaly localization performance on the left MCI/AD subjects (n=154) and the preclinical subjects of the A4 clinical trial (n=447). We further train a CNN classifier on the derived 3D anomaly maps from ADNI, including CN and MCI/AD, to classify subjects into two groups and test classification performance on A4. We demonstrate that our method outperforms baselines in anomaly localization. Additionally, we show that our method can successfully group preclinical subjects with significantly different cognitive functions, highlighting the potential of our approach for application in preclinical screening tests. The code will be publicly available.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIC: Circular Image Compression</title>
<link>https://arxiv.org/abs/2407.15870</link>
<guid>https://arxiv.org/abs/2407.15870</guid>
<content:encoded><![CDATA[
arXiv:2407.15870v3 Announce Type: replace-cross 
Abstract: Learned image compression (LIC) is currently the cutting-edge method. However, the inherent difference between testing and training images of LIC results in performance degradation to some extent. Especially for out-of-sample, out-of-distribution, or out-of-domain testing images, the performance of LIC dramatically degraded. Classical LIC is a serial image compression (SIC) approach that utilizes an open-loop architecture with serial encoding and decoding units. Nevertheless, according to the theory of automatic control, a closed-loop architecture holds the potential to improve the dynamic and static performance of LIC. Therefore, a circular image compression (CIC) approach with closed-loop encoding and decoding elements is proposed to minimize the gap between testing and training images and upgrade the capability of LIC. The proposed CIC establishes a nonlinear loop equation and proves that steady-state error between reconstructed and original images is close to zero by Taylor series expansion. The proposed CIC method possesses the property of Post-Training and plug-and-play which can be built on any existing advanced SIC methods. Experimental results on five public image compression datasets demonstrate that the proposed CIC outperforms five competing state-of-the-art open-source SIC algorithms in reconstruction capacity. Experimental results further show that the proposed method is suitable for out-of-sample testing images with dark backgrounds, sharp edges, high contrast, grid shapes, or complex patterns.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Simultaneous Multislice MRI Reconstruction Using Slice-Wise Learned Generative Diffusion Priors</title>
<link>https://arxiv.org/abs/2407.21600</link>
<guid>https://arxiv.org/abs/2407.21600</guid>
<content:encoded><![CDATA[
arXiv:2407.21600v3 Announce Type: replace-cross 
Abstract: Simultaneous multislice (SMS) imaging is a powerful technique for accelerating magnetic resonance imaging (MRI) acquisitions. However, SMS reconstruction remains challenging due to complex signal interactions between and within the excited slices. In this study, we introduce ROGER, a robust SMS MRI reconstruction method based on deep generative priors. Utilizing denoising diffusion probabilistic models (DDPM), ROGER begins with Gaussian noise and gradually recovers individual slices through reverse diffusion iterations while enforcing data consistency from measured k-space data within the readout concatenation framework. The posterior sampling procedure is designed such that the DDPM training can be performed on single-slice images without requiring modifications for SMS tasks. Additionally, our method incorporates a low-frequency enhancement (LFE) module to address the practical issue that SMS-accelerated fast spin echo (FSE) and echo planar imaging (EPI) sequences cannot easily embed fully-sampled autocalibration signals. Extensive experiments on both retrospectively and prospectively accelerated datasets demonstrate that ROGER consistently outperforms existing methods, enhancing both anatomical and functional imaging with strong out-of-distribution generalization. The source code and sample data for ROGER are available at https://github.com/Solor-pikachu/ROGER.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAISI: Medical AI for Synthetic Imaging</title>
<link>https://arxiv.org/abs/2409.11169</link>
<guid>https://arxiv.org/abs/2409.11169</guid>
<content:encoded><![CDATA[
arXiv:2409.11169v3 Announce Type: replace-cross 
Abstract: Medical imaging analysis faces challenges such as data scarcity, high annotation costs, and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI), an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 x 512 x 768 ) with flexible volume dimensions and voxel spacing. By incorporating ControlNet, MAISI can process organ segmentation, including 127 anatomical structures, as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI's capabilities in generating realistic, anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WALINET: A water and lipid identification convolutional Neural Network for nuisance signal removal in 1H MR Spectroscopic Imaging</title>
<link>https://arxiv.org/abs/2410.00746</link>
<guid>https://arxiv.org/abs/2410.00746</guid>
<content:encoded><![CDATA[
arXiv:2410.00746v2 Announce Type: replace-cross 
Abstract: Purpose. Proton Magnetic Resonance Spectroscopic Imaging (1H-MRSI) provides non-invasive spectral-spatial mapping of metabolism. However, long-standing problems in whole-brain 1H-MRSI are spectral overlap of metabolite peaks with large lipid signal from scalp, and overwhelming water signal that distorts spectra. Fast and effective methods are needed for high-resolution 1H-MRSI to accurately remove lipid and water signals while preserving the metabolite signal. The potential of supervised neural networks for this task remains unexplored, despite their success for other MRSI processing.
  Methods. We introduce a deep-learning method based on a modified Y-NET network for water and lipid removal in whole-brain 1H-MRSI. The WALINET (WAter and LIpid neural NETwork) was compared to conventional methods such as the state-of-the-art lipid L2 regularization and Hankel-Lanczos singular value decomposition (HLSVD) water suppression. Methods were evaluated on simulated and in-vivo whole-brain MRSI using NMRSE, SNR, CRLB, and FWHM metrics.
  Results. WALINET is significantly faster and needs 8s for high-resolution whole-brain MRSI, compared to 42 minutes for conventional HLSVD+L2. Quantitative analysis shows WALINET has better performance than HLSVD+L2: 1) more lipid removal with 41% lower NRMSE, 2) better metabolite signal preservation with 71% lower NRMSE in simulated data, 155% higher SNR and 50% lower CRLB in in-vivo data. Metabolic maps obtained by WALINET in healthy subjects and patients show better gray/white-matter contrast with more visible structural details.
  Conclusions. WALINET has superior performance for nuisance signal removal and metabolite quantification on whole-brain 1H-MRSI compared to conventional state-of-the-art techniques. This represents a new application of deep-learning for MRSI processing, with potential for automated high-throughput workflow.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Quantum Machine Learning for Multispectral Images Segmentation: Case Study</title>
<link>https://arxiv.org/abs/2503.08962</link>
<guid>https://arxiv.org/abs/2503.08962</guid>
<content:encoded><![CDATA[
arXiv:2503.08962v3 Announce Type: replace-cross 
Abstract: The emergence of Big Data changed how we approach information systems engineering. Nowadays, when we can use remote sensing techniques for Big Data acquisition, the issues such data introduce are as important as ever. One of those concerns is the processing of the data. Classical methods often fail to address that problem or are incapable of processing the data in a reasonable time. With that in mind information system engineers are required to investigate different approaches to the data processing. The recent advancements in noisy intermediate-scale quantum (NISQ) devices implementation allow us to investigate their application to real-life computational problem. This field of study is called quantum (information) systems engineering and usually focuses on technical problems with the contemporary devices. However, hardware challenges are not the only ones that hinder our quantum computation capabilities. Software limitations are the other, less explored side of this medal. Using multispectral image segmentation as a task example, we investigated how difficult it is to run a hybrid quantum-classical model on a real, publicly available quantum device. To quantify how and explain why the performance of our model changed when ran on a real device, we propose new explainability metrics. These metrics introduce new meaning to the explainable quantum machine learning; the explanation of the performance issue comes from the quantum device behavior. We also analyzed the expected money costs of running similar experiment on contemporary quantum devices using standard market prices.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reference-Free 3D Reconstruction of Brain Dissection Slabs via Learned Atlas Coordinates</title>
<link>https://arxiv.org/abs/2503.09963</link>
<guid>https://arxiv.org/abs/2503.09963</guid>
<content:encoded><![CDATA[
arXiv:2503.09963v2 Announce Type: replace-cross 
Abstract: Correlation of neuropathology with MRI has the potential to transfer microscopic signatures of pathology to in vivo scans. There is increasing interest in building these correlations from 3D reconstructed stacks of slab photographs, which are routinely taken during dissection at brain banks. These photographs bypass the need for ex vivo MRI, which is not widely accessible. However, existing methods either require a corresponding 3D reference (e.g., an ex vivo MRI scans, or a brain surface acquired with a structured light scanner) or a full stack of brain slabs, which severely limits applicability. Here we propose RefFree, a 3D reconstruction method for dissection photographs that does not require an external reference. RefFree coherently reconstructs a 3D volume for an arbitrary set of slabs (including a single slab) using predicted 3D coordinates in the standard atlas space (MNI) as guidance. To support RefFree's pipeline, we train an atlas coordinate prediction network that estimates the coordinate map from a 2D photograph, using synthetic photographs generated from digitally sliced 3D MRI data with randomized appearance for enhanced generalization. As a by-product, RefFree can propagate information (e.g., anatomical labels) from atlas space to one single photograph even without reconstruction. Experiments on simulated and real data show that, when all slabs are available, RefFree achieves performance comparable to existing classical methods but at substantially higher speed. Moreover, RefFree yields accurate reconstruction and registration for partial stacks or even a single slab. Our code is available at https://github.com/lintian-a/reffree.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LookAhead Tuning: Safer Language Models via Partial Answer Previews</title>
<link>https://arxiv.org/abs/2503.19041</link>
<guid>https://arxiv.org/abs/2503.19041</guid>
<content:encoded><![CDATA[
arXiv:2503.19041v3 Announce Type: replace-cross 
Abstract: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score-Based Turbo Message Passing for Plug-and-Play Compressive Image Recovery</title>
<link>https://arxiv.org/abs/2503.22140</link>
<guid>https://arxiv.org/abs/2503.22140</guid>
<content:encoded><![CDATA[
arXiv:2503.22140v2 Announce Type: replace-cross 
Abstract: Message passing algorithms have been tailored for compressive imaging applications by plugging in different types of off-the-shelf image denoisers. These off-the-shelf denoisers mostly rely on some generic or hand-crafted priors for denoising. Due to their insufficient accuracy in capturing the true image prior, these methods often fail to produce satisfactory results, especially in highly underdetermined scenarios. On the other hand, score-based generative modeling offers a promising way to accurately characterize the sophisticated image distribution. In this paper, by exploiting the close relation between score-based modeling and empirical Bayes-optimal denoising, we devise a message passing framework that integrates a score-based minimum mean squared error (MMSE) denoiser for compressive image recovery. Experiments on the FFHQ dataset demonstrate that our method strikes a significantly better performance-complexity tradeoff than conventional message passing, regularized linear regression, and score-based posterior sampling baselines. Remarkably, our method typically converges in fewer than 20 neural function evaluations (NFEs).
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2505.04050</link>
<guid>https://arxiv.org/abs/2505.04050</guid>
<content:encoded><![CDATA[
arXiv:2505.04050v2 Announce Type: replace-cross 
Abstract: 3D terrain models are essential in fields such as video game development and film production. Since surface color often correlates with terrain geometry, capturing this relationship is crucial to achieving realism. However, most existing methods generate either a heightmap or a texture, without sufficiently accounting for the inherent correlation. In this paper, we propose a method that jointly generates terrain heightmaps and textures using a latent diffusion model. First, we train the model in an unsupervised manner to randomly generate paired heightmaps and textures. Then, we perform supervised learning of an external adapter to enable user control via hand-drawn sketches. Experiments show that our approach allows intuitive terrain generation while preserving the correlation between heightmaps and textures.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions</title>
<link>https://arxiv.org/abs/2505.08919</link>
<guid>https://arxiv.org/abs/2505.08919</guid>
<content:encoded><![CDATA[
arXiv:2505.08919v2 Announce Type: replace-cross 
Abstract: High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical planning for the treatment of lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to comprehensively assess the quality of the reconstruction. Furthermore, to address the lack of publicly available shape datasets for benchmarking reconstruction algorithms, we developed a shape dataset named Lung3D, which includes the 3D models of 800 labeled pulmonary segments and their corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/HINTLab/ImPulSe.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes</title>
<link>https://arxiv.org/abs/2506.01950</link>
<guid>https://arxiv.org/abs/2506.01950</guid>
<content:encoded><![CDATA[
arXiv:2506.01950v4 Announce Type: replace-cross 
Abstract: We introduce DualMap, an online open-vocabulary mapping system that enables robots to understand and navigate dynamically changing environments through natural language queries. Designed for efficient semantic mapping and adaptability to changing environments, DualMap meets the essential requirements for real-world robot navigation applications. Our proposed hybrid segmentation frontend and object-level status check eliminate the costly 3D object merging required by prior methods, enabling efficient online scene mapping. The dual-map representation combines a global abstract map for high-level candidate selection with a local concrete map for precise goal-reaching, effectively managing and updating dynamic changes in the environment. Through extensive experiments in both simulation and real-world scenarios, we demonstrate state-of-the-art performance in 3D open-vocabulary segmentation, efficient scene mapping, and online language-guided navigation. Project page: https://eku127.github.io/DualMap/
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta Pruning via Graph Metanetworks : A Universal Meta Learning Framework for Network Pruning</title>
<link>https://arxiv.org/abs/2506.12041</link>
<guid>https://arxiv.org/abs/2506.12041</guid>
<content:encoded><![CDATA[
arXiv:2506.12041v3 Announce Type: replace-cross 
Abstract: We propose an entirely new meta-learning framework for network pruning. It is a general framework that can be theoretically applied to almost all types of networks with all kinds of pruning and has great generality and transferability. Experiments have shown that it can achieve outstanding results on many popular and representative pruning tasks (including both CNNs and Transformers). Unlike all prior works that either rely on fixed, hand-crafted criteria to prune in a coarse manner, or employ learning to prune ways that require special training during each pruning and lack generality. Our framework can learn complex pruning rules automatically via a neural network (metanetwork) and has great generality that can prune without any special training. More specifically, we introduce the newly developed idea of metanetwork from meta-learning into pruning. A metanetwork is a network that takes another network as input and produces a modified network as output. In this paper, we first establish a bijective mapping between neural networks and graphs, and then employ a graph neural network as our metanetwork. We train a metanetwork that learns the pruning strategy automatically and can transform a network that is hard to prune into another network that is much easier to prune. Once the metanetwork is trained, our pruning needs nothing more than a feedforward through the metanetwork and some standard finetuning to prune at state-of-the-art. Our code is available at https://github.com/Yewei-Liu/MetaPruning
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions</title>
<link>https://arxiv.org/abs/2506.23046</link>
<guid>https://arxiv.org/abs/2506.23046</guid>
<content:encoded><![CDATA[
arXiv:2506.23046v3 Announce Type: replace-cross 
Abstract: Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba</title>
<link>https://arxiv.org/abs/2508.11849</link>
<guid>https://arxiv.org/abs/2508.11849</guid>
<content:encoded><![CDATA[
arXiv:2508.11849v3 Announce Type: replace-cross 
Abstract: We introduce LocoMamba, a vision-driven cross-modal DRL framework built on selective state-space models, specifically leveraging Mamba, that achieves near-linear-time sequence modeling, effectively captures long-range dependencies, and enables efficient training with longer sequences. First, we embed proprioceptive states with a multilayer perceptron and patchify depth images with a lightweight convolutional neural network, producing compact tokens that improve state representation. Second, stacked Mamba layers fuse these tokens via near-linear-time selective scanning, reducing latency and memory footprint, remaining robust to token length and image resolution, and providing an inductive bias that mitigates overfitting. Third, we train the policy end-to-end with Proximal Policy Optimization under terrain and appearance randomization and an obstacle-density curriculum, using a compact state-centric reward that balances progress, smoothness, and safety. We evaluate our method in challenging simulated environments with static and moving obstacles as well as uneven terrain. Compared with state-of-the-art baselines, our method achieves higher returns and success rates with fewer collisions, exhibits stronger generalization to unseen terrains and obstacle densities, and improves training efficiency by converging in fewer updates under the same compute budget.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model</title>
<link>https://arxiv.org/abs/2508.14681</link>
<guid>https://arxiv.org/abs/2508.14681</guid>
<content:encoded><![CDATA[
arXiv:2508.14681v3 Announce Type: replace-cross 
Abstract: Multiplex imaging is revolutionizing pathology by enabling the simultaneous visualization of multiple biomarkers within tissue samples, providing molecular-level insights that traditional hematoxylin and eosin (H&amp;E) staining cannot provide. However, the complexity and cost of multiplex data acquisition have hindered its widespread adoption. Additionally, most existing large repositories of H&amp;E images lack corresponding multiplex images, limiting opportunities for multimodal analysis. To address these challenges, we leverage recent advances in latent diffusion models (LDMs), which excel at modeling complex data distributions by utilizing their powerful priors for fine-tuning to a target domain. In this paper, we introduce a novel framework for virtual multiplex staining that utilizes pretrained LDM parameters to generate multiplex images from H&amp;E images using a conditional diffusion model. Our approach enables marker-by-marker generation by conditioning the diffusion model on each marker, while sharing the same architecture across all markers. To tackle the challenge of varying pixel value distributions across different marker stains and to improve inference speed, we fine-tune the model for single-step sampling, enhancing both color contrast fidelity and inference efficiency through pixel-level loss functions. We validate our framework on two publicly available datasets, notably demonstrating its effectiveness in generating up to 18 different marker types with improved accuracy, a substantial increase over the 2-3 marker types achieved in previous approaches. This validation highlights the potential of our framework, pioneering virtual multiplex staining. Finally, this paper bridges the gap between H&amp;E and multiplex imaging, potentially enabling retrospective studies and large-scale analyses of existing H&amp;E image repositories.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patch-Based Diffusion for Data-Efficient, Radiologist-Preferred MRI Reconstruction</title>
<link>https://arxiv.org/abs/2509.21531</link>
<guid>https://arxiv.org/abs/2509.21531</guid>
<content:encoded><![CDATA[
arXiv:2509.21531v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) requires long acquisition times, raising costs, reducing accessibility, and making scans more susceptible to motion artifacts. Diffusion probabilistic models that learn data-driven priors can potentially assist in reducing acquisition time. However, they typically require large training datasets that can be prohibitively expensive to collect. Patch-based diffusion models have shown promise in learning effective data-driven priors over small real-valued datasets, but have not yet demonstrated clinical value in MRI. We extend the Patch-based Diffusion Inverse Solver (PaDIS) to complex-valued, multi-coil MRI reconstruction, and compare it against a state-of-the-art whole-image diffusion baseline (FastMRI-EDM) for 7x undersampled MRI reconstruction on the FastMRI brain dataset. We show that PaDIS-MRI models trained on small datasets of as few as 25 k-space images outperform FastMRI-EDM on image quality metrics (PSNR, SSIM, NRMSE), pixel-level uncertainty, cross-contrast generalization, and robustness to severe k-space undersampling. In a blinded study with three radiologists, PaDIS-MRI reconstructions were chosen as diagnostically superior in 91.7% of cases, compared to baselines (i) FastMRI-EDM and (ii) classical convex reconstruction with wavelet sparsity. These findings highlight the potential of patch-based diffusion priors for high-fidelity MRI reconstruction in data-scarce clinical settings where diagnostic confidence matters.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</title>
<link>https://arxiv.org/abs/2510.14974</link>
<guid>https://arxiv.org/abs/2510.14974</guid>
<content:encoded><![CDATA[
arXiv:2510.14974v2 Announce Type: replace-cross 
Abstract: Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming previous 1-NFE models of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art DMD models, while maintaining teacher-level quality.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPR-1: Interactive Physical Reasoner</title>
<link>https://arxiv.org/abs/2511.15407</link>
<guid>https://arxiv.org/abs/2511.15407</guid>
<content:encoded><![CDATA[
arXiv:2511.15407v2 Announce Type: replace-cross 
Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherent Multi-Agent Trajectory Forecasting in Team Sports with CausalTraj</title>
<link>https://arxiv.org/abs/2511.18248</link>
<guid>https://arxiv.org/abs/2511.18248</guid>
<content:encoded><![CDATA[
arXiv:2511.18248v2 Announce Type: replace-cross 
Abstract: Jointly forecasting trajectories of multiple interacting agents is a core challenge in sports analytics and other domains involving complex group dynamics. Accurate prediction enables realistic simulation and strategic understanding of gameplay evolution. Most existing models are evaluated solely on per-agent accuracy metrics (minADE, minFDE), which assess each agent independently on its best-of-k prediction. However these metrics overlook whether the model learns which predicted trajectories can jointly form a plausible multi-agent future. Many state-of-the-art models are designed and optimized primarily based on these metrics. As a result, they may underperform on joint predictions and also fail to generate coherent, interpretable multi-agent scenarios in team sports. We propose CausalTraj, a temporally causal, likelihood-based model that is built to generate jointly probable multi-agent trajectory forecasts. To better assess collective modeling capability, we emphasize joint metrics (minJADE, minJFDE) that measure joint accuracy across agents within the best generated scenario sample. Evaluated on the NBA SportVU, Basketball-U, and Football-U datasets, CausalTraj achieves competitive per-agent accuracy and the best recorded results on joint metrics, while yielding qualitatively coherent and realistic gameplay evolutions.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Histopathologic Assessment of Hirschsprung Disease Using a Multi-Stage Vision Transformer Framework</title>
<link>https://arxiv.org/abs/2511.20734</link>
<guid>https://arxiv.org/abs/2511.20734</guid>
<content:encoded><![CDATA[
arXiv:2511.20734v2 Announce Type: replace-cross 
Abstract: Hirschsprung Disease is characterized by the absence of ganglion cells in the myenteric plexus. Therefore, the correct identification of ganglion cells is crucial for diagnosing Hirschsprung disease. We introduce a three-stage analysis framework that mimics the pathologist's diagnostic approach. The framework, based on a Vision Transformer model (ViT-B/16), sequentially segments the muscularis propria, segments the myenteric plexus, and detects ganglion cells within anatomically valid regions. 30 whole-slide images of colon tissue were used, each containing manual annotations of muscularis, plexus, and ganglion cells. A 5-fold cross-validation scheme was applied to each stage, along with resolution-specific tiling strategies and tailored postprocessing to ensure anatomical consistency. The proposed method achieved a Dice coefficient of 89.9% and a Plexus Inclusion Rate of 100% for muscularis segmentation. Plexus segmentation reached a recall of 94.8%, a precision of 84.2% and a Ganglia Inclusion Rate of 99.7%. For ganglion cells annotated with high certainty, the model achieved 62.1\% precision and 89.1% recall. When considering all annotated ganglion cells, regardless of certainty level, the overall precision was 67.0%. These results indicate that ViT-based models are effective at leveraging global tissue context and capturing cellular morphology at small scales, even within complex histological tissue structures. This multi-stage methodology has great potential to support digital pathology workflows by reducing inter-observer variability and assisting in the evaluation of Hirschsprung disease. The clinical impact will be evaluated in future work with larger multi-center datasets and additional expert annotations.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI</title>
<link>https://arxiv.org/abs/2512.02020</link>
<guid>https://arxiv.org/abs/2512.02020</guid>
<content:encoded><![CDATA[
arXiv:2512.02020v2 Announce Type: replace-cross 
Abstract: Generative modeling has recently shown remarkable promise for visuomotor policy learning, enabling flexible and expressive control across diverse embodied AI tasks. However, existing generative policies often struggle with data inefficiency, requiring large-scale demonstrations, and sampling inefficiency, incurring slow action generation during inference. We introduce EfficientFlow, a unified framework for efficient embodied AI with flow-based policy learning. To enhance data efficiency, we bring equivariance into flow matching. We theoretically prove that when using an isotropic Gaussian prior and an equivariant velocity prediction network, the resulting action distribution remains equivariant, leading to improved generalization and substantially reduced data demands. To accelerate sampling, we propose a novel acceleration regularization strategy. As direct computation of acceleration is intractable for marginal flow trajectories, we derive a novel surrogate loss that enables stable and scalable training using only conditional trajectories. Across a wide range of robotic manipulation benchmarks, the proposed algorithm achieves competitive or superior performance under limited data while offering dramatically faster inference. These results highlight EfficientFlow as a powerful and efficient paradigm for high-performance embodied AI.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSG-Loc: Multi-Label Likelihood-based Semantic Graph Matching for Object-Level Global Localization</title>
<link>https://arxiv.org/abs/2512.03522</link>
<guid>https://arxiv.org/abs/2512.03522</guid>
<content:encoded><![CDATA[
arXiv:2512.03522v2 Announce Type: replace-cross 
Abstract: Robots are often required to localize in environments with unknown object classes and semantic ambiguity. However, when performing global localization using semantic objects, high semantic ambiguity intensifies object misclassification and increases the likelihood of incorrect associations, which in turn can cause significant errors in the estimated pose. Thus, in this letter, we propose a multi-label likelihood-based semantic graph matching framework for object-level global localization. The key idea is to exploit multi-label graph representations, rather than single-label alternatives, to capture and leverage the inherent semantic context of object observations. Based on these representations, our approach enhances semantic correspondence across graphs by combining the likelihood of each node with the maximum likelihood of its neighbors via context-aware likelihood propagation. For rigorous validation, data association and pose estimation performance are evaluated under both closed-set and open-set detection configurations. In addition, we demonstrate the scalability of our approach to large-vocabulary object categories in both real-world indoor scenes and synthetic environments.
]]></content:encoded>
<pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-SeR: Multimodal Self-Refinement for Lightweight Image Captioning</title>
<link>https://arxiv.org/abs/2508.21451</link>
<guid>https://arxiv.org/abs/2508.21451</guid>
<content:encoded><![CDATA[
<div> Streaming image captioning, multimodal language models, lightweight model, self-refinement, video QA<br /><br />Summary:<br /><br />1. The paper addresses the challenge of streaming image captioning, crucial for systems like video chatbots and navigation robots, which interpret continuous visual inputs. 2. Existing methods rely on large multimodal language models (MLLMs) that are computationally expensive and impractical for many applications. 3. The authors propose a lightweight captioning model by replacing the large language component of MLLMs with a compact 125M-parameter model, achieving similar performance despite a 93x reduction in size. 4. This outcome suggests that factual image captioning does not heavily depend on the complex reasoning capabilities of large language models. 5. To improve reliability, they introduce a multimodal self-refinement framework inspired by human vision, where the model first generates a coarse global caption and then refines it by focusing on salient regions referenced by the previous caption. 6. Experiments show that this approach outperforms existing models in both single-sentence and detailed image captioning tasks and extends effectively to long-range video question answering scenarios. <div>
arXiv:2508.21451v4 Announce Type: replace 
Abstract: Systems such as video chatbots and navigation robots often depend on streaming image captioning to interpret visual inputs. Existing approaches typically employ large multimodal language models (MLLMs) for this purpose, but their substantial computational cost hinders practical application. This limitation motivates our development of a lightweight captioning model. Our investigation begins by replacing the large-scale language component in MLLMs with a compact 125M-parameter model. Surprisingly, this compact model, despite a 93x reduction in size, achieves comparable performance to MLLMs, suggesting that factual image captioning does not significantly require the complex reasoning abilities of LLMs. Despite this promising result, our lightweight model still lacks reliability. To address this, we draw inspiration from the human visual process: perceiving a global and coarse understanding of the scene before attending to finer details. Accordingly, we propose a multimodal self-refinement framework that guides the model to utilize features from salient regions, identified by referencing the previous coarse caption, and to produce a refined description. Experimental results demonstrate the superiority of our model in both single-sentence and detailed captioning, extending even to long-range video QA tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Text Guidance for Enhancing Demographic Fairness in Gender Classification</title>
<link>https://arxiv.org/abs/2512.11015</link>
<guid>https://arxiv.org/abs/2512.11015</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, gender classification, image-text fusion, bias mitigation, multimodal representation<br /><br />Summary:<br /><br />This article addresses the challenge of fairness in AI with a focus on facial image-based gender classification. It introduces novel text-guided approaches to improve fairness by integrating semantic information extracted from image captions into the training process. Two primary strategies are proposed: Image Text Matching (ITM) guidance, which trains the model to identify fine-grained alignments between facial images and descriptive texts for enhanced multimodal understanding, and Image Text Fusion, which merges image and text modalities into unified representations that help reduce demographic biases. Experimental results on benchmark datasets demonstrate that these methods significantly improve accuracy and mitigate bias across different gender and racial groups, outperforming current approaches. A distinctive feature of this research is its reliance on textual guidance without requiring explicit demographic labels, making the technique applicable to a broad range of scenarios. The study also highlights the interpretability and intuitiveness of the training paradigm established through semantic information, linking improved fairness with better model generalization. Overall, the proposed methodologies offer meaningful advancements toward combating demographic bias in gender classification systems while maintaining application agnosticism and enhancing model transparency. <div>
arXiv:2512.11015v1 Announce Type: new 
Abstract: In the quest for fairness in artificial intelligence, novel approaches to enhance it in facial image based gender classification algorithms using text guided methodologies are presented. The core methodology involves leveraging semantic information from image captions during model training to improve generalization capabilities. Two key strategies are presented: Image Text Matching (ITM) guidance and Image Text fusion. ITM guidance trains the model to discern fine grained alignments between images and texts to obtain enhanced multimodal representations. Image text fusion combines both modalities into comprehensive representations for improved fairness. Exensive experiments conducted on benchmark datasets demonstrate these approaches effectively mitigate bias and improve accuracy across gender racial groups compared to existing methods. Additionally, the unique integration of textual guidance underscores an interpretable and intuitive training paradigm for computer vision systems. By scrutinizing the extent to which semantic information reduces disparities, this research offers valuable insights into cultivating more equitable facial analysis algorithms. The proposed methodologies contribute to addressing the pivotal challenge of demographic bias in gender classification from facial images. Furthermore, this technique operates in the absence of demographic labels and is application agnostic.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoccerMaster: A Vision Foundation Model for Soccer Understanding</title>
<link>https://arxiv.org/abs/2512.11016</link>
<guid>https://arxiv.org/abs/2512.11016</guid>
<content:encoded><![CDATA[
<div> SoccerUnderstanding, VisionFoundationModel, MultiTaskPretraining, DataCuration, SoccerVideoDatasets<br /><br />Summary:<br /><br />This paper introduces SoccerMaster, the first soccer-specific vision foundation model designed to unify multiple soccer visual understanding tasks in a single framework through supervised multi-task pretraining. Unlike previous approaches that rely on isolated, task-specific expert models, SoccerMaster is built to handle a wide range of tasks from fine-grained perception such as athlete detection to high-level semantic reasoning like event classification. To support this, the authors developed an automated data curation pipeline that generates scalable spatial annotations, which are integrated with existing soccer video datasets to form SoccerFactory, a comprehensive pretraining data resource. This extensive and diverse dataset enables more effective model training across different understanding tasks. Evaluation experiments demonstrate that SoccerMaster consistently outperforms expert models tailored for individual tasks, confirming its superior generalization and versatility. The paper also emphasizes the openness of their work by committing to publicly releasing the data, code, and final model, encouraging further research and application in soccer visual analysis. <div>
arXiv:2512.11016v1 Announce Type: new 
Abstract: Soccer understanding has recently garnered growing research interest due to its domain-specific complexity and unique challenges. Unlike prior works that typically rely on isolated, task-specific expert models, this work aims to propose a unified model to handle diverse soccer visual understanding tasks, ranging from fine-grained perception (e.g., athlete detection) to semantic reasoning (e.g., event classification). Specifically, our contributions are threefold: (i) we present SoccerMaster, the first soccer-specific vision foundation model that unifies diverse understanding tasks within a single framework via supervised multi-task pretraining; (ii) we develop an automated data curation pipeline to generate scalable spatial annotations, and integrate them with various existing soccer video datasets to construct SoccerFactory, a comprehensive pretraining data resource; and (iii) we conduct extensive evaluations demonstrating that SoccerMaster consistently outperforms task-specific expert models across diverse downstream tasks, highlighting its breadth and superiority. The data, code, and model will be publicly available.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weakly Supervised Tuberculosis Localization in Chest X-rays through Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.11057</link>
<guid>https://arxiv.org/abs/2512.11057</guid>
<content:encoded><![CDATA[
<div> Keywords: Tuberculosis, Chest X-ray, Knowledge Distillation, CNN, TBX11k Dataset  

<br /><br />Summary:  
1. Tuberculosis (TB) remains a major global health issue, especially in resource-limited countries, with chest X-ray (CXR) imaging being a common diagnostic tool that requires expert interpretation.  
2. Machine learning models have demonstrated high accuracy in TB classification, but they frequently rely on spurious correlations, limiting their generalizability across different datasets and clinical environments.  
3. Creating large, well-annotated medical image datasets is costly and labor-intensive, involving domain experts and multiple annotators to achieve reliable labels.  
4. This study employs a knowledge distillation approach using a teacher-student framework based on the ResNet50 architecture to train convolutional neural networks (CNNs) that not only reduce spurious correlations but also localize TB-related abnormalities without needing bounding-box annotations.  
5. Experiments conducted on the TBX11k dataset show that the student model achieves a significant mean Intersection over Union (mIOU) score of 0.2428, outperforming the teacher model and exhibiting enhanced robustness and potential for practical clinical deployment in diverse healthcare settings. <div>
arXiv:2512.11057v1 Announce Type: new 
Abstract: Tuberculosis (TB) remains one of the leading causes of mortality worldwide, particularly in resource-limited countries. Chest X-ray (CXR) imaging serves as an accessible and cost-effective diagnostic tool but requires expert interpretation, which is often unavailable. Although machine learning models have shown high performance in TB classification, they often depend on spurious correlations and fail to generalize. Besides, building large datasets featuring high-quality annotations for medical images demands substantial resources and input from domain specialists, and typically involves several annotators reaching agreement, which results in enormous financial and logistical expenses. This study repurposes knowledge distillation technique to train CNN models reducing spurious correlations and localize TB-related abnormalities without requiring bounding-box annotations. By leveraging a teacher-student framework with ResNet50 architecture, the proposed method trained on TBX11k dataset achieve impressive 0.2428 mIOU score. Experimental results further reveal that the student model consistently outperforms the teacher, underscoring improved robustness and potential for broader clinical deployment in diverse settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning</title>
<link>https://arxiv.org/abs/2512.11060</link>
<guid>https://arxiv.org/abs/2512.11060</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Optical Coherence Tomography Angiography, Synthetic Vasculature Reasoning, Diabetic Retinopathy, Image-Text Dataset<br /><br />Summary:  
This paper addresses the challenge of training Vision-Language Models (VLMs) for detailed medical reasoning, emphasizing the scarcity of large-scale, precisely annotated image-text datasets in specialized domains such as Optical Coherence Tomography Angiography (OCTA). To overcome this, the authors introduce Synthetic Vasculature Reasoning (SVR), a framework that generates realistic synthetic retinal vasculature images incorporating key Diabetic Retinopathy (DR) features such as capillary dropout, microaneurysms, neovascularization, and tortuosity, alongside automatically produced granular explanatory texts. Leveraging SVR, they curate OCTA-100K-SVR, a large-scale dataset containing 100,000 OCTA image and reasoning text pairs. Training a general-purpose Vision-Language Model (Qwen3-VL-8b) on this dataset, the model achieves a zero-shot balanced classification accuracy of 89.67% on real-world OCTA images, significantly outperforming existing supervised baselines. Furthermore, human expert evaluation confirms that the model notably improves explanation quality and the localization of pathological features in clinical data. Overall, the work demonstrates that synthetic data generation can effectively bridge data scarcity in medical imaging and enhance interpretable diagnostic performance of Vision-Language Models. <div>
arXiv:2512.11060v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) offer a promising path toward interpretable medical diagnosis by allowing users to ask about clinical explanations alongside predictions and across different modalities. However, training VLMs for detailed reasoning requires large-scale image-text datasets. In many specialized domains, for example in reading Optical Coherence Tomography Angiography (OCTA) images, such precise text with grounded description of pathologies is scarce or even non-existent. To overcome this bottleneck, we introduce Synthetic Vasculature Reasoning (SVR), a framework that controllably synthesizes images and corresponding text, specifically: realistic retinal vasculature with Diabetic Retinopathy (DR) features: capillary dropout, microaneurysms, neovascularization, and tortuosity, while automatically generating granular reasoning texts. Based on this we curate OCTA-100K-SVR, an OCTA image-reasoning dataset with 100,000 pairs. Our experiments show that a general-purpose VLM (Qwen3-VL-8b) trained on the dataset achieves a zero-shot balanced classification accuracy of 89.67% on real OCTA images, outperforming supervised baselines. Through human expert evaluation we also demonstrate that it significantly enhances explanation quality and pathology localization on clinical data.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation</title>
<link>https://arxiv.org/abs/2512.11061</link>
<guid>https://arxiv.org/abs/2512.11061</guid>
<content:encoded><![CDATA[
<div> Keywords: generative video models, vision-language model, scene representation, physics simulation, world modeling  

<br /><br />Summary: Generative video models currently face major challenges such as violating physical and logical constraints, lacking interactivity, and operating as opaque black boxes, which limits their use for creating structured, queryable environments. To address these issues, the paper introduces a new paradigm that distills image-caption pairs into an abstract, tractable representation optimized for simulation. The proposed framework, VDAWorld, leverages a Vision-Language Model (VLM) as an intelligent agent to autonomously build grounded 2D or 3D scene representations by selecting appropriate vision tools. The VLM then chooses a compatible physics simulator, such as rigid body or fluid simulations, to operate on the scene representation. This approach allows VDAWorld to infer latent dynamics from static scenes and predict plausible future states. Experimental results demonstrate that combining intelligent abstraction with adaptive simulation enables the creation of a versatile world model capable of producing high-quality simulations across diverse dynamic scenarios. Overall, VDAWorld advances world modeling by integrating vision, language, and physics simulation into a coherent framework for interactive and physically consistent video generation. <div>
arXiv:2512.11061v1 Announce Type: new 
Abstract: Generative video models, a leading approach to world modeling, face fundamental limitations. They often violate physical and logical rules, lack interactivity, and operate as opaque black boxes ill-suited for building structured, queryable worlds. To overcome these challenges, we propose a new paradigm focused on distilling an image caption pair into a tractable, abstract representation optimized for simulation. We introduce VDAWorld, a framework where a Vision-Language Model (VLM) acts as an intelligent agent to orchestrate this process. The VLM autonomously constructs a grounded (2D or 3D) scene representation by selecting from a suite of vision tools, and accordingly chooses a compatible physics simulator (e.g., rigid body, fluid) to act upon it. VDAWorld can then infer latent dynamics from the static scene to predict plausible future states. Our experiments show that this combination of intelligent abstraction and adaptive simulation results in a versatile world model capable of producing high quality simulations across a wide range of dynamic scenarios.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-CHUM: Event-based Cameras for Human Detection and Urban Monitoring</title>
<link>https://arxiv.org/abs/2512.11076</link>
<guid>https://arxiv.org/abs/2512.11076</guid>
<content:encoded><![CDATA[
<div> Keywords: urban dynamics, event-based cameras, sensor fusion, privacy, machine learning<br /><br />Summary:<br /><br />1. The paper addresses the ongoing challenges in understanding human movement and urban dynamics, highlighting the evolution from traditional observation methods to advanced sensing technologies.<br />2. It provides a focused survey on event-based cameras, which differ from conventional RGB cameras by capturing changes in light intensity rather than color values.<br />3. Event-based cameras offer distinct advantages, such as effective operation in low-light conditions and enhanced privacy preservation, making them suitable for urban monitoring.<br />4. The paper discusses the applications, benefits, and challenges of event-based cameras and explores the integration of machine learning techniques to leverage their data effectively.<br />5. To overcome limitations inherent to event-based cameras, the authors propose multi-sensor fusion approaches, combining event-based cameras with infrared, event-LiDAR, or vibration sensors, aiming to improve accuracy and robustness in studying city dynamics. <div>
arXiv:2512.11076v1 Announce Type: new 
Abstract: Understanding human movement and city dynamics has always been challenging. From traditional methods of manually observing the city's inhabitant, to using cameras, to now using sensors and more complex technology, the field of urban monitoring has evolved greatly. Still, there are more that can be done to unlock better practices for understanding city dynamics. This paper surveys how the landscape of urban dynamics studying has evolved with a particular focus on event-based cameras. Event-based cameras capture changes in light intensity instead of the RGB values that traditional cameras do. They offer unique abilities, like the ability to work in low-light, that can make them advantageous compared to other sensors. Through an analysis of event-based cameras, their applications, their advantages and challenges, and machine learning applications, we propose event-based cameras as a medium for capturing information to study urban dynamics. They offer the ability to capture important information while maintaining privacy. We also suggest multi-sensor fusion of event-based cameras and other sensors in the study of urban dynamics. Combining event-based cameras and infrared, event-LiDAR, or vibration has to potential to enhance the ability of event-based cameras and overcome the challenges that event-based cameras have.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description</title>
<link>https://arxiv.org/abs/2512.11098</link>
<guid>https://arxiv.org/abs/2512.11098</guid>
<content:encoded><![CDATA[
<div> Keywords: infrared imaging, zero-shot learning, vision-language models, industrial sensing, thermal monitoring<br /><br />Summary:  
This paper presents VLM-IRIS, a novel zero-shot framework designed to adapt vision-language models (VLMs) to process infrared (IR) camera data, which traditionally poses challenges due to VLMs being trained on RGB images. The key innovation is preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible formats, specifically by converting them to a magma colormap representation. This preprocessing allows existing CLIP-based vision encoders, such as ViT-B/32, to effectively interpret infrared data without requiring any model retraining. The framework leverages centroid prompt ensembling to enhance zero-shot prediction accuracy. VLM-IRIS is demonstrated on the practical task of detecting the presence of workpieces on a 3D printer bed, where thermal contrasts between the build plate and objects enable precise monitoring. The results show that the approach achieves high accuracy in infrared imagery analysis, highlighting its potential for label-free, real-time industrial monitoring in low-light or enclosed machine environments where traditional vision systems fail. This introduces an important advancement for industrial applications by integrating thermal sensing with foundation VLMs, expanding their utility beyond RGB visual domains. <div>
arXiv:2512.11098v1 Announce Type: new 
Abstract: Many manufacturing environments operate in low-light conditions or within enclosed machines where conventional vision systems struggle. Infrared cameras provide complementary advantages in such environments. Simultaneously, supervised AI systems require large labeled datasets, which makes zero-shot learning frameworks more practical for applications including infrared cameras. Recent advances in vision-language foundation models (VLMs) offer a new path in zero-shot predictions from paired image-text representations. However, current VLMs cannot understand infrared camera data since they are trained on RGB data. This work introduces VLM-IRIS (Vision-Language Models for InfraRed Industrial Sensing), a zero-shot framework that adapts VLMs to infrared data by preprocessing infrared images captured by a FLIR Boson sensor into RGB-compatible inputs suitable for CLIP-based encoders. We demonstrate zero-shot workpiece presence detection on a 3D printer bed where temperature differences between the build plate and workpieces make the task well-suited for thermal imaging. VLM-IRIS converts the infrared images to magma representation and applies centroid prompt ensembling with a CLIP ViT-B/32 encoder to achieve high accuracy on infrared images without any model retraining. These findings demonstrate that the proposed improvements to VLMs can be effectively extended to thermal applications for label-free monitoring.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VGent: Visual Grounding via Modular Design for Disentangling Reasoning and Prediction</title>
<link>https://arxiv.org/abs/2512.11099</link>
<guid>https://arxiv.org/abs/2512.11099</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual grounding, Modular encoder-decoder, Multimodal Large Language Model, Object detection, Reinforcement learning<br /><br />Summary:<br /><br />The paper introduces VGent, a novel modular encoder-decoder architecture for visual grounding that separates high-level reasoning from low-level bounding box prediction. Unlike existing approaches that rely on auto-regressive decoding in Multimodal Large Language Models (MLLMs) or re-aligning LLMs with vision features—which are slow and prone to hallucinations or degrade reasoning ability—VGent uses a frozen MLLM encoder to maintain pretrained reasoning strength. A decoder then selects target boxes by cross-attending to the encoder’s hidden states using high-quality boxes from object detectors as queries, enabling fast and efficient inference. The design synergizes advances in both object detection and MLLMs while avoiding common pitfalls like hallucination and slow decoding. The system supports modular upgrades including QuadThinker, an RL-based method to improve multi-target reasoning; mask-aware labels to resolve detection-segmentation ambiguities; and a global target recognition component to better identify all targets, enhancing proposal selection. Experiments on multi-target visual grounding benchmarks demonstrate state-of-the-art performance with a +20.6% F1 score improvement over previous methods, along with +8.2% gIoU and +5.8% cIoU gains under visual reference challenges, all achieved without sacrificing inference speed. <div>
arXiv:2512.11099v1 Announce Type: new 
Abstract: Current visual grounding models are either based on a Multimodal Large Language Model (MLLM) that performs auto-regressive decoding, which is slow and risks hallucinations, or on re-aligning an LLM with vision features to learn new special or object tokens for grounding, which may undermine the LLM's pretrained reasoning ability. In contrast, we propose VGent, a modular encoder-decoder architecture that explicitly disentangles high-level reasoning and low-level bounding box prediction. Specifically, a frozen MLLM serves as the encoder to provide untouched powerful reasoning capabilities, while a decoder takes high-quality boxes proposed by detectors as queries and selects target box(es) via cross-attending on encoder's hidden states. This design fully leverages advances in both object detection and MLLM, avoids the pitfalls of auto-regressive decoding, and enables fast inference. Moreover, it supports modular upgrades of both the encoder and decoder to benefit the whole system: we introduce (i) QuadThinker, an RL-based training paradigm for enhancing multi-target reasoning ability of the encoder; (ii) mask-aware label for resolving detection-segmentation ambiguity; and (iii) global target recognition to improve the recognition of all the targets which benefits the selection among augmented proposals. Experiments on multi-target visual grounding benchmarks show that VGent achieves a new state-of-the-art with +20.6% F1 improvement over prior methods, and further boosts gIoU by +8.2% and cIoU by +5.8% under visual reference challenges, while maintaining constant, fast inference latency.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-driven Fusion of Pathology Foundation Models for Enhanced Disease Characterization</title>
<link>https://arxiv.org/abs/2512.11104</link>
<guid>https://arxiv.org/abs/2512.11104</guid>
<content:encoded><![CDATA[
<div> Keywords: foundation models, pathology, feature fusion, cancer grading, whole-slide images  

<br /><br />Summary:  
This study investigates the integration of multiple pathology foundation models (FMs) to improve cancer grading and staging performance across kidney, prostate, and rectal cancers. Researchers used diagnostic H&amp;E whole-slide images for three cancer types, classifying them into low or high grade/stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were employed to train classifiers. Three FM fusion strategies were compared: majority-vote ensembling, naive feature concatenation, and an intelligent, correlation-guided pruning method to remove redundant features. Under patient-stratified cross-validation with hold-out testing, the intelligent fusion of tile-level embeddings consistently outperformed single FMs and naive fusion methods across all cancers. Similarity metrics revealed that, while FM embedding spaces were globally aligned, their local neighborhood structures showed less agreement, suggesting complementary detailed information across models. Attention map visualizations showed that intelligent fusion focused more specifically on tumor regions and reduced attention to benign tissue, improving interpretability. Overall, the study demonstrates that correlation-guided fusion creates compact, task-specific representations, enhancing both predictive accuracy and biological interpretability in computational pathology applications. <div>
arXiv:2512.11104v1 Announce Type: new 
Abstract: Foundation models (FMs) have demonstrated strong performance across diverse pathology tasks. While there are similarities in the pre-training objectives of FMs, there is still limited understanding of their complementarity, redundancy in embedding spaces, or biological interpretation of features. In this study, we propose an information-driven, intelligent fusion strategy for integrating multiple pathology FMs into a unified representation and systematically evaluate its performance for cancer grading and staging across three distinct diseases. Diagnostic H&amp;E whole-slide images from kidney (519 slides), prostate (490 slides), and rectal (200 slides) cancers were dichotomized into low versus high grade or stage. Both tile-level FMs (Conch v1.5, MUSK, Virchow2, H-Optimus1, Prov-Gigapath) and slide-level FMs (TITAN, CHIEF, MADELEINE) were considered to train downstream classifiers. We then evaluated three FM fusion schemes at both tile and slide levels: majority-vote ensembling, naive feature concatenation, and intelligent fusion based on correlation-guided pruning of redundant features. Under patient-stratified cross-validation with hold-out testing, intelligent fusion of tile-level embeddings yielded consistent gains in classification performance across all three cancers compared with the best single FMs and naive fusion. Global similarity metrics revealed substantial alignment of FM embedding spaces, contrasted by lower local neighborhood agreement, indicating complementary fine-grained information across FMs. Attention maps showed that intelligent fusion yielded concentrated attention on tumor regions while reducing spurious focus on benign regions. Our findings suggest that intelligent, correlation-guided fusion of pathology FMs can yield compact, task-tailored representations that enhance both predictive performance and interpretability in downstream computational pathology tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from a Generative Oracle: Domain Adaptation for Restoration</title>
<link>https://arxiv.org/abs/2512.11121</link>
<guid>https://arxiv.org/abs/2512.11121</guid>
<content:encoded><![CDATA[
<div> Keywords: image restoration, domain adaptation, generative oracle, pseudo-supervision, unsupervised learning  

<br /><br />Summary:  
This paper addresses the challenge of adapting pre-trained image restoration models to real-world, out-of-distribution degradations where ground truth data is unavailable. Traditional methods often struggle because they require architectural changes and paired data, which are impractical in many scenarios. The authors propose LEGO (Learning from a Generative Oracle), a novel three-stage framework designed for post-training domain adaptation without the need for paired data. First, LEGO generates initial restorations using the existing pre-trained model on the new domain. Second, a frozen, large-scale generative oracle refines these initial outputs to produce high-quality pseudo-ground-truth images. Third, the original model is fine-tuned with a mixed-supervision strategy that combines the original in-distribution data and the newly created pseudo-pairs. This process enables effective domain adaptation while maintaining the model’s original robustness and requires no changes to the network architecture. Experimental results demonstrate that LEGO significantly improves performance on diverse real-world benchmarks by bridging the domain gap in an unsupervised manner. Overall, LEGO provides a practical, scalable, and effective solution to adapt image restoration models to unseen domains without sacrificing existing capabilities. <div>
arXiv:2512.11121v1 Announce Type: new 
Abstract: Pre-trained image restoration models often fail on real-world, out-of-distribution degradations due to significant domain gaps. Adapting to these unseen domains is challenging, as out-of-distribution data lacks ground truth, and traditional adaptation methods often require complex architectural changes. We propose LEGO (Learning from a Generative Oracle), a practical three-stage framework for post-training domain adaptation without paired data. LEGO converts this unsupervised challenge into a tractable pseudo-supervised one. First, we obtain initial restorations from the pre-trained model. Second, we leverage a frozen, large-scale generative oracle to refine these estimates into high-quality pseudo-ground-truths. Third, we fine-tune the original model using a mixed-supervision strategy combining in-distribution data with these new pseudo-pairs. This approach adapts the model to the new distribution without sacrificing its original robustness or requiring architectural modifications. Experiments demonstrate that LEGO effectively bridges the domain gap, significantly improving performance on diverse real-world benchmarks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast-FoundationStereo: Real-Time Zero-Shot Stereo Matching</title>
<link>https://arxiv.org/abs/2512.11130</link>
<guid>https://arxiv.org/abs/2512.11130</guid>
<content:encoded><![CDATA[
<div> Keywords: stereo vision, zero-shot generalization, real-time processing, knowledge distillation, neural architecture search<br /><br />Summary:<br /><br />This paper introduces Fast-FoundationStereo, a novel family of stereo vision architectures designed to deliver strong zero-shot generalization while maintaining real-time frame rates. The authors identify the existing challenge where stereo foundation models excel in generalization but are computationally expensive, whereas efficient stereo models are faster but require domain-specific fine-tuning and compromise robustness. To address this, Fast-FoundationStereo uses a divide-and-conquer acceleration approach consisting of three key components: (1) knowledge distillation to compress a complex hybrid backbone into a single efficient student network, enabling faster inference; (2) blockwise neural architecture search to automatically discover optimal cost filtering designs within strict latency constraints, significantly reducing search complexity; and (3) structured pruning to remove redundancies in the iterative refinement module, enhancing efficiency further. Additionally, the authors developed an automatic pseudo-labeling pipeline to generate a large-scale dataset of 1.4 million in-the-wild stereo pairs, supplementing synthetic data and aiding effective knowledge distillation. The resulting model attains over 10 times speed improvement compared to FoundationStereo while closely matching its zero-shot accuracy performance. This work sets a new state-of-the-art benchmark for real-time stereo vision methods, combining accuracy and speed effectively. <div>
arXiv:2512.11130v1 Announce Type: new 
Abstract: Stereo foundation models achieve strong zero-shot generalization but remain computationally prohibitive for real-time applications. Efficient stereo architectures, on the other hand, sacrifice robustness for speed and require costly per-domain fine-tuning. To bridge this gap, we present Fast-FoundationStereo, a family of architectures that achieve, for the first time, strong zero-shot generalization at real-time frame rate. We employ a divide-and-conquer acceleration strategy with three components: (1) knowledge distillation to compress the hybrid backbone into a single efficient student; (2) blockwise neural architecture search for automatically discovering optimal cost filtering designs under latency budgets, reducing search complexity exponentially; and (3) structured pruning for eliminating redundancy in the iterative refinement module. Furthermore, we introduce an automatic pseudo-labeling pipeline used to curate 1.4M in-the-wild stereo pairs to supplement synthetic training data and facilitate knowledge distillation. The resulting model can run over 10x faster than FoundationStereo while closely matching its zero-shot accuracy, thus establishing a new state-of-the-art among real-time methods. Project page: https://nvlabs.github.io/Fast-FoundationStereo/
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning complete and explainable visual representations from itemized text supervision</title>
<link>https://arxiv.org/abs/2512.11141</link>
<guid>https://arxiv.org/abs/2512.11141</guid>
<content:encoded><![CDATA[
<div> itemized text supervision, cross-attention, visual representations, medical imaging, zero-shot learning  

<br /><br />Summary:  
Training vision models using language supervision allows for more general and transferable visual representations. However, typical approaches struggle with itemized text supervision found in domains like medical imaging and remote sensing, where multiple independent text annotations correspond to distinct findings within a single image. Unlike conventional multi-caption setups that are often redundant or overlapping, itemized text provides separate, semantically distinct descriptions. To address this, the paper introduces ItemizedCLIP, a framework that uses a cross-attention module to generate visual embeddings conditioned on individual text items. ItemizedCLIP incorporates customized training objectives to ensure item independence, encouraging distinct image regions to correspond to different items, and representation completeness, ensuring all text items are covered. The method was evaluated across four real-world domains with natural itemized supervision (brain MRI, head CT, chest CT, remote sensing) and one synthetic dataset. Results show that ItemizedCLIP significantly improves zero-shot classification performance and fine-grained interpretability compared to baseline models. The learned visual representations are semantically grounded, differentiable by item, complete in coverage, and visually interpretable. The code implementation has been made publicly available for further research and application. <div>
arXiv:2512.11141v1 Announce Type: new 
Abstract: Training vision models with language supervision enables general and transferable representations. However, many visual domains, especially non-object-centric domains such as medical imaging and remote sensing, contain itemized text annotations: multiple text items describing distinct and semantically independent findings within a single image. Such supervision differs from standard multi-caption supervision, where captions are redundant or highly overlapping. Here, we introduce ItemizedCLIP, a framework for learning complete and explainable visual representations from itemized text supervision. ItemizedCLIP employs a cross-attention module to produce text item-conditioned visual embeddings and a set of tailored objectives that jointly enforce item independence (distinct regions for distinct items) and representation completeness (coverage of all items). Across four domains with naturally itemized text supervision (brain MRI, head CT, chest CT, remote sensing) and one additional synthetically itemized dataset, ItemizedCLIP achieves substantial improvements in zero-shot performance and fine-grained interpretability over baselines. The resulting ItemizedCLIP representations are semantically grounded, item-differentiable, complete, and visually interpretable. Our code is available at https://github.com/MLNeurosurg/ItemizedCLIP.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context</title>
<link>https://arxiv.org/abs/2512.11167</link>
<guid>https://arxiv.org/abs/2512.11167</guid>
<content:encoded><![CDATA[
<div> Monkey Vision-Language Model, image tiling, high-resolution, reproducibility, global context<br /><br />Summary:<br /><br />This study focuses on reproducing and critically analyzing the Monkey Vision-Language Model (VLM) introduced by Li et al. (2023b), which aims at improving high-resolution image understanding by using image tiling to balance fine-grained detail recovery with computational efficiency. The researchers successfully replicate the tile-based approach using publicly available checkpoints and reimplement the original training pipeline. They confirm the original finding that tiling significantly aids in recovering local visual details in large images. Beyond reproduction, the study investigates the role of global context inclusion, offering practical insights that can guide future multimodal models handling high-resolution images. Despite confirming key findings, the authors observe variations in results, with the impact of tiling and global context differing depending on the nature of the task and the granularity of image tiles used. These deviations underscore the complexity and task sensitivity in applying tiling strategies to multimodal learning. Overall, the work stresses the importance of transparency and accessible infrastructure in reproducing complex models and contributes valuable empirical knowledge regarding tiling and context effects in vision-language modeling. <div>
arXiv:2512.11167v1 Announce Type: new 
Abstract: Reproducibility remains a cornerstone of scientific progress, yet complex multimodal models often lack transparent implementation details and accessible training infrastructure. In this work, we present a detailed reproduction and critical analysis of the Monkey Vision-Language Model (VLM) (Li et al. 2023b) published in CVPR24, a recent approach to high-resolution image understanding via image tiling. The original paper proposed splitting large images into tiles to recover fine-grained visual details while maintaining computational efficiency. Our study replicates this strategy using open checkpoints and reimplements the training pipeline. We confirm the key finding of the original Monkey VLM work, namely that tiling effectively recovers local details. We then extend this work further, by investigating the effect of the inclusion of the global context, which provide practical insights for future high-resolution multimodal modeling. However, we also report deviations in the results, with the magnitude of these effects depending heavily on task type and tile granularity.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight 3D Gaussian Splatting Compression via Video Codec</title>
<link>https://arxiv.org/abs/2512.11186</link>
<guid>https://arxiv.org/abs/2512.11186</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian Splatting, Video Compression, Morton Scan, Principal Component Analysis, Rate-Distortion Performance  

<br /><br />Summary:  
This paper addresses the inefficiency of current video-based Gaussian Splatting (GS) compression methods, which rely on the computationally heavy Parallel Linear Assignment Sorting (PLAS) for converting 3D GS data into smooth 2D maps. To overcome these limitations on lightweight devices, the authors propose a Lightweight 3D Gaussian Splatting Compression method based on video codec, termed LGSCV. The approach features a novel two-stage Morton scan process: a 3D Morton scan permutes the GS primitives, followed by a 2D Morton scan mapping these into blockwise 2D maps compatible with square block coding units (CU) of standard video codecs. Although blockwise 2D maps perform comparably to PLAS at high bitrates, quality deteriorates at medium-to-low bitrates. To address this, Principal Component Analysis (PCA) reduces the dimensionality of spherical harmonics (SH), and a flexible, faster MiniPLAS is introduced to permute primitives within certain block sizes. Combining SH PCA and MiniPLAS significantly improves rate-distortion (RD) performance, especially at lower bitrates, while guiding CU size configuration and halving encoding time. Experiments on the MPEG dataset demonstrate LGSCV achieves over 20% RD gain over state-of-the-art methods, reduces 2D map generation time to about 1 second, and cuts overall encoding time by 50%. The source code is publicly available on GitHub. <div>
arXiv:2512.11186v1 Announce Type: new 
Abstract: Current video-based GS compression methods rely on using Parallel Linear Assignment Sorting (PLAS) to convert 3D GS into smooth 2D maps, which are computationally expensive and time-consuming, limiting the application of GS on lightweight devices. In this paper, we propose a Lightweight 3D Gaussian Splatting (GS) Compression method based on Video codec (LGSCV). First, a two-stage Morton scan is proposed to generate blockwise 2D maps that are friendly for canonical video codecs in which the coding units (CU) are square blocks. A 3D Morton scan is used to permute GS primitives, followed by a 2D Morton scan to map the ordered GS primitives to 2D maps in a blockwise style. However, although the blockwise 2D maps report close performance to the PLAS map in high-bitrate regions, they show a quality collapse at medium-to-low bitrates. Therefore, a principal component analysis (PCA) is used to reduce the dimensionality of spherical harmonics (SH), and a MiniPLAS, which is flexible and fast, is designed to permute the primitives within certain block sizes. Incorporating SH PCA and MiniPLAS leads to a significant gain in rate-distortion (RD) performance, especially at medium and low bitrates. MiniPLAS can also guide the setting of the codec CU size configuration and significantly reduce encoding time. Experimental results on the MPEG dataset demonstrate that the proposed LGSCV achieves over 20% RD gain compared with state-of-the-art methods, while reducing 2D map generation time to approximately 1 second and cutting encoding time by 50%. The code is available at https://github.com/Qi-Yangsjtu/LGSCV .
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-task Learning with Extended Temporal Shift Module for Temporal Action Localization</title>
<link>https://arxiv.org/abs/2512.11189</link>
<guid>https://arxiv.org/abs/2512.11189</guid>
<content:encoded><![CDATA[
<div> BinEgo-360, Temporal Action Localization, Multi-task Learning, Ensemble Learning, Temporal Shift Module<br /><br />Summary: This paper presents a winning solution to the BinEgo-360 Challenge at ICCV 2025, focusing on temporal action localization (TAL) within multi-perspective and multi-modal video inputs. The dataset includes panoramic, third-person, and egocentric videos annotated with detailed action classes, posing unique challenges. The authors extend the Temporal Shift Module (TSM) to TAL tasks by incorporating a background class and segmenting videos into fixed-length, non-overlapping intervals for classification. A multi-task learning framework is proposed that jointly optimizes scene classification and TAL, exploiting contextual relationships between actions and their environments to enhance prediction accuracy. To further improve robustness and consistency, multiple models are integrated through a weighted ensemble strategy. This combination of multi-task learning, an efficient backbone architecture, and ensemble learning led the method to achieve first place in both the initial and extended rounds of the competition. The results demonstrate the effectiveness of leveraging contextual cues and ensemble techniques in handling the complex nature of multi-perspective, multi-modal video action localization tasks. <div>
arXiv:2512.11189v1 Announce Type: new 
Abstract: We present our solution to the BinEgo-360 Challenge at ICCV 2025, which focuses on temporal action localization (TAL) in multi-perspective and multi-modal video settings. The challenge provides a dataset containing panoramic, third-person, and egocentric recordings, annotated with fine-grained action classes. Our approach is built on the Temporal Shift Module (TSM), which we extend to handle TAL by introducing a background class and classifying fixed-length non-overlapping intervals. We employ a multi-task learning framework that jointly optimizes for scene classification and TAL, leveraging contextual cues between actions and environments. Finally, we integrate multiple models through a weighted ensemble strategy, which improves robustness and consistency of predictions. Our method is ranked first in both the initial and extended rounds of the competition, demonstrating the effectiveness of combining multi-task learning, an efficient backbone, and ensemble learning for TAL.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADKnitter: Compositional CAD Generation from Text and Geometry Guidance</title>
<link>https://arxiv.org/abs/2512.11199</link>
<guid>https://arxiv.org/abs/2512.11199</guid>
<content:encoded><![CDATA[
<div> CAD generation, diffusion sampling, compositional design, CAD assembly, KnitCAD dataset<br /><br />Summary:<br /><br />1. Designing computer-aided design (CAD) models traditionally requires significant expertise and time, with a focus on precision and detailed functionality. 2. Recent advances in 3D generation have shifted the focus from merely visual quality to also enabling editable and functional CAD designs. 3. Existing methods primarily handle single-part CAD generation but fall short when applied to multi-part assemblies, which are essential in real-world applications due to semantic and geometric constraints among parts. 4. The paper introduces CADKnitter, a novel framework for compositional CAD generation that uses a geometry-guided diffusion sampling strategy to create complementary CAD parts that respect both geometric compatibility with the existing model and semantic alignment with user-provided text prompts. 5. To support training and evaluation, the authors curated KnitCAD, a comprehensive dataset of over 310,000 CAD models enriched with textual prompts and detailed assembly metadata, enabling semantic and geometric constraint consideration. Experimental results demonstrate that CADKnitter outperforms state-of-the-art baselines in generating coherent and semantically meaningful multi-part CAD assemblies, highlighting its effectiveness for practical CAD design workflows. <div>
arXiv:2512.11199v1 Announce Type: new 
Abstract: Crafting computer-aided design (CAD) models has long been a painstaking and time-intensive task, demanding both precision and expertise from designers. With the emergence of 3D generation, this task has undergone a transformative impact, shifting not only from visual fidelity to functional utility but also enabling editable CAD designs. Prior works have achieved early success in single-part CAD generation, which is not well-suited for real-world applications, as multiple parts need to be assembled under semantic and geometric constraints. In this paper, we propose CADKnitter, a compositional CAD generation framework with a geometry-guided diffusion sampling strategy. CADKnitter is able to generate a complementary CAD part that follows both the geometric constraints of the given CAD model and the semantic constraints of the desired design text prompt. We also curate a dataset, so-called KnitCAD, containing over 310,000 samples of CAD models, along with textual prompts and assembly metadata that provide semantic and geometric constraints. Intensive experiments demonstrate that our proposed method outperforms other state-of-the-art baselines by a clear margin.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path</title>
<link>https://arxiv.org/abs/2512.11203</link>
<guid>https://arxiv.org/abs/2512.11203</guid>
<content:encoded><![CDATA[
<div> Autoregressive video diffusion models, inference-time alignment, noise refiner, AutoRefiner, sample fidelity<br /><br />Summary:<br /><br />Autoregressive video diffusion models (AR-VDMs) offer scalable and efficient alternatives to bidirectional video diffusion models, enabling real-time and interactive video generation applications. However, challenges persist in enhancing the sample fidelity produced by these models. One approach to improve fidelity is inference-time alignment, which optimizes the noise space for better outputs without updating the model parameters. Traditional optimization or search-based methods for inference-time alignment, however, are computationally expensive and impractical for AR-VDMs. Inspired by advancements in text-to-image (T2I) diffusion models, where feedforward noise refiners modulate sampled noises in a single forward pass, the authors explore extending this idea to AR-VDMs. They find that naïvely applying T2I noise refiners to AR-VDMs fails to improve performance. To address this, they propose AutoRefiner, a noise refiner specifically designed for AR-VDMs. AutoRefiner incorporates two key innovations: pathwise noise refinement, which refines noise along stochastic denoising paths, and a reflective key-value cache (KV-cache) to maintain consistency across steps. Experimental results demonstrate that AutoRefiner effectively enhances sample fidelity while serving as an efficient, plug-in component compatible with existing AR-VDMs. <div>
arXiv:2512.11203v1 Announce Type: new 
Abstract: Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmokeBench: Evaluating Multimodal Large Language Models for Wildfire Smoke Detection</title>
<link>https://arxiv.org/abs/2512.11215</link>
<guid>https://arxiv.org/abs/2512.11215</guid>
<content:encoded><![CDATA[
<div> Wildfire smoke, multimodal large language models, smoke detection, smoke localization, early-stage detection<br /><br />Summary:<br /><br />This paper presents SmokeBench, a benchmark designed to evaluate the ability of multimodal large language models (MLLMs) to identify and localize wildfire smoke in images. The benchmark includes four specific tasks: smoke classification, tile-based smoke localization, grid-based smoke localization, and overall smoke detection. Several state-of-the-art MLLMs such as Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro were tested using this benchmark. The results indicate that although some models can successfully classify the presence of smoke when it occupies a large area, all models face significant challenges in precisely localizing smoke, particularly in the early stages of a wildfire. Further analysis suggests that the volume of smoke is a strong predictor of model performance, while visual contrast has a lesser impact. These findings emphasize the limitations of current MLLMs in safety-critical tasks like wildfire monitoring and spotlight the need for advanced techniques to enhance early-stage smoke detection and localization accuracy. <div>
arXiv:2512.11215v1 Announce Type: new 
Abstract: Wildfire smoke is transparent, amorphous, and often visually confounded with clouds, making early-stage detection particularly challenging. In this work, we introduce a benchmark, called SmokeBench, to evaluate the ability of multimodal large language models (MLLMs) to recognize and localize wildfire smoke in images. The benchmark consists of four tasks: (1) smoke classification, (2) tile-based smoke localization, (3) grid-based smoke localization, and (4) smoke detection. We evaluate several MLLMs, including Idefics2, Qwen2.5-VL, InternVL3, Unified-IO 2, Grounding DINO, GPT-4o, and Gemini-2.5 Pro. Our results show that while some models can classify the presence of smoke when it covers a large area, all models struggle with accurate localization, especially in the early stages. Further analysis reveals that smoke volume is strongly correlated with model performance, whereas contrast plays a comparatively minor role. These findings highlight critical limitations of current MLLMs for safety-critical wildfire monitoring and underscore the need for methods that improve early-stage smoke localization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VFMF: World Modeling by Forecasting Vision Foundation Model Features</title>
<link>https://arxiv.org/abs/2512.11225</link>
<guid>https://arxiv.org/abs/2512.11225</guid>
<content:encoded><![CDATA[
<div> Keywords: world modeling, vision foundation models, generative forecasting, autoregressive flow matching, latent space<br /><br />Summary:<br /><br />1. The paper addresses the challenge of forecasting future world states from partial observations, which is a fundamental problem in world modeling. 2. Traditional methods focus on stochastic video generation by predicting future pixels, which is computationally expensive and not directly useful for decision-making tasks because it requires interpreting RGB images into actionable signals. 3. An alternative approach leverages features from vision foundation models (VFMs) to represent the world state and performs deterministic regression to predict these features, allowing direct translation into useful outputs like semantic segmentation and depth, while being computationally more efficient. 4. However, deterministic regression suffers from averaging over multiple plausible futures, thus failing to properly capture uncertainty and reducing forecast accuracy. 5. To overcome this limitation, the authors propose a generative forecaster that uses autoregressive flow matching in the VFM feature space, encoding the features into a compact latent space optimized for diffusion-based generative modeling. 6. This latent space encoding preserves information better than PCA-based approaches and can decode predictions into multiple interpretable modalities including semantic segmentation, depth, surface normals, and RGB images. 7. Experimental results show that the proposed method produces sharper and more accurate multimodal predictions than regression under matched computational budgets and architectures. 8. The study concludes that stochastic conditional generation in VFM feature space is a promising and scalable direction for developing future world models. <div>
arXiv:2512.11225v1 Announce Type: new 
Abstract: Forecasting from partial observations is central to world modeling. Many recent methods represent the world through images, and reduce forecasting to stochastic video generation. Although such methods excel at realism and visual fidelity, predicting pixels is computationally intensive and not directly useful in many applications, as it requires translating RGB into signals useful for decision making. An alternative approach uses features from vision foundation models (VFMs) as world representations, performing deterministic regression to predict future world states. These features can be directly translated into actionable signals such as semantic segmentation and depth, while remaining computationally efficient. However, deterministic regression averages over multiple plausible futures, undermining forecast accuracy by failing to capture uncertainty. To address this crucial limitation, we introduce a generative forecaster that performs autoregressive flow matching in VFM feature space. Our key insight is that generative modeling in this space requires encoding VFM features into a compact latent space suitable for diffusion. We show that this latent space preserves information more effectively than previously used PCA-based alternatives, both for forecasting and other applications, such as image generation. Our latent predictions can be easily decoded into multiple useful and interpretable output modalities: semantic segmentation, depth, surface normals, and even RGB. With matched architecture and compute, our method produces sharper and more accurate predictions than regression across all modalities. Our results suggest that stochastic conditional generation of VFM features offers a promising and scalable foundation for future world models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FutureX: Enhance End-to-End Autonomous Driving via Latent Chain-of-Thought World Model</title>
<link>https://arxiv.org/abs/2512.11226</link>
<guid>https://arxiv.org/abs/2512.11226</guid>
<content:encoded><![CDATA[
<div> Future scene prediction, Chain of Thought, end-to-end planner, autonomous driving, motion planning<br /><br />Summary:<br /><br />1. Autonomous driving systems typically use end-to-end planners that process raw sensor data to generate motion plans or control actions based solely on the current scene. 2. This approach can be suboptimal in dynamic traffic environments where the ego vehicle’s actions affect future scenes, necessitating predictive reasoning about scene evolution. 3. The authors propose FutureX, a Chain of Thought (CoT)-driven pipeline that enhances motion planning by modeling future scenes through a Latent World Model and refining trajectories accordingly. 4. FutureX includes an Auto-think Switch that evaluates the scene complexity and decides whether to engage in additional reasoning (Thinking mode) or produce an immediate plan (Instant mode). 5. In Thinking mode, the system predicts future scene states using CoT-guided rollouts, allowing a Summarizer Module to improve the motion plan iteratively. 6. Experiments show that FutureX improves the rationality of motion plans and reduces collision rates compared to existing methods without sacrificing efficiency. 7. Notably, it achieves a 6.2-point improvement in PDMS (Performance Driving Metric Score) for the TransFuser model evaluated on NAVSIM. 8. The authors plan to release the code, facilitating adoption and further research in this area. <div>
arXiv:2512.11226v1 Announce Type: new 
Abstract: In autonomous driving, end-to-end planners learn scene representations from raw sensor data and utilize them to generate a motion plan or control actions. However, exclusive reliance on the current scene for motion planning may result in suboptimal responses in highly dynamic traffic environments where ego actions further alter the future scene. To model the evolution of future scenes, we leverage the World Model to represent how the ego vehicle and its environment interact and change over time, which entails complex reasoning. The Chain of Thought (CoT) offers a promising solution by forecasting a sequence of future thoughts that subsequently guide trajectory refinement. In this paper, we propose FutureX, a CoT-driven pipeline that enhances end-to-end planners to perform complex motion planning via future scene latent reasoning and trajectory refinement. Specifically, the Auto-think Switch examines the current scene and decides whether additional reasoning is required to yield a higher-quality motion plan. Once FutureX enters the Thinking mode, the Latent World Model conducts a CoT-guided rollout to predict future scene representation, enabling the Summarizer Module to further refine the motion plan. Otherwise, FutureX operates in an Instant mode to generate motion plans in a forward pass for relatively simple scenes. Extensive experiments demonstrate that FutureX enhances existing methods by producing more rational motion plans and fewer collisions without compromising efficiency, thereby achieving substantial overall performance gains, e.g., 6.2 PDMS improvement for TransFuser on NAVSIM. Code will be released.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation</title>
<link>https://arxiv.org/abs/2512.11229</link>
<guid>https://arxiv.org/abs/2512.11229</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, talking head generation, real-time streaming, video latent space, autoregressive generation<br /><br />Summary:<br /><br />1. The paper introduces REST, a novel diffusion-based framework designed for real-time, end-to-end streaming audio-driven talking head generation.<br />2. To facilitate real-time performance, REST employs a high spatiotemporal Variational Autoencoder (VAE) to compress video data into a compact latent space.<br />3. An ID-Context Cache mechanism is developed to support autoregressive streaming in the latent space by combining ID-Sink and Context-Cache techniques, which ensures temporal consistency and identity preservation over long streams.<br />4. The Asynchronous Streaming Distillation (ASD) training strategy is proposed to reduce error accumulation common in autoregressive models, by using a non-streaming teacher model with an asynchronous noise schedule to guide the streaming student model.<br />5. Experimental results demonstrate that REST significantly outperforms existing state-of-the-art methods with faster generation speeds and improved overall talking head quality, bridging the gap between diffusion-based and autoregressive paradigms for real-time applications. <div>
arXiv:2512.11229v1 Announce Type: new 
Abstract: Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoomPilot: Controllable Synthesis of Interactive Indoor Environments via Multimodal Semantic Parsing</title>
<link>https://arxiv.org/abs/2512.11234</link>
<guid>https://arxiv.org/abs/2512.11234</guid>
<content:encoded><![CDATA[
<div> Keywords: controllable scene generation, indoor scenes, multi-modal inputs, Indoor Domain-Specific Language, interaction-annotated assets  

<br /><br />Summary:  
1. The paper presents RoomPilot, a unified framework designed to generate controllable and interactive indoor scenes suitable for applications such as game development, architectural visualization, and embodied AI training.  
2. RoomPilot supports diverse multi-modal inputs including textual descriptions and CAD floor plans, parsing them into an Indoor Domain-Specific Language (IDSL) that serves as a shared semantic representation for scene synthesis.  
3. The IDSL enables the generation of coherent, high-quality indoor scenes from any single input modality while preserving interaction semantics, overcoming limitations of prior methods constrained by narrow input types or stochastic processes.  
4. Unlike traditional procedural approaches which often yield visually plausible but functionally inert layouts, RoomPilot integrates a curated dataset of interaction-annotated assets to produce environments with realistic object behaviors and physical consistency.  
5. Extensive experiments demonstrate RoomPilot’s strong multi-modal understanding, fine-grained controllability in scene generation, and superior visual fidelity, marking a significant advancement toward general-purpose controllable 3D indoor scene generation. <div>
arXiv:2512.11234v1 Announce Type: new 
Abstract: Generating controllable and interactive indoor scenes is fundamental to applications in game development, architectural visualization, and embodied AI training. Yet existing approaches either handle a narrow range of input modalities or rely on stochastic processes that hinder controllability. To overcome these limitations, we introduce RoomPilot, a unified framework that parses diverse multi-modal inputs--textual descriptions or CAD floor plans--into an Indoor Domain-Specific Language (IDSL) for indoor structured scene generation. The key insight is that a well-designed IDSL can act as a shared semantic representation, enabling coherent, high-quality scene synthesis from any single modality while maintaining interaction semantics. In contrast to conventional procedural methods that produce visually plausible but functionally inert layouts, RoomPilot leverages a curated dataset of interaction-annotated assets to synthesize environments exhibiting realistic object behaviors. Extensive experiments further validate its strong multi-modal understanding, fine-grained controllability in scene generation, and superior physical consistency and visual fidelity, marking a significant step toward general-purpose controllable 3D indoor scene generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WildCap: Facial Appearance Capture in the Wild via Hybrid Inverse Rendering</title>
<link>https://arxiv.org/abs/2512.11237</link>
<guid>https://arxiv.org/abs/2512.11237</guid>
<content:encoded><![CDATA[
<div> Keywords: facial appearance capture, inverse rendering, in-the-wild, texel grid lighting, reflectance optimization  

<br /><br />Summary:  
1. The paper introduces WildCap, a novel method for capturing high-quality facial appearance from smartphone videos recorded in the wild, addressing cost and usability limitations of traditional controlled lighting setups.  
2. To disentangle reflectance properties from complex lighting in unconstrained conditions, the method uses a hybrid inverse rendering framework combining a data-driven approach called SwitchLight with model-based inverse rendering techniques.  
3. The authors identify that local artifacts generated by neural network predictions, such as shadow-baking, are non-physical and interfere with accurate modeling of lighting and material properties.  
4. To overcome this, a new texel grid lighting model is proposed, which explains these non-physical effects as clean albedo maps illuminated by local physical lighting sources, enabling a more physically consistent interpretation.  
5. During optimization, the method jointly samples a diffusion prior for reflectance maps while optimizing lighting parameters, effectively resolving the scale ambiguity between lighting intensity and albedo reflectance.  
6. Experimental results demonstrate that WildCap significantly outperforms prior methods under similar capture conditions, substantially narrowing the quality gap between in-the-wild and controlled facial appearance capture scenarios.  
7. The authors will make their code publicly available, supporting further research and practical applications in the area. <div>
arXiv:2512.11237v1 Announce Type: new 
Abstract: Existing methods achieve high-quality facial appearance capture under controllable lighting, which increases capture cost and limits usability. We propose WildCap, a novel method for high-quality facial appearance capture from a smartphone video recorded in the wild. To disentangle high-quality reflectance from complex lighting effects in in-the-wild captures, we propose a novel hybrid inverse rendering framework. Specifically, we first apply a data-driven method, i.e., SwitchLight, to convert the captured images into more constrained conditions and then adopt model-based inverse rendering. However, unavoidable local artifacts in network predictions, such as shadow-baking, are non-physical and thus hinder accurate inverse rendering of lighting and material. To address this, we propose a novel texel grid lighting model to explain non-physical effects as clean albedo illuminated by local physical lighting. During optimization, we jointly sample a diffusion prior for reflectance maps and optimize the lighting, effectively resolving scale ambiguity between local lights and albedo. Our method achieves significantly better results than prior arts in the same capture setup, closing the quality gap between in-the-wild and controllable recordings by a large margin. Our code will be released \href{https://yxuhan.github.io/WildCap/index.html}{\textcolor{magenta}{here}}.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Prompting for Balanced Incomplete Multi-modal Emotion Recognition</title>
<link>https://arxiv.org/abs/2512.11239</link>
<guid>https://arxiv.org/abs/2512.11239</guid>
<content:encoded><![CDATA[
<div> Keywords: Incomplete multi-modal emotion recognition, cross-modal prompting, modality-specific features, knowledge propagation, dynamic re-weighting

<br /><br />Summary:  
Incomplete multi-modal emotion recognition (IMER) focuses on interpreting human intentions and emotions using partially observed multi-source data. Although multi-modal data offers richer information, challenges like performance gaps and under-optimized modalities limit its effectiveness, especially when confronted with missing data. To tackle these issues, the authors propose a novel Cross-modal Prompting (ComP) method designed to enhance coherent information by improving modality-specific features and boosting each modality's accuracy. The method introduces a progressive prompt generation module equipped with a dynamic gradient modulator to create concise, consistent semantic cues for each modality. Additionally, cross-modal knowledge propagation is employed to selectively amplify shared information across modalities through the generated prompts, strengthening the discrimination power of modality-specific outputs. A coordinator component dynamically re-weights modality outputs to complement existing balance strategies, further enhancing model performance. The effectiveness of the ComP method is validated through extensive experiments conducted on four diverse datasets, compared against seven state-of-the-art approaches under various missing data rates. The results demonstrate significant improvements in multi-modal emotion recognition accuracy and robustness, confirming the superiority of the proposed framework in handling incomplete multi-modal inputs. <div>
arXiv:2512.11239v1 Announce Type: new 
Abstract: Incomplete multi-modal emotion recognition (IMER) aims at understanding human intentions and sentiments by comprehensively exploring the partially observed multi-source data. Although the multi-modal data is expected to provide more abundant information, the performance gap and modality under-optimization problem hinder effective multi-modal learning in practice, and are exacerbated in the confrontation of the missing data. To address this issue, we devise a novel Cross-modal Prompting (ComP) method, which emphasizes coherent information by enhancing modality-specific features and improves the overall recognition accuracy by boosting each modality's performance. Specifically, a progressive prompt generation module with a dynamic gradient modulator is proposed to produce concise and consistent modality semantic cues. Meanwhile, cross-modal knowledge propagation selectively amplifies the consistent information in modality features with the delivered prompts to enhance the discrimination of the modality-specific output. Additionally, a coordinator is designed to dynamically re-weight the modality outputs as a complement to the balance strategy to improve the model's efficacy. Extensive experiments on 4 datasets with 7 SOTA methods under different missing rates validate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PersonaLive! Expressive Portrait Image Animation for Live Streaming</title>
<link>https://arxiv.org/abs/2512.11253</link>
<guid>https://arxiv.org/abs/2512.11253</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion-based portrait animation, real-time performance, hybrid implicit signals, appearance distillation, autoregressive streaming generation

<br /><br />Summary:  
This paper introduces PersonaLive, a novel diffusion-based framework designed for streaming real-time portrait animation, addressing limitations in generation latency and real-time usability seen in previous models. The method utilizes hybrid implicit signals—combining implicit facial representations and 3D implicit keypoints—to enable expressive and precise image-level motion control. To improve inference speed, the authors propose a fewer-step appearance distillation strategy that reduces redundancy during the denoising process, significantly enhancing efficiency. PersonaLive also features an autoregressive micro-chunk streaming generation approach, integrating a sliding training strategy and a historical keyframe mechanism to achieve low-latency and stable long-term video generation. Extensive experimental results demonstrate that PersonaLive considerably outperforms prior diffusion-based portrait animation models, offering speedups ranging from 7 to 22 times while maintaining state-of-the-art performance. This work significantly advances the practical application of diffusion-based portrait animation in live streaming scenarios by balancing quality, realism, and real-time efficiency. <div>
arXiv:2512.11253v1 Announce Type: new 
Abstract: Current diffusion-based portrait animation models predominantly focus on enhancing visual quality and expression realism, while overlooking generation latency and real-time performance, which restricts their application range in the live streaming scenario. We propose PersonaLive, a novel diffusion-based framework towards streaming real-time portrait animation with multi-stage training recipes. Specifically, we first adopt hybrid implicit signals, namely implicit facial representations and 3D implicit keypoints, to achieve expressive image-level motion control. Then, a fewer-step appearance distillation strategy is proposed to eliminate appearance redundancy in the denoising process, greatly improving inference efficiency. Finally, we introduce an autoregressive micro-chunk streaming generation paradigm equipped with a sliding training strategy and a historical keyframe mechanism to enable low-latency and stable long-term video generation. Extensive experiments demonstrate that PersonaLive achieves state-of-the-art performance with up to 7-22x speedup over prior diffusion-based portrait animation models.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do We Need Reformer for Vision? An Experimental Comparison with Vision Transformers</title>
<link>https://arxiv.org/abs/2512.11260</link>
<guid>https://arxiv.org/abs/2512.11260</guid>
<content:encoded><![CDATA[
<div> Transformers, Vision Transformers, Reformer, Locality-Sensitive Hashing, Computational Efficiency  

<br /><br />Summary:  
1. The paper explores the use of the Reformer architecture as an alternative backbone for vision tasks, aiming to address the high computational cost of standard Vision Transformers (ViTs) due to quadratic scaling of global self-attention.  
2. The Reformer model integrates patch-based tokenization with locality-sensitive hashing (LSH) attention to approximate global self-attention, reducing theoretical time complexity from 𝒪(n²) to 𝒪(n log n), where n is the sequence length.  
3. The model is evaluated on three datasets: CIFAR-10 for small-scale experimentation, ImageNet-100 for assessing accuracy-efficiency trade-offs under more typical settings, and a high-resolution medical imaging dataset to test performance with longer token sequences.  
4. Results show the Reformer achieves better accuracy than a ViT-style baseline on CIFAR-10, indicating promise on smaller datasets.  
5. However, in larger and higher-resolution experiments, the standard ViT outperforms the Reformer in practical efficiency and overall computation time. This suggests that the theoretical benefits of LSH-based attention methods like the Reformer are only realized when dealing with significantly longer sequences than those encountered in most high-resolution images. <div>
arXiv:2512.11260v1 Announce Type: new 
Abstract: Transformers have recently demonstrated strong performance in computer vision, with Vision Transformers (ViTs) leveraging self-attention to capture both low-level and high-level image features. However, standard ViTs remain computationally expensive, since global self-attention scales quadratically with the number of tokens, which limits their practicality for high-resolution inputs and resource-constrained settings.
  In this work, we investigate the Reformer architecture as an alternative vision backbone. By combining patch-based tokenization with locality-sensitive hashing (LSH) attention, our model approximates global self-attention while reducing its theoretical time complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ in the sequence length $n$. We evaluate the proposed Reformer-based vision model on CIFAR-10 to assess its behavior on small-scale datasets, on ImageNet-100 to study its accuracy--efficiency trade-off in a more realistic setting, and on a high-resolution medical imaging dataset to evaluate the model under longer token sequences.
  While the Reformer achieves higher accuracy on CIFAR-10 compared to our ViT-style baseline, the ViT model consistently outperforms the Reformer in our experiments in terms of practical efficiency and end-to-end computation time across the larger and higher-resolution settings. These results suggest that, despite the theoretical advantages of LSH-based attention, meaningful computation gains require sequence lengths substantially longer than those produced by typical high-resolution images.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Efficacy of Sentinel-2 versus Aerial Imagery in Serrated Tussock Classification</title>
<link>https://arxiv.org/abs/2512.11267</link>
<guid>https://arxiv.org/abs/2512.11267</guid>
<content:encoded><![CDATA[
<div> Invasive species, Serrated tussock, Sentinel-2 imagery, Remote sensing, Random forest<br /><br />Summary:<br /><br />1. Invasive species like Serrated tussock (Nassella trichotoma) severely threaten ecosystems and agriculture by disrupting native grasslands and reducing pasture productivity, with notable impacts in Victoria, Australia.<br />2. Traditional ground surveys help manage infestations at small scales but are inadequate for landscape-scale monitoring due to feasibility constraints.<br />3. Aerial imagery, while offering high spatial resolution necessary for detailed classification, poses cost and scalability challenges.<br />4. Satellite-based remote sensing, especially using Sentinel-2 images with higher spectral resolution and seasonal phenological data, offers a more cost-effective and scalable alternative despite lower spatial resolution.<br />5. The study develops eleven models combining spectral bands, texture features, vegetation indices, and seasonal data to classify Serrated tussock.<br />6. Using a random forest classifier, the best Sentinel-2 model (M76*) achieved 68% Overall Accuracy and 0.55 Overall Kappa, surpassing the best aerial imaging model’s 67% OA and 0.52 OK on the same data.<br />7. Results demonstrate the effectiveness of multi-temporal, feature-enhanced satellite imagery for scalable, landscape-scale invasive species classification, presenting a promising tool for ecological monitoring and management. <div>
arXiv:2512.11267v1 Announce Type: new 
Abstract: Invasive species pose major global threats to ecosystems and agriculture. Serrated tussock (\textit{Nassella trichotoma}) is a highly competitive invasive grass species that disrupts native grasslands, reduces pasture productivity, and increases land management costs. In Victoria, Australia, it presents a major challenge due to its aggressive spread and ecological impact. While current ground surveys and subsequent management practices are effective at small scales, they are not feasible for landscape-scale monitoring. Although aerial imagery offers high spatial resolution suitable for detailed classification, its high cost limits scalability. Satellite-based remote sensing provides a more cost-effective and scalable alternative, though often with lower spatial resolution. This study evaluates whether multi-temporal Sentinel-2 imagery, despite its lower spatial resolution, can provide a comparable and cost-effective alternative for landscape-scale monitoring of serrated tussock by leveraging its higher spectral resolution and seasonal phenological information. A total of eleven models have been developed using various combinations of spectral bands, texture features, vegetation indices, and seasonal data. Using a random forest classifier, the best-performing Sentinel-2 model (M76*) has achieved an Overall Accuracy (OA) of 68\% and an Overall Kappa (OK) of 0.55, slightly outperforming the best-performing aerial imaging model's OA of 67\% and OK of 0.52 on the same dataset. These findings highlight the potential of multi-seasonal feature-enhanced satellite-based models for scalable invasive species classification.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion</title>
<link>https://arxiv.org/abs/2512.11274</link>
<guid>https://arxiv.org/abs/2512.11274</guid>
<content:encoded><![CDATA[
<div> FilmWeaver, multi-shot video generation, inter-shot consistency, autoregressive diffusion, dual-level cache<br /><br />Summary:  
The paper presents FilmWeaver, a new framework addressing the challenges of generating multi-shot videos with consistent characters and backgrounds over arbitrary lengths and shot counts. The approach uses an autoregressive diffusion paradigm to enable the generation of videos of any length flexibly. A crucial innovation is the decoupling of the video consistency problem into inter-shot consistency and intra-shot coherence. This is achieved by a dual-level cache system: the shot memory stores keyframes from previous shots to maintain scene and character identity, while the temporal memory holds frame history from the current shot to ensure smooth and continuous motion. FilmWeaver supports multi-round user interaction, allowing flexible creation of multi-shot videos. Its design also allows for additional capabilities such as multi-concept injection and video extension, demonstrating high versatility. To support training, the authors developed a comprehensive pipeline to create a high-quality multi-shot video dataset. Experimental results show that FilmWeaver outperforms existing methods in both consistency and aesthetic quality metrics, enabling more controllable, consistent, and narrative-driven video content creation. The project page is available at https://filmweaver.github.io. <div>
arXiv:2512.11274v1 Announce Type: new 
Abstract: Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RcAE: Recursive Reconstruction Framework for Unsupervised Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.11284</link>
<guid>https://arxiv.org/abs/2512.11284</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised anomaly detection, recursive autoencoder, cross recursion detection, detail preservation, industrial defects<br /><br />Summary:<br /><br />This paper addresses the challenge of unsupervised industrial anomaly detection, focusing on identifying defects without relying on labeled data. Traditional autoencoder-based approaches fall short due to their single-pass decoding, which often leads to incomplete anomaly suppression and loss of fine details. To overcome this, the authors propose a Recursive Autoencoder (RcAE) architecture that iteratively reconstructs input images, progressively suppressing anomalies while refining normal patterns. This recursive process generates a sequence of reconstructions that reveal abnormalities obscured in earlier passes. To exploit this dynamic, a Cross Recursion Detection (CRD) module is introduced, which monitors inconsistencies across recursive steps to improve detection of both subtle and large-scale defects. Furthermore, a Detail Preservation Network (DPN) is integrated to recover fine textures typically lost during reconstruction. Experimental results show that the proposed method significantly outperforms existing non-diffusion based models and matches the performance of state-of-the-art diffusion models, but with only 10% of their parameters and much faster inference speeds. These advantages demonstrate the approach's practical efficiency and effectiveness for real-world unsupervised industrial anomaly detection tasks. <div>
arXiv:2512.11284v1 Announce Type: new 
Abstract: Unsupervised industrial anomaly detection requires accurately identifying defects without labeled data. Traditional autoencoder-based methods often struggle with incomplete anomaly suppression and loss of fine details, as their single-pass decoding fails to effectively handle anomalies with varying severity and scale. We propose a recursive architecture for autoencoder (RcAE), which performs reconstruction iteratively to progressively suppress anomalies while refining normal structures. Unlike traditional single-pass models, this recursive design naturally produces a sequence of reconstructions, progressively exposing suppressed abnormal patterns. To leverage this reconstruction dynamics, we introduce a Cross Recursion Detection (CRD) module that tracks inconsistencies across recursion steps, enhancing detection of both subtle and large-scale anomalies. Additionally, we incorporate a Detail Preservation Network (DPN) to recover high-frequency textures typically lost during reconstruction. Extensive experiments demonstrate that our method significantly outperforms existing non-diffusion methods, and achieves performance on par with recent diffusion models with only 10% of their parameters and offering substantially faster inference. These results highlight the practicality and efficiency of our approach for real-world applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Video Autoencoder with Decoupled Temporal and Spatial Context</title>
<link>https://arxiv.org/abs/2512.11293</link>
<guid>https://arxiv.org/abs/2512.11293</guid>
<content:encoded><![CDATA[
<div> Keywords: video autoencoder, temporal-spatial decoupling, autoregressive model, video compression, video generation<br /><br />Summary: The paper proposes a novel Autoregressive Video Autoencoder (ARVAE) aimed at improving video compression and reconstruction by addressing the limitations of existing video autoencoders that entangle spatial and temporal information. ARVAE compresses and reconstructs each video frame conditioned on its predecessor in an autoregressive manner, enabling flexible handling of videos with arbitrary lengths. The model introduces a temporal-spatial decoupled representation that combines a downsampled flow field to maintain temporal coherence with spatial relative compensation to capture newly emerged content, enhancing compression efficiency without information loss. The encoder compresses the current and previous frames into a temporal motion component and a spatial supplement, respectively, while the decoder uses these latent representations along with the preceding frame to reconstruct the original frame. A multi-stage training strategy is employed to progressively improve model performance. Extensive experiments demonstrate that ARVAE surpasses previous methods in reconstruction quality while relying on lightweight models and small-scale training data. Furthermore, evaluations on downstream video generation tasks illustrate the strong potential of ARVAE for practical applications in video synthesis and processing. <div>
arXiv:2512.11293v1 Announce Type: new 
Abstract: Video autoencoders compress videos into compact latent representations for efficient reconstruction, playing a vital role in enhancing the quality and efficiency of video generation. However, existing video autoencoders often entangle spatial and temporal information, limiting their ability to capture temporal consistency and leading to suboptimal performance. To address this, we propose Autoregressive Video Autoencoder (ARVAE), which compresses and reconstructs each frame conditioned on its predecessor in an autoregressive manner, allowing flexible processing of videos with arbitrary lengths. ARVAE introduces a temporal-spatial decoupled representation that combines downsampled flow field for temporal coherence with spatial relative compensation for newly emerged content, achieving high compression efficiency without information loss. Specifically, the encoder compresses the current and previous frames into the temporal motion and spatial supplement, while the decoder reconstructs the original frame from the latent representations given the preceding frame. A multi-stage training strategy is employed to progressively optimize the model. Extensive experiments demonstrate that ARVAE achieves superior reconstruction quality with extremely lightweight models and small-scale training data. Moreover, evaluations on video generation tasks highlight its strong potential for downstream applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining</title>
<link>https://arxiv.org/abs/2512.11296</link>
<guid>https://arxiv.org/abs/2512.11296</guid>
<content:encoded><![CDATA[
<div> Keywords: G-code verification, CNC machining, Human-Machine Interface, Vision-Language Models, few-shot learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of verifying manually generated G-code used in CNC machines, recognizing that prior verification methods rely heavily on Large Language Models that only analyze written code.<br /><br />2. It highlights the limitation of LLMs in incorporating information from the Human-Machine Interface (HMI), which visually displays machine status and errors crucial for CNC operations.<br /><br />3. The authors propose a novel few-shot learning approach using Vision-Language Models (VLMs) that can simultaneously analyze the G-code text and paired HMI screenshots from a 15-slant-PRO lathe to detect errors and safety issues.<br /><br />4. Their input dataset consists of both error-free and error-prone paired samples, and the VLM is guided via a structured JSON schema derived from heuristic knowledge to improve understanding.<br /><br />5. Experimental results demonstrate that few-shot prompting of the VLM improves detection accuracy of discrepancies between G-code and HMI errors over zero-shot methods, making their framework effective for comprehensive debugging of CNC training-generated G-code. <div>
arXiv:2512.11296v1 Announce Type: new 
Abstract: Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiEgo: A Multi-View Egocentric Video Dataset for 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2512.11301</link>
<guid>https://arxiv.org/abs/2512.11301</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view egocentric, dynamic scene reconstruction, AR glasses, temporal synchronization, free-viewpoint video  

<br /><br />Summary:  
This paper introduces MultiEgo, the first dataset designed specifically for multi-view egocentric dynamic scene reconstruction, addressing a gap in existing datasets that mainly focus on static multi-view or single-egocentric setups. The dataset captures five canonical social interaction scenes, including meetings, performances, and presentations, each recorded from five participants wearing augmented reality (AR) glasses. A custom hardware-based data acquisition system and processing pipeline ensure sub-millisecond temporal synchronization across multiple viewpoints while providing accurate pose annotations. The dataset's design supports high-fidelity reconstruction and analysis of 4D dynamic social scenes from multiple egocentric perspectives. Experimentation validates MultiEgo’s practical utility and effectiveness for free-viewpoint video (FVV) applications, highlighting its potential to advance research in holographic documentation and realistic scene rendering. MultiEgo thus establishes itself as a foundational resource for the research community focused on multi-view egocentric dynamic scene reconstruction, enabling novel explorations in social interaction holography and immersive media technologies. <div>
arXiv:2512.11301v1 Announce Type: new 
Abstract: Multi-view egocentric dynamic scene reconstruction holds significant research value for applications in holographic documentation of social interactions. However, existing reconstruction datasets focus on static multi-view or single-egocentric view setups, lacking multi-view egocentric datasets for dynamic scene reconstruction. Therefore, we present MultiEgo, the first multi-view egocentric dataset for 4D dynamic scene reconstruction. The dataset comprises five canonical social interaction scenes: meetings, performances, and a presentation. Each scene provides five authentic egocentric videos captured by participants wearing AR glasses. We design a hardware-based data acquisition system and processing pipeline, achieving sub-millisecond temporal synchronization across views, coupled with accurate pose annotations. Experiment validation demonstrates the practical utility and effectiveness of our dataset for free-viewpoint video (FVV) applications, establishing MultiEgo as a foundational resource for advancing multi-view egocentric dynamic scene reconstruction research.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SATMapTR: Satellite Image Enhanced Online HD Map Construction</title>
<link>https://arxiv.org/abs/2512.11319</link>
<guid>https://arxiv.org/abs/2512.11319</guid>
<content:encoded><![CDATA[
<div> Keywords: HD maps, satellite images, feature fusion, autonomous driving, nuScenes dataset<br /><br />Summary:<br /><br />1. The paper addresses challenges in real-time HD map construction for autonomous driving, specifically the issues caused by low-quality input data from onboard sensors, such as incomplete, noisy, or missing information due to limited capabilities and occlusions.<br /><br />2. Satellite images provide a complementary, stable, wide-area perspective but suffer from degradation due to shadows and occlusions from vegetation and buildings, limiting their direct use in Bird’s Eye View (BEV) mapping.<br /><br />3. The authors propose SATMapTR, a novel online map construction model that effectively incorporates satellite images by introducing two main components: a gated feature refinement module that adaptively filters satellite features using high-level semantic and low-level structural information to enhance signal-to-noise ratio of map-relevant features.<br /><br />4. The second component is a geometry-aware fusion module that fuses satellite and BEV features at a precise grid-to-grid level, reducing interference from irrelevant or degraded regions, improving map accuracy and robustness.<br /><br />5. Experimental results on the nuScenes dataset demonstrate SATMapTR’s superior performance with a mean average precision (mAP) of 73.8, outperforming previous satellite-enhanced models by up to 14.2 mAP, showing robustness under adverse weather and sensor failure conditions, and achieving nearly threefold higher mAP at extended perception ranges. <div>
arXiv:2512.11319v1 Announce Type: new 
Abstract: High-definition (HD) maps are evolving from pre-annotated to real-time construction to better support autonomous driving in diverse scenarios. However, this process is hindered by low-quality input data caused by onboard sensors limited capability and frequent occlusions, leading to incomplete, noisy, or missing data, and thus reduced mapping accuracy and robustness. Recent efforts have introduced satellite images as auxiliary input, offering a stable, wide-area view to complement the limited ego perspective. However, satellite images in Bird's Eye View are often degraded by shadows and occlusions from vegetation and buildings. Prior methods using basic feature extraction and fusion remain ineffective. To address these challenges, we propose SATMapTR, a novel online map construction model that effectively fuses satellite image through two key components: (1) a gated feature refinement module that adaptively filters satellite image features by integrating high-level semantics with low-level structural cues to extract high signal-to-noise ratio map-relevant representations; and (2) a geometry-aware fusion module that consistently fuse satellite and BEV features at a grid-to-grid level, minimizing interference from irrelevant regions and low-quality inputs. Experimental results on the nuScenes dataset show that SATMapTR achieves the highest mean average precision (mAP) of 73.8, outperforming state-of-the-art satellite-enhanced models by up to 14.2 mAP. It also shows lower mAP degradation under adverse weather and sensor failures, and achieves nearly 3 times higher mAP at extended perception ranges.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeyframeFace: From Text to Expressive Facial Keyframes</title>
<link>https://arxiv.org/abs/2512.11321</link>
<guid>https://arxiv.org/abs/2512.11321</guid>
<content:encoded><![CDATA[
<div> Keywords: dynamic 3D facial animation, text-to-animation, KeyframeFace dataset, Large Language Models, ARKit coefficients<br /><br />Summary:  
Generating dynamic 3D facial animations from natural language input requires understanding both the temporal semantics and fine-grained facial expression changes. Current datasets and approaches generally focus on speech-driven animation or lack structured semantic and temporal guidance, limiting expressive and human-like animation outputs. To address these limitations, the authors introduce KeyframeFace, a large-scale multimodal dataset tailored for text-to-animation tasks with keyframe-level supervision. KeyframeFace includes 2,100 expressive scripts paired with monocular videos, frame-by-frame ARKit facial coefficients, contextual backgrounds, complex emotional expressions, manually annotated keyframes, and multi-perspective annotations generated using Large Language Models (LLMs) and Multimodal LLMs. Building upon this dataset, the authors propose the first text-to-animation framework that explicitly integrates prior knowledge from LLMs to guide interpretable facial motion synthesis. This approach exploits the semantic understanding capability of LLMs combined with the interpretability and temporal structure embedded in ARKit coefficients, enabling the generation of high-fidelity, expressive facial animations. Together, the KeyframeFace dataset and the LLM-based framework establish a new foundation for interpretable, keyframe-guided, and context-aware text-driven 3D facial animation. The code and dataset have been made publicly available for further research and development. <div>
arXiv:2512.11321v1 Announce Type: new 
Abstract: Generating dynamic 3D facial animation from natural language requires understanding both temporally structured semantics and fine-grained expression changes. Existing datasets and methods mainly focus on speech-driven animation or unstructured expression sequences and therefore lack the semantic grounding and temporal structures needed for expressive human performance generation. In this work, we introduce KeyframeFace, a large-scale multimodal dataset designed for text-to-animation research through keyframe-level supervision. KeyframeFace provides 2,100 expressive scripts paired with monocular videos, per-frame ARKit coefficients, contextual backgrounds, complex emotions, manually defined keyframes, and multi-perspective annotations based on ARKit coefficients and images via Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs). Beyond the dataset, we propose the first text-to-animation framework that explicitly leverages LLM priors for interpretable facial motion synthesis. This design aligns the semantic understanding capabilities of LLMs with the interpretable structure of ARKit's coefficients, enabling high-fidelity expressive animation. KeyframeFace and our LLM-based framework together establish a new foundation for interpretable, keyframe-guided, and context-aware text-to-animation. Code and data are available at https://github.com/wjc12345123/KeyframeFace.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MLLM Machine Unlearning via Visual Knowledge Distillation</title>
<link>https://arxiv.org/abs/2512.11325</link>
<guid>https://arxiv.org/abs/2512.11325</guid>
<content:encoded><![CDATA[
<div> Keywords: machine unlearning, multi-modal large models (MLLM), visual knowledge distillation, selective unlearning, model robustness  

<br /><br />Summary:  
This paper addresses the challenge of machine unlearning specifically for multi-modal large language models (MLLMs), which combine both visual and textual knowledge. Unlike most existing unlearning methods designed for large language models (LLMs), this work focuses on the underexplored area of MLLM unlearning. The authors propose a novel approach that disentangles visual and textual knowledge within MLLMs, allowing selective erasure of target visual knowledge while preserving textual information. Their method introduces a Visual Knowledge Distillation (VKD) scheme, which uses intermediate visual representations in the MLLM as supervision signals, contrasting with previous output-level supervision approaches. This leads to improved unlearning effectiveness and better utility of the model after unlearning. Additionally, the approach only fine-tunes visual components of the MLLM, resulting in notable efficiency gains during the unlearning process. Extensive experiments demonstrate that this method outperforms current state-of-the-art unlearning techniques on both effectiveness and efficiency metrics. Lastly, the paper is the first to evaluate the robustness of MLLM unlearning methods against relearning attacks, highlighting the security and reliability of their approach in practical settings. <div>
arXiv:2512.11325v1 Announce Type: new 
Abstract: Recently, machine unlearning approaches have been proposed to remove sensitive information from well-trained large models. However, most existing methods are tailored for LLMs, while MLLM-oriented unlearning remains at its early stage. Inspired by recent studies exploring the internal mechanisms of MLLMs, we propose to disentangle the visual and textual knowledge embedded within MLLMs and introduce a dedicated approach to selectively erase target visual knowledge while preserving textual knowledge. Unlike previous unlearning methods that rely on output-level supervision, our approach introduces a Visual Knowledge Distillation (VKD) scheme, which leverages intermediate visual representations within the MLLM as supervision signals. This design substantially enhances both unlearning effectiveness and model utility. Moreover, since our method only fine-tunes the visual components of the MLLM, it offers significant efficiency advantages. Extensive experiments demonstrate that our approach outperforms state-of-the-art unlearning methods in terms of both effectiveness and efficiency. Moreover, we are the first to evaluate the robustness of MLLM unlearning against relearning attacks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Video Flare Synthesis and Removal Leveraging Motion Independence between Flare and Scene</title>
<link>https://arxiv.org/abs/2512.11327</link>
<guid>https://arxiv.org/abs/2512.11327</guid>
<content:encoded><![CDATA[
<div> Lens flare, video restoration, spatiotemporal modeling, dynamic synthesis, attention mechanism<br /><br />Summary:<br /><br />1. The paper addresses the challenge of removing lens flare in videos, a degradation caused by strong light sources, which is more complex than in images due to independent motion between flare, light sources, and scene content. 2. It introduces a physics-informed dynamic flare synthesis pipeline that simulates light source motion with optical flow and models temporal behaviors of scattering and reflective flares, capturing their dynamic nature accurately. 3. A novel video flare removal network is designed, incorporating an attention module to spatially suppress flare regions and a Mamba-based temporal modeling component that captures long-range spatiotemporal dependencies without requiring multi-frame alignment. 4. To evaluate their method, the authors build the first comprehensive video flare dataset, including synthetic paired videos and real-world videos sourced from the internet to test generalization capabilities. 5. Extensive experiments demonstrate that the proposed approach significantly outperforms existing video-based and image-based flare removal techniques by effectively removing dynamic flares while preserving the integrity of light sources and maintaining spatiotemporal consistency across frames. <div>
arXiv:2512.11327v1 Announce Type: new 
Abstract: Lens flare is a degradation phenomenon caused by strong light sources. Existing researches on flare removal have mainly focused on images, while the spatiotemporal characteristics of video flare remain largely unexplored. Video flare synthesis and removal pose significantly greater challenges than in image, owing to the complex and mutually independent motion of flare, light sources, and scene content. This motion independence further affects restoration performance, often resulting in flicker and artifacts. To address this issue, we propose a physics-informed dynamic flare synthesis pipeline, which simulates light source motion using optical flow and models the temporal behaviors of both scattering and reflective flares. Meanwhile, we design a video flare removal network that employs an attention module to spatially suppress flare regions and incorporates a Mamba-based temporal modeling component to capture long range spatio-temporal dependencies. This motion-independent spatiotemporal representation effectively eliminates the need for multi-frame alignment, alleviating temporal aliasing between flares and scene content and thereby improving video flare removal performance. Building upon this, we construct the first video flare dataset to comprehensively evaluate our method, which includes a large set of synthetic paired videos and additional real-world videos collected from the Internet to assess generalization capability. Extensive experiments demonstrate that our method consistently outperforms existing video-based restoration and image-based flare removal methods on both real and synthetic videos, effectively removing dynamic flares while preserving light source integrity and maintaining spatiotemporal consistency of scene.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreqDINO: Frequency-Guided Adaptation for Generalized Boundary-Aware Ultrasound Image Segmentation</title>
<link>https://arxiv.org/abs/2512.11335</link>
<guid>https://arxiv.org/abs/2512.11335</guid>
<content:encoded><![CDATA[
<div> Keywords: Ultrasound segmentation, frequency-guided, boundary refinement, multi-scale extraction, deep learning<br /><br />Summary:<br /><br />1. Ultrasound image segmentation is crucial for clinical diagnosis but is challenged by speckle noise and imaging artifacts that degrade boundary clarity.<br />2. DINOv3, a powerful medical image segmentation model pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation.<br />3. The proposed FreqDINO framework addresses this limitation by incorporating frequency-guided strategies to enhance boundary perception and structural consistency.<br />4. FreqDINO introduces a Multi-scale Frequency Extraction and Alignment (MFEA) module that separates low-frequency structural information from high-frequency boundary details and aligns them through learnable attention mechanisms.<br />5. A Frequency-Guided Boundary Refinement (FGBR) module is designed to extract boundary prototypes from high-frequency components, refining spatial features.<br />6. Furthermore, a Multi-task Boundary-Guided Decoder (MBGD) ensures spatial coherence between boundary and semantic segmentation predictions.<br />7. Extensive experiments demonstrate that FreqDINO outperforms state-of-the-art ultrasound segmentation methods, showing superior accuracy and generalization.<br />8. The code for FreqDINO is publicly available at https://github.com/MingLang-FD/FreqDINO. <div>
arXiv:2512.11335v1 Announce Type: new 
Abstract: Ultrasound image segmentation is pivotal for clinical diagnosis, yet challenged by speckle noise and imaging artifacts. Recently, DINOv3 has shown remarkable promise in medical image segmentation with its powerful representation capabilities. However, DINOv3, pre-trained on natural images, lacks sensitivity to ultrasound-specific boundary degradation. To address this limitation, we propose FreqDINO, a frequency-guided segmentation framework that enhances boundary perception and structural consistency. Specifically, we devise a Multi-scale Frequency Extraction and Alignment (MFEA) strategy to separate low-frequency structures and multi-scale high-frequency boundary details, and align them via learnable attention. We also introduce a Frequency-Guided Boundary Refinement (FGBR) module that extracts boundary prototypes from high-frequency components and refines spatial features. Furthermore, we design a Multi-task Boundary-Guided Decoder (MBGD) to ensure spatial coherence between boundary and semantic predictions. Extensive experiments demonstrate that FreqDINO surpasses state-of-the-art methods with superior achieves remarkable generalization capability. The code is at https://github.com/MingLang-FD/FreqDINO.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UFVideo: Towards Unified Fine-Grained Video Cooperative Understanding with Large Language Models</title>
<link>https://arxiv.org/abs/2512.11336</link>
<guid>https://arxiv.org/abs/2512.11336</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-grained video understanding, Video LLM, Unified visual-language alignment, UFVideo-Bench, Temporal localization  

<br /><br />Summary: With advancements in multi-modal Large Language Models (LLMs), video-specific LLMs have emerged but remain limited to specialized tasks without achieving comprehensive, multi-grained video understanding. To address this, the paper introduces UFVideo, the first Video LLM designed for unified multi-grained cooperative understanding across global, pixel, and temporal scales within a single model. UFVideo employs a unified visual-language guided alignment approach, enabling flexible handling of video inputs and tasks such as textual response generation, temporal localization, and grounded mask prediction. The model dynamically encodes visual and text inputs depending on the task requirements. To rigorously assess UFVideo’s multi-grained understanding capabilities, the authors construct UFVideo-Bench, a benchmark comprising three distinct collaborative tasks spanning different scales, demonstrating the model’s adaptability and superior performance compared to GPT-4o. Furthermore, the effectiveness of UFVideo is validated across nine public benchmarks covering a variety of common video understanding tasks. The results provide insights and set the groundwork for the development of future Video LLMs, highlighting the importance of unified multi-scale visual-language integration for holistic video perception. <div>
arXiv:2512.11336v1 Announce Type: new 
Abstract: With the advancement of multi-modal Large Language Models (LLMs), Video LLMs have been further developed to perform on holistic and specialized video understanding. However, existing works are limited to specialized video understanding tasks, failing to achieve a comprehensive and multi-grained video perception. To bridge this gap, we introduce UFVideo, the first Video LLM with unified multi-grained cooperative understanding capabilities. Specifically, we design unified visual-language guided alignment to flexibly handle video understanding across global, pixel and temporal scales within a single model. UFVideo dynamically encodes the visual and text inputs of different tasks and generates the textual response, temporal localization, or grounded mask. Additionally, to evaluate challenging multi-grained video understanding tasks, we construct the UFVideo-Bench consisting of three distinct collaborative tasks within the scales, which demonstrates UFVideo's flexibility and advantages over GPT-4o. Furthermore, we validate the effectiveness of our model across 9 public benchmarks covering various common video understanding tasks, providing valuable insights for future Video LLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Specific Distance Correlation Matching for Few-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2512.11340</link>
<guid>https://arxiv.org/abs/2512.11340</guid>
<content:encoded><![CDATA[
<div> Few-shot action recognition, CLIP fine-tuning, distance correlation, task-specific matching, Ladder Side Network  

<br /><br />Summary:  
This paper addresses key limitations in few-shot action recognition (FSAR). First, it critiques existing set matching methods that rely on cosine similarity, which capture only linear inter-frame relationships and use instance-level information, thus missing nonlinear dependencies and task-specific cues. Second, it notes that while fine-tuning CLIP with skip-fusion (side) layers reduces memory usage, these layers are difficult to optimize with limited training data. To overcome these issues, the authors propose TS-FSAR, a novel framework with three key components. The Visual Ladder Side Network (LSN) enables efficient fine-tuning of CLIP, improving adaptation under few-shot conditions. The Task-Specific Distance Correlation Matching (TS-DCM) metric uses α-distance correlation to model both linear and nonlinear dependencies between video frames and incorporates task prototypes for tailored matching. The Guiding LSN with Adapted CLIP (GLAC) module regularizes LSN training by leveraging adapted frozen CLIP features to improve α-distance correlation estimation despite limited supervision. Experiments on five standard benchmarks demonstrate that TS-FSAR surpasses prior state-of-the-art methods, showing the effectiveness of the proposed components in improving few-shot action recognition performance. <div>
arXiv:2512.11340v1 Announce Type: new 
Abstract: Few-shot action recognition (FSAR) has recently made notable progress through set matching and efficient adaptation of large-scale pre-trained models. However, two key limitations persist. First, existing set matching metrics typically rely on cosine similarity to measure inter-frame linear dependencies and then perform matching with only instance-level information, thus failing to capture more complex patterns such as nonlinear relationships and overlooking task-specific cues. Second, for efficient adaptation of CLIP to FSAR, recent work performing fine-tuning via skip-fusion layers (which we refer to as side layers) has significantly reduced memory cost. However, the newly introduced side layers are often difficult to optimize under limited data conditions. To address these limitations, we propose TS-FSAR, a framework comprising three components: (1) a visual Ladder Side Network (LSN) for efficient CLIP fine-tuning; (2) a metric called Task-Specific Distance Correlation Matching (TS-DCM), which uses $\alpha$-distance correlation to model both linear and nonlinear inter-frame dependencies and leverages a task prototype to enable task-specific matching; and (3) a Guiding LSN with Adapted CLIP (GLAC) module, which regularizes LSN using the adapted frozen CLIP to improve training for better $\alpha$-distance correlation estimation under limited supervision. Extensive experiments on five widely-used benchmarks demonstrate that our TS-FSAR yields superior performance compared to prior state-of-the-arts.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture</title>
<link>https://arxiv.org/abs/2512.11350</link>
<guid>https://arxiv.org/abs/2512.11350</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic accident detection, transformer architecture, motion cues, optical flow, video features<br /><br />Summary:<br />Road traffic accidents are a major cause of global mortality, with increasing rates driven by population growth, urbanization, and motorization, raising concerns about current traffic surveillance systems. Traditional computer vision methods face challenges due to their limited understanding of spatiotemporal dynamics and poor ability to generalize across different domains. Transformer architectures have shown promise in modeling complex spatial-temporal dependencies and support efficient parallel processing, but their application in automated traffic accident detection is hindered by the lack of large, diverse datasets. To address this, the authors curated a comprehensive and balanced dataset encompassing various traffic environments, accident types, and contextual situations. Using this dataset, a novel transformer-based accident detection model was developed, combining convolutional layers for extracting local spatial correlations within video frames and transformers to capture sequential-temporal relationships across frames. Importantly, the study highlights the critical role of motion cues in understanding dynamic scenes, especially during accidents, which are often overlooked in previous work relying on static or coarse temporal features. Multiple methods for integrating motion information were tested, with the best results achieved by concatenating RGB video features with optical flow data, yielding an accuracy of 88.3%. The proposed model's performance was also benchmarked against vision-language models like GPT, Gemini, and LLaVA-NeXT-Video, demonstrating its effectiveness in accident detection tasks. <div>
arXiv:2512.11350v1 Announce Type: new 
Abstract: Road traffic accidents represent a leading cause of mortality globally, with incidence rates rising due to increasing population, urbanization, and motorization. Rising accident rates raise concerns about traffic surveillance effectiveness. Traditional computer vision methods for accident detection struggle with limited spatiotemporal understanding and poor cross-domain generalization. Recent advances in transformer architectures excel at modeling global spatial-temporal dependencies and parallel computation. However, applying these models to automated traffic accident detection is limited by small, non-diverse datasets, hindering the development of robust, generalizable systems. To address this gap, we curated a comprehensive and balanced dataset that captures a wide spectrum of traffic environments, accident types, and contextual variations. Utilizing the curated dataset, we propose an accident detection model based on a transformer architecture using pre-extracted spatial video features. The architecture employs convolutional layers to extract local correlations across diverse patterns within a frame, while leveraging transformers to capture sequential-temporal dependencies among the retrieved features. Moreover, most existing studies neglect the integration of motion cues, which are essential for understanding dynamic scenes, especially during accidents. These approaches typically rely on static features or coarse temporal information. In this study, multiple methods for incorporating motion cues were evaluated to identify the most effective strategy. Among the tested input approaches, concatenating RGB features with optical flow achieved the highest accuracy at 88.3%. The results were further compared with vision language models (VLM) such as GPT, Gemini, and LLaVA-NeXT-Video to assess the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection</title>
<link>https://arxiv.org/abs/2512.11354</link>
<guid>https://arxiv.org/abs/2512.11354</guid>
<content:encoded><![CDATA[
<div> Underwater pipelines, structured light 3D imaging, multi-source information fusion, pose estimation, defect detection<br /><br />Summary:<br /><br />This paper addresses the critical issue of corrosion in underwater pipelines by developing an intelligent real-time imaging system for underwater pipeline detection called the UW-SLD system. The system leverages structured light 3D imaging to capture detailed spatial information necessary for precise defect characterization. To process underwater images efficiently, a fast distortion correction (FDC) method is introduced for image rectification. The authors tackle the difficulty of extrinsic calibration between underwater sensors by proposing a factor graph-based parameter optimization to accurately estimate the transformation between structured light and acoustic sensors. A multi-mode 3D imaging strategy is designed to handle variations in pipeline geometry. To counter disturbances common in underwater environments, a multi-source information fusion strategy combined with an adaptive extended Kalman filter (AEKF) is employed, ensuring stable pose estimation and accurate measurements. The paper also introduces an edge detection-based ICP (ED-ICP) algorithm that integrates a pipeline edge detection network with enhanced point cloud registration, enabling robust reconstruction of defect structures under variable motion. Extensive experiments demonstrate the system’s superior accuracy, adaptability, and robustness across different operation modes, speeds, and depths, proving its strong potential for autonomous underwater pipeline inspection. <div>
arXiv:2512.11354v1 Announce Type: new 
Abstract: Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prior-Enhanced Gaussian Splatting for Dynamic Scene Reconstruction from Casual Video</title>
<link>https://arxiv.org/abs/2512.11356</link>
<guid>https://arxiv.org/abs/2512.11356</guid>
<content:encoded><![CDATA[
<div> dynamic scene reconstruction, monocular RGB video, Dynamic Gaussian Splatting, video segmentation, skeleton-based sampling  

<br /><br />Summary:  
This paper proposes a fully automatic pipeline aimed at reconstructing dynamic scenes from casually captured monocular RGB videos. Instead of creating a new scene representation, the approach enhances existing priors used in Dynamic Gaussian Splatting. The method begins by combining video segmentation with epipolar-error maps to generate detailed object-level masks that accurately capture thin structures. These masks serve two key roles: guiding an object-depth loss to refine consistent video depth and enabling skeleton-based sampling along with mask-guided re-identification for producing reliable and comprehensive 2-D object tracks. Additionally, the system incorporates two objectives within the reconstruction stage—a virtual-view depth loss designed to eliminate floating artifacts and a scaffold-projection loss that links motion nodes to the 2-D tracks to maintain fine geometric details and coherent object motion. The integration of these refined priors and losses enables the pipeline to outperform previous monocular dynamic scene reconstruction methods. Notably, it achieves visibly superior rendering results, providing enhanced fidelity and more accurate dynamic scene representations from single-camera RGB video input. <div>
arXiv:2512.11356v1 Announce Type: new 
Abstract: We introduce a fully automatic pipeline for dynamic scene reconstruction from casually captured monocular RGB videos. Rather than designing a new scene representation, we enhance the priors that drive Dynamic Gaussian Splatting. Video segmentation combined with epipolar-error maps yields object-level masks that closely follow thin structures; these masks (i) guide an object-depth loss that sharpens the consistent video depth, and (ii) support skeleton-based sampling plus mask-guided re-identification to produce reliable, comprehensive 2-D tracks. Two additional objectives embed the refined priors in the reconstruction stage: a virtual-view depth loss removes floaters, and a scaffold-projection loss ties motion nodes to the tracks, preserving fine geometry and coherent motion. The resulting system surpasses previous monocular dynamic scene reconstruction methods and delivers visibly superior renderings
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Detection of Minute Targets in High-Resolution Aerial Imagery across Temporal Shifts</title>
<link>https://arxiv.org/abs/2512.11360</link>
<guid>https://arxiv.org/abs/2512.11360</guid>
<content:encoded><![CDATA[
<div> Keywords: UAV, crop detection, rice seedlings, Faster R-CNN, transfer learning<br /><br />Summary:<br /><br />1. The paper focuses on efficient crop detection using Unmanned Aerial Vehicles (UAVs), which is crucial for advancing precision agriculture but is challenged by the small size of targets and variable environmental conditions.<br /><br />2. Specifically, it targets the detection of rice seedlings in paddy fields by utilizing a Faster R-CNN architecture enhanced through transfer learning to improve detection accuracy and speed.<br /><br />3. To address the difficulty of detecting small objects in high-resolution aerial images, the authors compiled a large UAV image dataset specially curated for training the model.<br /><br />4. The model’s robustness and generalization capability were rigorously tested across three distinct test sets collected at different times, allowing evaluation of performance under varying imaging conditions.<br /><br />5. Experimental results showed that transfer learning significantly accelerates model convergence in agricultural object detection tasks and provides consistent performance despite domain shifts caused by different image acquisition conditions, demonstrating the method's effectiveness and reliability. <div>
arXiv:2512.11360v1 Announce Type: new 
Abstract: Efficient crop detection via Unmanned Aerial Vehicles is critical for scaling precision agriculture, yet it remains challenging due to the small scale of targets and environmental variability. This paper addresses the detection of rice seedlings in paddy fields by leveraging a Faster R-CNN architecture initialized via transfer learning. To overcome the specific difficulties of detecting minute objects in high-resolution aerial imagery, we curate a significant UAV dataset for training and rigorously evaluate the model's generalization capabilities. Specifically, we validate performance across three distinct test sets acquired at different temporal intervals, thereby assessing robustness against varying imaging conditions. Our empirical results demonstrate that transfer learning not only facilitates the rapid convergence of object detection models in agricultural contexts but also yields consistent performance despite domain shifts in image acquisition.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assisted Refinement Network Based on Channel Information Interaction for Camouflaged and Salient Object Detection</title>
<link>https://arxiv.org/abs/2512.11369</link>
<guid>https://arxiv.org/abs/2512.11369</guid>
<content:encoded><![CDATA[
<div> Camouflaged Object Detection, Channel Information Interaction, Boundary Extraction, Region Extraction, Multi-scale Enhancement<br /><br />Summary:<br /><br />1. The paper addresses challenges in Camouflaged Object Detection (COD), focusing on identifying and segmenting objects that visually blend into backgrounds. <br /><br />2. It identifies two main decoding stage issues: insufficient cross-channel information interaction within same-layer features and ineffective co-modeling of boundary and region information.<br /><br />3. To improve cross-channel interaction, the authors propose the Channel Information Interaction Module (CIIM), which reorganizes and integrates features across channels both horizontally and vertically to capture complementary information.<br /><br />4. To jointly model boundary and region information, they design a collaborative decoding architecture using Boundary Extraction (BE) and Region Extraction (RE) modules to generate boundary priors and object localization maps. This process incorporates hybrid attention for feature calibration to resolve semantic ambiguity and produce sharp boundaries.<br /><br />5. The Multi-scale Enhancement (MSE) module is introduced to enrich contextual feature representations.<br /><br />6. Extensive experiments on four benchmark COD datasets demonstrate state-of-the-art performance and validate the model's effectiveness.<br /><br />7. The model is further applied successfully to related tasks such as Salient Object Detection (SOD), polyp segmentation, transparent object detection, and industrial/road defect detection, proving its adaptability.<br /><br />8. The code and experimental results are publicly accessible at https://github.com/akuan1234/ARNet-v2. <div>
arXiv:2512.11369v1 Announce Type: new 
Abstract: Camouflaged Object Detection (COD) stands as a significant challenge in computer vision, dedicated to identifying and segmenting objects visually highly integrated with their backgrounds. Current mainstream methods have made progress in cross-layer feature fusion, but two critical issues persist during the decoding stage. The first is insufficient cross-channel information interaction within the same-layer features, limiting feature expressiveness. The second is the inability to effectively co-model boundary and region information, making it difficult to accurately reconstruct complete regions and sharp boundaries of objects. To address the first issue, we propose the Channel Information Interaction Module (CIIM), which introduces a horizontal-vertical integration mechanism in the channel dimension. This module performs feature reorganization and interaction across channels to effectively capture complementary cross-channel information. To address the second issue, we construct a collaborative decoding architecture guided by prior knowledge. This architecture generates boundary priors and object localization maps through Boundary Extraction (BE) and Region Extraction (RE) modules, then employs hybrid attention to collaboratively calibrate decoded features, effectively overcoming semantic ambiguity and imprecise boundaries. Additionally, the Multi-scale Enhancement (MSE) module enriches contextual feature representations. Extensive experiments on four COD benchmark datasets validate the effectiveness and state-of-the-art performance of the proposed model. We further transferred our model to the Salient Object Detection (SOD) task and demonstrated its adaptability across downstream tasks, including polyp segmentation, transparent object detection, and industrial and road defect detection. Code and experimental results are publicly available at: https://github.com/akuan1234/ARNet-v2.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Distribution Segmentation via Wasserstein-Based Evidential Uncertainty</title>
<link>https://arxiv.org/abs/2512.11373</link>
<guid>https://arxiv.org/abs/2512.11373</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic segmentation, out-of-distribution, Wasserstein loss, Kullback-Leibler regularization, Dice consistency<br /><br />Summary:<br /><br />1. Deep neural networks (DNNs) have achieved state-of-the-art results in semantic segmentation tasks but face limitations when handling objects not seen during training, referred to as out-of-distribution (OOD) objects. <br />2. The detection and segmentation of OOD objects are critical in applications where safety is paramount, such as autonomous driving, where unknown objects can pose serious risks.<br />3. The paper proposes a novel evidence segmentation framework leveraging a Wasserstein loss function. This loss effectively measures distributional differences by considering the geometry of the probability simplex, offering a more natural and informative way to quantify uncertainty.<br />4. To enhance the robustness and structural accuracy of the segmentation, the framework integrates Kullback-Leibler divergence as a regularization term and incorporates Dice loss-based structural consistency constraints.<br />5. Experimental results demonstrate that the proposed approach surpasses traditional uncertainty-based methods in OOD segmentation performance, indicating better identification and delineation of unknown objects.<br /><br />This work thus contributes a theoretically motivated and practically effective method for improving open-world semantic segmentation by combining advanced distributional metrics and structural regularization techniques. <div>
arXiv:2512.11373v1 Announce Type: new 
Abstract: Deep neural networks achieve superior performance in semantic segmentation, but are limited to a predefined set of classes, which leads to failures when they encounter unknown objects in open-world scenarios. Recognizing and segmenting these out-of-distribution (OOD) objects is crucial for safety-critical applications such as automated driving. In this work, we present an evidence segmentation framework using a Wasserstein loss, which captures distributional distances while respecting the probability simplex geometry. Combined with Kullback-Leibler regularization and Dice structural consistency terms, our approach leads to improved OOD segmentation performance compared to uncertainty-based approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The N-Body Problem: Parallel Execution from Single-Person Egocentric Video</title>
<link>https://arxiv.org/abs/2512.11393</link>
<guid>https://arxiv.org/abs/2512.11393</guid>
<content:encoded><![CDATA[
<div> N-Body Problem, parallelisation, Vision-Language Model, EPIC-Kitchens, task scheduling<br /><br />Summary:<br /><br />This paper introduces the "N-Body Problem" in the context of egocentric video analysis, where the task is to determine how N individuals can collectively perform the same activities observed from a single person's video. The main objective is to maximize speed-up via parallelisation but ensuring the feasibility of such task assignments is challenging due to real-world constraints such as spatial collisions and object usage conflicts. To tackle this, the authors formalise the N-Body Problem and develop a comprehensive set of evaluation metrics that assess both the performance (speed-up and task coverage) and feasibility (spatial collisions, object conflicts, causal constraints) of the parallel execution. They propose a structured prompting strategy to guide a Vision-Language Model (VLM) to comprehend the 3D environment, object interactions, and temporal dependencies to generate physically plausible and efficient multi-agent task schedules. Empirically, on 100 videos from EPIC-Kitchens and HD-EPIC datasets with N=2 agents, their method significantly outperforms baseline approaches based on Gemini 2.5 Pro. It achieves a 45% increase in action coverage while simultaneously reducing collision rates by 55%, object conflicts by 45%, and causal conflicts by 55%, demonstrating improved effectiveness in feasible task parallelisation from a single egocentric video. <div>
arXiv:2512.11393v1 Announce Type: new 
Abstract: Humans can intuitively parallelise complex activities, but can a model learn this from observing a single person? Given one egocentric video, we introduce the N-Body Problem: how N individuals, can hypothetically perform the same set of tasks observed in this video. The goal is to maximise speed-up, but naive assignment of video segments to individuals often violates real-world constraints, leading to physically impossible scenarios like two people using the same object or occupying the same space. To address this, we formalise the N-Body Problem and propose a suite of metrics to evaluate both performance (speed-up, task coverage) and feasibility (spatial collisions, object conflicts and causal constraints). We then introduce a structured prompting strategy that guides a Vision-Language Model (VLM) to reason about the 3D environment, object usage, and temporal dependencies to produce a viable parallel execution. On 100 videos from EPIC-Kitchens and HD-EPIC, our method for N = 2 boosts action coverage by 45% over a baseline prompt for Gemini 2.5 Pro, while simultaneously slashing collision rates, object and causal conflicts by 55%, 45% and 55% respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowDC: Flow-Based Decoupling-Decay for Complex Image Editing</title>
<link>https://arxiv.org/abs/2512.11395</link>
<guid>https://arxiv.org/abs/2512.11395</guid>
<content:encoded><![CDATA[
<div> flow matching, text-to-image editing, complex editing, velocity decomposition, source consistency<br /><br />Summary:<br /><br />This paper addresses the challenge of complex text-based image editing, which involves multiple editing targets. Current solutions fall short: single-round editing struggles with long text input while multi-round editing suffers from cumulative inconsistency, making it difficult to balance semantic alignment and source structure preservation. The authors propose FlowDC, a novel approach that decouples complex edits into multiple sub-editing effects applied in parallel, improving efficiency and consistency. Additionally, they identify that the velocity component orthogonal to the editing displacement negatively impacts source structure preservation. To counter this, FlowDC decomposes the velocity and attenuates the orthogonal part to maintain better source consistency. To effectively evaluate complex editing methods, the authors create a new benchmark named Complex-PIE-Bench. Experimental results on Complex-PIE-Bench and an additional benchmark demonstrate that FlowDC outperforms existing methods in both semantic alignment and source consistency. Furthermore, ablation studies are conducted to analyze and validate the contributions of different modules within FlowDC. The study highlights FlowDC’s ability to meet the escalating demands in complex text-based image editing, advancing toward more accurate and consistent multi-target image modifications. <div>
arXiv:2512.11395v1 Announce Type: new 
Abstract: With the surge of pre-trained text-to-image flow matching models, text-based image editing performance has gained remarkable improvement, especially for \underline{simple editing} that only contains a single editing target. To satisfy the exploding editing requirements, the \underline{complex editing} which contains multiple editing targets has posed as a more challenging task. However, current complex editing solutions: single-round and multi-round editing are limited by long text following and cumulative inconsistency, respectively. Thus, they struggle to strike a balance between semantic alignment and source consistency. In this paper, we propose \textbf{FlowDC}, which decouples the complex editing into multiple sub-editing effects and superposes them in parallel during the editing process. Meanwhile, we observed that the velocity quantity that is orthogonal to the editing displacement harms the source structure preserving. Thus, we decompose the velocity and decay the orthogonal part for better source consistency. To evaluate the effectiveness of complex editing settings, we construct a complex editing benchmark: Complex-PIE-Bench. On two benchmarks, FlowDC shows superior results compared with existing methods. We also detail the ablations of our module designs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Reconstruction and Repair for Multi-class Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.11401</link>
<guid>https://arxiv.org/abs/2512.11401</guid>
<content:encoded><![CDATA[
<div> Industrial anomaly detection, multi-class anomaly detection, identity mapping problem, Collaborative Reconstruction and Repair (CRR), feature-level random masking<br /><br />Summary:<br /><br />Industrial anomaly detection aims to identify unknown anomalous patterns that deviate from normal data distributions. This task is challenging, especially in an open-set setting where anomalies are unknown and diverse. Traditional approaches often build separate models for each class, leading to high memory consumption and limited generalizability. To overcome this, the study proposes a unified framework called Collaborative Reconstruction and Repair (CRR) for multi-class anomaly detection. Conventional reconstruction-based networks tend to suffer from an identity mapping problem, where the network simply replicates input features regardless of anomaly presence, resulting in detection failures. CRR addresses this by transforming the reconstruction task into repairation: the decoder is optimized to reconstruct normal samples while repairing synthesized anomalies, generating distinct representations for anomalous regions and similar ones for normal regions relative to the encoder output. Additionally, the method employs feature-level random masking to ensure decoder representations maintain sufficient local information. To further enhance anomaly localization and reduce detection errors from encoder-decoder feature discrepancies, a segmentation network is trained with synthetic anomaly masks as supervision. Extensive experiments on industrial datasets demonstrate that CRR effectively alleviates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection. <div>
arXiv:2512.11401v1 Announce Type: new 
Abstract: Industrial anomaly detection is a challenging open-set task that aims to identify unknown anomalous patterns deviating from normal data distribution. To avoid the significant memory consumption and limited generalizability brought by building separate models per class, we focus on developing a unified framework for multi-class anomaly detection. However, under this challenging setting, conventional reconstruction-based networks often suffer from an identity mapping problem, where they directly replicate input features regardless of whether they are normal or anomalous, resulting in detection failures. To address this issue, this study proposes a novel framework termed Collaborative Reconstruction and Repair (CRR), which transforms the reconstruction to repairation. First, we optimize the decoder to reconstruct normal samples while repairing synthesized anomalies. Consequently, it generates distinct representations for anomalous regions and similar representations for normal areas compared to the encoder's output. Second, we implement feature-level random masking to ensure that the representations from decoder contain sufficient local information. Finally, to minimize detection errors arising from the discrepancies between feature representations from the encoder and decoder, we train a segmentation network supervised by synthetic anomaly masks, thereby enhancing localization performance. Extensive experiments on industrial datasets that CRR effectively mitigates the identity mapping issue and achieves state-of-the-art performance in multi-class industrial anomaly detection.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion</title>
<link>https://arxiv.org/abs/2512.11423</link>
<guid>https://arxiv.org/abs/2512.11423</guid>
<content:encoded><![CDATA[
<div> Keywords: audio-driven avatar, autoregressive diffusion, Progressive Step Bootstrapping, Motion Condition Injection, Unbounded RoPE  

<br /><br />Summary: This paper introduces JoyAvatar, an advanced audio-driven autoregressive model designed for real-time inference and infinite-length video generation in avatar synthesis. The first key contribution is Progressive Step Bootstrapping (PSB), which strategically assigns more denoising steps to initial frames to stabilize video generation and minimize error accumulation over time. The second innovation, Motion Condition Injection (MCI), improves temporal coherence by incorporating noise-corrupted previous frames as motion conditions, ensuring smoother and more consistent motion transitions. Third, Unbounded RoPE via Cache-Resetting (URCR) enables the system to generate videos of unlimited length by dynamically managing positional encoding, overcoming traditional fixed-length limitations. The model consists of 1.3 billion parameters and achieves an efficient generation speed of 16 frames per second on a single GPU, making it suitable for practical applications. In addition to real-time performance and infinite video duration, JoyAvatar delivers competitive results in visual quality, temporal consistency, and lip synchronization, which are critical for high-fidelity avatar generation. Overall, JoyAvatar advances the state of the art in audio-driven avatar generation by addressing key challenges of computational overhead, long-duration video synthesis, and quality degradation in autoregressive diffusion methods. <div>
arXiv:2512.11423v1 Announce Type: new 
Abstract: Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowception: Temporally Expansive Flow Matching for Video Generation</title>
<link>https://arxiv.org/abs/2512.11438</link>
<guid>https://arxiv.org/abs/2512.11438</guid>
<content:encoded><![CDATA[
<div> Flowception, video generation, non-autoregressive, frame insertion, denoising<br /><br />Summary:<br /><br />We introduce Flowception, a cutting-edge video generation framework that operates in a non-autoregressive and variable-length manner. Unlike traditional autoregressive models, Flowception mitigates error accumulation and drift by employing a novel frame insertion mechanism that compresses long-term context efficiently. This mechanism interleaves discrete frame insertions with continuous frame denoising, enabling effective handling of temporal dependencies. Compared to full-sequence flow models, Flowception achieves a three-fold reduction in floating-point operations (FLOPs) during training, making it computationally more efficient. Additionally, the framework supports local attention variations, enhancing its flexibility. A key innovation is the joint learning of video length alongside content, allowing adaptive video generation. Experimental evaluation demonstrates superior performance through improved Fréchet Video Distance (FVD) and VBench metrics relative to both autoregressive and full-sequence baselines. Qualitative results further confirm these quantitative benefits. Finally, Flowception’s design naturally integrates multiple tasks such as image-to-video generation and video interpolation by learning to insert and denoise frames within a sequence, illustrating its versatility across video-related applications. <div>
arXiv:2512.11438v1 Announce Type: new 
Abstract: We present Flowception, a novel non-autoregressive and variable-length video generation framework. Flowception learns a probability path that interleaves discrete frame insertions with continuous frame denoising. Compared to autoregressive methods, Flowception alleviates error accumulation/drift as the frame insertion mechanism during sampling serves as an efficient compression mechanism to handle long-term context. Compared to full-sequence flows, our method reduces FLOPs for training three-fold, while also being more amenable to local attention variants, and allowing to learn the length of videos jointly with their content. Quantitative experimental results show improved FVD and VBench metrics over autoregressive and full-sequence baselines, which is further validated with qualitative results. Finally, by learning to insert and denoise frames in a sequence, Flowception seamlessly integrates different tasks such as image-to-video generation and video interpolation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YawDD+: Frame-level Annotations for Accurate Yawn Prediction</title>
<link>https://arxiv.org/abs/2512.11446</link>
<guid>https://arxiv.org/abs/2512.11446</guid>
<content:encoded><![CDATA[
arXiv:2512.11446v1 Announce Type: new 
Abstract: Driver fatigue remains a leading cause of road accidents, with 24\% of crashes involving drowsy drivers. While yawning serves as an early behavioral indicator of fatigue, existing machine learning approaches face significant challenges due to video-annotated datasets that introduce systematic noise from coarse temporal annotations. We develop a semi-automated labeling pipeline with human-in-the-loop verification, which we apply to YawDD, enabling more accurate model training. Training the established MNasNet classifier and YOLOv11 detector architectures on YawDD+ improves frame accuracy by up to 6\% and mAP by 5\% over video-level supervision, achieving 99.34\% classification accuracy and 95.69\% detection mAP. The resulting approach deliver up to 59.8 FPS on edge AI hardware (NVIDIA Jetson Nano), confirming that enhanced data quality alone supports on-device yawning monitoring without server-side computation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.11458</link>
<guid>https://arxiv.org/abs/2512.11458</guid>
<content:encoded><![CDATA[
arXiv:2512.11458v1 Announce Type: new 
Abstract: We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring MLLM-Diffusion Information Transfer with MetaCanvas</title>
<link>https://arxiv.org/abs/2512.11464</link>
<guid>https://arxiv.org/abs/2512.11464</guid>
<content:encoded><![CDATA[
arXiv:2512.11464v1 Announce Type: new 
Abstract: Multimodal learning has rapidly advanced visual understanding, largely via multimodal large language models (MLLMs) that use powerful LLMs as cognitive cores. In visual generation, however, these powerful core models are typically reduced to global text encoders for diffusion models, leaving most of their reasoning and planning ability unused. This creates a gap: current multimodal LLMs can parse complex layouts, attributes, and knowledge-intensive scenes, yet struggle to generate images or videos with equally precise and structured control. We propose MetaCanvas, a lightweight framework that lets MLLMs reason and plan directly in spatial and spatiotemporal latent spaces and interface tightly with diffusion generators. We empirically implement MetaCanvas on three different diffusion backbones and evaluate it across six tasks, including text-to-image generation, text/image-to-video generation, image/video editing, and in-context video generation, each requiring precise layouts, robust attribute binding, and reasoning-intensive control. MetaCanvas consistently outperforms global-conditioning baselines, suggesting that treating MLLMs as latent-space planners is a promising direction for narrowing the gap between multimodal understanding and generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DOS: Distilling Observable Softmaps of Zipfian Prototypes for Self-Supervised Point Representation</title>
<link>https://arxiv.org/abs/2512.11465</link>
<guid>https://arxiv.org/abs/2512.11465</guid>
<content:encoded><![CDATA[
arXiv:2512.11465v1 Announce Type: new 
Abstract: Recent advances in self-supervised learning (SSL) have shown tremendous potential for learning 3D point cloud representations without human annotations. However, SSL for 3D point clouds still faces critical challenges due to irregular geometry, shortcut-prone reconstruction, and unbalanced semantics distribution. In this work, we propose DOS (Distilling Observable Softmaps), a novel SSL framework that self-distills semantic relevance softmaps only at observable (unmasked) points. This strategy prevents information leakage from masked regions and provides richer supervision than discrete token-to-prototype assignments. To address the challenge of unbalanced semantics in an unsupervised setting, we introduce Zipfian prototypes and incorporate them using a modified Sinkhorn-Knopp algorithm, Zipf-Sinkhorn, which enforces a power-law prior over prototype usage and modulates the sharpness of the target softmap during training. DOS outperforms current state-of-the-art methods on semantic segmentation and 3D object detection across multiple benchmarks, including nuScenes, Waymo, SemanticKITTI, ScanNet, and ScanNet200, without relying on extra data or annotations. Our results demonstrate that observable-point softmaps distillation offers a scalable and effective paradigm for learning robust 3D representations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADMorph: Geometry-Driven Parametric CAD Editing via a Plan-Generate-Verify Loop</title>
<link>https://arxiv.org/abs/2512.11480</link>
<guid>https://arxiv.org/abs/2512.11480</guid>
<content:encoded><![CDATA[
arXiv:2512.11480v1 Announce Type: new 
Abstract: A Computer-Aided Design (CAD) model encodes an object in two coupled forms: a parametric construction sequence and its resulting visible geometric shape. During iterative design, adjustments to the geometric shape inevitably require synchronized edits to the underlying parametric sequence, called geometry-driven parametric CAD editing. The task calls for 1) preserving the original sequence's structure, 2) ensuring each edit's semantic validity, and 3) maintaining high shape fidelity to the target shape, all under scarce editing data triplets. We present CADMorph, an iterative plan-generate-verify framework that orchestrates pretrained domain-specific foundation models during inference: a parameter-to-shape (P2S) latent diffusion model and a masked-parameter-prediction (MPP) model. In the planning stage, cross-attention maps from the P2S model pinpoint the segments that need modification and offer editing masks. The MPP model then infills these masks with semantically valid edits in the generation stage. During verification, the P2S model embeds each candidate sequence in shape-latent space, measures its distance to the target shape, and selects the closest one. The three stages leverage the inherent geometric consciousness and design knowledge in pretrained priors, and thus tackle structure preservation, semantic validity, and shape fidelity respectively. Besides, both P2S and MPP models are trained without triplet data, bypassing the data-scarcity bottleneck. CADMorph surpasses GPT-4o and specialized CAD baselines, and supports downstream applications such as iterative editing and reverse-engineering enhancement.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing</title>
<link>https://arxiv.org/abs/2512.11490</link>
<guid>https://arxiv.org/abs/2512.11490</guid>
<content:encoded><![CDATA[
arXiv:2512.11490v1 Announce Type: new 
Abstract: Satellite imagery differs fundamentally from natural images: its aerial viewpoint, very high resolution, diverse scale variations, and abundance of small objects demand both region-level spatial reasoning and holistic scene understanding. Current remote-sensing approaches remain fragmented between dual-encoder retrieval models, which excel at large-scale cross-modal search but cannot interleave modalities, and generative assistants, which support region-level interpretation but lack scalable retrieval capabilities. We propose $\textbf{VLM2GeoVec}$, an instruction-following, single-encoder vision-language model trained contrastively to embed interleaved inputs (images, text, bounding boxes, and geographic coordinates) in a unified vector space. Our single encoder interleaves all inputs into one joint embedding trained with a contrastive loss, eliminating multi-stage pipelines and task-specific modules. To evaluate its versatility, we introduce $\textbf{RSMEB}$, a novel benchmark covering key remote-sensing embedding applications: scene classification; cross-modal search; compositional retrieval; visual-question answering; visual grounding and region-level reasoning; and semantic geospatial retrieval. On RSMEB, it achieves $\textbf{26.6%}$ P@1 on region-caption retrieval (+25 pp vs. dual-encoder baselines), $\textbf{32.5%}$ P@1 on referring-expression retrieval (+19 pp), and $\textbf{17.8%}$ P@1 on semantic geo-localization retrieval (over $3\times$ prior best), while matching or exceeding specialized baselines on conventional tasks such as scene classification and cross-modal retrieval. VLM2GeoVec unifies scalable retrieval with region-level spatial reasoning, enabling cohesive multimodal analysis in remote sensing. We will publicly release the code, checkpoints, and data upon acceptance.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSkel-Mamba: Temporal Dynamic Modeling via State Space Model for Human Skeleton-based Action Recognition</title>
<link>https://arxiv.org/abs/2512.11503</link>
<guid>https://arxiv.org/abs/2512.11503</guid>
<content:encoded><![CDATA[
arXiv:2512.11503v1 Announce Type: new 
Abstract: Skeleton-based action recognition has garnered significant attention in the computer vision community. Inspired by the recent success of the selective state-space model (SSM) Mamba in modeling 1D temporal sequences, we propose TSkel-Mamba, a hybrid Transformer-Mamba framework that effectively captures both spatial and temporal dynamics. In particular, our approach leverages Spatial Transformer for spatial feature learning while utilizing Mamba for temporal modeling. Mamba, however, employs separate SSM blocks for individual channels, which inherently limits its ability to model inter-channel dependencies. To better adapt Mamba for skeleton data and enhance Mamba`s ability to model temporal dependencies, we introduce a Temporal Dynamic Modeling (TDM) block, which is a versatile plug-and-play component that integrates a novel Multi-scale Temporal Interaction (MTI) module. The MTI module employs multi-scale Cycle operators to capture cross-channel temporal interactions, a critical factor in action recognition. Extensive experiments on NTU-RGB+D 60, NTU-RGB+D 120, NW-UCLA and UAV-Human datasets demonstrate that TSkel-Mamba achieves state-of-the-art performance while maintaining low inference time, making it both efficient and highly effective.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSA3D: Text-Conditioned Assisted Self-Supervised Framework for Automatic Dental Abutment Design</title>
<link>https://arxiv.org/abs/2512.11507</link>
<guid>https://arxiv.org/abs/2512.11507</guid>
<content:encoded><![CDATA[
arXiv:2512.11507v1 Announce Type: new 
Abstract: Abutment design is a critical step in dental implant restoration. However, manual design involves tedious measurement and fitting, and research on automating this process with AI is limited, due to the unavailability of large annotated datasets. Although self-supervised learning (SSL) can alleviate data scarcity, its need for pre-training and fine-tuning results in high computational costs and long training times. In this paper, we propose a Self-supervised assisted automatic abutment design framework (SS$A^3$D), which employs a dual-branch architecture with a reconstruction branch and a regression branch. The reconstruction branch learns to restore masked intraoral scan data and transfers the learned structural information to the regression branch. The regression branch then predicts the abutment parameters under supervised learning, which eliminates the separate pre-training and fine-tuning process. We also design a Text-Conditioned Prompt (TCP) module to incorporate clinical information (such as implant location, system, and series) into SS$A^3$D. This guides the network to focus on relevant regions and constrains the parameter predictions. Extensive experiments on a collected dataset show that SS$A^3$D saves half of the training time and achieves higher accuracy than traditional SSL methods. It also achieves state-of-the-art performance compared to other methods, significantly improving the accuracy and efficiency of automated abutment design.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Geometric Understanding and Learned Data Priors in VGGT</title>
<link>https://arxiv.org/abs/2512.11508</link>
<guid>https://arxiv.org/abs/2512.11508</guid>
<content:encoded><![CDATA[
arXiv:2512.11508v1 Announce Type: new 
Abstract: The Visual Geometry Grounded Transformer (VGGT) is a 3D foundation model that infers camera geometry and scene structure in a single feed-forward pass. Trained in a supervised, single-step fashion on large datasets, VGGT raises a key question: does it build upon geometric concepts like traditional multi-view methods, or does it rely primarily on learned appearance-based data-driven priors? In this work, we conduct a systematic analysis of VGGT's internal mechanisms to uncover whether geometric understanding emerges within its representations. By probing intermediate features, analyzing attention patterns, and performing interventions, we examine how the model implements its functionality. Our findings reveal that VGGT implicitly performs correspondence matching within its global attention layers and encodes epipolar geometry, despite being trained without explicit geometric constraints. We further investigate VGGT's dependence on its learned data priors. Using spatial input masking and perturbation experiments, we assess its robustness to occlusions, appearance variations, and camera configurations, comparing it with classical multi-stage pipelines. Together, these insights highlight how VGGT internalizes geometric structure while using learned data-driven priors.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction as a Bridge for Event-Based Visual Question Answering</title>
<link>https://arxiv.org/abs/2512.11510</link>
<guid>https://arxiv.org/abs/2512.11510</guid>
<content:encoded><![CDATA[
arXiv:2512.11510v1 Announce Type: new 
Abstract: Integrating event cameras with Multimodal Large Language Models (MLLMs) promises general scene understanding in challenging visual conditions, yet requires navigating a trade-off between preserving the unique advantages of event data and ensuring compatibility with frame-based models. We address this challenge by using reconstruction as a bridge, proposing a straightforward Frame-based Reconstruction and Tokenization (FRT) method and designing an efficient Adaptive Reconstruction and Tokenization (ART) method that leverages event sparsity. For robust evaluation, we introduce EvQA, the first objective, real-world benchmark for event-based MLLMs, comprising 1,000 event-Q&amp;A pairs from 22 public datasets. Our experiments demonstrate that our methods achieve state-of-the-art performance on EvQA, highlighting the significant potential of MLLMs in event-based vision.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Super-Resolved Canopy Height Mapping from Sentinel-2 Time Series Using LiDAR HD Reference Data across Metropolitan France</title>
<link>https://arxiv.org/abs/2512.11524</link>
<guid>https://arxiv.org/abs/2512.11524</guid>
<content:encoded><![CDATA[
arXiv:2512.11524v1 Announce Type: new 
Abstract: Fine-scale forest monitoring is essential for understanding canopy structure and its dynamics, which are key indicators of carbon stocks, biodiversity, and forest health. Deep learning is particularly effective for this task, as it integrates spectral, temporal, and spatial signals that jointly reflect the canopy structure. To address this need, we introduce THREASURE-Net, a novel end-to-end framework for Tree Height Regression And Super-Resolution. The model is trained on Sentinel-2 time series using reference height metrics derived from LiDAR HD data at multiple spatial resolutions over Metropolitan France to produce annual height maps. We evaluate three model variants, producing tree-height predictions at 2.5 m, 5 m, and 10 m resolution. THREASURE-Net does not rely on any pretrained model nor on reference very high resolution optical imagery to train its super-resolution module; instead, it learns solely from LiDAR-derived height information. Our approach outperforms existing state-of-the-art methods based on Sentinel data and is competitive with methods based on very high resolution imagery. It can be deployed to generate high-precision annual canopy-height maps, achieving mean absolute errors of 2.62 m, 2.72 m, and 2.88 m at 2.5 m, 5 m, and 10 m resolution, respectively. These results highlight the potential of THREASURE-Net for scalable and cost-effective structural monitoring of temperate forests using only freely available satellite data. The source code for THREASURE-Net is available at: https://github.com/Global-Earth-Observation/threasure-net.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HFS: Holistic Query-Aware Frame Selection for Efficient Video Reasoning</title>
<link>https://arxiv.org/abs/2512.11534</link>
<guid>https://arxiv.org/abs/2512.11534</guid>
<content:encoded><![CDATA[
arXiv:2512.11534v1 Announce Type: new 
Abstract: Key frame selection in video understanding presents significant challenges. Traditional top-K selection methods, which score frames independently, often fail to optimize the selection as a whole. This independent scoring frequently results in selecting frames that are temporally clustered and visually redundant. Additionally, training lightweight selectors using pseudo labels generated offline by Multimodal Large Language Models (MLLMs) prevents the supervisory signal from dynamically adapting to task objectives. To address these limitations, we propose an end-to-end trainable, task-adaptive framework for frame selection. A Chain-of-Thought approach guides a Small Language Model (SLM) to generate task-specific implicit query vectors, which are combined with multimodal features to enable dynamic frame scoring. We further define a continuous set-level objective function that incorporates relevance, coverage, and redundancy, enabling differentiable optimization via Gumbel-Softmax to select optimal frame combinations at the set level. Finally, student-teacher mutual learning is employed, where the student selector (SLM) and teacher reasoner (MLLM) are trained to align their frame importance distributions via KL divergence. Combined with cross-entropy loss, this enables end-to-end optimization, eliminating reliance on static pseudo labels. Experiments across various benchmarks, including Video-MME, LongVideoBench, MLVU, and NExT-QA, demonstrate that our method significantly outperforms existing approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infinity and Beyond: Compositional Alignment in VAR and Diffusion T2I Models</title>
<link>https://arxiv.org/abs/2512.11542</link>
<guid>https://arxiv.org/abs/2512.11542</guid>
<content:encoded><![CDATA[
arXiv:2512.11542v1 Announce Type: new 
Abstract: Achieving compositional alignment between textual descriptions and generated images - covering objects, attributes, and spatial relationships - remains a core challenge for modern text-to-image (T2I) models. Although diffusion-based architectures have been widely studied, the compositional behavior of emerging Visual Autoregressive (VAR) models is still largely unexamined. We benchmark six diverse T2I systems - SDXL, PixArt-$\alpha$, Flux-Dev, Flux-Schnell, Infinity-2B, and Infinity-8B - across the full T2I-CompBench++ and GenEval suites, evaluating alignment in color and attribute binding, spatial relations, numeracy, and complex multi-object prompts. Across both benchmarks, Infinity-8B achieves the strongest overall compositional alignment, while Infinity-2B also matches or exceeds larger diffusion models in several categories, highlighting favorable efficiency-performance trade-offs. In contrast, SDXL and PixArt-$\alpha$ show persistent weaknesses in attribute-sensitive and spatial tasks. These results provide the first systematic comparison of VAR and diffusion approaches to compositional alignment and establish unified baselines for the future development of the T2I model.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSL-MedSAM2: A Semi-supervised Medical Image Segmentation Framework Powered by Few-shot Learning of SAM2</title>
<link>https://arxiv.org/abs/2512.11548</link>
<guid>https://arxiv.org/abs/2512.11548</guid>
<content:encoded><![CDATA[
arXiv:2512.11548v1 Announce Type: new 
Abstract: Despite the success of deep learning based models in medical image segmentation, most state-of-the-art (SOTA) methods perform fully-supervised learning, which commonly rely on large scale annotated training datasets. However, medical image annotation is highly time-consuming, hindering its clinical applications. Semi-supervised learning (SSL) has been emerged as an appealing strategy in training with limited annotations, largely reducing the labelling cost. We propose a novel SSL framework SSL-MedSAM2, which contains a training-free few-shot learning branch TFFS-MedSAM2 based on the pretrained large foundation model Segment Anything Model 2 (SAM2) for pseudo label generation, and an iterative fully-supervised learning branch FSL-nnUNet based on nnUNet for pseudo label refinement. The results on MICCAI2025 challenge CARE-LiSeg (Liver Segmentation) demonstrate an outstanding performance of SSL-MedSAM2 among other methods. The average dice scores on the test set in GED4 and T1 MRI are 0.9710 and 0.9648 respectively, and the Hausdorff distances are 20.07 and 21.97 respectively. The code is available via https://github.com/naisops/SSL-MedSAM2/tree/main.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3DTeethSAM: Taming SAM2 for 3D Teeth Segmentation</title>
<link>https://arxiv.org/abs/2512.11557</link>
<guid>https://arxiv.org/abs/2512.11557</guid>
<content:encoded><![CDATA[
arXiv:2512.11557v1 Announce Type: new 
Abstract: 3D teeth segmentation, involving the localization of tooth instances and their semantic categorization in 3D dental models, is a critical yet challenging task in digital dentistry due to the complexity of real-world dentition. In this paper, we propose 3DTeethSAM, an adaptation of the Segment Anything Model 2 (SAM2) for 3D teeth segmentation. SAM2 is a pretrained foundation model for image and video segmentation, demonstrating a strong backbone in various downstream scenarios. To adapt SAM2 for 3D teeth data, we render images of 3D teeth models from predefined views, apply SAM2 for 2D segmentation, and reconstruct 3D results using 2D-3D projections. Since SAM2's performance depends on input prompts and its initial outputs often have deficiencies, and given its class-agnostic nature, we introduce three light-weight learnable modules: (1) a prompt embedding generator to derive prompt embeddings from image embeddings for accurate mask decoding, (2) a mask refiner to enhance SAM2's initial segmentation results, and (3) a mask classifier to categorize the generated masks. Additionally, we incorporate Deformable Global Attention Plugins (DGAP) into SAM2's image encoder. The DGAP enhances both the segmentation accuracy and the speed of the training process. Our method has been validated on the 3DTeethSeg benchmark, achieving an IoU of 91.90% on high-resolution 3D teeth meshes, establishing a new state-of-the-art in the field.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DentalGPT: Incentivizing Multimodal Complex Reasoning in Dentistry</title>
<link>https://arxiv.org/abs/2512.11558</link>
<guid>https://arxiv.org/abs/2512.11558</guid>
<content:encoded><![CDATA[
arXiv:2512.11558v1 Announce Type: new 
Abstract: Reliable interpretation of multimodal data in dentistry is essential for automated oral healthcare, yet current multimodal large language models (MLLMs) struggle to capture fine-grained dental visual details and lack sufficient reasoning ability for precise diagnosis. To address these limitations, we present DentalGPT, a specialized dental MLLM developed through high-quality domain knowledge injection and reinforcement learning. Specifically, the largest annotated multimodal dataset for dentistry to date was constructed by aggregating over 120k dental images paired with detailed descriptions that highlight diagnostically relevant visual features, making it the multimodal dataset with the most extensive collection of dental images to date. Training on this dataset significantly enhances the MLLM's visual understanding of dental conditions, while the subsequent reinforcement learning stage further strengthens its capability for multimodal complex reasoning. Comprehensive evaluations on intraoral and panoramic benchmarks, along with dental subsets of medical VQA benchmarks, show that DentalGPT achieves superior performance in disease classification and dental VQA tasks, outperforming many state-of-the-art MLLMs despite having only 7B parameters. These results demonstrate that high-quality dental data combined with staged adaptation provides an effective pathway for building capable and domain-specialized dental MLLMs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-temporal Calving Front Segmentation</title>
<link>https://arxiv.org/abs/2512.11560</link>
<guid>https://arxiv.org/abs/2512.11560</guid>
<content:encoded><![CDATA[
arXiv:2512.11560v1 Announce Type: new 
Abstract: The calving fronts of marine-terminating glaciers undergo constant changes. These changes significantly affect the glacier's mass and dynamics, demanding continuous monitoring. To address this need, deep learning models were developed that can automatically delineate the calving front in Synthetic Aperture Radar imagery. However, these models often struggle to correctly classify areas affected by seasonal conditions such as ice melange or snow-covered surfaces. To address this issue, we propose to process multiple frames from a satellite image time series of the same glacier in parallel and exchange temporal information between the corresponding feature maps to stabilize each prediction. We integrate our approach into the current state-of-the-art architecture Tyrion and accomplish a new state-of-the-art performance on the CaFFe benchmark dataset. In particular, we achieve a Mean Distance Error of 184.4 m and a mean Intersection over Union of 83.6.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Foundation Models' 3D Understanding Through Multi-View Correspondence Analysis</title>
<link>https://arxiv.org/abs/2512.11574</link>
<guid>https://arxiv.org/abs/2512.11574</guid>
<content:encoded><![CDATA[
arXiv:2512.11574v1 Announce Type: new 
Abstract: Benchmarking 3D spatial understanding of foundation models is essential for real-world applications such as robotics and autonomous driving. Existing evaluations often rely on downstream finetuning with linear heads or task-specific decoders, making it difficult to isolate the intrinsic 3D reasoning ability of pretrained encoders. In this work, we introduce a novel benchmark for in-context 3D scene understanding that requires no finetuning and directly probes the quality of dense visual features. Building on the Hummingbird framework, which evaluates in-context 2D scene understanding, we extend the setup to the 3D Multi-View ImageNet (MVImgNet) dataset. Given a set of images from objects in specific angles (keys), we benchmark the performance of segmenting novel views (queries) and report the scores in 4 categories of easy, medium, hard, and extreme based on the key-query view contrast. We benchmark 8 state-of-the-art foundation models and show DINO-based encoders remain competitive across large viewpoint shifts, while 3D-aware models like VGGT require dedicated multi-view adjustments. Our code is publicly available at https://github.com/ToyeshC/open-hummingbird-3d-eval .
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning for Seismic Data Processing</title>
<link>https://arxiv.org/abs/2512.11575</link>
<guid>https://arxiv.org/abs/2512.11575</guid>
<content:encoded><![CDATA[
arXiv:2512.11575v1 Announce Type: new 
Abstract: Seismic processing transforms raw data into subsurface images essential for geophysical applications. Traditional methods face challenges, such as noisy data, and manual parameter tuning, among others. Recently deep learning approaches have proposed alternative solutions to some of these problems. However, important challenges of existing deep learning approaches are spatially inconsistent results across neighboring seismic gathers and lack of user-control. We address these limitations by introducing ContextSeisNet, an in-context learning model, to seismic demultiple processing. Our approach conditions predictions on a support set of spatially related example pairs: neighboring common-depth point gathers from the same seismic line and their corresponding labels. This allows the model to learn task-specific processing behavior at inference time by observing how similar gathers should be processed, without any retraining. This method provides both flexibility through user-defined examples and improved lateral consistency across seismic lines. On synthetic data, ContextSeisNet outperforms a U-Net baseline quantitatively and demonstrates enhanced spatial coherence between neighboring gathers. On field data, our model achieves superior lateral consistency compared to both traditional Radon demultiple and the U-Net baseline. Relative to the U-Net, ContextSeisNet also delivers improved near-offset performance and more complete multiple removal. Notably, ContextSeisNet achieves comparable field data performance despite being trained on 90% less data, demonstrating substantial data efficiency. These results establish ContextSeisNet as a practical approach for spatially consistent seismic demultiple with potential applicability to other seismic processing tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using GUI Agent for Electronic Design Automation</title>
<link>https://arxiv.org/abs/2512.11611</link>
<guid>https://arxiv.org/abs/2512.11611</guid>
<content:encoded><![CDATA[
arXiv:2512.11611v1 Announce Type: new 
Abstract: Graphical User Interface (GUI) agents adopt an end-to-end paradigm that maps a screenshot to an action sequence, thereby automating repetitive tasks in virtual environments. However, existing GUI agents are evaluated almost exclusively on commodity software such as Microsoft Word and Excel. Professional Computer-Aided Design (CAD) suites promise an order-of-magnitude higher economic return, yet remain the weakest performance domain for existing agents and are still far from replacing expert Electronic-Design-Automation (EDA) engineers. We therefore present the first systematic study that deploys GUI agents for EDA workflows. Our contributions are: (1) a large-scale dataset named GUI-EDA, including 5 CAD tools and 5 physical domains, comprising 2,000+ high-quality screenshot-answer-action pairs recorded by EDA scientists and engineers during real-world component design; (2) a comprehensive benchmark that evaluates 30+ mainstream GUI agents, demonstrating that EDA tasks constitute a major, unsolved challenge; and (3) an EDA-specialized metric named EDAgent, equipped with a reflection mechanism that achieves reliable performance on industrial CAD software and, for the first time, outperforms Ph.D. students majored in Electrical Engineering. This work extends GUI agents from generic office automation to specialized, high-value engineering domains and offers a new avenue for advancing EDA productivity. The dataset will be released at: https://github.com/aiben-ch/GUI-EDA.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Image Compression</title>
<link>https://arxiv.org/abs/2512.11612</link>
<guid>https://arxiv.org/abs/2512.11612</guid>
<content:encoded><![CDATA[
arXiv:2512.11612v1 Announce Type: new 
Abstract: Image Compression for Machines (ICM) has emerged as a pivotal research direction in the field of visual data compression. However, with the rapid evolution of machine intelligence, the target of compression has shifted from task-specific virtual models to Embodied agents operating in real-world environments. To address the communication constraints of Embodied AI in multi-agent systems and ensure real-time task execution, this paper introduces, for the first time, the scientific problem of Embodied Image Compression. We establish a standardized benchmark, EmbodiedComp, to facilitate systematic evaluation under ultra-low bitrate conditions in a closed-loop setting. Through extensive empirical studies in both simulated and real-world settings, we demonstrate that existing Vision-Language-Action models (VLAs) fail to reliably perform even simple manipulation tasks when compressed below the Embodied bitrate threshold. We anticipate that EmbodiedComp will catalyze the development of domain-specific compression tailored for Embodied agents , thereby accelerating the Embodied AI deployment in the Real-world.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Explicit: Slice-to-Volume Reconstruction via 3D Gaussian Primitives with Analytic Point Spread Function Modeling</title>
<link>https://arxiv.org/abs/2512.11624</link>
<guid>https://arxiv.org/abs/2512.11624</guid>
<content:encoded><![CDATA[
arXiv:2512.11624v1 Announce Type: new 
Abstract: Recovering high-fidelity 3D images from sparse or degraded 2D images is a fundamental challenge in medical imaging, with broad applications ranging from 3D ultrasound reconstruction to MRI super-resolution. In the context of fetal MRI, high-resolution 3D reconstruction of the brain from motion-corrupted low-resolution 2D acquisitions is a prerequisite for accurate neurodevelopmental diagnosis. While implicit neural representations (INRs) have recently established state-of-the-art performance in self-supervised slice-to-volume reconstruction (SVR), they suffer from a critical computational bottleneck: accurately modeling the image acquisition physics requires expensive stochastic Monte Carlo sampling to approximate the point spread function (PSF). In this work, we propose a shift from neural network based implicit representations to Gaussian based explicit representations. By parameterizing the HR 3D image volume as a field of anisotropic Gaussian primitives, we leverage the property of Gaussians being closed under convolution and thus derive a \textit{closed-form analytical solution} for the forward model. This formulation reduces the previously intractable acquisition integral to an exact covariance addition ($\mathbf{\Sigma}_{obs} = \mathbf{\Sigma}_{HR} + \mathbf{\Sigma}_{PSF}$), effectively bypassing the need for compute-intensive stochastic sampling while ensuring exact gradient propagation. We demonstrate that our approach matches the reconstruction quality of self-supervised state-of-the-art SVR frameworks while delivering a 5$\times$--10$\times$ speed-up on neonatal and fetal data. With convergence often reached in under 30 seconds, our framework paves the way towards translation into clinical routine of real-time fetal 3D MRI. Code will be public at {https://github.com/m-dannecker/Gaussian-Primitives-for-Fast-SVR}.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FactorPortrait: Controllable Portrait Animation via Disentangled Expression, Pose, and Viewpoint</title>
<link>https://arxiv.org/abs/2512.11645</link>
<guid>https://arxiv.org/abs/2512.11645</guid>
<content:encoded><![CDATA[
arXiv:2512.11645v1 Announce Type: new 
Abstract: We introduce FactorPortrait, a video diffusion method for controllable portrait animation that enables lifelike synthesis from disentangled control signals of facial expressions, head movement, and camera viewpoints. Given a single portrait image, a driving video, and camera trajectories, our method animates the portrait by transferring facial expressions and head movements from the driving video while simultaneously enabling novel view synthesis from arbitrary viewpoints. We utilize a pre-trained image encoder to extract facial expression latents from the driving video as control signals for animation generation. Such latents implicitly capture nuanced facial expression dynamics with identity and pose information disentangled, and they are efficiently injected into the video diffusion transformer through our proposed expression controller. For camera and head pose control, we employ Pl\"ucker ray maps and normal maps rendered from 3D body mesh tracking. To train our model, we curate a large-scale synthetic dataset containing diverse combinations of camera viewpoints, head poses, and facial expression dynamics. Extensive experiments demonstrate that our method outperforms existing approaches in realism, expressiveness, control accuracy, and view consistency.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation</title>
<link>https://arxiv.org/abs/2512.11654</link>
<guid>https://arxiv.org/abs/2512.11654</guid>
<content:encoded><![CDATA[
arXiv:2512.11654v1 Announce Type: new 
Abstract: The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at (https://lucazzola.github.io/publications/kinemic).
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-modal Context-aware Learning for Visual Prompt Guided Multimodal Image Understanding in Remote Sensing</title>
<link>https://arxiv.org/abs/2512.11680</link>
<guid>https://arxiv.org/abs/2512.11680</guid>
<content:encoded><![CDATA[
arXiv:2512.11680v1 Announce Type: new 
Abstract: Recent advances in image understanding have enabled methods that leverage large language models for multimodal reasoning in remote sensing. However, existing approaches still struggle to steer models to the user-relevant regions when only simple, generic text prompts are available. Moreover, in large-scale aerial imagery many objects exhibit highly similar visual appearances and carry rich inter-object relationships, which further complicates accurate recognition. To address these challenges, we propose Cross-modal Context-aware Learning for Visual Prompt-Guided Multimodal Image Understanding (CLV-Net). CLV-Net lets users supply a simple visual cue, a bounding box, to indicate a region of interest, and uses that cue to guide the model to generate correlated segmentation masks and captions that faithfully reflect user intent. Central to our design is a Context-Aware Mask Decoder that models and integrates inter-object relationships to strengthen target representations and improve mask quality. In addition, we introduce a Semantic and Relationship Alignment module: a Cross-modal Semantic Consistency Loss enhances fine-grained discrimination among visually similar targets, while a Relationship Consistency Loss enforces alignment between textual relations and visual interactions. Comprehensive experiments on two benchmark datasets show that CLV-Net outperforms existing methods and establishes new state-of-the-art results. The model effectively captures user intent and produces precise, intention-aligned multimodal outputs.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth-Copy-Paste: Multimodal and Depth-Aware Compositing for Robust Face Detection</title>
<link>https://arxiv.org/abs/2512.11683</link>
<guid>https://arxiv.org/abs/2512.11683</guid>
<content:encoded><![CDATA[
arXiv:2512.11683v1 Announce Type: new 
Abstract: Data augmentation is crucial for improving the robustness of face detection systems, especially under challenging conditions such as occlusion, illumination variation, and complex environments. Traditional copy paste augmentation often produces unrealistic composites due to inaccurate foreground extraction, inconsistent scene geometry, and mismatched background semantics. To address these limitations, we propose Depth Copy Paste, a multimodal and depth aware augmentation framework that generates diverse and physically consistent face detection training samples by copying full body person instances and pasting them into semantically compatible scenes. Our approach first employs BLIP and CLIP to jointly assess semantic and visual coherence, enabling automatic retrieval of the most suitable background images for the given foreground person. To ensure high quality foreground masks that preserve facial details, we integrate SAM3 for precise segmentation and Depth-Anything to extract only the non occluded visible person regions, preventing corrupted facial textures from being used in augmentation. For geometric realism, we introduce a depth guided sliding window placement mechanism that searches over the background depth map to identify paste locations with optimal depth continuity and scale alignment. The resulting composites exhibit natural depth relationships and improved visual plausibility. Extensive experiments show that Depth Copy Paste provides more diverse and realistic training data, leading to significant performance improvements in downstream face detection tasks compared with traditional copy paste and depth free augmentation methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text images processing system using artificial intelligence models</title>
<link>https://arxiv.org/abs/2512.11691</link>
<guid>https://arxiv.org/abs/2512.11691</guid>
<content:encoded><![CDATA[
arXiv:2512.11691v1 Announce Type: new 
Abstract: This is to present a text image classifier device that identifies textual content in images and then categorizes each image into one of four predefined categories, including Invoice, Form, Letter, or Report. The device supports a gallery mode, in which users browse files on flash disks, hard disk drives, or microSD cards, and a live mode which renders feeds of cameras connected to it. Its design is specifically aimed at addressing pragmatic challenges, such as changing light, random orientation, curvature or partial coverage of text, low resolution, and slightly visible text. The steps of the processing process are divided into four steps: image acquisition and preprocessing, textual elements detection with the help of DBNet++ (Differentiable Binarization Network Plus) model, BART (Bidirectional Auto-Regressive Transformers) model that classifies detected textual elements, and the presentation of the results through a user interface written in Python and PyQt5. All the stages are connected in such a way that they form a smooth workflow. The system achieved a text recognition rate of about 94.62% when tested over ten hours on the mentioned Total-Text dataset, that includes high resolution images, created so as to represent a wide range of problematic conditions. These experimental results support the effectiveness of the suggested methodology to practice, mixed-source text categorization, even in uncontrolled imaging conditions.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EditMGT: Unleashing Potentials of Masked Generative Transformers in Image Editing</title>
<link>https://arxiv.org/abs/2512.11715</link>
<guid>https://arxiv.org/abs/2512.11715</guid>
<content:encoded><![CDATA[
arXiv:2512.11715v1 Announce Type: new 
Abstract: Recent advances in diffusion models (DMs) have achieved exceptional visual quality in image editing tasks. However, the global denoising dynamics of DMs inherently conflate local editing targets with the full-image context, leading to unintended modifications in non-target regions. In this paper, we shift our attention beyond DMs and turn to Masked Generative Transformers (MGTs) as an alternative approach to tackle this challenge. By predicting multiple masked tokens rather than holistic refinement, MGTs exhibit a localized decoding paradigm that endows them with the inherent capacity to explicitly preserve non-relevant regions during the editing process. Building upon this insight, we introduce the first MGT-based image editing framework, termed EditMGT. We first demonstrate that MGT's cross-attention maps provide informative localization signals for localizing edit-relevant regions and devise a multi-layer attention consolidation scheme that refines these maps to achieve fine-grained and precise localization. On top of these adaptive localization results, we introduce region-hold sampling, which restricts token flipping within low-attention areas to suppress spurious edits, thereby confining modifications to the intended target regions and preserving the integrity of surrounding non-target areas. To train EditMGT, we construct CrispEdit-2M, a high-resolution dataset spanning seven diverse editing categories. Without introducing additional parameters, we adapt a pre-trained text-to-image MGT into an image editing model through attention injection. Extensive experiments across four standard benchmarks demonstrate that, with fewer than 1B parameters, our model achieves similarity performance while enabling 6 times faster editing. Moreover, it delivers comparable or superior editing quality, with improvements of 3.6% and 17.6% on style change and style transfer tasks, respectively.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referring Change Detection in Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.11719</link>
<guid>https://arxiv.org/abs/2512.11719</guid>
<content:encoded><![CDATA[
arXiv:2512.11719v1 Announce Type: new 
Abstract: Change detection in remote sensing imagery is essential for applications such as urban planning, environmental monitoring, and disaster management. Traditional change detection methods typically identify all changes between two temporal images without distinguishing the types of transitions, which can lead to results that may not align with specific user needs. Although semantic change detection methods have attempted to address this by categorizing changes into predefined classes, these methods rely on rigid class definitions and fixed model architectures, making it difficult to mix datasets with different label sets or reuse models across tasks, as the output channels are tightly coupled with the number and type of semantic classes. To overcome these limitations, we introduce Referring Change Detection (RCD), which leverages natural language prompts to detect specific classes of changes in remote sensing images. By integrating language understanding with visual analysis, our approach allows users to specify the exact type of change they are interested in. However, training models for RCD is challenging due to the limited availability of annotated data and severe class imbalance in existing datasets. To address this, we propose a two-stage framework consisting of (I) \textbf{RCDNet}, a cross-modal fusion network designed for referring change detection, and (II) \textbf{RCDGen}, a diffusion-based synthetic data generation pipeline that produces realistic post-change images and change maps for a specified category using only pre-change image, without relying on semantic segmentation masks and thereby significantly lowering the barrier to scalable data creation. Experiments across multiple datasets show that our framework enables scalable and targeted change detection. Project website is here: https://yilmazkorkmaz1.github.io/RCD.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reframing Music-Driven 2D Dance Pose Generation as Multi-Channel Image Generation</title>
<link>https://arxiv.org/abs/2512.11720</link>
<guid>https://arxiv.org/abs/2512.11720</guid>
<content:encoded><![CDATA[
arXiv:2512.11720v1 Announce Type: new 
Abstract: Recent pose-to-video models can translate 2D pose sequences into photorealistic, identity-preserving dance videos, so the key challenge is to generate temporally coherent, rhythm-aligned 2D poses from music, especially under complex, high-variance in-the-wild distributions. We address this by reframing music-to-dance generation as a music-token-conditioned multi-channel image synthesis problem: 2D pose sequences are encoded as one-hot images, compressed by a pretrained image VAE, and modeled with a DiT-style backbone, allowing us to inherit architectural and training advances from modern text-to-image models and better capture high-variance 2D pose distributions. On top of this formulation, we introduce (i) a time-shared temporal indexing scheme that explicitly synchronizes music tokens and pose latents over time and (ii) a reference-pose conditioning strategy that preserves subject-specific body proportions and on-screen scale while enabling long-horizon segment-and-stitch generation. Experiments on a large in-the-wild 2D dance corpus and the calibrated AIST++2D benchmark show consistent improvements over representative music-to-dance methods in pose- and video-space metrics and human preference, and ablations validate the contributions of the representation, temporal indexing, and reference conditioning. See supplementary videos at https://hot-dance.github.io
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weak-to-Strong Generalization Enables Fully Automated De Novo Training of Multi-head Mask-RCNN Model for Segmenting Densely Overlapping Cell Nuclei in Multiplex Whole-slice Brain Images</title>
<link>https://arxiv.org/abs/2512.11722</link>
<guid>https://arxiv.org/abs/2512.11722</guid>
<content:encoded><![CDATA[
arXiv:2512.11722v1 Announce Type: new 
Abstract: We present a weak to strong generalization methodology for fully automated training of a multi-head extension of the Mask-RCNN method with efficient channel attention for reliable segmentation of overlapping cell nuclei in multiplex cyclic immunofluorescent (IF) whole-slide images (WSI), and present evidence for pseudo-label correction and coverage expansion, the key phenomena underlying weak to strong generalization. This method can learn to segment de novo a new class of images from a new instrument and/or a new imaging protocol without the need for human annotations. We also present metrics for automated self-diagnosis of segmentation quality in production environments, where human visual proofreading of massive WSI images is unaffordable. Our method was benchmarked against five current widely used methods and showed a significant improvement. The code, sample WSI images, and high-resolution segmentation results are provided in open form for community adoption and adaptation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.11749</link>
<guid>https://arxiv.org/abs/2512.11749</guid>
<content:encoded><![CDATA[
arXiv:2512.11749v1 Announce Type: new 
Abstract: Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting</title>
<link>https://arxiv.org/abs/2512.11763</link>
<guid>https://arxiv.org/abs/2512.11763</guid>
<content:encoded><![CDATA[
arXiv:2512.11763v1 Announce Type: new 
Abstract: Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.
  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</title>
<link>https://arxiv.org/abs/2512.11771</link>
<guid>https://arxiv.org/abs/2512.11771</guid>
<content:encoded><![CDATA[
arXiv:2512.11771v1 Announce Type: new 
Abstract: Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator</title>
<link>https://arxiv.org/abs/2512.11782</link>
<guid>https://arxiv.org/abs/2512.11782</guid>
<content:encoded><![CDATA[
arXiv:2512.11782v1 Announce Type: new 
Abstract: Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs</title>
<link>https://arxiv.org/abs/2512.11791</link>
<guid>https://arxiv.org/abs/2512.11791</guid>
<content:encoded><![CDATA[
arXiv:2512.11791v1 Announce Type: new 
Abstract: Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</title>
<link>https://arxiv.org/abs/2512.11792</link>
<guid>https://arxiv.org/abs/2512.11792</guid>
<content:encoded><![CDATA[
arXiv:2512.11792v1 Announce Type: new 
Abstract: Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particulate: Feed-Forward 3D Object Articulation</title>
<link>https://arxiv.org/abs/2512.11798</link>
<guid>https://arxiv.org/abs/2512.11798</guid>
<content:encoded><![CDATA[
arXiv:2512.11798v1 Announce Type: new 
Abstract: We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network's feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</title>
<link>https://arxiv.org/abs/2512.11799</link>
<guid>https://arxiv.org/abs/2512.11799</guid>
<content:encoded><![CDATA[
arXiv:2512.11799v1 Announce Type: new 
Abstract: Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance</title>
<link>https://arxiv.org/abs/2512.11800</link>
<guid>https://arxiv.org/abs/2512.11800</guid>
<content:encoded><![CDATA[
arXiv:2512.11800v1 Announce Type: new 
Abstract: The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Fusion of Regional Brain Experts for Interpretable Alzheimer's Disease Diagnosis</title>
<link>https://arxiv.org/abs/2512.10966</link>
<guid>https://arxiv.org/abs/2512.10966</guid>
<content:encoded><![CDATA[
arXiv:2512.10966v1 Announce Type: cross 
Abstract: Accurate and early diagnosis of Alzheimer's disease (AD) can benefit from integrating complementary information from multiple modalities, mirroring clinical practice. However, conventional fusion approaches often rely on simple concatenation of features, which cannot adaptively balance the contributions of biomarkers such as amyloid PET and MRI across brain regions. In this work, we propose MREF-AD, a Multimodal Regional Expert Fusion model for AD diagnosis. It is a Mixture-of-Experts (MoE) framework that models meso-scale brain regions in each modality as an independent expert and employs two-level gating networks to learn subject-specific fusion weights. Beyond improving diagnostic performance, MREF-AD provides modality- and region-level insight into how structural and molecular imaging jointly contribute to disease diagnosis. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), MREF-AD achieves state-of-the-art performance over baselines while providing enhanced interpretability of brain region-specific biomarker relevance, underscoring its utility as a general framework for adaptive and interpretable multimodal fusion in neuroimaging.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control</title>
<link>https://arxiv.org/abs/2512.11047</link>
<guid>https://arxiv.org/abs/2512.11047</guid>
<content:encoded><![CDATA[
arXiv:2512.11047v1 Announce Type: cross 
Abstract: Humanoid robots require precise locomotion and dexterous manipulation to perform challenging loco-manipulation tasks. Yet existing approaches, modular or end-to-end, are deficient in manipulation-aware locomotion. This confines the robot to a limited workspace, preventing it from performing large-space loco-manipulation. We attribute this to: (1) the challenge of acquiring loco-manipulation knowledge due to the scarcity of humanoid teleoperation data, and (2) the difficulty of faithfully and reliably executing locomotion commands, stemming from the limited precision and stability of existing RL controllers. To acquire richer loco-manipulation knowledge, we propose a unified latent learning framework that enables Vision-Language-Action (VLA) system to learn from low-cost action-free egocentric videos. Moreover, an efficient human data collection pipeline is devised to augment the dataset and scale the benefits. To more precisely execute the desired locomotion commands, we present a loco-manipulation-oriented (LMO) RL policy specifically tailored for accurate and stable core loco-manipulation movements, such as advancing, turning, and squatting. Building on these components, we introduce WholeBodyVLA, a unified framework for humanoid loco-manipulation. To the best of our knowledge, WholeBodyVLA is one of its kind enabling large-space humanoid loco-manipulation. It is verified via comprehensive experiments on the AgiBot X2 humanoid, outperforming prior baseline by 21.3%. It also demonstrates strong generalization and high extensibility across a broad range of tasks.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles</title>
<link>https://arxiv.org/abs/2512.11145</link>
<guid>https://arxiv.org/abs/2512.11145</guid>
<content:encoded><![CDATA[
arXiv:2512.11145v1 Announce Type: cross 
Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models</title>
<link>https://arxiv.org/abs/2512.11194</link>
<guid>https://arxiv.org/abs/2512.11194</guid>
<content:encoded><![CDATA[
arXiv:2512.11194v1 Announce Type: cross 
Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy</title>
<link>https://arxiv.org/abs/2512.11218</link>
<guid>https://arxiv.org/abs/2512.11218</guid>
<content:encoded><![CDATA[
arXiv:2512.11218v1 Announce Type: cross 
Abstract: The pursuit of out-of-distribution generalization in Vision-Language-Action (VLA) models is often hindered by catastrophic forgetting of the Vision-Language Model (VLM) backbone during fine-tuning. While co-training with external reasoning data helps, it requires experienced tuning and data-related overhead. Beyond such external dependencies, we identify an intrinsic cause within VLA datasets: modality imbalance, where language diversity is much lower than visual and action diversity. This imbalance biases the model toward visual shortcuts and language forgetting. To address this, we introduce BayesVLA, a Bayesian factorization that decomposes the policy into a visual-action prior, supporting seeing-to-act, and a language-conditioned likelihood, enabling prompt-to-specify. This inherently preserves generalization and promotes instruction following. We further incorporate pre- and post-contact phases to better leverage pre-trained foundation models. Information-theoretic analysis formally validates our effectiveness in mitigating shortcut learning. Extensive experiments show superior generalization to unseen instructions, objects, and environments compared to existing methods. Project page is available at: https://xukechun.github.io/papers/BayesVLA.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Aware Multi-Expert Architecture For Lifelong Deep Learning</title>
<link>https://arxiv.org/abs/2512.11243</link>
<guid>https://arxiv.org/abs/2512.11243</guid>
<content:encoded><![CDATA[
arXiv:2512.11243v1 Announce Type: cross 
Abstract: Lifelong deep learning (LDL) trains neural networks to learn sequentially across tasks while preserving prior knowledge. We propose Task-Aware Multi-Expert (TAME), a continual learning algorithm that leverages task similarity to guide expert selection and knowledge transfer. TAME maintains a pool of pretrained neural networks and activates the most relevant expert for each new task. A shared dense layer integrates features from the chosen expert to generate predictions. To reduce catastrophic forgetting, TAME uses a replay buffer that stores representative samples and embeddings from previous tasks and reuses them during training. An attention mechanism further prioritizes the most relevant stored information for each prediction. Together, these components allow TAME to adapt flexibly while retaining important knowledge across evolving task sequences. Experiments on binary classification tasks derived from CIFAR-100 show that TAME improves accuracy on new tasks while sustaining performance on earlier ones, highlighting its effectiveness in balancing adaptation and retention in lifelong learning settings.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction</title>
<link>https://arxiv.org/abs/2512.11399</link>
<guid>https://arxiv.org/abs/2512.11399</guid>
<content:encoded><![CDATA[
arXiv:2512.11399v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Baseline: Examining Baseline Effects on Explainability Metrics</title>
<link>https://arxiv.org/abs/2512.11433</link>
<guid>https://arxiv.org/abs/2512.11433</guid>
<content:encoded><![CDATA[
arXiv:2512.11433v1 Announce Type: cross 
Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems</title>
<link>https://arxiv.org/abs/2512.11532</link>
<guid>https://arxiv.org/abs/2512.11532</guid>
<content:encoded><![CDATA[
arXiv:2512.11532v1 Announce Type: cross 
Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brain-Semantoks: Learning Semantic Tokens of Brain Dynamics with a Self-Distilled Foundation Model</title>
<link>https://arxiv.org/abs/2512.11582</link>
<guid>https://arxiv.org/abs/2512.11582</guid>
<content:encoded><![CDATA[
arXiv:2512.11582v1 Announce Type: cross 
Abstract: The development of foundation models for functional magnetic resonance imaging (fMRI) time series holds significant promise for predicting phenotypes related to disease and cognition. Current models, however, are often trained using a mask-and-reconstruct objective on small brain regions. This focus on low-level information leads to representations that are sensitive to noise and temporal fluctuations, necessitating extensive fine-tuning for downstream tasks. We introduce Brain-Semantoks, a self-supervised framework designed specifically to learn abstract representations of brain dynamics. Its architecture is built on two core innovations: a semantic tokenizer that aggregates noisy regional signals into robust tokens representing functional networks, and a self-distillation objective that enforces representational stability across time. We show that this objective is stabilized through a novel training curriculum, ensuring the model robustly learns meaningful features from low signal-to-noise time series. We demonstrate that learned representations enable strong performance on a variety of downstream tasks even when only using a linear probe. Furthermore, we provide comprehensive scaling analyses indicating more unlabeled data reliably results in out-of-distribution performance gains without domain adaptation.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastics of shapes and Kunita flows</title>
<link>https://arxiv.org/abs/2512.11676</link>
<guid>https://arxiv.org/abs/2512.11676</guid>
<content:encoded><![CDATA[
arXiv:2512.11676v1 Announce Type: cross 
Abstract: Stochastic processes of evolving shapes are used in applications including evolutionary biology, where morphology changes stochastically as a function of evolutionary processes. Due to the non-linear and often infinite-dimensional nature of shape spaces, the mathematical construction of suitable stochastic shape processes is far from immediate. We define and formalize properties that stochastic shape processes should ideally satisfy to be compatible with the shape structure, and we link this to Kunita flows that, when acting on shape spaces, induce stochastic processes that satisfy these criteria by their construction. We couple this with a survey of other relevant shape stochastic processes and show how bridge sampling techniques can be used to condition shape stochastic processes on observed data thereby allowing for statistical inference of parameters of the stochastic dynamics.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Particle Image Velocimetry Refinement via Consensus ADMM</title>
<link>https://arxiv.org/abs/2512.11695</link>
<guid>https://arxiv.org/abs/2512.11695</guid>
<content:encoded><![CDATA[
arXiv:2512.11695v1 Announce Type: cross 
Abstract: Particle Image Velocimetry (PIV) is an imaging technique in experimental fluid dynamics that quantifies flow fields around bluff bodies by analyzing the displacement of neutrally buoyant tracer particles immersed in the fluid. Traditional PIV approaches typically depend on tuning parameters specific to the imaging setup, making the performance sensitive to variations in illumination, flow conditions, and seeding density. On the other hand, even state-of-the-art machine learning methods for flow quantification are fragile outside their training set. In our experiments, we observed that flow quantification would improve if different tunings (or algorithms) were applied to different regions of the same image pair. In this work, we parallelize the instantaneous flow quantification with multiple algorithms and adopt a consensus framework based on the alternating direction method of multipliers, seamlessly incorporating priors such as smoothness and incompressibility. We perform several numerical experiments to demonstrate the benefits of this approach. For instance, we achieve a decrease in end-point-error of up to 20% of a dense-inverse-search estimator at an inference rate of 60Hz, and we show how this performance boost can be increased further with outlier rejection. Our method is implemented in JAX, effectively exploiting hardware acceleration, and integrated in Flow Gym, enabling (i) reproducible comparisons with the state-of-the-art, (ii) testing different base algorithms, (iii) straightforward deployment for active fluids control applications.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images</title>
<link>https://arxiv.org/abs/2512.11745</link>
<guid>https://arxiv.org/abs/2512.11745</guid>
<content:encoded><![CDATA[
arXiv:2512.11745v1 Announce Type: cross 
Abstract: Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE's ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis</title>
<link>https://arxiv.org/abs/2512.11797</link>
<guid>https://arxiv.org/abs/2512.11797</guid>
<content:encoded><![CDATA[
arXiv:2512.11797v1 Announce Type: cross 
Abstract: The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot's kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Action Counting with Dynamic Queries</title>
<link>https://arxiv.org/abs/2403.01543</link>
<guid>https://arxiv.org/abs/2403.01543</guid>
<content:encoded><![CDATA[
arXiv:2403.01543v4 Announce Type: replace 
Abstract: Temporal repetition counting aims to quantify the repeated action cycles within a video. The majority of existing methods rely on the similarity correlation matrix to characterize the repetitiveness of actions, but their scalability is hindered due to the quadratic computational complexity. In this work, we introduce a novel approach that employs an action query representation to localize repeated action cycles with linear computational complexity. Based on this representation, we further develop two key components to tackle the essential challenges of temporal repetition counting. Firstly, to facilitate open-set action counting, we propose the dynamic update scheme on action queries. Unlike static action queries, this approach dynamically embeds video features into action queries, offering a more flexible and generalizable representation. Secondly, to distinguish between actions of interest and background noise actions, we incorporate inter-query contrastive learning to regularize the video representations corresponding to different action queries. As a result, our method significantly outperforms previous works, particularly in terms of long video sequences, unseen actions, and actions at various speeds. On the challenging RepCountA benchmark, we outperform the state-of-the-art method TransRAC by 26.5% in OBO accuracy, with a 22.7% mean error decrease and 94.1% computational burden reduction. Code is available at https://github.com/lizishi/DeTRC.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecDETR: A transformer-based hyperspectral point object detection network</title>
<link>https://arxiv.org/abs/2405.10148</link>
<guid>https://arxiv.org/abs/2405.10148</guid>
<content:encoded><![CDATA[
arXiv:2405.10148v4 Announce Type: replace 
Abstract: Hyperspectral target detection (HTD) aims to identify specific materials based on spectral information in hyperspectral imagery and can detect extremely small-sized objects, some of which occupy a smaller than one-pixel area. However, existing HTD methods are developed based on per-pixel binary classification, neglecting the three-dimensional cube structure of hyperspectral images (HSIs) that integrates both spatial and spectral dimensions. The synergistic existence of spatial and spectral features in HSIs enable objects to simultaneously exhibit both, yet the per-pixel HTD framework limits the joint expression of these features. In this paper, we rethink HTD from the perspective of spatial-spectral synergistic representation and propose hyperspectral point object detection as an innovative task framework. We introduce SpecDETR, the first specialized network for hyperspectral multi-class point object detection, which eliminates dependence on pre-trained backbone networks commonly required by vision-based object detectors. SpecDETR uses a multi-layer Transformer encoder with self-excited subpixel-scale attention modules to directly extract deep spatial-spectral joint features from hyperspectral cubes. We develop a simulated hyperspectral point object detection benchmark termed SPOD, and for the first time, evaluate and compare the performance of visual object detection networks and HTD methods on hyperspectral point object detection. Extensive experiments demonstrate that our proposed SpecDETR outperforms SOTA visual object detection networks and HTD methods. Our code and dataset are available at https://github.com/ZhaoxuLi123/SpecDETR.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual-Friendly Concept Protection via Selective Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2408.08518</link>
<guid>https://arxiv.org/abs/2408.08518</guid>
<content:encoded><![CDATA[
arXiv:2408.08518v3 Announce Type: replace 
Abstract: Personalized concept generation by tuning diffusion models with a few images raises potential legal and ethical concerns regarding privacy and intellectual property rights. Researchers attempt to prevent malicious personalization using adversarial perturbations. However, previous efforts have mainly focused on the effectiveness of protection while neglecting the visibility of perturbations. They utilize global adversarial perturbations, which introduce noticeable alterations to original images and significantly degrade visual quality. In this work, we propose the Visual-Friendly Concept Protection (VCPro) framework, which prioritizes the protection of key concepts chosen by the image owner through adversarial perturbations with lower perceptibility. To ensure these perturbations are as inconspicuous as possible, we introduce a relaxed optimization objective to identify the least perceptible yet effective adversarial perturbations, solved using the Lagrangian multiplier method. Qualitative and quantitative experiments validate that VCPro achieves a better trade-off between the visibility of perturbations and protection effectiveness, effectively prioritizing the protection of target concepts in images with less perceptible perturbations.
]]></content:encoded>
<pubDate>Mon, 15 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>