<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.CV updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.CV</link>


<item>
<title>ARIAL: An Agentic Framework for Document VQA with Precise Answer Localization</title>
<link>https://arxiv.org/abs/2511.18192</link>
<guid>https://arxiv.org/abs/2511.18192</guid>
<content:encoded><![CDATA[
<div> Keywords: Document Visual Question Answering, spatial grounding, ARIAL, modular framework, interpretability<br /><br />Summary: Document Visual Question Answering (VQA) demands systems to accurately extract textual answers and localize them precisely within document images to ensure interpretability, especially in critical applications. Existing methods either focus on textual accuracy with insufficient spatial grounding or compromise performance for better explainability. ARIAL (Agentic Reasoning for Interpretable Answer Localization) addresses this by using an LLM-based planning agent to coordinate specialized tools, enabling both high-accuracy answer extraction and reliable spatial localization. The framework decomposes Document VQA into four key subtasks: OCR text extraction using TrOCR, context selection via retrieval-augmented semantic search, answer generation through a fine-tuned Gemma 3-27B model, and precise bounding-box localization by aligning extracted text to image regions. ARIAL’s modular design facilitates transparent reasoning, auditability at the tool level, and independent optimization of components. Evaluated on four benchmarks—DocVQA, FUNSD, CORD, and SROIE—ARIAL achieves state-of-the-art textual accuracy (ANLS) and spatial precision (mAP), exceeding previous best results such as those by DLaVA. These results underscore how agentic orchestration of specialized tools enhances both performance and interpretability, advancing trustworthy and explainable document AI systems. <div>
arXiv:2511.18192v2 Announce Type: replace 
Abstract: Document Visual Question Answering (VQA) requires models to not only extract accurate textual answers but also precisely localize them within document images, a capability critical for interpretability in high-stakes applications. However, existing systems achieve strong textual accuracy while producing unreliable spatial grounding, or sacrifice performance for interpretability. We present ARIAL (Agentic Reasoning for Interpretable Answer Localization), a modular framework that orchestrates specialized tools through an LLM-based planning agent to achieve both precise answer extraction and reliable spatial grounding. ARIAL decomposes Document VQA into structured subtasks: OCR-based text extraction with TrOCR, retrieval-augmented context selection using semantic search, answer generation via a fine-tuned Gemma 3-27B model, and explicit bounding-box localization through text-to-region alignment. This modular architecture produces transparent reasoning traces, enabling tool-level auditability and independent component optimization. We evaluate ARIAL on four benchmarks (DocVQA, FUNSD, CORD, and SROIE) using both textual accuracy (ANLS) and spatial precision (mAP at IoU 0.50 to 0.95). ARIAL achieves state-of-the-art results across all datasets: 88.7 ANLS and 50.1 mAP on DocVQA, 90.0 ANLS and 50.3 mAP on FUNSD, 85.5 ANLS and 60.2 mAP on CORD, and 93.1 ANLS on SROIE, surpassing the previous best method (DLaVA) by +2.8 ANLS and +3.9 mAP on DocVQA. Our work demonstrates how agentic orchestration of specialized tools can simultaneously improve performance and interpretability, providing a pathway toward trustworthy, explainable document AI systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing What Matters: Visual Preference Policy Optimization for Visual Generation</title>
<link>https://arxiv.org/abs/2511.18719</link>
<guid>https://arxiv.org/abs/2511.18719</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Visual Generative Models, Group Relative Policy Optimization, Pixel-Level Advantages, Perceptual Structuring Module<br /><br />Summary:<br /><br />Reinforcement learning (RL) has proven effective for improving visual generative models by aligning them with human preferences using Group Relative Policy Optimization (GRPO). However, conventional GRPO methods rely solely on a single scalar reward per sample, treating the entire image or video as a whole and neglecting detailed spatial and temporal information. This coarse feedback limits the ability to correct localized imperfections and to model subtle perceptual details. To address these limitations, the authors propose Visual Preference Policy Optimization (ViPO), a novel GRPO variant. ViPO transforms scalar rewards into structured pixel-level advantage maps, enabling finer and more informative supervision. At the core of ViPO is a Perceptual Structuring Module that leverages pretrained vision models to generate spatially and temporally aware advantage maps. This approach effectively reallocates the optimization focus towards perceptually significant regions while maintaining training stability comparable to standard GRPO. Extensive experiments on image and video benchmarks demonstrate that ViPO consistently outperforms vanilla GRPO by achieving better alignment with human-preference rewards within the training domain and improving generalization to out-of-domain scenarios. ViPO is architecture-agnostic, lightweight, and seamlessly integrates with existing GRPO pipelines, offering a more expressive learning signal for visual generation tasks. <div>
arXiv:2511.18719v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become a powerful tool for post-training visual generative models, with Group Relative Policy Optimization (GRPO) increasingly used to align generators with human preferences. However, existing GRPO pipelines rely on a single scalar reward per sample, treating each image or video as a holistic entity and ignoring the rich spatial and temporal structure of visual content. This coarse supervision hinders the correction of localized artifacts and the modeling of fine-grained perceptual cues. We introduce Visual Preference Policy Optimization (ViPO), a GRPO variant that lifts scalar feedback into structured, pixel-level advantages. ViPO employs a Perceptual Structuring Module that uses pretrained vision backbones to construct spatially and temporally aware advantage maps, redistributing optimization pressure toward perceptually important regions while preserving the stability of standard GRPO. Across both image and video benchmarks, ViPO consistently outperforms vanilla GRPO, improving in-domain alignment with human-preference rewards and enhancing generalization on out-of-domain evaluations. The method is architecture-agnostic, lightweight, and fully compatible with existing GRPO training pipelines, providing a more expressive and informative learning signal for visual generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yo'City: Personalized and Boundless 3D Realistic City Scene Generation via Self-Critic Expansion</title>
<link>https://arxiv.org/abs/2511.18734</link>
<guid>https://arxiv.org/abs/2511.18734</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D city generation, large models, hierarchical planning, image-to-3D synthesis, city expansion<br /><br />Summary:<br /><br />Yo'City is a novel framework aimed at realistic 3D city generation that addresses the limitations of traditional single diffusion model approaches. It enables user-customized and infinitely expandable city-scale scenes by leveraging the reasoning and compositional abilities of large pre-trained models. The method uses a top-down hierarchical planning strategy that organizes the city into a "City-District-Grid" structure, where a Global Planner outlines the overall layout and functional districts, and a Local Designer provides detailed grid-level descriptions. The framework employs a "produce-refine-evaluate" loop for isometric image synthesis to generate detailed grid-level visuals, which are then converted into 3D models through image-to-3D generation techniques. To simulate ongoing urban evolution, Yo'City incorporates an interactive, relationship-driven expansion mechanism that performs scene graph-based layout optimization, taking into account spatial coherence, distances, and semantic relationships. The authors also construct a comprehensive benchmark dataset with six multi-dimensional metrics assessing semantics, geometry, texture, and city layout quality. Extensive experimental results show that Yo'City consistently outperforms current state-of-the-art methods across all evaluation criteria, demonstrating improvements in generation quality and scalability for practical applications such as virtual reality and digital twins. <div>
arXiv:2511.18734v2 Announce Type: replace 
Abstract: Realistic 3D city generation is fundamental to a wide range of applications, including virtual reality and digital twins. However, most existing methods rely on training a single diffusion model, which limits their ability to generate personalized and boundless city-scale scenes. In this paper, we present Yo'City, a novel agentic framework that enables user-customized and infinitely expandable 3D city generation by leveraging the reasoning and compositional capabilities of off-the-shelf large models. Specifically, Yo'City first conceptualize the city through a top-down planning strategy that defines a hierarchical "City-District-Grid" structure. The Global Planner determines the overall layout and potential functional districts, while the Local Designer further refines each district with detailed grid-level descriptions. Subsequently, the grid-level 3D generation is achieved through a "produce-refine-evaluate" isometric image synthesis loop, followed by image-to-3D generation. To simulate continuous city evolution, Yo'City further introduces a user-interactive, relationship-guided expansion mechanism, which performs scene graph-based distance- and semantics-aware layout optimization, ensuring spatially coherent city growth. To comprehensively evaluate our method, we construct a diverse benchmark dataset and design six multi-dimensional metrics that assess generation quality from the perspectives of semantics, geometry, texture, and layout. Extensive experiments demonstrate that Yo'City consistently outperforms existing state-of-the-art methods across all evaluation aspects.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProxT2I: Efficient Reward-Guided Text-to-Image Generation via Proximal Diffusion</title>
<link>https://arxiv.org/abs/2511.18742</link>
<guid>https://arxiv.org/abs/2511.18742</guid>
<content:encoded><![CDATA[
<div> Diffusion models, text-to-image, proximal operators, reinforcement learning, LAION-Face-T2I-15M<br /><br />Summary:<br /><br />This work introduces ProxT2I, a novel text-to-image diffusion model that departs from the traditional forward discretization approach by using backward discretizations and conditional proximal operators learned from data instead of score functions. This method addresses key issues in existing samplers, such as slowness and instability, which require many sampling steps for generating high-quality images. The authors incorporate reinforcement learning and policy optimization techniques to directly optimize samplers for task-specific rewards, enhancing both efficiency and alignment with human preferences. A major contribution is the creation of LAION-Face-T2I-15M, a large-scale, open-source dataset consisting of 15 million high-quality human images with detailed captions, enabling robust training and evaluation of the model. ProxT2I consistently outperforms score-based baseline models in terms of sampling speed and preference alignment while matching state-of-the-art results in quality. Notably, it achieves this with a smaller model size and reduced computational requirements, offering a lightweight yet effective solution for human-centric text-to-image generation tasks. This work advances the field by combining novel modeling techniques with reinforcement learning and large-scale data to improve generative performance and practicality. <div>
arXiv:2511.18742v2 Announce Type: replace 
Abstract: Diffusion models have emerged as a dominant paradigm for generative modeling across a wide range of domains, including prompt-conditional generation. The vast majority of samplers, however, rely on forward discretization of the reverse diffusion process and use score functions that are learned from data. Such forward and explicit discretizations can be slow and unstable, requiring a large number of sampling steps to produce good-quality samples. In this work we develop a text-to-image (T2I) diffusion model based on backward discretizations, dubbed ProxT2I, relying on learned and conditional proximal operators instead of score functions. We further leverage recent advances in reinforcement learning and policy optimization to optimize our samplers for task-specific rewards. Additionally, we develop a new large-scale and open-source dataset comprising 15 million high-quality human images with fine-grained captions, called LAION-Face-T2I-15M, for training and evaluation. Our approach consistently enhances sampling efficiency and human-preference alignment compared to score-based baselines, and achieves results on par with existing state-of-the-art and open-source text-to-image models while requiring lower compute and smaller model size, offering a lightweight yet performant solution for human text-to-image generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiP: Taming Diffusion Models in Pixel Space</title>
<link>https://arxiv.org/abs/2511.18822</link>
<guid>https://arxiv.org/abs/2511.18822</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Latent Diffusion Models, pixel space diffusion, Diffusion Transformer, ImageNet FID score<br /><br />Summary:<br /><br />1. Diffusion models typically face a trade-off between generation quality and computational efficiency, with Latent Diffusion Models (LDMs) offering efficiency but potentially losing information and lacking end-to-end training. 2. Pixel space diffusion models avoid reliance on VAEs but are computationally expensive for high-resolution image synthesis. 3. The paper proposes DiP, an efficient pixel space diffusion framework that decouples image generation into two stages: a global stage using a Diffusion Transformer (DiT) operating on large patches to build global structure, and a local stage with a lightweight Patch Detailer Head that restores fine details using contextual information. 4. This two-stage, synergistic approach achieves computational efficiency on par with LDMs without using a VAE, improving inference speeds by up to 10× compared to previous methods while increasing parameters by only 0.3%. 5. DiP achieves state-of-the-art performance with a Fréchet Inception Distance (FID) score of 1.79 on ImageNet at 256×256 resolution, demonstrating both high-quality generation and efficient computation. <div>
arXiv:2511.18822v2 Announce Type: replace 
Abstract: Diffusion models face a fundamental trade-off between generation quality and computational efficiency. Latent Diffusion Models (LDMs) offer an efficient solution but suffer from potential information loss and non-end-to-end training. In contrast, existing pixel space models bypass VAEs but are computationally prohibitive for high-resolution synthesis. To resolve this dilemma, we propose DiP, an efficient pixel space diffusion framework. DiP decouples generation into a global and a local stage: a Diffusion Transformer (DiT) backbone operates on large patches for efficient global structure construction, while a co-trained lightweight Patch Detailer Head leverages contextual features to restore fine-grained local details. This synergistic design achieves computational efficiency comparable to LDMs without relying on a VAE. DiP is accomplished with up to 10$\times$ faster inference speeds than previous method while increasing the total number of parameters by only 0.3%, and achieves an 1.79 FID score on ImageNet 256$\times$256.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2511.19145</link>
<guid>https://arxiv.org/abs/2511.19145</guid>
<content:encoded><![CDATA[
<div> Activation Boundary Matching, Low-Rank Adaptation, ABM-LoRA, convergence acceleration, VTAB-1K

<br /><br />Summary: This paper introduces Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a novel initialization method designed to accelerate the convergence of low-rank adapters in neural networks. Unlike the conventional LoRA approach that starts from random initialization, ABM-LoRA aligns the adapter's activation boundaries with those of a pretrained model, improving the transfer of gradient information. This alignment minimizes information loss at initialization by ensuring the projection of full-parameter gradients is maximized within the adapter's subspace. As a result, ABM-LoRA achieves a lower initial loss and faster convergence during training. The authors validate their approach on a diverse set of tasks and architectures, including language understanding with T5-Base on GLUE, dialogue generation using LLaMA2-7B on WizardLM, and vision recognition with ViT-B/16 on VTAB-1K. Notably, on VTAB-1K, ABM-LoRA attains the highest accuracy compared to existing methods. The method also demonstrates substantial improvements on structured reasoning tasks that require geometric understanding, highlighting its potential for broad applicability across domains. This principled initialization strategy represents a significant step forward in enhancing the efficiency and effectiveness of low-rank adaptation techniques. <div>
arXiv:2511.19145v3 Announce Type: replace 
Abstract: We propose Activation Boundary Matching for Low-Rank Adaptation (ABM-LoRA), a principled initialization strategy that substantially accelerates the convergence of low-rank adapters. While LoRA offers high parameter efficiency, its random initialization restricts gradient updates to a mismatched tangent space, causing significant information loss and hindering early convergence. Our ABM-LoRA addresses this by aligning the adapter's activation boundaries with those of the pretrained model before downstream training, thereby maximizing the projection of full-parameter gradients into the adapter subspace. This alignment sharply reduces information loss at initialization, yields a lower starting loss, and accelerates convergence. We demonstrate ABM-LoRA's effectiveness across diverse architectures and tasks: language understanding (T5-Base on GLUE), dialogue generation (LLaMA2-7B on WizardLM), and vision recognition (ViT-B/16 on VTAB-1K). On VTAB-1K, it achieves the highest accuracy among all methods, with strong gains on structured reasoning tasks requiring geometric understanding.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.19220</link>
<guid>https://arxiv.org/abs/2511.19220</guid>
<content:encoded><![CDATA[
<div> Keywords: vision language models, medical visual question answering, visual grounding, Italy, EuropeMedQA<br /><br />Summary:<br /><br />1. The study evaluates the visual grounding capabilities of four cutting-edge vision language models (VLMs)—Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp—on Italian medical visual question answering tasks.<br /><br />2. Using 60 image-dependent questions from the EuropeMedQA Italian dataset, the researchers replaced the original medical images with blank placeholders to assess whether these models genuinely rely on visual input when answering questions.<br /><br />3. The findings reveal substantial variability in the degree to which models depend on visual information. GPT-4o demonstrated the strongest visual dependency, with a 27.9 percentage point drop in accuracy when images were omitted, decreasing from 83.2% to 55.3%.<br /><br />4. In contrast, GPT-5-mini, Gemini, and Claude showed only modest accuracy drops of 8.5, 2.4, and 5.6 percentage points respectively, indicating they rely more on textual shortcuts or reasoning than actual image analysis.<br /><br />5. Further examination of explanations generated by the models indicated that all provided confident but sometimes fabricated justifications for visual interpretations, highlighting differences in robustness and the critical need for thorough evaluations before clinical application. <div>
arXiv:2511.19220v2 Announce Type: replace 
Abstract: Large vision language models (VLMs) have achieved impressive performance on medical visual question answering benchmarks, yet their reliance on visual information remains unclear. We investigate whether frontier VLMs demonstrate genuine visual grounding when answering Italian medical questions by testing four state-of-the-art models: Claude Sonnet 4.5, GPT-4o, GPT-5-mini, and Gemini 2.0 flash exp. Using 60 questions from the EuropeMedQA Italian dataset that explicitly require image interpretation, we substitute correct medical images with blank placeholders to test whether models truly integrate visual and textual information. Our results reveal striking variability in visual dependency: GPT-4o shows the strongest visual grounding with a 27.9pp accuracy drop (83.2% [74.6%, 91.7%] to 55.3% [44.1%, 66.6%]), while GPT-5-mini, Gemini, and Claude maintain high accuracy with modest drops of 8.5pp, 2.4pp, and 5.6pp respectively. Analysis of model-generated reasoning reveals confident explanations for fabricated visual interpretations across all models, suggesting varying degrees of reliance on textual shortcuts versus genuine visual analysis. These findings highlight critical differences in model robustness and the need for rigorous evaluation before clinical deployment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Plug-and-play Memory for Guiding Video Diffusion Models</title>
<link>https://arxiv.org/abs/2511.19229</link>
<guid>https://arxiv.org/abs/2511.19229</guid>
<content:encoded><![CDATA[
<div> Diffusion Transformer, Video Generation, Memory Encoder, Physical Consistency, Temporal Coherence  

<br /><br />Summary:  
This paper addresses the challenge of physical law violations and lack of commonsense dynamics in Diffusion Transformer (DiT) based video generation models, which, despite producing high-quality and temporally coherent videos, fail to incorporate explicit world knowledge. Inspired by in-context memory in Transformer-based large language models, the authors empirically demonstrate that DiT’s hidden states can be manipulated via simple interventions. They find that low-pass and high-pass filters in the embedding space disentangle low-level appearance features from high-level physical and semantic information, facilitating targeted guidance. Based on these insights, the paper proposes a learnable memory encoder, DiT-Mem, built with stacked 3D CNNs, spectral filters, and self-attention layers, which encodes reference videos into compact memory tokens. These tokens are integrated into DiT’s self-attention mechanism as a plug-and-play memory during inference. Training freezes the diffusion backbone and optimizes only the memory encoder, allowing efficient learning with 150M parameters and 10K data samples. Extensive experiments demonstrate that DiT-Mem improves adherence to physical rules and enhances video fidelity. The code and datasets are publicly available to foster further research in memory-augmented video generation. <div>
arXiv:2511.19229v2 Announce Type: replace 
Abstract: Diffusion Transformer(DiT) based video generation models have recently achieved impressive visual quality and temporal coherence, but they still frequently violate basic physical laws and commonsense dynamics, revealing a lack of explicit world knowledge. In this work, we explore how to equip them with a plug-and-play memory that injects useful world knowledge. Motivated by in-context memory in Transformer-based LLMs, we conduct empirical studies to show that DiT can be steered via interventions on its hidden states, and simple low-pass and high-pass filters in the embedding space naturally disentangle low-level appearance and high-level physical/semantic cues, enabling targeted guidance. Building on these observations, we propose a learnable memory encoder DiT-Mem, composed of stacked 3D CNNs, low-/high-pass filters, and self-attention layers. The encoder maps reference videos into a compact set of memory tokens, which are concatenated as the memory within the DiT self-attention layers. During training, we keep the diffusion backbone frozen, and only optimize the memory encoder. It yields a rather efficient training process on few training parameters (150M) and 10K data samples, and enables plug-and-play usage at inference time. Extensive experiments on state-of-the-art models demonstrate the effectiveness of our method in improving physical rule following and video fidelity. Our code and data are publicly released here: https://thrcle421.github.io/DiT-Mem-Web/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrismAudio: Decomposed Chain-of-Thoughts and Multi-dimensional Rewards for Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2511.18833</link>
<guid>https://arxiv.org/abs/2511.18833</guid>
<content:encoded><![CDATA[
<div> Keywords: Video-to-Audio generation, Reinforcement Learning, Chain-of-Thought, Fast-GRPO, AudioCanvas<br /><br />Summary:  
1. PrismAudio is a novel framework designed for Video-to-Audio (V2A) generation that addresses the challenge of balancing four key perceptual aspects: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy.  
2. The framework innovatively applies Reinforcement Learning (RL) with specialized Chain-of-Thought (CoT) planning to break down complex perceptual goals into four distinct CoT modules: Semantic, Temporal, Aesthetic, and Spatial, each with its own targeted reward function.  
3. This modular CoT-reward setup allows multidimensional RL optimization that resolves the objective entanglement problem by guiding the model to improve jointly across all perceptual dimensions while maintaining interpretability.  
4. To enhance computational efficiency, the authors propose Fast-GRPO, a hybrid ODE-SDE sampling technique that significantly reduces training overhead compared to traditional GRPO implementations.  
5. The work also introduces AudioCanvas, a new V2A benchmark dataset featuring 300 single-event classes and 501 multi-event samples, designed to be more diverse, balanced, and realistic than existing datasets.  
6. Experimental evaluations demonstrate that PrismAudio achieves state-of-the-art performance on both the in-domain VGGSound dataset and the out-of-domain AudioCanvas benchmark, excelling across all four perceptual metrics.  
7. The project and codebase details are publicly accessible at https://PrismAudio-Project.github.io. <div>
arXiv:2511.18833v3 Announce Type: replace-cross 
Abstract: Video-to-Audio (V2A) generation requires balancing four critical perceptual dimensions: semantic consistency, audio-visual temporal synchrony, aesthetic quality, and spatial accuracy; yet existing methods suffer from objective entanglement that conflates competing goals in single loss functions and lack human preference alignment. We introduce PrismAudio, the first framework to integrate Reinforcement Learning into V2A generation with specialized Chain-of-Thought (CoT) planning. Our approach decomposes monolithic reasoning into four specialized CoT modules (Semantic, Temporal, Aesthetic, and Spatial CoT), each paired with targeted reward functions. This CoT-reward correspondence enables multidimensional RL optimization that guides the model to jointly generate better reasoning across all perspectives, solving the objective entanglement problem while preserving interpretability. To make this optimization computationally practical, we propose Fast-GRPO, which employs hybrid ODE-SDE sampling that dramatically reduces the training overhead compared to existing GRPO implementations. We also introduce AudioCanvas, a rigorous benchmark that is more distributionally balanced and covers more realistically diverse and challenging scenarios than existing datasets, with 300 single-event classes and 501 multi-event samples. Experimental results demonstrate that PrismAudio achieves state-of-the-art performance across all four perceptual dimensions on both the in-domain VGGSound test set and out-of-domain AudioCanvas benchmark. The project page is available at https://PrismAudio-Project.github.io.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SO-Bench: A Structural Output Evaluation of Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.21750</link>
<guid>https://arxiv.org/abs/2511.21750</guid>
<content:encoded><![CDATA[
<div> arXiv:2511.21750v1  
Keywords: Multimodal Large Language Models, Schema-grounded Extraction, Visual Structured Output, SO-Bench Benchmark, Multimodal Reasoning  

<br /><br />Summary:  
This paper addresses the challenge of evaluating schema-grounded information extraction and reasoning within multimodal large language models (MLLMs), particularly focusing on their capacity to produce structured outputs that comply with predefined data formats. Unlike existing benchmarks primarily centered on textual data, the study introduces SO-Bench, a comprehensive benchmark designed to assess and improve MLLMs' structured output capabilities across four diverse visual domains: user interface (UI) screens, natural images, documents, and charts. SO-Bench is constructed using more than 6,500 diverse JSON schemas and 1,800 curated image-schema pairs, all validated for high quality by human annotators. The authors conduct systematic benchmarking experiments involving both open-source and cutting-edge proprietary MLLMs, revealing persistent deficiencies in generating accurate, schema-compliant responses. These findings highlight the current limitations in multimodal structured reasoning. To address this gap, the study additionally performs training interventions that significantly enhance the structured generation performance of MLLMs. The paper emphasizes the importance of improved benchmarks like SO-Bench for fostering progress in the field and expresses the intent to publicly release the benchmark to benefit the research community developing multimodal structured reasoning systems. <div>
arXiv:2511.21750v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saddle-Free Guidance: Improved On-Manifold Sampling without Labels or Additional Training</title>
<link>https://arxiv.org/abs/2511.21863</link>
<guid>https://arxiv.org/abs/2511.21863</guid>
<content:encoded><![CDATA[
<div> Keywords: score-based generative models, saddle-free guidance, classifier-free guidance, image generation, diffusion models<br /><br />Summary: Score-based generative models rely on guidance to produce plausible, on-manifold samples, but existing methods like Classifier-Free Guidance (CFG) and Auto-Guidance have limitations such as requiring labeled data or additional model training. The authors discover that the positive curvature of log density estimates in saddle regions can serve as an effective guidance signal for these models. They introduce a novel method called saddle-free guidance (SFG), which tracks the maximal positive curvature of the log density during generation to guide a single score-based model without the need for extra training or labeled datasets. SFG has computational costs comparable to CFG and is compatible with standard diffusion and flow matching models. Experimental results show that SFG achieves state-of-the-art performance in unconditional ImageNet-512 generation based on FID and FD-DINOv2 metrics. Furthermore, combining SFG with Auto-Guidance leads to even better FD-DINOv2 scores on unconditional samples. Additional tests with FLUX.1-dev and Stable Diffusion v3.5 demonstrate that SFG enhances the diversity of generated images while preserving strong prompt adherence and image fidelity, making it a versatile and efficient approach for score-based generative modeling. <div>
arXiv:2511.21863v1 Announce Type: new 
Abstract: Score-based generative models require guidance in order to generate plausible, on-manifold samples. The most popular guidance method, Classifier-Free Guidance (CFG), is only applicable in settings with labeled data and requires training an additional unconditional score-based model. More recently, Auto-Guidance adopts a smaller, less capable version of the original model to guide generation. While each method effectively promotes the fidelity of generated data, each requires labeled data or the training of additional models, making it challenging to guide score-based models when (labeled) training data are not available or training new models is not feasible.
  We make the surprising discovery that the positive curvature of log density estimates in saddle regions provides strong guidance for score-based models. Motivated by this, we develop saddle-free guidance (SFG) which maintains estimates of maximal positive curvature of the log density to guide individual score-based models. SFG has the same computational cost of classifier-free guidance, does not require additional training, and works with off-the-shelf diffusion and flow matching models. Our experiments indicate that SFG achieves state-of-the-art FID and FD-DINOv2 metrics in single-model unconditional ImageNet-512 generation. When SFG is combined with Auto-Guidance, its unconditional samples achieve general state-of-the-art in FD-DINOv2 score. Our experiments with FLUX.1-dev and Stable Diffusion v3.5 indicate that SFG boosts the diversity of output images compared to CFG while maintaining excellent prompt adherence and image fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniArt: Unified 3D Representation for Generating 3D Articulated Objects with Open-Set Articulation</title>
<link>https://arxiv.org/abs/2511.21887</link>
<guid>https://arxiv.org/abs/2511.21887</guid>
<content:encoded><![CDATA[
<div> articulated 3D objects, diffusion framework, unified latent representation, articulation prediction, PartNet-Mobility benchmark  

<br /><br />Summary:  
The paper introduces UniArt, a diffusion-based framework designed to synthesize fully articulated 3D objects directly from a single input image in an end-to-end manner. This approach contrasts with prior multi-stage techniques by establishing a unified latent representation that simultaneously encodes geometry, texture, part segmentation, and kinematic parameters, enabling a more cohesive output. A novel reversible joint-to-voxel embedding is proposed, which spatially aligns articulation features with volumetric geometry, allowing the model to learn coherent motion patterns along with the structural formation of the object. Additionally, the authors reformulate the articulation type prediction task as an open-set problem, removing the constraint of fixed joint categories, and thereby improving generalization to novel joints and unseen object types. The effectiveness and versatility of UniArt are validated through experiments conducted on the PartNet-Mobility benchmark, where it achieves state-of-the-art performance in both mesh quality and articulation accuracy. This work addresses the challenge of scalable and automated creation of articulated 3D assets, which are crucial for applications in realistic simulation and embodied robotics. <div>
arXiv:2511.21887v1 Announce Type: new 
Abstract: Articulated 3D objects play a vital role in realistic simulation and embodied robotics, yet manually constructing such assets remains costly and difficult to scale. In this paper, we present UniArt, a diffusion-based framework that directly synthesizes fully articulated 3D objects from a single image in an end-to-end manner. Unlike prior multi-stage techniques, UniArt establishes a unified latent representation that jointly encodes geometry, texture, part segmentation, and kinematic parameters. We introduce a reversible joint-to-voxel embedding, which spatially aligns articulation features with volumetric geometry, enabling the model to learn coherent motion behaviors alongside structural formation. Furthermore, we formulate articulation type prediction as an open-set problem, removing the need for fixed joint semantics and allowing generalization to novel joint categories and unseen object types. Experiments on the PartNet-Mobility benchmark demonstrate that UniArt achieves state-of-the-art mesh quality and articulation accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathReasoning: A multimodal reasoning agent for query-based ROI navigation on whole-slide images</title>
<link>https://arxiv.org/abs/2511.21902</link>
<guid>https://arxiv.org/abs/2511.21902</guid>
<content:encoded><![CDATA[
<div> Keywords: tumor microenvironment, Whole Slide Images, PathReasoning, multi-modal reasoning, digital pathology<br /><br />Summary:<br /><br />1. The paper addresses the challenge of analyzing Whole Slide Images (WSIs) in cancer diagnosis, prognosis, and treatment by deciphering the tumor microenvironment. WSIs are extremely large, containing over 10 billion pixels, making manual navigation difficult and time-consuming.<br /><br />2. Inspired by how pathologists navigate WSIs through sampling, reasoning, and self-reflection, the authors propose "PathReasoning," a multi-modal reasoning agent that iteratively navigates WSIs by multiple rounds of reasoning and refinement.<br /><br />3. PathReasoning starts with randomly sampled candidate regions and improves selections through self-reflection and reasoning that connects visual data with clinical questions, ultimately proposing new regions to explore.<br /><br />4. This iterative process forms a reasoning chain that guides attention to diagnostically relevant areas, converting each slide into a sequence of question-guided views enabling efficient identification of informative regions of interest (ROIs) without dense pixel-level annotations.<br /><br />5. The approach outperforms strong ROI-selection baselines by 6.7% and 3.1% AUROC on cancer subtyping and longitudinal analysis tasks, respectively. It also enhances breast cancer report generation accuracy by 10% over GPT-4o.<br /><br />6. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, facilitating efficient slide review, consistent diagnostics, comprehensive reporting, and evidence traceability in digital pathology workflows. <div>
arXiv:2511.21902v1 Announce Type: new 
Abstract: Deciphering tumor microenvironment from Whole Slide Images (WSIs) is intriguing as it is key to cancer diagnosis, prognosis and treatment response. While these gigapixel images on one hand offer a comprehensive portrait of cancer, on the other hand, the extremely large size, as much as more than 10 billion pixels, make it challenging and time-consuming to navigate to corresponding regions to support diverse clinical inspection. Inspired by pathologists who conducted navigation on WSIs with a combination of sampling, reasoning and self-reflection, we proposed "PathReasoning", a multi-modal reasoning agent that iteratively navigates across WSIs through multiple rounds of reasoning and refinements. Specifically, starting with randomly sampled candidate regions, PathReasoning reviews current selections with self-reflection, reasoning over the correspondence between visual observations and clinical questions, and concludes by proposing new regions to explore. Across rounds, PathReasoning builds a reasoning chain that gradually directs attention to diagnostically relevant areas. PathReasoning turns each whole slide into a sequence of question-guided views, allowing the model to efficiently find informative ROIs within a fixed number of steps, without the need for dense pixel-level annotations. PathReasoning can substantially outperform strong ROI-selection approaches by 6.7% and 3.1% of AUROC on subtyping and longitudinal analysis tasks. The high-quality ROIs further support accurate report generation on breast cancer, significantly outperforming the standard GPT-4o by 10% in accuracy. PathReasoning prioritizes question-specific regions and constructs interpretable reasoning chains, supporting efficient slide review, consistent diagnostic interpretations, comprehensive reporting, and evidence traceability in digital pathology.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Parameter Optimization for Robust Remote Photoplethysmography</title>
<link>https://arxiv.org/abs/2511.21903</link>
<guid>https://arxiv.org/abs/2511.21903</guid>
<content:encoded><![CDATA[
<div> rPPG, PRISM, photometric detrending, online parameter adaptation, vital sign monitoring

<br /><br />Summary: This paper presents Projection-based Robust Signal Mixing (PRISM), a novel training-free algorithm for remote photoplethysmography (rPPG) that enables contactless vital sign monitoring using standard RGB cameras. Unlike existing methods that depend on fixed parameters tailored to specific lighting and camera setups, PRISM adapts parameters online through signal quality assessment, enhancing robustness across diverse environments. The method jointly optimizes photometric detrending and color mixing, improving signal quality without requiring any training phase. PRISM achieves state-of-the-art results among unsupervised rPPG methods, with mean absolute errors (MAE) of 0.77 bpm on the PURE dataset and 0.66 bpm on UBFC-rPPG, alongside accuracies of 97.3% and 97.5%, respectively, under a 5 bpm threshold. Statistical analysis confirms that PRISM performs on par with the best supervised methods (with p-values greater than 0.2), despite being training-free. The algorithm also operates efficiently in real-time on standard CPUs, supporting practical applications. Overall, PRISM validates that adaptive time series optimization significantly advances the accuracy and adaptability of rPPG vital sign monitoring across varying conditions without the need for supervised training. <div>
arXiv:2511.21903v1 Announce Type: new 
Abstract: Remote photoplethysmography (rPPG) enables contactless vital sign monitoring using standard RGB cameras. However, existing methods rely on fixed parameters optimized for particular lighting conditions and camera setups, limiting adaptability to diverse deployment environments. This paper introduces the Projection-based Robust Signal Mixing (PRISM) algorithm, a training-free method that jointly optimizes photometric detrending and color mixing through online parameter adaptation based on signal quality assessment. PRISM achieves state-of-the-art performance among unsupervised methods, with MAE of 0.77 bpm on PURE and 0.66 bpm on UBFC-rPPG, and accuracy of 97.3\% and 97.5\% respectively at a 5 bpm threshold. Statistical analysis confirms PRISM performs equivalently to leading supervised methods ($p > 0.2$), while maintaining real-time CPU performance without training. This validates that adaptive time series optimization significantly improves rPPG across diverse conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Multimodal Cancer Prototyping with Whole Slide Images and Incompletely Paired Genomics</title>
<link>https://arxiv.org/abs/2511.21937</link>
<guid>https://arxiv.org/abs/2511.21937</guid>
<content:encoded><![CDATA[
<div> Multimodal learning, Precision oncology, Genomics imputation, Whole slide images, Prototype-based fusion  

<br /><br />Summary:  
This paper introduces a flexible multimodal prototyping framework designed to integrate histology whole slide images (WSIs) and incomplete genomic data for precision oncology. The framework addresses challenges posed by phenotypic and genotypic heterogeneity that affect intra-modal feature quality and complicate cross-modal integration. The proposed method includes four key components: (1) Biological Prototyping leverages text prompting and prototype-wise weighting to enhance biological relevance in data representation; (2) Multiview Alignment aligns data across samples and distributions to improve consistency between modalities; (3) Bipartite Fusion captures both shared and modality-specific features to achieve effective multimodal fusion; (4) Semantic Genomics Imputation handles scenarios where genomic data is partially or completely missing, ensuring robustness in real-world clinical settings. Extensive experiments on multiple downstream tasks demonstrate that this approach consistently outperforms current state-of-the-art multimodal methods. The framework facilitates better integration of histological and genomic information for precision oncology, even when data are incomplete. The authors provide the code for reproducibility and further research at their GitHub repository, supporting transparency and accessibility. <div>
arXiv:2511.21937v1 Announce Type: new 
Abstract: Multimodal approaches that integrate histology and genomics hold strong potential for precision oncology. However, phenotypic and genotypic heterogeneity limits the quality of intra-modal representations and hinders effective inter-modal integration. Furthermore, most existing methods overlook real-world clinical scenarios where genomics may be partially missing or entirely unavailable. We propose a flexible multimodal prototyping framework to integrate whole slide images and incomplete genomics for precision oncology. Our approach has four key components: 1) Biological Prototyping using text prompting and prototype-wise weighting; 2) Multiview Alignment through sample- and distribution-wise alignments; 3) Bipartite Fusion to capture both shared and modality-specific information for multimodal fusion; and 4) Semantic Genomics Imputation to handle missing data. Extensive experiments demonstrate the consistent superiority of the proposed method compared to other state-of-the-art approaches on multiple downstream tasks. The code is available at https://github.com/helenypzhang/Interpretable-Multimodal-Prototyping.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AmodalGen3D: Generative Amodal 3D Object Reconstruction from Sparse Unposed Views</title>
<link>https://arxiv.org/abs/2511.21945</link>
<guid>https://arxiv.org/abs/2511.21945</guid>
<content:encoded><![CDATA[
<div> Keywords: AmodalGen3D, 3D reconstruction, occlusion, sparse views, cross attention<br /><br />Summary:<br /><br />1. AmodalGen3D is a novel generative framework designed for reconstructing complete 3D objects from a limited number of unposed and partially occluded views, addressing challenges posed by unseen object surfaces in real-world scenarios.<br /><br />2. Unlike traditional multi-view and inpainting-based methods, which often produce incomplete or inconsistent reconstructions under sparse and occluded conditions, AmodalGen3D infers full, occlusion-free geometry and appearance.<br /><br />3. The model uniquely integrates 2D amodal completion priors with multi-view stereo geometry and incorporates a View-Wise Cross Attention mechanism to effectively fuse sparse-view features, alongside a Stereo-Conditioned Cross Attention module for inferring unobserved structures.<br /><br />4. By jointly modeling visible and hidden parts of objects, AmodalGen3D generates reconstructions that comply with sparse-view constraints while plausibly hallucinating unseen regions, achieving higher fidelity and completeness.<br /><br />5. Extensive experiments on synthetic and real datasets confirm that AmodalGen3D outperforms existing methods under occlusion-heavy, sparse-view scenarios, offering significant benefits for applications in robotics, augmented/virtual reality, and embodied AI. <div>
arXiv:2511.21945v1 Announce Type: new 
Abstract: Reconstructing 3D objects from a few unposed and partially occluded views is a common yet challenging problem in real-world scenarios, where many object surfaces are never directly observed. Traditional multi-view or inpainting-based approaches struggle under such conditions, often yielding incomplete or geometrically inconsistent reconstructions. We introduce AmodalGen3D, a generative framework for amodal 3D object reconstruction that infers complete, occlusion-free geometry and appearance from arbitrary sparse inputs. The model integrates 2D amodal completion priors with multi-view stereo geometry conditioning, supported by a View-Wise Cross Attention mechanism for sparse-view feature fusion and a Stereo-Conditioned Cross Attention module for unobserved structure inference. By jointly modeling visible and hidden regions, AmodalGen3D faithfully reconstructs 3D objects that are consistent with sparse-view constraints while plausibly hallucinating unseen parts. Experiments on both synthetic and real-world datasets demonstrate that AmodalGen3D achieves superior fidelity and completeness under occlusion-heavy sparse-view settings, addressing a pressing need for object-level 3D scene reconstruction in robotics, AR/VR, and embodied AI applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAPVid-360: Tracking Any Point in 360 from Narrow Field of View Video</title>
<link>https://arxiv.org/abs/2511.21946</link>
<guid>https://arxiv.org/abs/2511.21946</guid>
<content:encoded><![CDATA[
<div> TAPVid-360, panoramic understanding, allocentric representation, 360 video, point tracking<br /><br />Summary:<br /><br />1. The paper addresses a key limitation in current artificial vision systems, which struggle to maintain persistent, panoramic, and allocentric understanding of scenes, especially beyond the visible field of view. This contrasts with human ability to create comprehensive mental models and maintain object permanence.<br /><br />2. The authors introduce TAPVid-360, a novel task designed to predict the 3D direction to queried scene points throughout a video sequence, including those far outside the narrow field of view, enabling allocentric scene representation.<br /><br />3. TAPVid-360 avoids the need for dynamic 4D ground truth scene models during training by leveraging 360-degree videos as supervision, which are resampled into narrow FOV perspectives. Ground truth directions for points are computed by tracking them across the full panorama using an efficient 2D pipeline.<br /><br />4. A new dataset and benchmark named TAPVid360-10k is presented, containing 10,000 perspective videos annotated with ground truth directional point tracking to facilitate research and evaluation on this task.<br /><br />5. The paper proposes a baseline method by adapting CoTracker v3 to predict per-point rotation updates for direction prediction, achieving superior performance compared to existing TAP and TAPVid 3D methods, thus demonstrating the effectiveness of the approach. <div>
arXiv:2511.21946v1 Announce Type: new 
Abstract: Humans excel at constructing panoramic mental models of their surroundings, maintaining object permanence and inferring scene structure beyond visible regions. In contrast, current artificial vision systems struggle with persistent, panoramic understanding, often processing scenes egocentrically on a frame-by-frame basis. This limitation is pronounced in the Track Any Point (TAP) task, where existing methods fail to track 2D points outside the field of view. To address this, we introduce TAPVid-360, a novel task that requires predicting the 3D direction to queried scene points across a video sequence, even when far outside the narrow field of view of the observed video. This task fosters learning allocentric scene representations without needing dynamic 4D ground truth scene models for training. Instead, we exploit 360 videos as a source of supervision, resampling them into narrow field-of-view perspectives while computing ground truth directions by tracking points across the full panorama using a 2D pipeline. We introduce a new dataset and benchmark, TAPVid360-10k comprising 10k perspective videos with ground truth directional point tracking. Our baseline adapts CoTracker v3 to predict per-point rotations for direction updates, outperforming existing TAP and TAPVid 3D methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WalkCLIP: Multimodal Learning for Urban Walkability Prediction</title>
<link>https://arxiv.org/abs/2511.21947</link>
<guid>https://arxiv.org/abs/2511.21947</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban walkability, multimodal framework, satellite imagery, GPT-4o captions, population dynamics<br /><br />Summary:<br /><br />1. Urban walkability is essential for public health, sustainability, and quality of life, but traditional assessment methods like surveys and field audits are costly and hard to scale.<br />2. Prior approaches using single data sources such as satellite imagery, street view imagery, or population indicators only capture limited dimensions of the walking environment.<br />3. Satellite data provides an overhead view of the built environment but lacks pedestrian-level details, street view imagery focuses on ground-level conditions but misses broader context, and population dynamics reflect activity patterns but not visual environment characteristics.<br />4. The introduced WalkCLIP framework integrates these complementary data sources — satellite images, GPT-4o-generated captions from street view images, and population dynamics — to learn rich, walkability-aware vision-language representations.<br />5. WalkCLIP incorporates a spatial aggregation module for neighborhood context and fuses visual with behavioral signals for accurate predictions.<br />6. When evaluated across 4,660 locations in Minneapolis-Saint Paul, WalkCLIP outperforms both unimodal and multimodal baseline models in predictive accuracy and spatial alignment, demonstrating the advantage of combining diverse data modalities to assess walkability effectively. <div>
arXiv:2511.21947v1 Announce Type: new 
Abstract: Urban walkability is a cornerstone of public health, sustainability, and quality of life. Traditional walkability assessments rely on surveys and field audits, which are costly and difficult to scale. Recent studies have used satellite imagery, street view imagery, or population indicators to estimate walkability, but these single-source approaches capture only one dimension of the walking environment. Satellite data describe the built environment from above, but overlook the pedestrian perspective. Street view imagery captures conditions at the ground level, but lacks broader spatial context. Population dynamics reveal patterns of human activity but not the visual form of the environment. We introduce WalkCLIP, a multimodal framework that integrates these complementary viewpoints to predict urban walkability. WalkCLIP learns walkability-aware vision-language representations from GPT-4o generated image captions, refines these representations with a spatial aggregation module that incorporates neighborhood context, and fuses the resulting features with representations from a population dynamics foundation model. Evaluated at 4,660 locations throughout Minneapolis-Saint Paul, WalkCLIP outperforms unimodal and multimodal baselines in both predictive accuracy and spatial alignment. These results show that the integration of visual and behavioral signals yields reliable predictions of the walking environment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepGI: Explainable Deep Learning for Gastrointestinal Image Classification</title>
<link>https://arxiv.org/abs/2511.21959</link>
<guid>https://arxiv.org/abs/2511.21959</guid>
<content:encoded><![CDATA[
<div> Gastrointestinal imaging, deep learning, disease classification, explainable AI, medical benchmarks<br /><br />Summary:<br /><br />This paper introduces a novel gastrointestinal medical imaging dataset containing 4,000 endoscopic images categorized into four disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging advanced deep learning models, the study addresses typical challenges in endoscopic imaging such as variable lighting, changing camera angles, and image artifacts. The evaluation shows that VGG16 and MobileNetV2 models achieved the highest test accuracy of 96.5%, while the Xception model scored 94.24%, setting strong benchmarks for automated disease classification in this domain. The work also integrates explainable AI techniques using Grad-CAM visualizations, which highlight the key image regions influencing the models' predictions, thereby improving the interpretability and potential clinical trustworthiness of the AI system. Through comprehensive experiments, the study demonstrates the feasibility of robust, accurate, and interpretable medical image analysis under realistic and challenging conditions. This contribution advances gastrointestinal computer-aided diagnosis by providing new comparative insights, original benchmark results, and visual explanation tools. It emphasizes the critical role of diverse, clinically relevant datasets and explainability in the development and deployment of medical AI solutions. <div>
arXiv:2511.21959v1 Announce Type: new 
Abstract: This paper presents a comprehensive comparative model analysis on a novel gastrointestinal medical imaging dataset, comprised of 4,000 endoscopic images spanning four critical disease classes: Diverticulosis, Neoplasm, Peritonitis, and Ureters. Leveraging state-of-the-art deep learning techniques, the study confronts common endoscopic challenges such as variable lighting, fluctuating camera angles, and frequent imaging artifacts. The best performing models, VGG16 and MobileNetV2, each achieved a test accuracy of 96.5%, while Xception reached 94.24%, establishing robust benchmarks and baselines for automated disease classification. In addition to strong classification performance, the approach includes explainable AI via Grad-CAM visualization, enabling identification of image regions most influential to model predictions and enhancing clinical interpretability. Experimental results demonstrate the potential for robust, accurate, and interpretable medical image analysis even in complex real-world conditions. This work contributes original benchmarks, comparative insights, and visual explanations, advancing the landscape of gastrointestinal computer-aided diagnosis and underscoring the importance of diverse, clinically relevant datasets and model explainability in medical AI research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAT3D: Physics-Augmented Text-to-3D Scene Generation</title>
<link>https://arxiv.org/abs/2511.21978</link>
<guid>https://arxiv.org/abs/2511.21978</guid>
<content:encoded><![CDATA[
<div> Keywords: PAT3D, text-to-3D generation, physics simulation, rigid-body simulation, scene optimization<br /><br />Summary:<br />1. PAT3D is introduced as the first framework that combines vision-language models with physics-based simulation to generate 3D scenes directly from text prompts, ensuring physical plausibility and simulation readiness. <br />2. The system generates 3D objects and infers their spatial relationships, organizing them into a hierarchical scene tree which serves as initial conditions for a differentiable rigid-body simulator. <br />3. This simulator models object interactions under gravity and guides the scene toward a stable static equilibrium without object interpenetrations. <br />4. A simulation-in-the-loop optimization procedure is employed to refine the scene by enforcing physical stability, eliminating intersections, and improving semantic alignment with the input text. <br />5. Experimental results demonstrate that PAT3D significantly surpasses existing methods in terms of physical realism, semantic consistency with prompts, and visual quality, while also providing simulation-ready scenes suitable for downstream applications such as scene editing and robotic manipulation. Code and data release are planned upon acceptance. <div>
arXiv:2511.21978v1 Announce Type: new 
Abstract: We introduce PAT3D, the first physics-augmented text-to-3D scene generation framework that integrates vision-language models with physics-based simulation to produce physically plausible, simulation-ready, and intersection-free 3D scenes. Given a text prompt, PAT3D generates 3D objects, infers their spatial relations, and organizes them into a hierarchical scene tree, which is then converted into initial conditions for simulation. A differentiable rigid-body simulator ensures realistic object interactions under gravity, driving the scene toward static equilibrium without interpenetrations. To further enhance scene quality, we introduce a simulation-in-the-loop optimization procedure that guarantees physical stability and non-intersection, while improving semantic consistency with the input prompt. Experiments demonstrate that PAT3D substantially outperforms prior approaches in physical plausibility, semantic consistency, and visual quality. Beyond high-quality generation, PAT3D uniquely enables simulation-ready 3D scenes for downstream tasks such as scene editing and robotic manipulation. Code and data will be released upon acceptance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DialBench: Towards Accurate Reading Recognition of Pointer Meter using Large Foundation Models</title>
<link>https://arxiv.org/abs/2511.21982</link>
<guid>https://arxiv.org/abs/2511.21982</guid>
<content:encoded><![CDATA[
<div> pointer meters, dial reading, vision-language model, physical relation injection, RPM-10K dataset<br /><br />Summary:<br /><br />1. The paper addresses the challenge of precise reading recognition for pointer meters, which is critical for smart power systems yet faces difficulties from reflections, occlusions, dynamic viewing angles, and pointer-scale similarity.<br /><br />2. To overcome the lack of large-scale datasets for robust model development, the authors introduce RPM-10K, a new comprehensive benchmark dataset comprising 10,730 images of meter dials that capture these challenging conditions.<br /><br />3. Built on this dataset, they propose MRLM, a novel vision-language model that integrates physical relation injection to improve understanding by explicitly encoding the geometric and causal relationships between meter pointers and scale markings.<br /><br />4. MRLM departs from typical image-level correlation learning by aligning perception with physical reasoning, leveraging world-model perspectives to more accurately interpret dial configurations.<br /><br />5. The model utilizes cross-attentional fusion and adaptive expert selection mechanisms to generate precise numeric readings, and extensive experiments validate its effectiveness on the RPM-10K benchmark.<br /><br />6. The authors commit to releasing both the RPM-10K dataset and the MRLM source code, facilitating further progress in dial reading recognition research. <div>
arXiv:2511.21982v1 Announce Type: new 
Abstract: The precise reading recognition of pointer meters plays a key role in smart power systems, but existing approaches remain fragile due to challenges like reflections, occlusions, dynamic viewing angles, and overly between thin pointers and scale markings. Up to now, this area still lacks large-scale datasets to support the development of robust algorithms. To address these challenges, this paper first presents a new large-scale benchmark dataset for dial reading, termed RPM-10K, which contains 10730 meter images that fully reflect the aforementioned key challenges. Built upon the dataset, we propose a novel vision-language model for pointer meter reading recognition, termed MRLM, based on physical relation injection. Instead of exhaustively learning image-level correlations, MRLM explicitly encodes the geometric and causal relationships between the pointer and the scale, aligning perception with physical reasoning in the spirit of world-model perspectives. Through cross-attentional fusion and adaptive expert selection, the model learns to interpret dial configurations and generate precise numeric readings. Extensive experiments fully validated the effectiveness of our proposed framework on the newly proposed benchmark dataset. Both the dataset and source code will be released on https://github.com/Event-AHU/DialBench
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PPBoost: Progressive Prompt Boosting for Text-Driven Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.21984</link>
<guid>https://arxiv.org/abs/2511.21984</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, text prompts, visual prompts, zero-shot learning, pseudo-bounding boxes<br /><br />Summary:<br /><br />1. The paper addresses the shortcomings of text-prompted medical image segmentation models, which struggle with spatial precision and domain shifts, and contrasts them with visual-prompted models that use precise bounding-box (bbox) prompts to improve segmentation accuracy.<br /><br />2. It highlights the challenge of obtaining precise visual prompts in clinical settings due to the high cost and difficulty.<br /><br />3. The authors propose PPBoost (Progressive Prompt-Boosting), a framework that converts weak text-derived signals into strong, spatially grounded visual prompts without using any image- or pixel-level segmentation labels, thus operating strictly in a zero-shot regime.<br /><br />4. PPBoost leverages a vision-language model to generate initial pseudo-bboxes from textual descriptions, applies an uncertainty-aware filtering to select reliable bboxes, and trains a pseudo-labeled detector on these bboxes to produce high-quality bounding boxes.<br /><br />5. During inference, PPBoost refines these bboxes by expanding them to tightly cover target anatomical structures, providing enhanced spatial cues that guide existing segmentation models to create more accurate dense masks.<br /><br />6. Empirical results on three datasets with diverse modalities and anatomies show PPBoost consistently improves metrics like Dice and Normalized Surface Distance, surpassing both text- and visual-prompt baselines and even outperforming few-shot segmentation methods without labeled data.<br /><br />7. The framework is compatible with multiple common visual segmentation model backbones, demonstrating its generalizability and practical utility in medical image segmentation tasks. <div>
arXiv:2511.21984v1 Announce Type: new 
Abstract: Text-prompted foundation models for medical image segmentation offer an intuitive way to delineate anatomical structures from natural language queries, but their predictions often lack spatial precision and degrade under domain shift. In contrast, visual-prompted models achieve strong segmentation performance across diverse modalities by leveraging spatial cues of precise bounding-box (bbox) prompts to guide the segmentation of target lesions. However, it is costly and challenging to obtain the precise visual prompts in clinical practice. We propose PPBoost (Progressive Prompt-Boosting), a framework that bridges these limitations by transforming weak text-derived signals into strong, spatially grounded visual prompts, operating under a strict zero-shot regime with no image- or pixel-level segmentation labels. PPBoost first uses a vision-language model to produce initial pseudo-bboxes conditioned on the textual object descriptions and applies an uncertainty-aware criterion to filter unreliable predictions. The retained image-bboxes pairs are then leveraged to train a pseudo-labeled detector, producing the high-quality bboxes for the query images. During inference, PPBoost further refines the generated bboxes by appropriately expanding them to tightly cover the target anatomical structures. The enhanced spatially-grounding bbox prompts guide existing segmentation models to generate final dense masks, effectively amplifying weak text cues into strong spatial guidance. Across three datasets spanning diverse modalities and anatomies, PPBoost consistently improves Dice and Normalized Surface Distance over text- and visual-prompted baselines and, notably, surpasses few-shot segmentation models without using labeled data. PPBoost can generalize to multiple typical visual segmentation model backbones.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?</title>
<link>https://arxiv.org/abs/2511.21998</link>
<guid>https://arxiv.org/abs/2511.21998</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-modal LLM, interactive guidance, live coaching, Qualcomm Interactive Cooking, mistake detection<br /><br />Summary:  
1. The paper addresses a key limitation in current multi-modal Large Language Models (LLMs), which excel in conversation but lack the capability to provide live, interactive, step-by-step guidance.  
2. Effective live guidance requires the model to not just give instructions but also detect whether those instructions have been successfully followed, identify user mistakes, and alert users immediately, all in real-time and asynchronously rather than turn-based interaction.  
3. To facilitate research in this area, the authors introduce the Qualcomm Interactive Cooking benchmark and dataset, based on CaptainCook4D, which uniquely includes annotations of user mistakes and their subsequent corrections during task execution.  
4. This dataset features densely timed annotations for both instructions and feedback messages, with mistake alerts precisely timestamped according to the visual occurrence in the video stream, enabling evaluation of models’ real-time interactive capabilities.  
5. The authors evaluate existing state-of-the-art multi-modal LLMs on this new benchmark and propose LiveMamba, a novel streaming multi-modal LLM designed specifically for interactive instructional guidance, establishing a strong baseline for future work in live, situated coaching scenarios. <div>
arXiv:2511.21998v1 Announce Type: new 
Abstract: Multi-modal Large Language Models (LLM) have advanced conversational abilities but struggle with providing live, interactive step-by-step guidance, a key capability for future AI assistants. Effective guidance requires not only delivering instructions but also detecting their successful execution, as well as identifying and alerting users to mistakes, all of which has to happen in real-time. This requires models that are not turn-based, but that can react asynchronously to a video stream, as well as video data showing users performing tasks including mistakes and their corrections. To this end, we introduce Qualcomm Interactive Cooking, a new benchmark and dataset built upon CaptainCook4D, which contains user mistakes during task execution. Our dataset and benchmark features densely annotated, timed instructions and feedback messages, specifically including mistake alerts precisely timestamped to their visual occurrence in the video. We evaluate state-of-the-art multi-modal LLMs on the Qualcomm Interactive Cooking benchmark and introduce LiveMamba, a streaming multi-modal LLM designed for interactive instructional guidance. This work provides the first dedicated benchmark and a strong baseline for developing and evaluating on live, situated coaching.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamFlow: Theory, Algorithm, and Implementation for High-Efficiency Rectified Flow Generation</title>
<link>https://arxiv.org/abs/2511.22009</link>
<guid>https://arxiv.org/abs/2511.22009</guid>
<content:encoded><![CDATA[
<div> Rectified Flow, Flow Matching, acceleration pipeline, dynamic TensorRT, image generation speed<br /><br />Summary:  
This article addresses the challenge of accelerating Rectified Flow generative models, which differ theoretically and structurally from existing diffusion models, making current acceleration techniques inapplicable. The authors present a comprehensive acceleration pipeline that innovates in theory, system design, and reasoning strategies to enhance generation speed significantly. Key methods include batch processing employing a newly designed velocity field, vectorization of heterogeneous time-step batch processing, and dynamic compilation using TensorRT tailored for these novel approaches. These combined techniques effectively optimize computational efficiency and throughput specifically for flow-based generative models. Experimentally, while current public acceleration methods only improve speed by about 18%, the proposed pipeline achieves an extraordinary acceleration of up to 611% for generating 512×512 images, demonstrating a marked advancement beyond existing methods. This work not only advances the state of acceleration in flow-based generative models but also provides a generalizable framework that can potentially be adapted to related models, thus significantly enhancing practical applicability in real-time and high-resolution image generation tasks. <div>
arXiv:2511.22009v1 Announce Type: new 
Abstract: New technologies such as Rectified Flow and Flow Matching have significantly improved the performance of generative models in the past two years, especially in terms of control accuracy, generation quality, and generation efficiency. However, due to some differences in its theory, design, and existing diffusion models, the existing acceleration methods cannot be directly applied to the Rectified Flow model. In this article, we have comprehensively implemented an overall acceleration pipeline from the aspects of theory, design, and reasoning strategies. This pipeline uses new methods such as batch processing with a new velocity field, vectorization of heterogeneous time-step batch processing, and dynamic TensorRT compilation for the new methods to comprehensively accelerate related models based on flow models. Currently, the existing public methods usually achieve an acceleration of 18%, while experiments have proved that our new method can accelerate the 512*512 image generation speed to up to 611%, which is far beyond the current non-generalized acceleration methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedEyes: Learning Dynamic Visual Focus for Medical Progressive Diagnosis</title>
<link>https://arxiv.org/abs/2511.22018</link>
<guid>https://arxiv.org/abs/2511.22018</guid>
<content:encoded><![CDATA[
<div> Keywords: medical diagnosis, reinforcement learning, visual reasoning, expert guidance, medical VQA<br /><br />Summary:<br /><br />1. The paper addresses the challenge of creating accurate medical diagnostic AI systems that mimic clinicians' progressive visual focusing and iterative reasoning during diagnosis.<br />2. Existing vision-language models using on-policy reinforcement learning with verifiable rewards (RLVR) often favor superficially coherent but clinically inaccurate reasoning paths.<br />3. MedEyes is introduced as a novel reinforcement learning framework that incorporates off-policy expert guidance by leveraging expert visual search trajectories as external behavioral signals, steering the model toward clinically aligned visual reasoning.<br />4. The Gaze-guided Reasoning Navigator (GRN) emulates clinical diagnostic strategies through a dual-mode exploration that both scans systematically for abnormalities and drills down for detailed regional analysis.<br />5. To encourage a balance between imitating expert behavior and autonomous exploration, the Confidence Value Sampler (CVS) applies nucleus sampling and adaptive termination to generate diverse yet credible reasoning paths.<br />6. The dual-stream GRPO optimization framework separates on-policy and off-policy learning signals, which reduces reward assimilation issues and entropy collapse.<br />7. Experimental results demonstrate MedEyes improves performance by +8.5% on multiple medical Visual Question Answering (VQA) benchmarks, showing its effectiveness in developing interpretable medical AI systems. <div>
arXiv:2511.22018v1 Announce Type: new 
Abstract: Accurate medical diagnosis often involves progressive visual focusing and iterative reasoning, characteristics commonly observed in clinical workflows. While recent vision-language models demonstrate promising chain-of-thought (CoT) reasoning capabilities via reinforcement learning with verifiable rewards (RLVR), their purely on-policy learning paradigm tends to reinforce superficially coherent but clinically inaccurate reasoning paths. We propose MedEyes, a novel reinforcement learning framework that dynamically models clinician-style diagnostic reasoning by progressively attending to and interpreting relevant medical image regions. By incorporating off-policy expert guidance, MedEyes converts expert visual search trajectories into structured external behavioral signals, guiding the model toward clinically aligned visual reasoning. We design the Gaze-guided Reasoning Navigator (GRN) to emulate the diagnostic process through a dual-mode exploration strategy, scanning for systematic abnormality localization and drilling for detailed regional analysis. To balance expert imitation and autonomous discovery, we introduce the Confidence Value Sampler (CVS), which employs nucleus sampling and adaptive termination to create diverse yet credible exploration paths. Finally, the dual-stream GRPO optimization framework decouples on-policy and off-policy learning signals, mitigating reward assimilation and entropy collapse. Experiments demonstrate that MedEyes achieves an average performance improvement of +8.5\% across multiple medical VQA benchmarks, validating MedEyes's potential in building interpretable medical AI systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intra-Class Probabilistic Embeddings for Uncertainty Estimation in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22019</link>
<guid>https://arxiv.org/abs/2511.22019</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language models, uncertainty estimation, contrastive learning, feature projection, error detection<br /><br />Summary:<br /><br />This paper addresses the issue of overconfident misclassifications in vision-language models (VLMs) such as CLIP, which limits their reliability in safety-critical applications. The authors propose a training-free, post-hoc uncertainty estimation method designed specifically for contrastive VLMs to detect erroneous predictions effectively. The core of the approach involves measuring visual feature consistency within a class by using a combination of feature projection and multivariate Gaussian modeling to produce class-specific probabilistic embeddings. This method is VLM-agnostic and does not require any fine-tuning, making it adaptable and easy to apply to different models. Additionally, it demonstrates robustness to distribution shifts and is effective even when only a few (as few as 10) training images per class are available. To validate the approach, extensive experiments are conducted on several benchmark datasets, including ImageNet, Flowers102, Food101, EuroSAT, and DTD. The results show that the proposed method achieves state-of-the-art performance in error detection, significantly outperforming both deterministic and probabilistic baseline methods applied to VLMs. The implementation code is made publicly available, promoting reproducibility and further research. <div>
arXiv:2511.22019v1 Announce Type: new 
Abstract: Vision-language models (VLMs), such as CLIP, have gained popularity for their strong open vocabulary classification performance, but they are prone to assigning high confidence scores to misclassifications, limiting their reliability in safety-critical applications. We introduce a training-free, post-hoc uncertainty estimation method for contrastive VLMs that can be used to detect erroneous predictions. The key to our approach is to measure visual feature consistency within a class, using feature projection combined with multivariate Gaussians to create class-specific probabilistic embeddings. Our method is VLM-agnostic, requires no fine-tuning, demonstrates robustness to distribution shift, and works effectively with as few as 10 training images per class. Extensive experiments on ImageNet, Flowers102, Food101, EuroSAT and DTD show state-of-the-art error detection performance, significantly outperforming both deterministic and probabilistic VLM baselines. Code is available at https://github.com/zhenxianglin/ICPE.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layover or Direct Flight: Rethinking Audio-Guided Image Segmentation</title>
<link>https://arxiv.org/abs/2511.22025</link>
<guid>https://arxiv.org/abs/2511.22025</guid>
<content:encoded><![CDATA[
<div> audio-visual grounding, human-robot interaction, spoken instructions, transcription-free, robustness  

<br /><br />Summary:  
This paper addresses the task of object grounding, which involves localizing an object in an image based on spoken human instructions, a key step toward better human-robot interaction. Traditional methods rely heavily on transcribing speech into text, extracting keywords, and then grounding objects using pretrained text-vision models. The authors challenge this transcription-dependent approach, questioning its efficiency and robustness. They propose exploring direct alignment between audio and visual data without converting speech to text. To test this, they simplify the problem to grounding based on single-word spoken instructions and introduce a new dataset containing diverse objects spoken with various human accents. The study benchmarks several adapted audio-visual models and shows that direct audio grounding is not only possible but in some cases more effective than transcription-based pipelines, especially in handling linguistic variability. These results advocate for a shift toward transcription-free multimodal understanding systems, promising enhanced robustness and efficiency for future audio-visual applications in robotics and AI. <div>
arXiv:2511.22025v1 Announce Type: new 
Abstract: Understanding human instructions is essential for enabling smooth human-robot interaction. In this work, we focus on object grounding, i.e., localizing an object of interest in a visual scene (e.g., an image) based on verbal human instructions. Despite recent progress, a dominant research trend relies on using text as an intermediate representation. These approaches typically transcribe speech to text, extract relevant object keywords, and perform grounding using models pretrained on large text-vision datasets. However, we question both the efficiency and robustness of such transcription-based pipelines. Specifically, we ask: Can we achieve direct audio-visual alignment without relying on text? To explore this possibility, we simplify the task by focusing on grounding from single-word spoken instructions. We introduce a new audio-based grounding dataset that covers a wide variety of objects and diverse human accents. We then adapt and benchmark several models from the closely audio-visual field. Our results demonstrate that direct grounding from audio is not only feasible but, in some cases, even outperforms transcription-based methods, especially in terms of robustness to linguistic variability. Our findings encourage a renewed interest in direct audio grounding and pave the way for more robust and efficient multimodal understanding systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAGen: Phase-guided Amplitude Generation for Domain-adaptive Object Detection</title>
<link>https://arxiv.org/abs/2511.22029</link>
<guid>https://arxiv.org/abs/2511.22029</guid>
<content:encoded><![CDATA[
<div> Unsupervised domain adaptation, frequency domain, image style adaptation, domain-adaptive object detection, lightweight preprocessing  

<br /><br />Summary:  
This paper addresses the challenge of unsupervised domain adaptation (UDA) to improve neural network deployment across varying environments without requiring extensive labeled data in target domains. It critiques existing methods for their complexity, including adversarial training and auxiliary model components, which increase computational cost and implementation difficulty. The authors propose a novel and simple UDA technique that adapts image styles by operating in the frequency domain, effectively reducing the domain gap between source and target data. A key advantage is the use of a lightweight preprocessing module applied only during training, which is completely removed during inference, thereby avoiding any inference overhead. The method is evaluated on domain-adaptive object detection (DAOD) tasks, particularly where annotations are readily available in source domains but scarce or unavailable in more challenging target domains like adverse weather or low-light conditions. Experimental results demonstrate substantial and consistent improvements across multiple benchmarks, validating both the practicality and effectiveness of the approach. This work highlights the potential of frequency-based style adaptation as a streamlined solution for UDA tasks requiring robust performance without sacrificing computational efficiency. <div>
arXiv:2511.22029v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) greatly facilitates the deployment of neural networks across diverse environments. However, most state-of-the-art approaches are overly complex, relying on challenging adversarial training strategies, or on elaborate architectural designs with auxiliary models for feature distillation and pseudo-label generation. In this work, we present a simple yet effective UDA method that learns to adapt image styles in the frequency domain to reduce the discrepancy between source and target domains. The proposed approach introduces only a lightweight pre-processing module during training and entirely discards it at inference time, thus incurring no additional computational overhead. We validate our method on domain-adaptive object detection (DAOD) tasks, where ground-truth annotations are easily accessible in source domains (e.g., normal-weather or synthetic conditions) but challenging to obtain in target domains (e.g., adverse weather or low-light scenes). Extensive experiments demonstrate that our method achieves substantial performance gains on multiple benchmarks, highlighting its practicality and effectiveness.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparseWorld-TC: Trajectory-Conditioned Sparse Occupancy World Model</title>
<link>https://arxiv.org/abs/2511.22039</link>
<guid>https://arxiv.org/abs/2511.22039</guid>
<content:encoded><![CDATA[
<div> trajectory-conditioned forecasting, 3D scene occupancy, transformer architecture, sparse representation, nuScenes benchmark<br /><br />Summary:<br /><br />This paper presents a novel transformer-based architecture designed for trajectory-conditioned forecasting of future 3D scene occupancy. Unlike prior approaches relying on variational autoencoders (VAEs) to generate discrete occupancy tokens with limited representational capacity, the proposed method predicts multi-frame future occupancy directly from raw image features in an end-to-end manner. The approach uses a sparse occupancy representation that removes the need for intermediate bird's eye view (BEV) projection and its associated geometric priors, enabling the model to better capture spatiotemporal dependencies. By avoiding tokenization constraints and BEV-induced structural limitations, the method significantly improves performance. Experiments on the nuScenes benchmark demonstrate state-of-the-art results for 1 to 3 second occupancy forecasting horizons, surpassing existing methods by a notable margin. Additionally, the model exhibits robust understanding of dynamic scene elements and maintains high accuracy when conditioned on arbitrary future trajectories. The work draws inspiration from the success of attention-based transformer architectures in foundational vision and language tasks, adapting these principles to achieve enhanced occupancy forecasting in complex 3D environments. <div>
arXiv:2511.22039v1 Announce Type: new 
Abstract: This paper introduces a novel architecture for trajectory-conditioned forecasting of future 3D scene occupancy. In contrast to methods that rely on variational autoencoders (VAEs) to generate discrete occupancy tokens, which inherently limit representational capacity, our approach predicts multi-frame future occupancy in an end-to-end manner directly from raw image features. Inspired by the success of attention-based transformer architectures in foundational vision and language models such as GPT and VGGT, we employ a sparse occupancy representation that bypasses the intermediate bird's eye view (BEV) projection and its explicit geometric priors. This design allows the transformer to capture spatiotemporal dependencies more effectively. By avoiding both the finite-capacity constraint of discrete tokenization and the structural limitations of BEV representations, our method achieves state-of-the-art performance on the nuScenes benchmark for 1-3 second occupancy forecasting, outperforming existing approaches by a significant margin. Furthermore, it demonstrates robust scene dynamics understanding, consistently delivering high accuracy under arbitrary future trajectory conditioning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICM-SR: Image-Conditioned Manifold Regularization for Image Super-Resoultion</title>
<link>https://arxiv.org/abs/2511.22048</link>
<guid>https://arxiv.org/abs/2511.22048</guid>
<content:encoded><![CDATA[
<div> Keywords: Real-world image super-resolution, diffusion models, manifold regularization, image-conditioned manifold, perceptual quality<br /><br />Summary:<br />1. Real-world image super-resolution (Real-ISR) methods commonly use text-to-image diffusion models to regularize outputs on a learned manifold; however, existing approaches typically use a text-conditioned manifold which is misaligned with Real-ISR goals.<br />2. This misalignment causes practical issues such as color distortions and blurred edges in the reconstructed high-quality images, indicating that text-conditioned generative priors are not optimal for Real-ISR.<br />3. To address these flaws, the study proposes image-conditioned manifold regularization (ICM), which regularizes outputs towards a manifold conditioned on sparse structural information derived from the input images, specifically a combination of colormap and Canny edges.<br />4. The ICM approach avoids the numerical instability caused by conditioning directly on dense raw input images and provides a more stable and task-aligned regularization signal.<br />5. Experimental results demonstrate that ICM significantly improves super-resolution performance, especially enhancing perceptual quality, confirming its effectiveness and suitability for real-world applications. The authors also plan to release the source code to support reproducibility. <div>
arXiv:2511.22048v1 Announce Type: new 
Abstract: Real world image super-resolution (Real-ISR) often leverages the powerful generative priors of text-to-image diffusion models by regularizing the output to lie on their learned manifold. However, existing methods often overlook the importance of the regularizing manifold, typically defaulting to a text-conditioned manifold. This approach suffers from two key limitations. Conceptually, it is misaligned with the Real-ISR task, which is to generate high quality (HQ) images directly tied to the low quality (LQ) images. Practically, the teacher model often reconstructs images with color distortions and blurred edges, indicating a flawed generative prior for this task. To correct these flaws and ensure conceptual alignment, a more suitable manifold must incorporate information from the images. While the most straightforward approach is to condition directly on the raw input images, their high information densities make the regularization process numerically unstable. To resolve this, we propose image-conditioned manifold regularization (ICM), a method that regularizes the output towards a manifold conditioned on the sparse yet essential structural information: a combination of colormap and Canny edges. ICM provides a task-aligned and stable regularization signal, thereby avoiding the instability of dense-conditioning and enhancing the final super-resolution quality. Our experiments confirm that the proposed regularization significantly enhances super-resolution performance, particularly in perceptual quality, demonstrating its effectiveness for real-world applications. We will release the source code of our work for reproducibility.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TPCNet: Triple physical constraints for Low-light Image Enhancement</title>
<link>https://arxiv.org/abs/2511.22052</link>
<guid>https://arxiv.org/abs/2511.22052</guid>
<content:encoded><![CDATA[
<div> Low-light enhancement, Retinex theory, specular reflection, Kubelka-Munk theory, triple physical constraints<br /><br />Summary:<br /><br />1. The paper addresses the challenge of enhancing low-light images by improving image contrast and reducing color bias and noise, which are critical tasks in computer vision.<br />2. Existing deep-learning methods frequently use the Retinex theory but often assume surfaces reflect light ideally as Lambertian, ignoring the effects of specular reflection.<br />3. The authors propose preserving the specular reflection coefficient and reformulate physical constraints based on Kubelka-Munk theory, which allows modeling illumination, reflection, and detection via triple physical constraints (TPCs).<br />4. Unlike previous approaches that apply constraints in image space, this work embeds the TPCs into the feature space of the deep learning model, leading to better generalization and adaptability.<br />5. The resulting architecture, named TPCNet, improves both quantitative metrics and visual quality across 10 benchmark datasets, without increasing the number of parameters, and outperforms existing state-of-the-art methods according to extensive experiments and ablation studies. <div>
arXiv:2511.22052v1 Announce Type: new 
Abstract: Low-light image enhancement is an essential computer vision task to improve image contrast and to decrease the effects of color bias and noise. Many existing interpretable deep-learning algorithms exploit the Retinex theory as the basis of model design. However, previous Retinex-based algorithms, that consider reflected objects as ideal Lambertian ignore specular reflection in the modeling process and construct the physical constraints in image space, limiting generalization of the model. To address this issue, we preserve the specular reflection coefficient and reformulate the original physical constraints in the imaging process based on the Kubelka-Munk theory, thereby constructing constraint relationship between illumination, reflection, and detection, the so-called triple physical constraints (TPCs)theory. Based on this theory, the physical constraints are constructed in the feature space of the model to obtain the TPC network (TPCNet). Comprehensive quantitative and qualitative benchmark and ablation experiments confirm that these constraints effectively improve the performance metrics and visual quality without introducing new parameters, and demonstrate that our TPCNet outperforms other state-of-the-art methods on 10 datasets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OralGPT-Omni: A Versatile Dental Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2511.22055</link>
<guid>https://arxiv.org/abs/2511.22055</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Dentistry, TRACE-CoT, OralGPT-Omni, MMOral-Uni<br /><br />Summary:<br /><br />This paper introduces OralGPT-Omni, the first dental-specialized multimodal large language model (MLLM) designed to analyze diverse dental imaging modalities and clinical tasks with high reliability. The development of OralGPT-Omni addresses challenges in dentistry AI such as limited domain-specific data, lack of expert annotations, and insufficient modality-specific modeling. The authors created TRACE-CoT, a clinically grounded chain-of-thought dataset that simulates the diagnostic reasoning process of dental radiologists, which enhances the model’s interpretability and accuracy. A novel four-stage training paradigm is proposed to further improve the model’s performance in understanding dental images. The study also presents MMOral-Uni, the first unified multimodal benchmark for dental image analysis, featuring 2,809 open-ended question-answer pairs across five modalities and five clinical tasks. OralGPT-Omni achieves a superior overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, significantly outperforming GPT-5. By releasing all code, benchmarks, and models publicly, this work promotes intelligent dentistry and lays the foundation for future advances in dental image analysis and AI-assisted diagnostics. <div>
arXiv:2511.22055v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have exhibited immense potential across numerous medical specialties; yet, dentistry remains underexplored, in part due to limited domain-specific data, scarce dental expert annotations, insufficient modality-specific modeling, and challenges in reliability. In this paper, we present OralGPT-Omni, the first dental-specialized MLLM designed for comprehensive and trustworthy analysis across diverse dental imaging modalities and clinical tasks. To explicitly capture dentists' diagnostic reasoning, we construct TRACE-CoT, a clinically grounded chain-of-thought dataset that mirrors dental radiologists' decision-making processes. This reasoning supervision, combined with our proposed four-stage training paradigm, substantially strengthens the model's capacity for dental image understanding and analysis. In parallel, we introduce MMOral-Uni, the first unified multimodal benchmark for dental image analysis. It comprises 2,809 open-ended question-answer pairs spanning five modalities and five tasks, offering a comprehensive evaluation suite to date for MLLMs in digital dentistry. OralGPT-Omni achieves an overall score of 51.84 on the MMOral-Uni benchmark and 45.31 on the MMOral-OPG benchmark, dramatically outperforming the scores of GPT-5. Our work promotes intelligent dentistry and paves the way for future advances in dental image analysis. All code, benchmark, and models will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DNA: Dual-branch Network with Adaptation for Open-Set Online Handwriting Generation</title>
<link>https://arxiv.org/abs/2511.22064</link>
<guid>https://arxiv.org/abs/2511.22064</guid>
<content:encoded><![CDATA[
<div> Online handwriting generation, unseen characters, dual-branch network, style adaptation, content generalization<br /><br />Summary:<br /><br />This paper addresses the challenge of online handwriting generation (OHG) specifically focusing on the difficulty of generating unseen characters, a common problem in glyph-based languages such as Chinese. The authors propose a novel method named Dual-branch Network with Adaptation (DNA), which is designed to generate handwriting samples featuring both unseen writer styles and previously unseen characters during testing. The DNA model incorporates two adaptive branches: a style branch and a content branch. The style branch captures stroke-level attributes such as writing direction, spacing, placement, and flow to mimic realistic handwriting styles. Simultaneously, the content branch enhances the model's ability to generalize to unseen characters by decomposing character content into structural information and texture details via local and global encoders. This dual approach allows the model to adapt efficiently to new writing styles and unseen character forms, overcoming key limitations of existing OHG models. Extensive experimental evaluations demonstrate that the DNA model outperforms prior methods, setting new state-of-the-art results in OHG tasks particularly under conditions involving unseen styles and characters, thereby increasing the model’s practical applicability in real-world handwriting recognition enhancement. <div>
arXiv:2511.22064v1 Announce Type: new 
Abstract: Online handwriting generation (OHG) enhances handwriting recognition models by synthesizing diverse, human-like samples. However, existing OHG methods struggle to generate unseen characters, particularly in glyph-based languages like Chinese, limiting their real-world applicability. In this paper, we introduce our method for OHG, where the writer's style and the characters generated during testing are unseen during training. To tackle this challenge, we propose a Dual-branch Network with Adaptation (DNA), which comprises an adaptive style branch and an adaptive content branch. The style branch learns stroke attributes such as writing direction, spacing, placement, and flow to generate realistic handwriting. Meanwhile, the content branch is designed to generalize effectively to unseen characters by decomposing character content into structural information and texture details, extracted via local and global encoders, respectively. Extensive experiments demonstrate that our DNA model is well-suited for the unseen OHG setting, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldWander: Bridging Egocentric and Exocentric Worlds in Video Generation</title>
<link>https://arxiv.org/abs/2511.22098</link>
<guid>https://arxiv.org/abs/2511.22098</guid>
<content:encoded><![CDATA[
<div> Keywords: video diffusion models, egocentric-exocentric translation, WorldWander, in-context learning, EgoExo-8K<br /><br />Summary:<br /><br />1. The paper addresses the challenge of translating video content seamlessly between egocentric (first-person) and exocentric (third-person) perspectives, which is important for applications like filmmaking, embodied AI, and world modeling.  
2. The authors propose WorldWander, an in-context learning framework built upon advanced video diffusion transformers, specifically designed to handle cross-view synchronization in video generation.  
3. WorldWander introduces two key technical components: In-Context Perspective Alignment, which helps the model understand and align different viewpoints within context, and Collaborative Position Encoding, which enhances cross-view temporal and spatial consistency.  
4. To support training and evaluation, the authors curate EgoExo-8K, a large-scale dataset consisting of synchronized egocentric-exocentric video triplets from both synthetic environments and real-world scenes, enabling robust and diverse learning scenarios.  
5. Experimental results demonstrate that WorldWander significantly improves perspective synchronization and preserves character consistency between views, outperforming existing approaches and setting a new benchmark for egocentric-exocentric video translation tasks. <div>
arXiv:2511.22098v1 Announce Type: new 
Abstract: Video diffusion models have recently achieved remarkable progress in realism and controllability. However, achieving seamless video translation across different perspectives, such as first-person (egocentric) and third-person (exocentric), remains underexplored. Bridging these perspectives is crucial for filmmaking, embodied AI, and world models. Motivated by this, we present WorldWander, an in-context learning framework tailored for translating between egocentric and exocentric worlds in video generation. Building upon advanced video diffusion transformers, WorldWander integrates (i) In-Context Perspective Alignment and (ii) Collaborative Position Encoding to efficiently model cross-view synchronization. To further support our task, we curate EgoExo-8K, a large-scale dataset containing synchronized egocentric-exocentric triplets from both synthetic and real-world scenarios. Experiments demonstrate that WorldWander achieves superior perspective synchronization, character consistency, and generalization, setting a new benchmark for egocentric-exocentric video translation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRI-Based Brain Age Estimation with Supervised Contrastive Learning of Continuous Representation</title>
<link>https://arxiv.org/abs/2511.22102</link>
<guid>https://arxiv.org/abs/2511.22102</guid>
<content:encoded><![CDATA[
<div> Keywords: brain age estimation, supervised contrastive learning, Rank-N-Contrast loss, Grad-RAM, neurodegenerative diseases<br /><br />Summary:<br />1. The study focuses on MRI-based brain age estimation models, which aim to determine an individual's biological brain age through neuroanatomical features, relevant for identifying accelerated brain aging linked to neurodegenerative diseases.<br />2. Traditional deep learning regression methods often neglect the continuous nature of neuromorphological changes, limiting feature representation and accuracy.<br />3. To overcome these limitations, the authors propose a novel approach using supervised contrastive learning combined with the Rank-N-Contrast (RNC) loss function applied to T1-weighted structural MRI data for brain age estimation.<br />4. The method incorporates Grad-RAM visualization to provide more detailed and interpretable explanations of the regression outputs.<br />5. Experimental results demonstrate that the proposed method achieves a mean absolute error (MAE) of 4.27 years and an R² of 0.93 using a limited training set, outperforming conventional deep regression models with the same ResNet backbone and matching or surpassing state-of-the-art methods trained on larger datasets.<br />6. Grad-RAM visualizations reveal that the RNC loss captures more nuanced age-related neuromorphological features compared to standard regression.<br />7. As an exploratory application, the model estimates the biological-chronological brain age gap in Alzheimer's and Parkinson's patients, finding correlations between this gap and disease severity.<br />8. These findings support the potential utility of the approach as a biomarker for diagnosing and monitoring neurodegenerative disorders. <div>
arXiv:2511.22102v1 Announce Type: new 
Abstract: MRI-based brain age estimation models aim to assess a subject's biological brain age based on information, such as neuroanatomical features. Various factors, including neurodegenerative diseases, can accelerate brain aging and measuring this phenomena could serve as a potential biomarker for clinical applications. While deep learning (DL)-based regression has recently attracted major attention, existing approaches often fail to capture the continuous nature of neuromorphological changes, potentially resulting in sub-optimal feature representation and results. To address this, we propose to use supervised contrastive learning with the recent Rank-N-Contrast (RNC) loss to estimate brain age based on widely used T1w structural MRI for the first time and leverage Grad-RAM to visually explain regression results. Experiments show that our proposed method achieves a mean absolute error (MAE) of 4.27 years and an $R^2$ of 0.93 with a limited dataset of training samples, significantly outperforming conventional deep regression with the same ResNet backbone while performing better or comparably with the state-of-the-art methods with significantly larger training data. Furthermore, Grad-RAM revealed more nuanced features related to age regression with the RNC loss than conventional deep regression. As an exploratory study, we employed the proposed method to estimate the gap between the biological and chronological brain ages in Alzheimer's Disease and Parkinson's disease patients, and revealed the correlation between the brain age gap and disease severity, demonstrating its potential as a biomarker in neurodegenerative disorders.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE3D: Mixture of Experts meets Multi-Modal 3D Understanding</title>
<link>https://arxiv.org/abs/2511.22103</link>
<guid>https://arxiv.org/abs/2511.22103</guid>
<content:encoded><![CDATA[
<div> Multi-modal learning, Mixture of Experts, 3D understanding, Transformer, Pre-training<br /><br />Summary:<br /><br />This paper addresses the challenges in multi-modal 3D understanding tasks within computer vision, where existing fusion methods with a single dense network struggle due to modality heterogeneity and complexity. The authors propose MoE3D, a novel framework that integrates Mixture of Experts (MoE) into multi-modal learning to better leverage complementary information across different data modalities. MoE3D features specialized "expert" networks tailored to specific modalities or cross-modal interactions, improving feature processing. A MoE-based transformer architecture is introduced, enhancing the utilization of visual features. An information aggregation module is designed to further boost fusion effectiveness. The approach employs top-1 gating, allowing only one expert or expert group to process inputs for greater computational efficiency. Additionally, a progressive pre-training strategy is developed to harness semantic and 2D prior knowledge, providing better initialization for the network. Experiments demonstrate MoE3D's competitive performance across four key 3D understanding tasks. Crucially, MoE3D outperforms state-of-the-art methods by a significant margin, notably surpassing the best previous model by 6.1 mIoU on the Multi3DRefer dataset, highlighting its effectiveness in multi-modal fusion for 3D vision problems. <div>
arXiv:2511.22103v1 Announce Type: new 
Abstract: Multi-modal 3D understanding is a fundamental task in computer vision. Previous multi-modal fusion methods typically employ a single, dense fusion network, struggling to handle the significant heterogeneity and complexity across modalities, leading to suboptimal performance. In this paper, we propose MoE3D, which integrates Mixture of Experts (MoE) into the multi-modal learning framework. The core is that we deploy a set of specialized "expert" networks, each adept at processing a specific modality or a mode of cross-modal interaction. Specifically, the MoE-based transformer is designed to better utilize the complementary information hidden in the visual features. Information aggregation module is put forward to further enhance the fusion performance. Top-1 gating is employed to make one expert process features with expert groups, ensuring high efficiency. We further propose a progressive pre-training strategy to better leverage the semantic and 2D prior, thus equipping the network with good initialization. Our MoE3D achieves competitive performance across four prevalent 3D understanding tasks. Notably, our MoE3D surpasses the top-performing counterpart by 6.1 mIoU on Multi3DRefer.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperST: Hierarchical Hyperbolic Learning for Spatial Transcriptomics Prediction</title>
<link>https://arxiv.org/abs/2511.22107</link>
<guid>https://arxiv.org/abs/2511.22107</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatial Transcriptomics, gene expression prediction, hyperbolic space, hierarchical alignment, multi-level representation<br /><br />Summary:<br /><br />1. Spatial Transcriptomics (ST) integrates pathology images with gene expression data to analyze tissue function at spot level, but current methods struggle to fully utilize the hierarchical structure present in ST data, especially regarding gene expression.<br />2. Existing approaches mostly perform spot-level image-to-gene matching but overlook multi-level contextual information and the complexity of gene expression profiles, which contain molecular details that do not always have clear visual counterparts.<br />3. HyperST is proposed as a novel framework that captures multi-level representations from both image and gene data by embedding them into hyperbolic space, which naturally models hierarchical structures.<br />4. The framework includes a Multi-Level Representation Extractors module to obtain spot-level and niche-level features, providing enriched contextual information beyond individual spots.<br />5. A Hierarchical Hyperbolic Alignment module aligns these multi-level embeddings in a shared space, enhancing image features with molecular semantics and bridging the modality gap.<br />6. HyperST outperforms existing methods on four public datasets across various tissues, demonstrating its superior ability to predict spatial gene expression effectively and providing a scalable, accurate tool for ST prediction. <div>
arXiv:2511.22107v1 Announce Type: new 
Abstract: Spatial Transcriptomics (ST) merges the benefits of pathology images and gene expression, linking molecular profiles with tissue structure to analyze spot-level function comprehensively. Predicting gene expression from histology images is a cost-effective alternative to expensive ST technologies. However, existing methods mainly focus on spot-level image-to-gene matching but fail to leverage the full hierarchical structure of ST data, especially on the gene expression side, leading to incomplete image-gene alignment. Moreover, a challenge arises from the inherent information asymmetry: gene expression profiles contain more molecular details that may lack salient visual correlates in histological images, demanding a sophisticated representation learning approach to bridge this modality gap. We propose HyperST, a framework for ST prediction that learns multi-level image-gene representations by modeling the data's inherent hierarchy within hyperbolic space, a natural geometric setting for such structures. First, we design a Multi-Level Representation Extractors to capture both spot-level and niche-level representations from each modality, providing context-aware information beyond individual spot-level image-gene pairs. Second, a Hierarchical Hyperbolic Alignment module is introduced to unify these representations, performing spatial alignment while hierarchically structuring image and gene embeddings. This alignment strategy enriches the image representations with molecular semantics, significantly improving cross-modal prediction. HyperST achieves state-of-the-art performance on four public datasets from different tissues, paving the way for more scalable and accurate spatial transcriptomics prediction.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization</title>
<link>https://arxiv.org/abs/2511.22119</link>
<guid>https://arxiv.org/abs/2511.22119</guid>
<content:encoded><![CDATA[
<div> Text-to-image generation, prompt stealing, reinforcement learning, black-box attack, diffusion models<br /><br />Summary:<br /><br />Text-to-image generative models like Stable Diffusion produce high-quality images based on textual prompts, making these prompts valuable digital assets. This value raises security and intellectual property concerns, particularly regarding prompt stealing attacks which aim to recover the textual prompts from generated images. Such attacks not only risk unauthorized reuse but can also have positive uses like data attribution and model provenance. Existing methods for prompt stealing often need white-box access, large labeled datasets, or rely on simple captioning, limiting their practicality. To overcome these challenges, the authors propose PROMPTMINER, a black-box framework that separates prompt recovery into two phases: reinforcement learning-based optimization for reconstructing the primary subject, and fuzzing-based search to recover style-related modifiers. Extensive experiments on various datasets and diffusion backbones show PROMPTMINER achieves superior performance with high CLIP similarity (up to 0.958) and SBERT textual alignment (up to 0.751), outperforming all baselines. PROMPTMINER also generalizes well to real-world images with unknown generators, surpassing the strongest baseline by 7.5% in CLIP similarity. Lastly, it demonstrates robustness against defensive perturbations, making it a strong and practical approach to prompt stealing. The authors provide code for reproducibility. <div>
arXiv:2511.22119v1 Announce Type: new 
Abstract: Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GoPrune: Accelerated Structured Pruning with $\ell_{2,p}$-Norm Optimization</title>
<link>https://arxiv.org/abs/2511.22120</link>
<guid>https://arxiv.org/abs/2511.22120</guid>
<content:encoded><![CDATA[
<div> Keywords: convolutional neural networks, structured pruning, ℓ₂,ₚ-norm, proximal alternating minimization, network compression

<br /><br />Summary:  
The paper addresses the challenge of high storage and computational costs in deep convolutional neural networks (CNNs), which limit their deployment on edge devices with restricted resources. It focuses on pruning as a compression technique, emphasizing structured pruning due to its effectiveness in accelerating inference. Existing methods applying the ℓₚ-norm for pruning mainly handle unstructured pruning with \( p \in (0,1) \) and suffer from low computational efficiency. To overcome these issues, the authors propose GoPrune, an accelerated structured pruning method that leverages the ℓ₂,ₚ-norm for sparse network learning with \( p \in [0,1) \), extending the possible values of \( p \). The method includes an efficient optimization algorithm based on proximal alternating minimization (PAM), which simplifies subproblems into closed-form solutions, enhancing compression speed and efficiency. Experimental results on CIFAR datasets with ResNet and VGG architectures demonstrate that GoPrune achieves superior pruning performance compared to existing techniques. The authors also provide their implementation on GitHub, facilitating reproducibility and further research. Overall, GoPrune offers a computationally efficient structured pruning framework that balances network sparsity and performance for resource-limited environments. <div>
arXiv:2511.22120v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) suffer from rapidly increasing storage and computational costs as their depth grows, which severely hinders their deployment on resource-constrained edge devices. Pruning is a practical approach for network compression, among which structured pruning is the most effective for inference acceleration. Although existing work has applied the $\ell_p$-norm to pruning, it only considers unstructured pruning with $p\in (0, 1)$ and has low computational efficiency. To overcome these limitations, we propose an accelerated structured pruning method called GoPrune. Our method employs the $\ell_{2,p}$-norm for sparse network learning, where the value of $p$ is extended to $[0, 1)$. Moreover, we develop an efficient optimization algorithm based on the proximal alternating minimization (PAM), and the resulting subproblems enjoy closed-form solutions, thus improving compression efficiency. Experiments on the CIFAR datasets using ResNet and VGG models demonstrate the superior performance of the proposed method in network pruning. Our code is available at https://github.com/xianchaoxiu/GoPrune.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation</title>
<link>https://arxiv.org/abs/2511.22121</link>
<guid>https://arxiv.org/abs/2511.22121</guid>
<content:encoded><![CDATA[
<div> Keywords: single-image 3D generation, monocular cues, Cue3D framework, shading, silhouette<br /><br />Summary:<br /><br />This paper introduces Cue3D, the first comprehensive, model-agnostic framework designed to quantify the influence of individual monocular image cues in single-image 3D generation. The study evaluates seven state-of-the-art 3D generation methods, including regression-based, multi-view, and native 3D generative models, under a unified benchmark. By systematically perturbing image cues such as shading, texture, silhouette, perspective, edges, and local continuity, the work measures their specific impact on the quality of 3D outputs. The analysis reveals that shape meaningfulness, rather than texture, is the primary factor dictating a model's ability to generalize across inputs. It highlights geometric cues, especially shading, as critical for effective 3D reconstruction from single images. The research further identifies that many models rely heavily on provided silhouettes, which may limit robustness, and they exhibit varied sensitivities to other cues like perspective and local continuity depending on their architectural family. By dissecting how modern 3D networks exploit these classical vision cues, Cue3D not only deepens understanding of current methods but also provides valuable insights and directions to develop more transparent, robust, and controllable models for single-image 3D generation. <div>
arXiv:2511.22121v1 Announce Type: new 
Abstract: Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GA2-CLIP: Generic Attribute Anchor for Efficient Prompt Tuningin Video-Language Models</title>
<link>https://arxiv.org/abs/2511.22125</link>
<guid>https://arxiv.org/abs/2511.22125</guid>
<content:encoded><![CDATA[
<div> Visual-Language Models, soft prompt tuning, semantic space, video tasks, generalization<br /><br />Summary:<br /><br />This work addresses the challenge of maintaining the generalization ability of Vision-Language Models (VLMs) when fine-tuned on video tasks, as fine-tuning often narrows the semantic space, causing overfitting to supervised categories and impairing unseen class recognition. To tackle this, the authors propose a plug-and-play coupling prompt learning framework designed to optimize generalization in video tasks by mitigating semantic space narrowing. The core innovation involves integrating externally supervised prompts into the model: for textual prompts, pre-trained hard prompt tokens from other datasets are concatenated with soft prompt tokens and coupled through a learnable mapping layer, creating a competitive prompting mechanism that prevents overfitting. Additionally, they introduce irrelevant video sets and negative prompts as generic attribute anchors to help maintain generic attribute relevance within the pre-trained semantic space, further preserving generalization ability. Experimental results on video benchmarks demonstrate that this method significantly outperforms existing state-of-the-art soft prompt tuning approaches, especially regarding performance on base-to-new class prediction tasks, showcasing its effectiveness in enhancing generalization to unseen classes. <div>
arXiv:2511.22125v1 Announce Type: new 
Abstract: Visual and textual soft prompt tuning can effectively improve the adaptability of Vision-Language Models (VLMs) in downstream tasks. However, fine-tuning on video tasks impairs the model's generalization ability to unseen classes. Existing methods attempt to mitigate this forgetting effect by regularizing the gap between hand-crafted prompts and soft prompts, but this also weakens the learning ability of soft prompts. To address this challenge, we propose a plug-and-play coupling prompt learning framework to optimize the generalization performance of V-L models in video tasks, with the core motivation of mitigating semantic space narrowing during fine-tuning by introducing an externally supervised prompt. Specifically, for textual prompts, we introduce pre-trained prompts from other datasets as hard prompt tokens. These are concatenated with soft prompt tokens and coupled via a learnable mapping layer. This competitive prompting approach prevents the semantic space from overfitting to supervised categories. In addition, we introduce a set of well-designed irrelevant video sets and negative prompts as generic attribute anchors to maintain the generic relevance of the attributes in the pre-trained semantic space, thus preserving the generalization ability. Experiments on video tasks demonstrate that our method significantly outperforms state-of-the-art prompt tuning approaches across generalization benchmarks, particularly on base-to-new class prediction.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous labeling of surgical resection margins using a foundation model</title>
<link>https://arxiv.org/abs/2511.22131</link>
<guid>https://arxiv.org/abs/2511.22131</guid>
<content:encoded><![CDATA[
<div> resection margins, virtual inking network, digital pathology, cautery artifacts, whole-slide images<br /><br />Summary:<br /><br />1. Assessing resection margins is critical for evaluating pathological specimens and significantly impacts patient outcomes. Current methods rely on physical inking, which is inconsistently applied and can be obscured by cautery artifacts in histological slides.<br /><br />2. The study introduces a Virtual Inking Network (VIN) designed to autonomously localize surgical cut surfaces on whole-slide images, aiming to reduce dependence on physical inks and standardize margin assessment.<br /><br />3. VIN leverages a frozen foundation model as its feature extractor combined with a two-layer multilayer perceptron trained for patch-level classification to detect features consistent with cautery artifacts.<br /><br />4. The dataset includes 120 hematoxylin and eosin (H&amp;E) stained slides from 12 human tonsil tissue blocks, totaling about 2 TB of data, with margin boundaries annotated by a board-certified pathologist.<br /><br />5. In blind testing on 20 slides from unseen blocks, VIN produced margin overlays that qualitatively matched expert annotations, achieving a region-level accuracy of approximately 73.3%, with minor errors that did not disrupt margin continuity.<br /><br />6. These findings demonstrate VIN's capability to capture cautery-related histomorphology and provide reproducible, ink-free margin delineations, supporting its integration into routine digital pathology workflows and enabling downstream margin distance measurements. <div>
arXiv:2511.22131v1 Announce Type: new 
Abstract: Assessing resection margins is central to pathological specimen evaluation and has profound implications for patient outcomes. Current practice employs physical inking, which is applied variably, and cautery artifacts can obscure the true margin on histological sections. We present a virtual inking network (VIN) that autonomously localizes the surgical cut surface on whole-slide images, reducing reliance on inks and standardizing margin-focused review. VIN uses a frozen foundation model as the feature extractor and a compact two-layer multilayer perceptron trained for patch-level classification of cautery-consistent features. The dataset comprised 120 hematoxylin and eosin (H&amp;E) stained slides from 12 human tonsil tissue blocks, resulting in ~2 TB of uncompressed raw image data, where a board-certified pathologist provided boundary annotations. In blind testing with 20 slides from previously unseen blocks, VIN produced coherent margin overlays that qualitatively aligned with expert annotations across serial sections. Quantitatively, region-level accuracy was ~73.3% across the test set, with errors largely confined to limited areas that did not disrupt continuity of the whole-slide margin map. These results indicate that VIN captures cautery-related histomorphology and can provide a reproducible, ink-free margin delineation suitable for integration into routine digital pathology workflows and for downstream measurement of margin distances.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action</title>
<link>https://arxiv.org/abs/2511.22134</link>
<guid>https://arxiv.org/abs/2511.22134</guid>
<content:encoded><![CDATA[
<div> Vision-Language-Action, action degeneration, dual-layer data pruning, dual-teacher adaptive distillation, VLA Score<br /><br />Summary:<br /><br />1. The paper addresses the challenge of building generalizable Vision-Language-Action (VLA) models that combine strong reasoning abilities with precise manipulation skills for robotics. 2. A common approach trains a specialist VLA on robot demonstrations to learn reliable actions, then fine-tunes it with mixed annotated robot and multimodal data to enhance reasoning; however, this often leads to "action degeneration," where action performance degrades after fine-tuning. 3. To counter this, the authors propose DualVLA, a model that improves action performance post-training while preserving reasoning ability. 4. DualVLA introduces a dual-layer data pruning method to eliminate redundant embodied reasoning data that negatively impacts action learning. 5. Furthermore, a dual-teacher adaptive distillation strategy is designed to give tailored supervision signals for different data domains, maintaining a balance between action accuracy and reasoning. 6. To better evaluate generalist VLAs, the authors propose the VLA Score, which breaks down VLA capabilities into reasoning, intention, action, and alignment dimensions for detailed assessment. 7. Experimental results demonstrate DualVLA achieves an average success rate of 61.0 in SimplerEnv and scores 65.4 on eight multimodal benchmarks, showing a stronger balance between precise action execution and multimodal understanding. 8. The research underscores the effectiveness of targeted data pruning and adaptive distillation in resolving the trade-off between reasoning and action in VLA models. <div>
arXiv:2511.22134v1 Announce Type: new 
Abstract: To build a generalizable Vision-Language-Action (VLA) model with strong reasoning ability, a common strategy is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then incorporate mixed annotated robot data together with multimodal data to restore broader reasoning capabilities. However, we observe that the resulting reasoning VLA often suffers from degraded action performance compared to the specialist model before fine-tuning, a phenomenon we refer to as action degeneration. To address this issue, we propose DualVLA, which enhances action performance through carefully designed post-training while still preserving reasoning capability. We first introduce a dual-layer data pruning method that removes redundant embodied reasoning, preventing it from adversely influencing action learning. To further strengthen action generation, we design a dual-teacher adaptive distillation strategy that assigns different supervision signals to different data domains while maintaining reasoning ability. To fill the evaluation gap for generalist VLAs, we also propose VLA Score, which decouples VLA capability into reasoning, intention, action, and alignment dimensions for a more fine-grained assessment. Experiments show that DualVLA achieves an average success rate of 61.0 in SimplerEnv and an average score of 65.4 across eight competitive multimodal benchmarks, demonstrating a stronger balance between precise action execution and multimodal understanding. Project Website: https://costaliya.github.io/DualVLA/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EASL: Multi-Emotion Guided Semantic Disentanglement for Expressive Sign Language Generation</title>
<link>https://arxiv.org/abs/2511.22135</link>
<guid>https://arxiv.org/abs/2511.22135</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, sign language generation, emotional expression, emotion-semantic disentanglement, diffusion models<br /><br />Summary:  
1. Large language models (LLMs) have greatly advanced sign language generation by converting text into high-quality sign language videos, enhancing communication accessibility for the Deaf community.  
2. Current LLM-based methods focus mainly on semantic accuracy but fail to capture emotional expressions, leading to generated sign language that lacks naturalness and expressiveness.  
3. The proposed EASL (Emotion-Aware Sign Language) framework introduces a multi-emotion-guided generation architecture designed to integrate fine-grained emotional information into sign language synthesis.  
4. EASL employs emotion-semantic disentanglement modules with progressive training to separately extract semantic features and affective (emotional) features, allowing better emotional representation.  
5. During pose decoding, EASL uses the emotional representations to guide semantic interactions, generating sign poses accompanied by 7-class emotion confidence scores, which enables recognition of emotional expressions.  
6. Experimental results show that EASL outperforms existing baseline methods in pose accuracy by effectively incorporating multi-emotion information.  
7. Additionally, EASL adapts well to diffusion models, facilitating the generation of more expressive and emotionally rich sign language videos, enhancing naturalness and communication quality. <div>
arXiv:2511.22135v1 Announce Type: new 
Abstract: Large language models have revolutionized sign language generation by automatically transforming text into high-quality sign language videos, providing accessible communication for the Deaf community. However, existing LLM-based approaches prioritize semantic accuracy while overlooking emotional expressions, resulting in outputs that lack naturalness and expressiveness. We propose EASL (Emotion-Aware Sign Language), a multi-emotion-guided generation architecture for fine-grained emotional integration. We introduce emotion-semantic disentanglement modules with progressive training to separately extract semantic and affective features. During pose decoding, the emotional representations guide semantic interaction to generate sign poses with 7-class emotion confidence scores, enabling emotional expression recognition. Experimental results demonstrate that EASL achieves pose accuracy superior to all compared baselines by integrating multi-emotion information and effectively adapts to diffusion models to generate expressive sign language videos.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SemOD: Semantic Enabled Object Detection Network under Various Weather Conditions</title>
<link>https://arxiv.org/abs/2511.22142</link>
<guid>https://arxiv.org/abs/2511.22142</guid>
<content:encoded><![CDATA[
<div> Keywords: semantic-enabled network, object detection, all-weather conditions, image enhancement, YOLO

<br /><br />Summary: This paper addresses the challenge of object detection in autonomous driving systems under diverse weather conditions, where most existing camera-based perception models are trained on clear weather data and lack adaptability. The authors propose a semantic-enabled network composed of two main components: a Preprocessing Unit (PPU) and a Detection Unit (DTU). The PPU uses a U-shaped network enriched by semantic information to refine degraded images caused by adverse weather, enhancing visual coherence and realism by generating plausible content in missing or degraded areas. The DTU then incorporates this semantic data within a modified YOLO detection framework to improve object recognition accuracy. The inclusion of semantic information helps the model understand object boundaries better and supports effective image transformation across various weather scenarios. Experimental results demonstrate that this approach yields improvements in mean Average Precision (mAP) by 1.47% to 8.80% compared to existing methods on multiple benchmark datasets, showcasing the effectiveness of semantics for both image enhancement and object detection. The approach pioneers the integration of semantic information throughout the all-weather detection pipeline, providing a comprehensive and robust solution to enhance autonomous vehicle perception systems. The related code is made publicly available for further research and development. <div>
arXiv:2511.22142v1 Announce Type: new 
Abstract: In the field of autonomous driving, camera-based perception models are mostly trained on clear weather data. Models that focus on addressing specific weather challenges are unable to adapt to various weather changes and primarily prioritize their weather removal characteristics. Our study introduces a semantic-enabled network for object detection in diverse weather conditions. In our analysis, semantics information can enable the model to generate plausible content for missing areas, understand object boundaries, and preserve visual coherency and realism across both filled-in and existing portions of the image, which are conducive to image transformation and object recognition. Specific in implementation, our architecture consists of a Preprocessing Unit (PPU) and a Detection Unit (DTU), where the PPU utilizes a U-shaped net enriched by semantics to refine degraded images, and the DTU integrates this semantic information for object detection using a modified YOLO network. Our method pioneers the use of semantic data for all-weather transformations, resulting in an increase between 1.47\% to 8.80\% in mAP compared to existing methods across benchmark datasets of different weather. This highlights the potency of semantics in image enhancement and object detection, offering a comprehensive approach to improving object detection performance. Code will be available at https://github.com/EnisZuo/SemOD.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stacked Ensemble of Fine-Tuned CNNs for Knee Osteoarthritis Severity Grading</title>
<link>https://arxiv.org/abs/2511.22143</link>
<guid>https://arxiv.org/abs/2511.22143</guid>
<content:encoded><![CDATA[
<div> Knee Osteoarthritis, Kellgren-Lawrence grading, stacked ensemble model, convolutional neural networks, CatBoost<br /><br />Summary:<br /><br />Knee Osteoarthritis (KOA) is a musculoskeletal disorder impacting daily activities, particularly among elderly individuals. KOA severity is traditionally assessed by analyzing knee X-rays with the Kellgren-Lawrence (KL) grading system, which classifies severity from 0 to 4, though this method requires expertise and is prone to subjective errors. To improve diagnostic accuracy and efficiency, the authors developed a stacked ensemble model that combines several fine-tuned Convolutional Neural Networks (CNNs) for two tasks: binary classification to detect the presence of KOA and multiclass classification to assign the precise KL grade. The ensemble includes MobileNetV2, YOLOv8, and DenseNet201 as base learners, with Categorical Boosting (CatBoost) acting as the meta-learner. The model achieved a balanced test accuracy of 87.5% for binary classification and 73% for multiclass grading, outperforming previous studies reported in the literature. This approach reduces subjectivity and can potentially streamline KOA diagnosis and grading, supporting clinicians with more reliable and automated assessments based on X-ray imaging. <div>
arXiv:2511.22143v1 Announce Type: new 
Abstract: Knee Osteoarthritis (KOA) is a musculoskeletal condition that can cause significant limitations and impairments in daily activities, especially among older individuals. To evaluate the severity of KOA, typically, X-ray images of the affected knee are analyzed, and a grade is assigned based on the Kellgren-Lawrence (KL) grading system, which classifies KOA severity into five levels, ranging from 0 to 4. This approach requires a high level of expertise and time and is susceptible to subjective interpretation, thereby introducing potential diagnostic inaccuracies. To address this problem a stacked ensemble model of fine-tuned Convolutional Neural Networks (CNNs) was developed for two classification tasks: a binary classifier for detecting the presence of KOA, and a multiclass classifier for precise grading across the KL spectrum. The proposed stacked ensemble model consists of a diverse set of pre-trained architectures, including MobileNetV2, You Only Look Once (YOLOv8), and DenseNet201 as base learners and Categorical Boosting (CatBoost) as the meta-learner. This proposed model had a balanced test accuracy of 73% in multiclass classification and 87.5% in binary classification, which is higher than previous works in extant literature.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks</title>
<link>https://arxiv.org/abs/2511.22147</link>
<guid>https://arxiv.org/abs/2511.22147</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian splatting, computation cost attacks, black-box defense, adversarial training, 3D reconstruction<br /><br />Summary:<br /><br />This paper addresses vulnerabilities in 3D Gaussian splatting (3DGS), a prominent technique for 3D reconstruction widely used in various applications. The authors highlight recent discoveries of critical computation cost attacks that exploit 3DGS, resulting in malicious resource consumption and denial-of-service (DoS) situations, thus threatening the reliability of 3DGS systems. To counter these threats, the paper introduces RemedyGS, the first comprehensive black-box defense framework designed specifically for 3DGS pipelines. RemedyGS consists of two main components: a detector that identifies input images affected by poisoned textures and a purifier that restores these attacked images back to a benign state, mitigating the attacks’ harmful impact. The framework also integrates adversarial training within the purifier to ensure alignment of the distribution between recovered images and original natural images, which strengthens the defense mechanism's effectiveness. Experimental evaluations demonstrate that RemedyGS not only successfully defends against white-box, black-box, and adaptive attacks but also maintains high utility and safety standards for 3DGS systems. This work paves the way for more secure and dependable deployment of 3D Gaussian splatting in real-world applications by effectively mitigating computationally costly adversarial threats. <div>
arXiv:2511.22147v1 Announce Type: new 
Abstract: As a mainstream technique for 3D reconstruction, 3D Gaussian splatting (3DGS) has been applied in a wide range of applications and services. Recent studies have revealed critical vulnerabilities in this pipeline and introduced computation cost attacks that lead to malicious resource occupancies and even denial-of-service (DoS) conditions, thereby hindering the reliable deployment of 3DGS. In this paper, we propose the first effective and comprehensive black-box defense framework, named RemedyGS, against such computation cost attacks, safeguarding 3DGS reconstruction systems and services. Our pipeline comprises two key components: a detector to identify the attacked input images with poisoned textures and a purifier to recover the benign images from their attacked counterparts, mitigating the adverse effects of these attacks. Moreover, we incorporate adversarial training into the purifier to enforce distributional alignment between the recovered and original natural images, thereby enhancing the defense efficacy. Experimental results demonstrate that our framework effectively defends against white-box, black-box, and adaptive attacks in 3DGS systems, achieving state-of-the-art performance in both safety and utility.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IMTalker: Efficient Audio-driven Talking Face Generation with Implicit Motion Transfer</title>
<link>https://arxiv.org/abs/2511.22167</link>
<guid>https://arxiv.org/abs/2511.22167</guid>
<content:encoded><![CDATA[
<div> Keywords: talking face generation, implicit motion transfer, identity preservation, cross-attention mechanism, audio-lip synchronization  

<br /><br />Summary:  
This paper introduces IMTalker, a new framework for talking face generation that improves upon existing methods by using implicit motion transfer instead of explicit optical flow and local warping techniques. The approach replaces traditional flow-based warping with a cross-attention mechanism to implicitly capture motion discrepancy and identity alignment within a unified latent space, enabling robust modeling of complex global motions and reducing identity drift. To better preserve speaker identity during cross-identity reenactment, an identity-adaptive module is integrated, which projects motion latents into personalized spaces, ensuring a clear separation between motion and identity features. Additionally, IMTalker employs a lightweight flow-matching motion generator that creates vivid and controllable implicit motion vectors from a combination of audio, pose, and gaze inputs. Experimental results demonstrate that IMTalker outperforms previous state-of-the-art methods in terms of motion accuracy, identity preservation, and audio-lip synchronization quality. The system is also highly efficient, operating at 40 frames per second (FPS) during video-driven generation and 42 FPS for audio-driven generation on an RTX 4090 GPU. The authors plan to release their code and pretrained models to support further applications and research in this domain. <div>
arXiv:2511.22167v1 Announce Type: new 
Abstract: Talking face generation aims to synthesize realistic speaking portraits from a single image, yet existing methods often rely on explicit optical flow and local warping, which fail to model complex global motions and cause identity drift. We present IMTalker, a novel framework that achieves efficient and high-fidelity talking face generation through implicit motion transfer. The core idea is to replace traditional flow-based warping with a cross-attention mechanism that implicitly models motion discrepancy and identity alignment within a unified latent space, enabling robust global motion rendering. To further preserve speaker identity during cross-identity reenactment, we introduce an identity-adaptive module that projects motion latents into personalized spaces, ensuring clear disentanglement between motion and identity. In addition, a lightweight flow-matching motion generator produces vivid and controllable implicit motion vectors from audio, pose, and gaze cues. Extensive experiments demonstrate that IMTalker surpasses prior methods in motion accuracy, identity preservation, and audio-lip synchronization, achieving state-of-the-art quality with superior efficiency, operating at 40 FPS for video-driven and 42 FPS for audio-driven generation on an RTX 4090 GPU. We will release our code and pre-trained models to facilitate applications and future research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Long Horizon Air Quality Forecasting via Group-Relative Policy Optimization</title>
<link>https://arxiv.org/abs/2511.22169</link>
<guid>https://arxiv.org/abs/2511.22169</guid>
<content:encoded><![CDATA[
<div> Keywords: particulate matter forecasting, East Asia, CMAQ-OBS dataset, Group-Relative Policy Optimization, False Alarm Rate reduction<br /><br />Summary:<br /><br />Accurate long-term forecasting of particulate matter (PM) concentration is vital for public health decision-making, especially in regions with complex atmospheric conditions like East Asia. Existing foundation models such as Aurora provide wide applicability but often fail to capture region-specific dynamics and depend on non-real-time inputs, limiting their effectiveness for localized and timely warnings. To overcome these limitations, the authors developed and released the CMAQ-OBS dataset, which combines real-world observations and high-resolution model outputs for East Asia. This dataset reduces regional forecasting error by 59.5% and supports real-time forecasts ranging from 48 to 120 hours, crucial for enabling prompt public health alerts. The study highlights the inadequacy of standard point-wise training objectives, which overlook the asymmetric costs of false alarms versus missed severe pollution events. Standard supervised fine-tuning (SFT) models tend to over-predict, leading to elevated false alarm rates and undermining public trust. To better align the models with operational priorities, the authors introduce Group-Relative Policy Optimization (GRPO), a method employing class-wise rewards and curriculum rollout strategies. Experimental results demonstrate that their approach significantly reduces false alarm rates by 47.3% compared to SFT baselines, while maintaining a competitive F1-score. This framework offers a practical and reliable solution for long horizon air quality forecasting in real-world applications. <div>
arXiv:2511.22169v1 Announce Type: new 
Abstract: Accurate long horizon forecasting of particulate matter (PM) concentration fields is essential for operational public health decisions. However, achieving reliable forecasts remains challenging in regions with complex terrain and strong atmospheric dynamics such as East Asia. While foundation models such as Aurora offer global generality, they often miss region-specific dynamics and rely on non-real-time inputs, limiting their practical utility for localized warning systems. To address this gap, we construct and release the real-world observations and high-resolution CMAQ-OBS dataset for East Asia, reducing regional error by 59.5% and enabling real-time 48-120 hour forecasts critical for public health alerts. However, standard point-wise objectives cannot reflect asymmetric operational costs, where false alarms deteriorate public trust while missed severe events endanger populations. This cost mismatch causes SFT models to over-predict and yield high False Alarm Rates. We introduce Group-Relative Policy Optimization (GRPO) with class-wise rewards and curriculum rollout to align predictions with operational priorities. Experimental results demonstrate that our framework significantly improves the reliability of the forecast. Compared to the SFT-only baseline, our model reduces the False Alarm Rate by 47.3% while achieving a competitive F1-score, proving its effectiveness for practical, real-world air quality forecasting systems on long lead time scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partially Shared Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2511.22170</link>
<guid>https://arxiv.org/abs/2511.22170</guid>
<content:encoded><![CDATA[
<div> Concept Bottleneck Models, Large Language Models, Vision-Language Models, interpretability, concept compactness<br /><br />Summary:<br /><br />1. Concept Bottleneck Models (CBMs) improve interpretability by inserting a layer of human-understandable concepts between input data and model predictions. 2. Existing automated concept generation methods using Large Language Models (LLMs) and Vision-Language Models (VLMs) suffer from issues like poor visual grounding, redundant concepts, and lack of suitable metrics to balance accuracy and compactness. 3. The authors propose PS-CBM, a Partially Shared Concept Bottleneck Model framework that mitigates these challenges with three key components: a multimodal concept generator integrating LLM-based semantic information with exemplar visual cues; a Partially Shared Concept Strategy that merges concepts based on their activation patterns to balance specificity and reduce redundancy; and a new post-hoc metric named Concept-Efficient Accuracy (CEA) that jointly evaluates predictive accuracy and concept compactness. 4. PS-CBM was rigorously tested on eleven diverse datasets, consistently outperforming state-of-the-art CBMs by improving classification accuracy by 1.0% to 7.4% and increasing CEA by 2.0% to 9.5%. 5. The results demonstrate that PS-CBM achieves a better trade-off between high classification accuracy and strong interpretability while requiring fewer concepts, highlighting its effectiveness in enhancing concept-based model design. <div>
arXiv:2511.22170v1 Announce Type: new 
Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by introducing a layer of human-understandable concepts between inputs and predictions. While recent methods automate concept generation using Large Language Models (LLMs) and Vision-Language Models (VLMs), they still face three fundamental challenges: poor visual grounding, concept redundancy, and the absence of principled metrics to balance predictive accuracy and concept compactness. We introduce PS-CBM, a Partially Shared CBM framework that addresses these limitations through three core components: (1) a multimodal concept generator that integrates LLM-derived semantics with exemplar-based visual cues; (2) a Partially Shared Concept Strategy that merges concepts based on activation patterns to balance specificity and compactness; and (3) Concept-Efficient Accuracy (CEA), a post-hoc metric that jointly captures both predictive accuracy and concept compactness. Extensive experiments on eleven diverse datasets show that PS-CBM consistently outperforms state-of-the-art CBMs, improving classification accuracy by 1.0%-7.4% and CEA by 2.0%-9.5%, while requiring significantly fewer concepts. These results underscore PS-CBM's effectiveness in achieving both high accuracy and strong interpretability.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BrepGPT: Autoregressive B-rep Generation with Voronoi Half-Patch</title>
<link>https://arxiv.org/abs/2511.22171</link>
<guid>https://arxiv.org/abs/2511.22171</guid>
<content:encoded><![CDATA[
<div> Boundary representation, B-rep, Voronoi Half-Patch, VQ-VAE, Transformer

<br /><br />Summary:  
Boundary representation (B-rep) is the standard format for CAD models but existing generative methods rely on complex multi-stage networks that cause errors and inefficiency. The authors introduce BrepGPT, a single-stage autoregressive model that generates B-rep structures more effectively. The core innovation is the Voronoi Half-Patch (VHP) representation, which breaks down B-reps into local units by assigning geometry to the closest half-edges and sampling their connections. This approach unifies geometric and topological data into a single coherent format, avoiding multiple hierarchical encodings. The method uses dual vector-quantized VAEs (VQ-VAEs) to encode vertex topology and VHP units into compact vertex-based tokens. These tokens are then autoregressively predicted by a decoder-only Transformer, enabling direct reconstruction of complete B-rep models. Experiments show that BrepGPT outperforms previous methods in unconditional B-rep generation. Additionally, the framework supports conditional generation based on category labels, point clouds, text, and images, as well as applications like B-rep autocompletion and interpolation. This demonstrates its versatility and potential for streamlined CAD model generation in diverse scenarios. <div>
arXiv:2511.22171v1 Announce Type: new 
Abstract: Boundary representation (B-rep) is the de facto standard for CAD model representation in modern industrial design. The intricate coupling between geometric and topological elements in B-rep structures has forced existing generative methods to rely on cascaded multi-stage networks, resulting in error accumulation and computational inefficiency. We present BrepGPT, a single-stage autoregressive framework for B-rep generation. Our key innovation lies in the Voronoi Half-Patch (VHP) representation, which decomposes B-reps into unified local units by assigning geometry to nearest half-edges and sampling their next pointers. Unlike hierarchical representations that require multiple distinct encodings for different structural levels, our VHP representation facilitates unifying geometric attributes and topological relations in a single, coherent format. We further leverage dual VQ-VAEs to encode both vertex topology and Voronoi Half-Patches into vertex-based tokens, achieving a more compact sequential encoding. A decoder-only Transformer is then trained to autoregressively predict these tokens, which are subsequently mapped to vertex-based features and decoded into complete B-rep models. Experiments demonstrate that BrepGPT achieves state-of-the-art performance in unconditional B-rep generation. The framework also exhibits versatility in various applications, including conditional generation from category labels, point clouds, text descriptions, and images, as well as B-rep autocompletion and interpolation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding the Inner Eye: A Framework for Hierarchical and Flexible Visual Grounded Reasoning</title>
<link>https://arxiv.org/abs/2511.22172</link>
<guid>https://arxiv.org/abs/2511.22172</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal AI, visual grounded reasoning, reinforcement learning, Salience-Weighted IoU Reward, cognitive flexibility<br /><br />Summary:  
The paper presents GRiP (Guided Reasoning and Perception), a novel two-stage training framework designed to enhance models' capabilities in visual grounded reasoning by explicitly guiding perceptual focus and logical reasoning pathways. It addresses limitations in current methods, which either suffer from instability in end-to-end reinforcement learning (RL) or lack flexibility due to supervised fine-tuning (SFT). GRiP introduces two main innovations during its cognitive-enhanced RL stage: (1) a Salience-Weighted Intersection over Union (IoU) Reward that encourages prioritizing mission-critical objects over irrelevant distractors, and (2) a Multi-Heuristic Reward that fosters cognitive flexibility by rewarding diverse yet logically valid reasoning strategies. Starting with the Qwen2.5-VL-7B model, GRiP significantly improves performance on multiple challenging benchmarks, achieving state-of-the-art results among open-source models on TreeBench and V* Bench, both known for their complex visual reasoning tasks. This demonstrates that going beyond simplistic reward functions and incorporating cognitively inspired signals about what to observe and how to reason is vital for advancing multimodal intelligence. The authors also commit to releasing the code publicly to support further research. <div>
arXiv:2511.22172v1 Announce Type: new 
Abstract: Models capable of "thinking with images" by dynamically grounding their reasoning in visual evidence represent a major leap in multimodal AI. However, replicating and advancing this ability is non-trivial, with current methods often trapped between the instability of end-to-end reinforcement learning (RL) and the rigidity of supervised fine-tuning (SFT). This leads to models that either struggle to learn or lack the cognitive flexibility required for complex, real-world scenes. To navigate this dilemma, we introduce GRiP (Guided Reasoning and Perception), a novel two-stage training framework that cultivates robust and flexible visual grounded reasoning by explicitly guiding the model's perceptual focus and logical pathways. GRiP's core lies in its cognitive-enhanced RL stage, which features two key innovations: (1) a Salience-Weighted IoU Reward that incentivizes the model to prioritize the localization of mission-critical objects over trivial distractors, and (2) a Multi-Heuristic Reward that encourages cognitive flexibility by rewarding diverse yet logically valid reasoning pathways. Initialized from the Qwen2.5-VL-7B model, GRiP demonstrates significant performance gains across multiple challenging benchmarks. It achieves state-of-the-art results among open-source models on the highly challenging TreeBench and V* Bench, proving its effectiveness in complex visual reasoning. Our work demonstrates that moving beyond simplistic rewards and instead guiding models with cognitively-inspired signals for what to see and how to think is crucial for unlocking the next level of multimodal intelligence. The code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Graph Convolutional Network with Chebyshev Spectral Graph and Graph Attention for Autism Spectrum Disorder Classification</title>
<link>https://arxiv.org/abs/2511.22178</link>
<guid>https://arxiv.org/abs/2511.22178</guid>
<content:encoded><![CDATA[
<div> Keywords: Autism Spectrum Disorder, Graph Convolutional Network, multimodal neuroimaging, Chebyshev Spectral Graph Convolution, Graph Attention Networks  

<br /><br />Summary:  
This paper addresses the challenges of early and objective diagnosis of Autism Spectrum Disorder (ASD), a neurodevelopmental disorder with diverse symptom manifestations and neurological bases. The authors propose a novel Graph Convolutional Network (GCN) model that integrates Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT) to improve classification accuracy of ASD by utilizing multimodal neuroimaging data and phenotypic information. The study leverages the ABIDE I dataset, consisting of resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic data from 870 individuals. The model employs a multi-branch architecture to process each data modality separately before merging them via concatenation. A population graph is constructed based on site similarity to capture inter-individual relationships. Chebyshev polynomial filters enable efficient localized spectral learning, while GAT layers enhance node representation through attention-weighted aggregation of neighboring nodes. The model is trained using stratified five-fold cross-validation with an input dimension of 5,206 features per subject. Experimental results demonstrate that the proposed model outperforms several state-of-the-art baselines, achieving a test accuracy of 74.82% along with an AUC of 0.82, highlighting its potential for more accurate and robust ASD diagnosis using multimodal data. <div>
arXiv:2511.22178v1 Announce Type: new 
Abstract: ASD is a complicated neurodevelopmental disorder marked by variation in symptom presentation and neurological underpinnings, making early and objective diagnosis extremely problematic. This paper presents a Graph Convolutional Network (GCN) model, incorporating Chebyshev Spectral Graph Convolution and Graph Attention Networks (GAT), to increase the classification accuracy of ASD utilizing multimodal neuroimaging and phenotypic data. Leveraging the ABIDE I dataset, which contains resting-state functional MRI (rs-fMRI), structural MRI (sMRI), and phenotypic variables from 870 patients, the model leverages a multi-branch architecture that processes each modality individually before merging them via concatenation. Graph structure is encoded using site-based similarity to generate a population graph, which helps in understanding relationship connections across individuals. Chebyshev polynomial filters provide localized spectral learning with lower computational complexity, whereas GAT layers increase node representations by attention-weighted aggregation of surrounding information. The proposed model is trained using stratified five-fold cross-validation with a total input dimension of 5,206 features per individual. Extensive trials demonstrate the enhanced model's superiority, achieving a test accuracy of 74.82\% and an AUC of 0.82 on the entire dataset, surpassing multiple state-of-the-art baselines, including conventional GCNs, autoencoder-based deep neural networks, and multimodal CNNs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction</title>
<link>https://arxiv.org/abs/2511.22181</link>
<guid>https://arxiv.org/abs/2511.22181</guid>
<content:encoded><![CDATA[
<div> trajectory planning, autonomous driving, Vision Transformer (ViT), motion prediction, multimodal trajectory

<br /><br />Summary:  
The paper introduces MTR-VP, a novel approach for trajectory planning in autonomous driving that relies on vision-based context embeddings instead of traditional map-based features. 1) The method uses a Vision Transformer (ViT) encoder to process raw camera images along with past vehicle kinematic states, producing context embeddings inspired by the Motion Transformer (MTR) framework. 2) MTR-VP replaces MTR’s learnable intention queries with a cross-attention mechanism that integrates intent information with scene and historical vehicle data embeddings. 3) The approach is evaluated on the Waymo End-to-End Driving Dataset, aiming to predict 5-second future trajectories in bird’s-eye-view coordinates by utilizing prior images, pose histories, and routing goals. 4) Ablation studies reveal that transformer-based fusion of visual and kinetic features alone struggles to create effective scene context embeddings, even when enhanced with foundation-model embeddings like CLIP and DINOv2. 5) However, forecasting a distribution over multiple future trajectories rather than a single one significantly improves planning quality, highlighting the benefit of multimodal prediction outputs for autonomous driving trajectory planning. <div>
arXiv:2511.22181v1 Announce Type: new 
Abstract: We present a method for trajectory planning for autonomous driving, learning image-based context embeddings that align with motion prediction frameworks and planning-based intention input. Within our method, a ViT encoder takes raw images and past kinematic state as input and is trained to produce context embeddings, drawing inspiration from those generated by the recent MTR (Motion Transformer) encoder, effectively substituting map-based features with learned visual representations. MTR provides a strong foundation for multimodal trajectory prediction by localizing agent intent and refining motion iteratively via motion query pairs; we name our approach MTR-VP (Motion Transformer for Vision-based Planning), and instead of the learnable intention queries used in the MTR decoder, we use cross attention on the intent and the context embeddings, which reflect a combination of information encoded from the driving scene and past vehicle states. We evaluate our methods on the Waymo End-to-End Driving Dataset, which requires predicting the agent's future 5-second trajectory in bird's-eye-view coordinates using prior camera images, agent pose history, and routing goals. We analyze our architecture using ablation studies, removing input images and multiple trajectory output. Our results suggest that transformer-based methods that are used to combine the visual features along with the kinetic features such as the past trajectory features are not effective at combining both modes to produce useful scene context embeddings, even when intention embeddings are augmented with foundation-model representations of scene context from CLIP and DINOv2, but that predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shoe Style-Invariant and Ground-Aware Learning for Dense Foot Contact Estimation</title>
<link>https://arxiv.org/abs/2511.22184</link>
<guid>https://arxiv.org/abs/2511.22184</guid>
<content:encoded><![CDATA[
<div> Keywords: foot contact estimation, shoe style-invariant, ground-aware learning, adversarial training, dense prediction<br /><br />Summary:<br /><br />Foot contact is a crucial aspect of human physical interaction and movement analysis, yet existing methods mostly rely on joint-level approximations using zero-velocity constraints which lack detailed interaction modeling between the foot and environment. This study addresses the challenge of predicting dense foot contact maps from a single RGB image, a largely underexplored area. Two primary difficulties are identified: (1) high variability in shoe appearances that hinder generalization across different shoe styles, and (2) the typically monotonous texture of the ground, which provides limited informative features for prediction. To overcome these challenges, the authors propose the FEet COntact estimation (FECO) framework, which learns dense foot contact by incorporating both shoe style-invariant and ground-aware learning components. The framework leverages adversarial training to enforce shoe style-invariant feature learning, allowing the model to generalize across diverse shoe types effectively. Additionally, a ground feature extractor is introduced to capture ground characteristics using spatial context, improving the utilization of ground information for contact estimation. Experimental results demonstrate that FECO attains robust and reliable foot contact predictions regardless of shoe appearance and effectively exploits ground cues. The authors plan to release their code to facilitate further research in this domain. <div>
arXiv:2511.22184v1 Announce Type: new 
Abstract: Foot contact plays a critical role in human interaction with the world, and thus exploring foot contact can advance our understanding of human movement and physical interaction. Despite its importance, existing methods often approximate foot contact using a zero-velocity constraint and focus on joint-level contact, failing to capture the detailed interaction between the foot and the world. Dense estimation of foot contact is crucial for accurately modeling this interaction, yet predicting dense foot contact from a single RGB image remains largely underexplored. There are two main challenges for learning dense foot contact estimation. First, shoes exhibit highly diverse appearances, making it difficult for models to generalize across different styles. Second, ground often has a monotonous appearance, making it difficult to extract informative features. To tackle these issues, we present a FEet COntact estimation (FECO) framework that learns dense foot contact with shoe style-invariant and ground-aware learning. To overcome the challenge of shoe appearance diversity, our approach incorporates shoe style adversarial training that enforces shoe style-invariant features for contact estimation. To effectively utilize ground information, we introduce a ground feature extractor that captures ground properties based on spatial context. As a result, our proposed method achieves robust foot contact estimation regardless of shoe appearance and effectively leverages ground information. Code will be released.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridWorldSim: A Scalable and Controllable High-fidelity Simulator for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.22187</link>
<guid>https://arxiv.org/abs/2511.22187</guid>
<content:encoded><![CDATA[
<div> Keywords: HybridWorldSim, multi-traversal neural reconstruction, generative modeling, autonomous driving simulation, MIRROR dataset<br /><br />Summary:<br /><br />1. The paper presents HybridWorldSim, a novel hybrid simulation framework designed to enhance end-to-end autonomous driving by tackling limitations found in existing simulation methods.<br />2. HybridWorldSim combines multi-traversal neural reconstruction techniques for accurately rendering static backgrounds with generative modeling approaches to simulate dynamic agents, thus improving both visual fidelity and spatial consistency.<br />3. This unified approach allows for high-quality novel view synthesis even under large viewpoint changes, addressing key challenges in realistic driving scenario simulation.<br />4. To support robust benchmarking and research, the authors introduce the MIRROR dataset, which consists of extensive multi-traversal data capturing diverse routes and varying environmental conditions across multiple cities.<br />5. Extensive experiments demonstrate that HybridWorldSim outperforms previous state-of-the-art methods, providing a practical, scalable, and high-fidelity simulation solution that can accelerate research and development in autonomous driving systems. <div>
arXiv:2511.22187v1 Announce Type: new 
Abstract: Realistic and controllable simulation is critical for advancing end-to-end autonomous driving, yet existing approaches often struggle to support novel view synthesis under large viewpoint changes or to ensure geometric consistency. We introduce HybridWorldSim, a hybrid simulation framework that integrates multi-traversal neural reconstruction for static backgrounds with generative modeling for dynamic agents. This unified design addresses key limitations of previous methods, enabling the creation of diverse and high-fidelity driving scenarios with reliable visual and spatial consistency. To facilitate robust benchmarking, we further release a new multi-traversal dataset MIRROR that captures a wide range of routes and environmental conditions across different cities. Extensive experiments demonstrate that HybridWorldSim surpasses previous state-of-the-art methods, providing a practical and scalable solution for high-fidelity simulation and a valuable resource for research and development in autonomous driving.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARPGNet: Appearance- and Relation-aware Parallel Graph Attention Fusion Network for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2511.22188</link>
<guid>https://arxiv.org/abs/2511.22188</guid>
<content:encoded><![CDATA[
<div> Keywords: facial expression recognition, spatial-temporal representation, graph attention, CNN, facial region relations<br /><br />Summary: This paper addresses the challenge of learning discriminative spatial-temporal representations crucial for facial expression recognition by capturing facial expression dynamics more effectively. Unlike previous methods that primarily depend on pre-trained Convolutional Neural Networks (CNNs) focusing on facial appearance and often neglecting the interrelations between facial regions, the authors introduce the Appearance- and Relation-aware Parallel Graph attention fusion Network (ARPGNet). ARPGNet constructs a facial region relation graph and utilizes a graph attention mechanism to explicitly model relationships between different facial regions, enabling enhanced relational representation sequences. Alongside these, CNN-based appearance representation sequences are concurrently processed through a parallel graph attention fusion module. This module facilitates mutual interaction and enhancement between the appearance and relation representations while simultaneously capturing the complementarity across different sequences as well as their temporal dynamics. Experimental evaluation on three facial expression recognition datasets demonstrates that ARPGNet either outperforms or matches state-of-the-art methods, proving its effectiveness in integrating appearance and relational information for improved recognition accuracy. The proposed approach highlights the importance of considering facial region relationships and temporal information jointly to advance facial expression recognition performance. <div>
arXiv:2511.22188v1 Announce Type: new 
Abstract: The key to facial expression recognition is to learn discriminative spatial-temporal representations that embed facial expression dynamics. Previous studies predominantly rely on pre-trained Convolutional Neural Networks (CNNs) to learn facial appearance representations, overlooking the relationships between facial regions. To address this issue, this paper presents an Appearance- and Relation-aware Parallel Graph attention fusion Network (ARPGNet) to learn mutually enhanced spatial-temporal representations of appearance and relation information. Specifically, we construct a facial region relation graph and leverage the graph attention mechanism to model the relationships between facial regions. The resulting relational representation sequences, along with CNN-based appearance representation sequences, are then fed into a parallel graph attention fusion module for mutual interaction and enhancement. This module simultaneously explores the complementarity between different representation sequences and the temporal dynamics within each sequence. Experimental results on three facial expression recognition datasets demonstrate that the proposed ARPGNet outperforms or is comparable to state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable 3D Object Generation with Single Image Prompt</title>
<link>https://arxiv.org/abs/2511.22194</link>
<guid>https://arxiv.org/abs/2511.22194</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, 3D object generation, textual inversion, image adapter, depth-conditioned warmup<br /><br />Summary: The paper addresses limitations in current 3D object generation methods that rely heavily on text-to-image diffusion models with textual inversion, which require additional training time and offer limited control. To overcome these issues, the authors propose two novel approaches. First, they introduce an off-the-shelf image adapter to generate 3D objects without the need for textual inversion, enabling better control over depth, pose, and textual conditions. Second, they present a depth-conditioned warmup strategy designed to improve 3D consistency in generated models. Experimental results demonstrate that their methods achieve comparable or better performance both qualitatively and quantitatively compared to existing text-inversion-based approaches, with noticeable improvements in maintaining 3D consistency. Additionally, a user study was conducted to evaluate how well the generated 3D objects match the input images and maintain 3D consistency. The results from this study show that their proposed model outperforms existing alternatives, confirming the effectiveness of the approaches. The authors also provide their implementation code on GitHub, facilitating further research and use. <div>
arXiv:2511.22194v1 Announce Type: new 
Abstract: Recently, the impressive generative capabilities of diffusion models have been demonstrated, producing images with remarkable fidelity. Particularly, existing methods for the 3D object generation tasks, which is one of the fastest-growing segments in computer vision, pre-dominantly use text-to-image diffusion models with textual inversion which train a pseudo text prompt to describe the given image. In practice, various text-to-image generative models employ textual inversion to learn concepts or styles of target object in the pseudo text prompt embedding space, thereby generating sophisticated outputs. However, textual inversion requires additional training time and lacks control ability. To tackle this issues, we propose two innovative methods: (1) using an off-the-shelf image adapter that generates 3D objects without textual inversion, offering enhanced control over conditions such as depth, pose, and text. (2) a depth conditioned warmup strategy to enhance 3D consistency. In experimental results, ours show qualitatively and quantitatively comparable performance and improved 3D consistency to the existing text-inversion-based alternatives. Furthermore, we conduct a user study to assess (i) how well results match the input image and (ii) whether 3D consistency is maintained. User study results show that our model outperforms the alternatives, validating the effectiveness of our approaches. Our code is available at GitHub repository:https://github.com/Seooooooogi/Control3D_IP/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-Consistent Multi-View Editing by Diffusion Guidance</title>
<link>https://arxiv.org/abs/2511.22228</link>
<guid>https://arxiv.org/abs/2511.22228</guid>
<content:encoded><![CDATA[
<div> Diffusion models, multi-view consistency, 3D image editing, NeRF, Gaussian Splat<br /><br />Summary:<br /><br />Recent innovations in diffusion models have significantly enhanced text-based image editing capabilities. However, conventional approaches that edit images individually often result in geometric and photometric inconsistencies across different views of the same 3D scene, which is especially problematic for 3D representations like Neural Radiance Fields (NeRFs) and Gaussian Splat models. To overcome these challenges, the authors propose a novel, training-free diffusion framework designed to enforce multi-view consistency during the image editing process. The core idea relies on the assumption that corresponding points in unedited multi-view images should undergo similar transformations after the editing step. To realize this, a novel consistency loss is introduced that steers the diffusion sampling process toward producing coherent and consistent edits across views. This framework is flexible and compatible with various existing image editing techniques, supporting scenarios involving both dense and sparse multi-view setups. Extensive experiments demonstrate that their method significantly improves 3D consistency when compared to prior multi-view editing approaches. Furthermore, the approach enables superior quality editing within Gaussian Splat models, producing sharp details and ensuring fidelity to user-provided text prompts. Additional visual and video results are accessible on their project webpage for deeper insights. <div>
arXiv:2511.22228v1 Announce Type: new 
Abstract: Recent advancements in diffusion models have greatly improved text-based image editing, yet methods that edit images independently often produce geometrically and photometrically inconsistent results across different views of the same scene. Such inconsistencies are particularly problematic for editing of 3D representations such as NeRFs or Gaussian Splat models. We propose a training-free diffusion framework that enforces multi-view consistency during the image editing process. The key assumption is that corresponding points in the unedited images should undergo similar transformations after editing. To achieve this, we introduce a consistency loss that guides the diffusion sampling toward coherent edits. The framework is flexible and can be combined with widely varying image editing methods, supporting both dense and sparse multi-view editing setups. Experimental results show that our approach significantly improves 3D consistency compared to existing multi-view editing methods. We also show that this increased consistency enables high-quality Gaussian Splat editing with sharp details and strong fidelity to user-specified text prompts. Please refer to our project page for video results: https://3d-consistent-editing.github.io/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Compound Figures to Composite Understanding: Developing a Multi-Modal LLM from Biomedical Literature with Medical Multiple-Image Benchmarking and Validation</title>
<link>https://arxiv.org/abs/2511.22232</link>
<guid>https://arxiv.org/abs/2511.22232</guid>
<content:encoded><![CDATA[
arXiv:2511.22232v1 Announce Type: new 
Abstract: Multi-modal large language models (MLLMs) have shown promise in advancing healthcare. However, most existing models remain confined to single-image understanding, which greatly limits their applicability in clinical workflows. In practice, medical diagnosis and progression often require synthesizing information across multiple images from different modalities or time points. The development of medical MLLMs capable of such multi-image understanding has been hindered by the lack of large-scale, high-quality annotated training data. To address this limitation, we propose a novel framework that leverages license-permissive compound images in biomedical literature, as a rich yet underutilized data source for multi-image analysis. Specifically, we design a five-stage, context-aware instruction generation paradigm underpinned by a divide-and-conquer strategy. By decomposing multi-image analysis into manageable sub-tasks, this paradigm empowers MLLMs to move beyond single-panel analysis and provide a composite understanding by learning the complex spatial, temporal, and cross-modal relationships inherent in these compound figures. By parsing over 237,000 compound figures and their contextual text for instruction generation, we develop M3LLM, a medical multi-image multi-modal large language model. For benchmarking, we construct PMC-MI-Bench for composite understanding, manually validated by medical experts. Extensive experiments show that M3LLM significantly outperforms both general-purpose and specialized medical MLLMs across multi-image, single-image, text-only, and multi-choice scenarios. Notably, M3LLM exhibits strong generalization to longitudinal chest X-ray analysis using the MIMIC dataset. This work establishes a scalable and efficient paradigm for developing medical MLLMs capable of composite reasoning, bridging the gap between biomedical literature and real-world clinical applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IE-SRGS: An Internal-External Knowledge Fusion Framework for High-Fidelity 3D Gaussian Splatting Super-Resolution</title>
<link>https://arxiv.org/abs/2511.22233</link>
<guid>https://arxiv.org/abs/2511.22233</guid>
<content:encoded><![CDATA[
arXiv:2511.22233v1 Announce Type: new 
Abstract: Reconstructing high-resolution (HR) 3D Gaussian Splatting (3DGS) models from low-resolution (LR) inputs remains challenging due to the lack of fine-grained textures and geometry. Existing methods typically rely on pre-trained 2D super-resolution (2DSR) models to enhance textures, but suffer from 3D Gaussian ambiguity arising from cross-view inconsistencies and domain gaps inherent in 2DSR models. We propose IE-SRGS, a novel 3DGS SR paradigm that addresses this issue by jointly leveraging the complementary strengths of external 2DSR priors and internal 3DGS features. Specifically, we use 2DSR and depth estimation models to generate HR images and depth maps as external knowledge, and employ multi-scale 3DGS models to produce cross-view consistent, domain-adaptive counterparts as internal knowledge. A mask-guided fusion strategy is introduced to integrate these two sources and synergistically exploit their complementary strengths, effectively guiding the 3D Gaussian optimization toward high-fidelity reconstruction. Extensive experiments on both synthetic and real-world benchmarks show that IE-SRGS consistently outperforms state-of-the-art methods in both quantitative accuracy and visual fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging 3D Deep Learning and Curation for Analysis and High-Quality Segmentation in Practice</title>
<link>https://arxiv.org/abs/2511.22236</link>
<guid>https://arxiv.org/abs/2511.22236</guid>
<content:encoded><![CDATA[
arXiv:2511.22236v1 Announce Type: new 
Abstract: Accurate 3D microscopy image segmentation is critical for quantitative bioimage analysis but even state-of-the-art foundation models yield error-prone results. Therefore, manual curation is still widely used for either preparing high-quality training data or fixing errors before analysis. We present VessQC, an open-source tool for uncertainty-guided curation of large 3D microscopy segmentations. By integrating uncertainty maps, VessQC directs user attention to regions most likely containing biologically meaningful errors. In a preliminary user study uncertainty-guided correction significantly improved error detection recall from 67% to 94.0% (p=0.007) without a significant increase in total curation time. VessQC thus enables efficient, human-in-the-loop refinement of volumetric segmentations and bridges a key gap in real-world applications between uncertainty estimation and practical human-computer interaction. The software is freely available at github.com/MMV-Lab/VessQC.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Creating Blank Canvas Against AI-enabled Image Forgery</title>
<link>https://arxiv.org/abs/2511.22237</link>
<guid>https://arxiv.org/abs/2511.22237</guid>
<content:encoded><![CDATA[
arXiv:2511.22237v1 Announce Type: new 
Abstract: AIGC-based image editing technology has greatly simplified the realistic-level image modification, causing serious potential risks of image forgery. This paper introduces a new approach to tampering detection using the Segment Anything Model (SAM). Instead of training SAM to identify tampered areas, we propose a novel strategy. The entire image is transformed into a blank canvas from the perspective of neural models. Any modifications to this blank canvas would be noticeable to the models. To achieve this idea, we introduce adversarial perturbations to prevent SAM from ``seeing anything'', allowing it to identify forged regions when the image is tampered with. Due to SAM's powerful perceiving capabilities, naive adversarial attacks cannot completely tame SAM. To thoroughly deceive SAM and make it blind to the image, we introduce a frequency-aware optimization strategy, which further enhances the capability of tamper localization. Extensive experimental results demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning</title>
<link>https://arxiv.org/abs/2511.22242</link>
<guid>https://arxiv.org/abs/2511.22242</guid>
<content:encoded><![CDATA[
arXiv:2511.22242v1 Announce Type: new 
Abstract: A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Anchoring for Robust Personalization in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.22245</link>
<guid>https://arxiv.org/abs/2511.22245</guid>
<content:encoded><![CDATA[
arXiv:2511.22245v1 Announce Type: new 
Abstract: Text-to-image diffusion models have achieved remarkable progress in generating diverse and realistic images from textual descriptions. However, they still struggle with personalization, which requires adapting a pretrained model to depict user-specific subjects from only a few reference images. The key challenge lies in learning a new visual concept from a limited number of reference images while preserving the pretrained semantic prior that maintains text-image alignment. When the model focuses on subject fidelity, it tends to overfit the limited reference images and fails to leverage the pretrained distribution. Conversely, emphasizing prior preservation maintains semantic consistency but prevents the model from learning new personalized attributes. Building on these observations, we propose the personalization process through a semantic anchoring that guides adaptation by grounding new concepts in their corresponding distributions. We therefore reformulate personalization as the process of learning a rare concept guided by its frequent counterpart through semantic anchoring. This anchoring encourages the model to adapt new concepts in a stable and controlled manner, expanding the pretrained distribution toward personalized regions while preserving its semantic structure. As a result, the proposed method achieves stable adaptation and consistent improvements in both subject fidelity and text-image alignment compared to baseline methods. Extensive experiments and ablation studies further demonstrate the robustness and effectiveness of the proposed anchoring strategy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Diffusible High-Dimensional Latent Spaces: A Frequency Perspective</title>
<link>https://arxiv.org/abs/2511.22249</link>
<guid>https://arxiv.org/abs/2511.22249</guid>
<content:encoded><![CDATA[
arXiv:2511.22249v1 Announce Type: new 
Abstract: Latent diffusion has become the default paradigm for visual generation, yet we observe a persistent reconstruction-generation trade-off as latent dimensionality increases: higher-capacity autoencoders improve reconstruction fidelity but generation quality eventually declines. We trace this gap to the different behaviors in high-frequency encoding and decoding. Through controlled perturbations in both RGB and latent domains, we analyze encoder/decoder behaviors and find that decoders depend strongly on high-frequency latent components to recover details, whereas encoders under-represent high-frequency contents, yielding insufficient exposure and underfitting in high-frequency bands for diffusion model training. To address this issue, we introduce FreqWarm, a plug-and-play frequency warm-up curriculum that increases early-stage exposure to high-frequency latent signals during diffusion or flow-matching training -- without modifying or retraining the autoencoder. Applied across several high-dimensional autoencoders, FreqWarm consistently improves generation quality: decreasing gFID by 14.11 on Wan2.2-VAE, 6.13 on LTX-VAE, and 4.42 on DC-AE-f32, while remaining architecture-agnostic and compatible with diverse backbones. Our study shows that explicitly managing frequency exposure can successfully turn high-dimensional latent spaces into more diffusible targets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UMind-VL: A Generalist Ultrasound Vision-Language Model for Unified Grounded Perception and Comprehensive Interpretation</title>
<link>https://arxiv.org/abs/2511.22256</link>
<guid>https://arxiv.org/abs/2511.22256</guid>
<content:encoded><![CDATA[
arXiv:2511.22256v1 Announce Type: new 
Abstract: Despite significant strides in medical foundation models, the ultrasound domain lacks a comprehensive solution capable of bridging low-level Ultrasound Grounded Perception (e.g., segmentation, localization) and high-level Ultrasound Comprehensive Interpretation (e.g., diagnosis, reasoning). To bridge this gap, we propose UMind-VL, a unified foundation model designed to synergize pixel-level structural understanding with complex clinical reasoning. We first introduce UMind-DS, a large-scale multimodal dataset comprising 1.2 million ultrasound image-text pairs across 16 anatomical regions, enriching standard data with pixel-level annotations and clinician-validated rationales. Architecturally, UMind-VL incorporates a lightweight Dynamic Convolutional Mask Decoder that generates masks via dynamic kernels conditioned on LLM outputs. This design, combined with task-specific tokens, unifies segmentation, detection, geometric measurement, and diagnosis tasks within a single framework. Extensive evaluations demonstrate that UMind-VL significantly outperforms existing generalist multimodal models and achieves performance on par with, or superior to, state-of-the-art specialist models across segmentation, detection, keypoint localization, and diagnostic reasoning benchmarks, while maintaining strong generalization ability. We demonstrate the capability of UMind-VL in Figure 1.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?</title>
<link>https://arxiv.org/abs/2511.22262</link>
<guid>https://arxiv.org/abs/2511.22262</guid>
<content:encoded><![CDATA[
arXiv:2511.22262v1 Announce Type: new 
Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for 3D scenes, widely adopted due to its exceptional efficiency and high-fidelity visual quality. Given the significant value of 3DGS assets, recent works have introduced specialized watermarking schemes to ensure copyright protection and ownership verification. However, can existing 3D Gaussian watermarking approaches genuinely guarantee robust protection of the 3D assets? In this paper, for the first time, we systematically explore and validate possible vulnerabilities of 3DGS watermarking frameworks. We demonstrate that conventional watermark removal techniques designed for 2D images do not effectively generalize to the 3DGS scenario due to the specialized rendering pipeline and unique attributes of each gaussian primitives. Motivated by this insight, we propose GSPure, the first watermark purification framework specifically for 3DGS watermarking representations. By analyzing view-dependent rendering contributions and exploiting geometrically accurate feature clustering, GSPure precisely isolates and effectively removes watermark-related Gaussian primitives while preserving scene integrity. Extensive experiments demonstrate that our GSPure achieves the best watermark purification performance, reducing watermark PSNR by up to 16.34dB while minimizing degradation to original scene fidelity with less than 1dB PSNR loss. Moreover, it consistently outperforms existing methods in both effectiveness and generalization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DriveVGGT: Visual Geometry Transformer for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.22264</link>
<guid>https://arxiv.org/abs/2511.22264</guid>
<content:encoded><![CDATA[
arXiv:2511.22264v1 Announce Type: new 
Abstract: Feed-forward reconstruction has recently gained significant attention, with VGGT being a notable example. However, directly applying VGGT to autonomous driving (AD) systems leads to sub-optimal results due to the different priors between the two tasks. In AD systems, several important new priors need to be considered: (i) The overlap between camera views is minimal, as autonomous driving sensor setups are designed to achieve coverage at a low cost. (ii) The camera intrinsics and extrinsics are known, which introduces more constraints on the output and also enables the estimation of absolute scale. (iii) Relative positions of all cameras remain fixed though the ego vehicle is in motion. To fully integrate these priors into a feed-forward framework, we propose DriveVGGT, a scale-aware 4D reconstruction framework specifically designed for autonomous driving data. Specifically, we propose a Temporal Video Attention (TVA) module to process multi-camera videos independently, which better leverages the spatiotemporal continuity within each single-camera sequence. Then, we propose a Multi-camera Consistency Attention (MCA) module to conduct window attention with normalized relative pose embeddings, aiming to establish consistency relationships across different cameras while restricting each token to attend only to nearby frames. Finally, we extend the standard VGGT heads by adding an absolute scale head and an ego vehicle pose head. Experiments show that DriveVGGT outperforms VGGT, StreamVGGT, fastVGGT on autonomous driving dataset while extensive ablation studies verify effectiveness of the proposed designs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Collapse of Patches</title>
<link>https://arxiv.org/abs/2511.22281</link>
<guid>https://arxiv.org/abs/2511.22281</guid>
<content:encoded><![CDATA[
arXiv:2511.22281v1 Announce Type: new 
Abstract: Observing certain patches in an image reduces the uncertainty of others. Their realization lowers the distribution entropy of each remaining patch feature, analogous to collapsing a particle's wave function in quantum mechanics. This phenomenon can intuitively be called patch collapse. To identify which patches are most relied on during a target region's collapse, we learn an autoencoder that softly selects a subset of patches to reconstruct each target patch. Graphing these learned dependencies for each patch's PageRank score reveals the optimal patch order to realize an image. We show that respecting this order benefits various masked image modeling methods. First, autoregressive image generation can be boosted by retraining the state-of-the-art model MAR. Next, we introduce a new setup for image classification by exposing Vision Transformers only to high-rank patches in the collapse order. Seeing 22\% of such patches is sufficient to achieve high accuracy. With these experiments, we propose patch collapse as a novel image modeling perspective that promotes vision efficiency. Our project is available at https://github.com/wguo-ai/CoP .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Match-and-Fuse: Consistent Generation from Unstructured Image Sets</title>
<link>https://arxiv.org/abs/2511.22287</link>
<guid>https://arxiv.org/abs/2511.22287</guid>
<content:encoded><![CDATA[
arXiv:2511.22287v1 Announce Type: new 
Abstract: We present Match-and-Fuse - a zero-shot, training-free method for consistent controlled generation of unstructured image sets - collections that share a common visual element, yet differ in viewpoint, time of capture, and surrounding content. Unlike existing methods that operate on individual images or densely sampled videos, our framework performs set-to-set generation: given a source set and user prompts, it produces a new set that preserves cross-image consistency of shared content. Our key idea is to model the task as a graph, where each node corresponds to an image and each edge triggers a joint generation of image pairs. This formulation consolidates all pairwise generations into a unified framework, enforcing their local consistency while ensuring global coherence across the entire set. This is achieved by fusing internal features across image pairs, guided by dense input correspondences, without requiring masks or manual supervision. It also allows us to leverage an emergent prior in text-to-image models that encourages coherent generation when multiple views share a single canvas. Match-and-Fuse achieves state-of-the-art consistency and visual quality, and unlocks new capabilities for content creation from image collections.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure is Supervision: Multiview Masked Autoencoders for Radiology</title>
<link>https://arxiv.org/abs/2511.22294</link>
<guid>https://arxiv.org/abs/2511.22294</guid>
<content:encoded><![CDATA[
arXiv:2511.22294v1 Announce Type: new 
Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small Object Detection for Birds with Swin Transformer</title>
<link>https://arxiv.org/abs/2511.22310</link>
<guid>https://arxiv.org/abs/2511.22310</guid>
<content:encoded><![CDATA[
arXiv:2511.22310v1 Announce Type: new 
Abstract: Object detection is the task of detecting objects in an image. In this task, the detection of small objects is particularly difficult. Other than the small size, it is also accompanied by difficulties due to blur, occlusion, and so on. Current small object detection methods are tailored to small and dense situations, such as pedestrians in a crowd or far objects in remote sensing scenarios. However, when the target object is small and sparse, there is a lack of objects available for training, making it more difficult to learn effective features. In this paper, we propose a specialized method for detecting a specific category of small objects; birds. Particularly, we improve the features learned by the neck; the sub-network between the backbone and the prediction head, to learn more effective features with a hierarchical design. We employ Swin Transformer to upsample the image features. Moreover, we change the shifted window size for adapting to small objects. Experiments show that the proposed Swin Transformer-based neck combined with CenterNet can lead to good performance by changing the window sizes. We further find that smaller window sizes (default 2) benefit mAPs for small object detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-based Consistent Video Colorization</title>
<link>https://arxiv.org/abs/2511.22330</link>
<guid>https://arxiv.org/abs/2511.22330</guid>
<content:encoded><![CDATA[
arXiv:2511.22330v1 Announce Type: new 
Abstract: Existing video colorization methods struggle with temporal flickering or demand extensive manual input. We propose a novel approach automating high-fidelity video colorization using rich semantic guidance derived from language and segmentation. We employ a language-conditioned diffusion model to colorize grayscale frames. Guidance is provided via automatically generated object masks and textual prompts; our primary automatic method uses a generic prompt, achieving state-of-the-art results without specific color input. Temporal stability is achieved by warping color information from previous frames using optical flow (RAFT); a correction step detects and fixes inconsistencies introduced by warping. Evaluations on standard benchmarks (DAVIS30, VIDEVO20) show our method achieves state-of-the-art performance in colorization accuracy (PSNR) and visual realism (Colorfulness, CDC), demonstrating the efficacy of automated prompt-based guidance for consistent video colorization.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unexplored flaws in multiple-choice VQA evaluations</title>
<link>https://arxiv.org/abs/2511.22341</link>
<guid>https://arxiv.org/abs/2511.22341</guid>
<content:encoded><![CDATA[
arXiv:2511.22341v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate strong capabilities in handling image-text inputs. A common way to assess this ability is through multiple-choice Visual Question Answering (VQA). Earlier works have already revealed that these benchmarks are sensitive to answer choice order, a limitation that can be mitigated through careful design. Yet, we highlight additional, unexplored biases in prompt formatting that question the reliability of current MLLM evaluations. Specifically, we identify three key variation factors in prompt formatting and analyze their impact through a large-scale study involving $\mathbf{\text{seven}}$ MLLMs and $\mathbf{\text{five}}$ VQA datasets, spanning $\mathbf{48}$ distinct $\mathbf{\text{prompt format variations}}$. Our findings reveal that multiple-choice VQA is highly sensitive to minor prompt format changes, even when these changes are semantically neutral. We further demonstrate that these biases persist independently of known order biases or the MLLM's confidence in the correct answer. Finally, we demonstrate that existing bias mitigation strategies fail to address these newly identified biases.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment</title>
<link>https://arxiv.org/abs/2511.22345</link>
<guid>https://arxiv.org/abs/2511.22345</guid>
<content:encoded><![CDATA[
arXiv:2511.22345v1 Announce Type: new 
Abstract: Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3$\times$, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64$\times$64 and 256$\times$256. Our code is available at https://github.com/MCG-NJU/FlowBack.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INSIGHT: An Interpretable Neural Vision-Language Framework for Reasoning of Generative Artifacts</title>
<link>https://arxiv.org/abs/2511.22351</link>
<guid>https://arxiv.org/abs/2511.22351</guid>
<content:encoded><![CDATA[
arXiv:2511.22351v1 Announce Type: new 
Abstract: The growing realism of AI-generated images produced by recent GAN and diffusion models has intensified concerns over the reliability of visual media. Yet, despite notable progress in deepfake detection, current forensic systems degrade sharply under real-world conditions such as severe downsampling, compression, and cross-domain distribution shifts. Moreover, most detectors operate as opaque classifiers, offering little insight into why an image is flagged as synthetic, undermining trust and hindering adoption in high-stakes settings.
  We introduce INSIGHT (Interpretable Neural Semantic and Image-based Generative-forensic Hallucination Tracing), a unified multimodal framework for robust detection and transparent explanation of AI-generated images, even at extremely low resolutions (16x16 - 64x64). INSIGHT combines hierarchical super-resolution for amplifying subtle forensic cues without inducing misleading artifacts, Grad-CAM driven multi-scale localization to reveal spatial regions indicative of generative patterns, and CLIP-guided semantic alignment to map visual anomalies to human-interpretable descriptors. A vision-language model is then prompted using a structured ReAct + Chain-of-Thought protocol to produce consistent, fine-grained explanations, verified through a dual-stage G-Eval + LLM-as-a-judge pipeline to minimize hallucinations and ensure factuality.
  Across diverse domains, including animals, vehicles, and abstract synthetic scenes, INSIGHT substantially improves both detection robustness and explanation quality under extreme degradation, outperforming prior detectors and black-box VLM baselines. Our results highlight a practical path toward transparent, reliable AI-generated image forensics and establish INSIGHT as a step forward in trustworthy multimodal content verification.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorFlow: Training-Free 3D Editing via Latent Anchor-Aligned Flows</title>
<link>https://arxiv.org/abs/2511.22357</link>
<guid>https://arxiv.org/abs/2511.22357</guid>
<content:encoded><![CDATA[
arXiv:2511.22357v1 Announce Type: new 
Abstract: Training-free 3D editing aims to modify 3D shapes based on human instructions without model finetuning. It plays a crucial role in 3D content creation. However, existing approaches often struggle to produce strong or geometrically stable edits, largely due to inconsistent latent anchors introduced by timestep-dependent noise during diffusion sampling. To address these limitations, we introduce AnchorFlow, which is built upon the principle of latent anchor consistency. Specifically, AnchorFlow establishes a global latent anchor shared between the source and target trajectories, and enforces coherence using a relaxed anchor-alignment loss together with an anchor-aligned update rule. This design ensures that transformations remain stable and semantically faithful throughout the editing process. By stabilizing the latent reference space, AnchorFlow enables more pronounced semantic modifications. Moreover, AnchorFlow is mask-free. Without mask supervision, it effectively preserves geometric fidelity. Experiments on the Eval3DEdit benchmark show that AnchorFlow consistently delivers semantically aligned and structurally robust edits across diverse editing types. Code is at https://github.com/ZhenglinZhou/AnchorFlow.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asking like Socrates: Socrates helps VLMs understand remote sensing images</title>
<link>https://arxiv.org/abs/2511.22396</link>
<guid>https://arxiv.org/abs/2511.22396</guid>
<content:encoded><![CDATA[
arXiv:2511.22396v1 Announce Type: new 
Abstract: Recent multimodal reasoning models, inspired by DeepSeek-R1, have significantly advanced vision-language systems. However, in remote sensing (RS) tasks, we observe widespread pseudo reasoning: models narrate the process of reasoning rather than genuinely reason toward the correct answer based on visual evidence. We attribute this to the Glance Effect, where a single, coarse perception of large-scale RS imagery results in incomplete understanding and reasoning based on linguistic self-consistency instead of visual evidence. To address this, we propose RS-EoT (Remote Sensing Evidence-of-Thought), a language-driven, iterative visual evidence-seeking paradigm. To instill this paradigm, we propose SocraticAgent, a self-play multi-agent system that synthesizes reasoning traces via alternating cycles of reasoning and visual inspection. To enhance and generalize these patterns, we propose a two-stage progressive RL strategy: first, RL on fine-grained Grounding tasks to enhance RS-EoT capabilities, followed by RL on RS VQA to generalize to broader understanding scenarios. Experiments show RS-EoT achieves state-of-the-art performance on multiple RS VQA and grounding benchmarks. Analyses reveal clear iterative cycles of reasoning and evidence seeking, confirming RS-EoT mitigates the Glance Effect and enables genuine evidence-grounded reasoning. Our code, data, and models are available at https://geox-lab.github.io/Asking_like_Socrates
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAV-MM3D: A Large-Scale Synthetic Benchmark for 3D Perception of Unmanned Aerial Vehicles with Multi-Modal Data</title>
<link>https://arxiv.org/abs/2511.22404</link>
<guid>https://arxiv.org/abs/2511.22404</guid>
<content:encoded><![CDATA[
arXiv:2511.22404v1 Announce Type: new 
Abstract: Accurate perception of UAVs in complex low-altitude environments is critical for airspace security and related intelligent systems. Developing reliable solutions requires large-scale, accurately annotated, and multimodal data. However, real-world UAV data collection faces inherent constraints due to airspace regulations, privacy concerns, and environmental variability, while manual annotation of 3D poses and cross-modal correspondences is time-consuming and costly. To overcome these challenges, we introduce UAV-MM3D, a high-fidelity multimodal synthetic dataset for low-altitude UAV perception and motion understanding. It comprises 400K synchronized frames across diverse scenes (urban areas, suburbs, forests, coastal regions) and weather conditions (clear, cloudy, rainy, foggy), featuring multiple UAV models (micro, small, medium-sized) and five modalities - RGB, IR, LiDAR, Radar, and DVS (Dynamic Vision Sensor). Each frame provides 2D/3D bounding boxes, 6-DoF poses, and instance-level annotations, enabling core tasks related to UAVs such as 3D detection, pose estimation, target tracking, and short-term trajectory forecasting. We further propose LGFusionNet, a LiDAR-guided multimodal fusion baseline, and a dedicated UAV trajectory prediction baseline to facilitate benchmarking. With its controllable simulation environment, comprehensive scenario coverage, and rich annotations, UAV3D offers a public benchmark for advancing 3D perception of UAVs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffStyle360: Diffusion-Based 360{\deg} Head Stylization via Style Fusion Attention</title>
<link>https://arxiv.org/abs/2511.22411</link>
<guid>https://arxiv.org/abs/2511.22411</guid>
<content:encoded><![CDATA[
arXiv:2511.22411v1 Announce Type: new 
Abstract: 3D head stylization has emerged as a key technique for reimagining realistic human heads in various artistic forms, enabling expressive character design and creative visual experiences in digital media. Despite the progress in 3D-aware generation, existing 3D head stylization methods often rely on computationally expensive optimization or domain-specific fine-tuning to adapt to new styles. To address these limitations, we propose DiffStyle360, a diffusion-based framework capable of producing multi-view consistent, identity-preserving 3D head stylizations across diverse artistic domains given a single style reference image, without requiring per-style training. Building upon the 3D-aware DiffPortrait360 architecture, our approach introduces two key components: the Style Appearance Module, which disentangles style from content, and the Style Fusion Attention mechanism, which adaptively balances structure preservation and stylization fidelity in the latent space. Furthermore, we employ a 3D GAN-generated multi-view dataset for robust fine-tuning and introduce a temperaturebased key scaling strategy to control stylization intensity during inference. Extensive experiments on FFHQ and RenderMe360 demonstrate that DiffStyle360 achieves superior style quality, outperforming state-of-the-art GAN- and diffusion-based stylization methods across challenging style domains.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models</title>
<link>https://arxiv.org/abs/2511.22425</link>
<guid>https://arxiv.org/abs/2511.22425</guid>
<content:encoded><![CDATA[
arXiv:2511.22425v1 Announce Type: new 
Abstract: We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fin3R: Fine-tuning Feed-forward 3D Reconstruction Models via Monocular Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.22429</link>
<guid>https://arxiv.org/abs/2511.22429</guid>
<content:encoded><![CDATA[
arXiv:2511.22429v1 Announce Type: new 
Abstract: We present Fin3R, a simple, effective, and general fine-tuning method for feed-forward 3D reconstruction models. The family of feed-forward reconstruction model regresses pointmap of all input images to a reference frame coordinate system, along with other auxiliary outputs, in a single forward pass. However, we find that current models struggle with fine geometry and robustness due to (\textit{i}) the scarcity of high-fidelity depth and pose supervision and (\textit{ii}) the inherent geometric misalignment from multi-view pointmap regression. Fin3R jointly tackles two issues with an extra lightweight fine-tuning step. We freeze the decoder, which handles view matching, and fine-tune only the image encoder-the component dedicated to feature extraction. The encoder is enriched with fine geometric details distilled from a strong monocular teacher model on large, unlabeled datasets, using a custom, lightweight LoRA adapter. We validate our method on a wide range of models, including DUSt3R, MASt3R, CUT3R, and VGGT. The fine-tuned models consistently deliver sharper boundaries, recover complex structures, and achieve higher geometric accuracy in both single- and multi-view settings, while adding only the tiny LoRA weights, which leave test-time memory and latency virtually unchanged. Project page: \href{http://visual-ai.github.io/fin3r}{https://visual-ai.github.io/fin3r}
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkeletonAgent: An Agentic Interaction Framework for Skeleton-based Action Recognition</title>
<link>https://arxiv.org/abs/2511.22433</link>
<guid>https://arxiv.org/abs/2511.22433</guid>
<content:encoded><![CDATA[
arXiv:2511.22433v1 Announce Type: new 
Abstract: Recent advances in skeleton-based action recognition increasingly leverage semantic priors from Large Language Models (LLMs) to enrich skeletal representations. However, the LLM is typically queried in isolation from the recognition model and receives no performance feedback. As a result, it often fails to deliver the targeted discriminative cues critical to distinguish similar actions. To overcome these limitations, we propose SkeletonAgent, a novel framework that bridges the recognition model and the LLM through two cooperative agents, i.e., Questioner and Selector. Specifically, the Questioner identifies the most frequently confused classes and supplies them to the LLM as context for more targeted guidance. Conversely, the Selector parses the LLM's response to extract precise joint-level constraints and feeds them back to the recognizer, enabling finer-grained cross-modal alignment. Comprehensive evaluations on five benchmarks, including NTU RGB+D, NTU RGB+D 120, Kinetics-Skeleton, FineGYM, and UAV-Human, demonstrate that SkeletonAgent consistently outperforms state-of-the-art benchmark methods. The code is available at https://github.com/firework8/SkeletonAgent.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ABounD: Adversarial Boundary-Driven Few-Shot Learning for Multi-Class Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.22436</link>
<guid>https://arxiv.org/abs/2511.22436</guid>
<content:encoded><![CDATA[
arXiv:2511.22436v1 Announce Type: new 
Abstract: Few-shot multi-class industrial anomaly detection remains a challenging task. Vision-language models need to be both category-adaptive and sharply discriminative, yet data scarcity often blurs the boundary between normal and abnormal states. This ambiguity leads to missed subtle defects and the rejection of atypical normal samples. We propose ABounD, an Adversarial Boundary-Driven few-shot learning for multi-class anomaly detection, which is a unified learning framework that integrates semantic concept learning with decision boundary shaping. The Dynamic Concept Fusion (DCF) module produces class-adaptive prompts by fusing generalizable priors with class-specific cues, conditioned on image features. Meanwhile, Adversarial Boundary Forging (ABF) sculpts a more precise decision margin by generating boundary-level fence features via PGD-style perturbations. Training is conducted in a single stage under a Concept-Boundary Loss, where ABF provides the main supervisory signal and semantic-spatial regularizers stabilize the optimization. This synergy yields a decision boundary that closely follows normal data while preserving flexibility and robust semantic alignment. Experiments on MVTec-AD and VisA datasets demonstrate state-of-the-art performance in the task of few-shot multi-class anomaly detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do You See What I Say? Generalizable Deepfake Detection based on Visual Speech Recognition</title>
<link>https://arxiv.org/abs/2511.22443</link>
<guid>https://arxiv.org/abs/2511.22443</guid>
<content:encoded><![CDATA[
arXiv:2511.22443v1 Announce Type: new 
Abstract: Deepfake generation has witnessed remarkable progress, contributing to highly realistic generated images, videos, and audio. While technically intriguing, such progress has raised serious concerns related to the misuse of manipulated media. To mitigate such misuse, robust and reliable deepfake detection is urgently needed. Towards this, we propose a novel network FauxNet, which is based on pre-trained Visual Speech Recognition (VSR) features. By extracting temporal VSR features from videos, we identify and segregate real videos from manipulated ones. The holy grail in this context has to do with zero-shot detection, i.e., generalizable detection, which we focus on in this work. FauxNet consistently outperforms the state-of-the-art in this setting. In addition, FauxNet is able to attribute - distinguish between generation techniques from which the videos stem. Finally, we propose new datasets, referred to as Authentica-Vox and Authentica-HDTF, comprising about 38,000 real and fake videos in total, the latter created with six recent deepfake generation techniques. We provide extensive analysis and results on the Authentica datasets and FaceForensics++, demonstrating the superiority of FauxNet. The Authentica datasets will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking machine learning models for multi-class state recognition in double duantum dot data</title>
<link>https://arxiv.org/abs/2511.22451</link>
<guid>https://arxiv.org/abs/2511.22451</guid>
<content:encoded><![CDATA[
arXiv:2511.22451v1 Announce Type: new 
Abstract: Semiconductor quantum dots (QDs) are a leading platform for scalable quantum processors. However, scaling to large arrays requires reliable, automated tuning strategies for devices' bootstrapping, calibration, and operation, with many tuning aspects depending on accurately identifying QD device states from charge-stability diagrams (CSDs). In this work, we present a comprehensive benchmarking study of four modern machine learning (ML) architectures for multi-class state recognition in double-QD CSDs. We evaluate their performance across different data budgets and normalization schemes using both synthetic and experimental data. We find that the more resource-intensive models -- U-Nets and visual transformers (ViTs) -- achieve the highest MSE score (defined as $1-\mathrm{MSE}$) on synthetic data (over $0.98$) but fail to generalize to experimental data. MDNs are the most computationally efficient and exhibit highly stable training, but with substantially lower peak performance. CNNs offer the most favorable trade-off on experimental CSDs, achieving strong accuracy with two orders of magnitude fewer parameters than the U-Nets and ViTs. Normalization plays a nontrivial role: min-max scaling generally yields higher MSE scores but less stable convergence, whereas z-score normalization produces more predictable training dynamics but at reduced accuracy for most models. Overall, our study shows that CNNs with min-max normalization are a practical approach for QD CSDs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Real versus Fake Towards Intent-Aware Video Analysis</title>
<link>https://arxiv.org/abs/2511.22455</link>
<guid>https://arxiv.org/abs/2511.22455</guid>
<content:encoded><![CDATA[
arXiv:2511.22455v1 Announce Type: new 
Abstract: The rapid advancement of generative models has led to increasingly realistic deepfake videos, posing significant societal and security risks. While existing detection methods focus on distinguishing real from fake videos, such approaches fail to address a fundamental question: What is the intent behind a manipulated video? Towards addressing this question, we introduce IntentHQ: a new benchmark for human-centered intent analysis, shifting the paradigm from authenticity verification to contextual understanding of videos. IntentHQ consists of 5168 videos that have been meticulously collected and annotated with 23 fine-grained intent-categories, including "Financial fraud", "Indirect marketing", "Political propaganda", as well as "Fear mongering". We perform intent recognition with supervised and self-supervised multi-modality models that integrate spatio-temporal video features, audio processing, and text analysis to infer underlying motivations and goals behind videos. Our proposed model is streamlined to differentiate between a wide range of intent-categories.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ITS3D: Inference-Time Scaling for Text-Guided 3D Diffusion Models</title>
<link>https://arxiv.org/abs/2511.22456</link>
<guid>https://arxiv.org/abs/2511.22456</guid>
<content:encoded><![CDATA[
arXiv:2511.22456v1 Announce Type: new 
Abstract: We explore inference-time scaling in text-guided 3D diffusion models to enhance generative quality without additional training. To this end, we introduce ITS3D, a framework that formulates the task as an optimization problem to identify the most effective Gaussian noise input. The framework is driven by a verifier-guided search algorithm, where the search algorithm iteratively refines noise candidates based on verifier feedback. To address the inherent challenges of 3D generation, we introduce three techniques for improved stability, efficiency, and exploration capability. 1) Gaussian normalization is applied to stabilize the search process. It corrects distribution shifts when noise candidates deviate from a standard Gaussian distribution during iterative updates. 2) The high-dimensional nature of the 3D search space increases computational complexity. To mitigate this, a singular value decomposition-based compression technique is employed to reduce dimensionality while preserving effective search directions. 3) To further prevent convergence to suboptimal local minima, a singular space reset mechanism dynamically updates the search space based on diversity measures. Extensive experiments demonstrate that ITS3D enhances text-to-3D generation quality, which shows the potential of computationally efficient search methods in generative processes. The source code is available at https://github.com/ZhenglinZhou/ITS3D.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussians on Fire: High-Frequency Reconstruction of Flames</title>
<link>https://arxiv.org/abs/2511.22459</link>
<guid>https://arxiv.org/abs/2511.22459</guid>
<content:encoded><![CDATA[
arXiv:2511.22459v1 Announce Type: new 
Abstract: We propose a method to reconstruct dynamic fire in 3D from a limited set of camera views with a Gaussian-based spatiotemporal representation. Capturing and reconstructing fire and its dynamics is highly challenging due to its volatile nature, transparent quality, and multitude of high-frequency features. Despite these challenges, we aim to reconstruct fire from only three views, which consequently requires solving for under-constrained geometry. We solve this by separating the static background from the dynamic fire region by combining dense multi-view stereo images with monocular depth priors. The fire is initialized as a 3D flow field, obtained by fusing per-view dense optical flow projections. To capture the high frequency features of fire, each 3D Gaussian encodes a lifetime and linear velocity to match the dense optical flow. To ensure sub-frame temporal alignment across cameras we employ a custom hardware synchronization pattern -- allowing us to reconstruct fire with affordable commodity hardware. Our quantitative and qualitative validations across numerous reconstruction experiments demonstrate robust performance for diverse and challenging real fire scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoadSceneBench: A Lightweight Benchmark for Mid-Level Road Scene Understanding</title>
<link>https://arxiv.org/abs/2511.22466</link>
<guid>https://arxiv.org/abs/2511.22466</guid>
<content:encoded><![CDATA[
arXiv:2511.22466v1 Announce Type: new 
Abstract: Understanding mid-level road semantics, which capture the structural and contextual cues that link low-level perception to high-level planning, is essential for reliable autonomous driving and digital map construction. However, existing benchmarks primarily target perception tasks such as detection or segmentation, overlooking the reasoning capabilities required to infer road topology and dynamic scene structure. To address this gap, we present RoadSceneBench, a lightweight yet information-rich benchmark designed to evaluate and advance visual reasoning in complex road environments. Unlike large-scale perception datasets, RoadSceneBench emphasizes relational understanding and structural consistency, encouraging models to capture the underlying logic of real-world road scenes. Furthermore, to enhance reasoning reliability, we propose Hierarchical Relational Reward Propagation with Temporal Consistency (HRRP-T), a training framework for Vision-Language Models (VLMs) in which reward signals adaptively promote spatial coherence and semantic alignment throughout the reasoning process. This paradigm enables models to move beyond static recognition toward geometry-aware and temporally consistent reasoning. Extensive experiments demonstrate that our method achieves state-of-the-art performance across diverse road configurations. RoadSceneBench thus provides a compact yet powerful foundation for studying mid-level road semantics and fostering structure-aware autonomous perception. Our dataset is available at https://github.com/XiyanLiu/RoadSceneBench.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid, Unified and Iterative: A Novel Framework for Text-based Person Anomaly Retrieval</title>
<link>https://arxiv.org/abs/2511.22470</link>
<guid>https://arxiv.org/abs/2511.22470</guid>
<content:encoded><![CDATA[
arXiv:2511.22470v1 Announce Type: new 
Abstract: Text-based person anomaly retrieval has emerged as a challenging task, with most existing approaches relying on complex deep-learning techniques. This raises a research question: How can the model be optimized to achieve greater fine-grained features? To address this, we propose a Local-Global Hybrid Perspective (LHP) module integrated with a Vision-Language Model (VLM), designed to explore the effectiveness of incorporating both fine-grained features alongside coarse-grained features. Additionally, we investigate a Unified Image-Text (UIT) model that combines multiple objective loss functions, including Image-Text Contrastive (ITC), Image-Text Matching (ITM), Masked Language Modeling (MLM), and Masked Image Modeling (MIM) loss. Beyond this, we propose a novel iterative ensemble strategy, by combining iteratively instead of using model results simultaneously like other ensemble methods. To take advantage of the superior performance of the LHP model, we introduce a novel feature selection algorithm based on its guidance, which helps improve the model's performance. Extensive experiments demonstrate the effectiveness of our method in achieving state-of-the-art (SOTA) performance on PAB dataset, compared with previous work, with a 9.70\% improvement in R@1, 1.77\% improvement in R@5, and 1.01\% improvement in R@10.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Cross-Generator Image Forgery Detection through DINOv3</title>
<link>https://arxiv.org/abs/2511.22471</link>
<guid>https://arxiv.org/abs/2511.22471</guid>
<content:encoded><![CDATA[
arXiv:2511.22471v1 Announce Type: new 
Abstract: As generative models become increasingly diverse and powerful, cross-generator detection has emerged as a new challenge. Existing detection methods often memorize artifacts of specific generative models rather than learning transferable cues, leading to substantial failures on unseen generators. Surprisingly, this work finds that frozen visual foundation models, especially DINOv3, already exhibit strong cross-generator detection capability without any fine-tuning. Through systematic studies on frequency, spatial, and token perspectives, we observe that DINOv3 tends to rely on global, low-frequency structures as weak but transferable authenticity cues instead of high-frequency, generator-specific artifacts. Motivated by this insight, we introduce a simple, training-free token-ranking strategy followed by a lightweight linear probe to select a small subset of authenticity-relevant tokens. This token subset consistently improves detection accuracy across all evaluated datasets. Our study provides empirical evidence and a feasible hypothesis for understanding why foundation models generalize across diverse generators, offering a universal, efficient, and interpretable baseline for image forgery detection.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI killed the video star. Audio-driven diffusion model for expressive talking head generation</title>
<link>https://arxiv.org/abs/2511.22488</link>
<guid>https://arxiv.org/abs/2511.22488</guid>
<content:encoded><![CDATA[
arXiv:2511.22488v1 Announce Type: new 
Abstract: We propose Dimitra++, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we propose a conditional Motion Diffusion Transformer (cMDT) to model facial motion sequences, employing a 3D representation. The cMDT is conditioned on two inputs: a reference facial image, which determines appearance, as well as an audio sequence, which drives the motion. Quantitative and qualitative experiments, as well as a user study on two widely employed datasets, i.e., VoxCeleb2 and CelebV-HQ, suggest that Dimitra++ is able to outperform existing approaches in generating realistic talking heads imparting lip motion, facial expression, and head pose.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciPostGen: Bridging the Gap between Scientific Papers and Poster Layouts</title>
<link>https://arxiv.org/abs/2511.22490</link>
<guid>https://arxiv.org/abs/2511.22490</guid>
<content:encoded><![CDATA[
arXiv:2511.22490v1 Announce Type: new 
Abstract: As the number of scientific papers continues to grow, there is a demand for approaches that can effectively convey research findings, with posters serving as a key medium for presenting paper contents. Poster layouts determine how effectively research is communicated and understood, highlighting their growing importance. In particular, a gap remains in understanding how papers correspond to the layouts that present them, which calls for datasets with paired annotations at scale. To bridge this gap, we introduce SciPostGen, a large-scale dataset for understanding and generating poster layouts from scientific papers. Our analyses based on SciPostGen show that paper structures are associated with the number of layout elements in posters. Based on this insight, we explore a framework, Retrieval-Augmented Poster Layout Generation, which retrieves layouts consistent with a given paper and uses them as guidance for layout generation. We conducted experiments under two conditions: with and without layout constraints typically specified by poster creators. The results show that the retriever estimates layouts aligned with paper structures, and our framework generates layouts that also satisfy given constraints.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Shape Is Optimal for Masks in Text Removal?</title>
<link>https://arxiv.org/abs/2511.22499</link>
<guid>https://arxiv.org/abs/2511.22499</guid>
<content:encoded><![CDATA[
arXiv:2511.22499v1 Announce Type: new 
Abstract: The advent of generative models has dramatically improved the accuracy of image inpainting. In particular, by removing specific text from document images, reconstructing original images is extremely important for industrial applications. However, most existing methods of text removal focus on deleting simple scene text which appears in images captured by a camera in an outdoor environment. There is little research dedicated to complex and practical images with dense text. Therefore, we created benchmark data for text removal from images including a large amount of text. From the data, we found that text-removal performance becomes vulnerable against mask profile perturbation. Thus, for practical text-removal tasks, precise tuning of the mask shape is essential. This study developed a method to model highly flexible mask profiles and learn their parameters using Bayesian optimization. The resulting profiles were found to be character-wise masks. It was also found that the minimum cover of a text region is not optimal. Our research is expected to pave the way for a user-friendly guideline for manual masking.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA</title>
<link>https://arxiv.org/abs/2511.22521</link>
<guid>https://arxiv.org/abs/2511.22521</guid>
<content:encoded><![CDATA[
arXiv:2511.22521v1 Announce Type: new 
Abstract: Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.22532</link>
<guid>https://arxiv.org/abs/2511.22532</guid>
<content:encoded><![CDATA[
arXiv:2511.22532v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have recently attracted growing attention in end-to-end autonomous driving for their strong reasoning capabilities and rich world knowledge. However, existing VLAs often suffer from limited numerical reasoning ability and overly simplified input-output mappings, which hinder their performance in complex driving scenarios requiring step-by-step causal reasoning. To address these challenges, we propose CoT4AD, a novel VLA framework that introduces Chain-of-Thought (CoT) reasoning for autonomous driving to enhance both numerical and causal reasoning in Vision-Language Models (VLMs). CoT4AD integrates visual observations and language instructions to perform semantic reasoning, scene understanding, and trajectory planning. During training, it explicitly models a perception-question-prediction-action CoT to align the reasoning space with the action space across multiple driving tasks. During inference, it performs implicit CoT reasoning to enable consistent numerical reasoning and robust decision-making in dynamic environments. Extensive experiments on both real-world and simulated benchmarks, including nuScenes and Bench2Drive, demonstrate that CoT4AD achieves state-of-the-art performance in both open-loop and closed-loop evaluations. Code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast3Dcache: Training-free 3D Geometry Synthesis Acceleration</title>
<link>https://arxiv.org/abs/2511.22533</link>
<guid>https://arxiv.org/abs/2511.22533</guid>
<content:encoded><![CDATA[
arXiv:2511.22533v1 Announce Type: new 
Abstract: Diffusion models have achieved impressive generative quality across modalities like 2D images, videos, and 3D shapes, but their inference remains computationally expensive due to the iterative denoising process. While recent caching-based methods effectively reuse redundant computations to speed up 2D and video generation, directly applying these techniques to 3D diffusion models can severely disrupt geometric consistency. In 3D synthesis, even minor numerical errors in cached latent features accumulate, causing structural artifacts and topological inconsistencies. To overcome this limitation, we propose Fast3Dcache, a training-free geometry-aware caching framework that accelerates 3D diffusion inference while preserving geometric fidelity. Our method introduces a Predictive Caching Scheduler Constraint (PCSC) to dynamically determine cache quotas according to voxel stabilization patterns and a Spatiotemporal Stability Criterion (SSC) to select stable features for reuse based on velocity magnitude and acceleration criterion. Comprehensive experiments show that Fast3Dcache accelerates inference significantly, achieving up to a 27.12% speed-up and a 54.8% reduction in FLOPs, with minimal degradation in geometric quality as measured by Chamfer Distance (2.48%) and F-Score (1.95%).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diff-ICMH: Harmonizing Machine and Human Vision in Image Compression with Generative Prior</title>
<link>https://arxiv.org/abs/2511.22549</link>
<guid>https://arxiv.org/abs/2511.22549</guid>
<content:encoded><![CDATA[
arXiv:2511.22549v1 Announce Type: new 
Abstract: Image compression methods are usually optimized isolatedly for human perception or machine analysis tasks. We reveal fundamental commonalities between these objectives: preserving accurate semantic information is paramount, as it directly dictates the integrity of critical information for intelligent tasks and aids human understanding. Concurrently, enhanced perceptual quality not only improves visual appeal but also, by ensuring realistic image distributions, benefits semantic feature extraction for machine tasks. Based on this insight, we propose Diff-ICMH, a generative image compression framework aiming for harmonizing machine and human vision in image compression. It ensures perceptual realism by leveraging generative priors and simultaneously guarantees semantic fidelity through the incorporation of Semantic Consistency loss (SC loss) during training. Additionally, we introduce the Tag Guidance Module (TGM) that leverages highly semantic image-level tags to stimulate the pre-trained diffusion model's generative capabilities, requiring minimal additional bit rates. Consequently, Diff-ICMH supports multiple intelligent tasks through a single codec and bitstream without any task-specific adaptation, while preserving high-quality visual experience for human perception. Extensive experimental results demonstrate Diff-ICMH's superiority and generalizability across diverse tasks, while maintaining visual appeal for human perception. Code is available at: https://github.com/RuoyuFeng/Diff-ICMH.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bringing Your Portrait to 3D Presence</title>
<link>https://arxiv.org/abs/2511.22553</link>
<guid>https://arxiv.org/abs/2511.22553</guid>
<content:encoded><![CDATA[
arXiv:2511.22553v1 Announce Type: new 
Abstract: We present a unified framework for reconstructing animatable 3D human avatars from a single portrait across head, half-body, and full-body inputs. Our method tackles three bottlenecks: pose- and framing-sensitive feature representations, limited scalable data, and unreliable proxy-mesh estimation. We introduce a Dual-UV representation that maps image features to a canonical UV space via Core-UV and Shell-UV branches, eliminating pose- and framing-induced token shifts. We also build a factorized synthetic data manifold combining 2D generative diversity with geometry-consistent 3D renderings, supported by a training scheme that improves realism and identity consistency. A robust proxy-mesh tracker maintains stability under partial visibility. Together, these components enable strong in-the-wild generalization. Trained only on half-body synthetic data, our model achieves state-of-the-art head and upper-body reconstruction and competitive full-body results. Extensive experiments and analyses further validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Condition Embedded Regression Network for Automated Dental Abutment Design</title>
<link>https://arxiv.org/abs/2511.22578</link>
<guid>https://arxiv.org/abs/2511.22578</guid>
<content:encoded><![CDATA[
arXiv:2511.22578v1 Announce Type: new 
Abstract: The abutment is an important part of artificial dental implants, whose design process is time-consuming and labor-intensive. Long-term use of inappropriate dental implant abutments may result in implant complications, including peri-implantitis. Using artificial intelligence to assist dental implant abutment design can quickly improve the efficiency of abutment design and enhance abutment adaptability. In this paper, we propose a text condition embedded abutment design framework (TCEAD), the novel automated abutment design solution available in literature. The proposed study extends the self-supervised learning framework of the mesh mask autoencoder (MeshMAE) by introducing a text-guided localization (TGL) module to facilitate abutment area localization. As the parameter determination of the abutment is heavily dependent on local fine-grained features (the width and height of the implant and the distance to the opposing tooth), we pre-train the encoder using oral scan data to improve the model's feature extraction ability. Moreover, considering that the abutment area is only a small part of the oral scan data, we designed a TGL module, which introduces the description of the abutment area through the text encoder of Contrastive Language-Image Pre-training (CLIP), enabling the network to quickly locate the abutment area. We validated the performance of TCEAD on a large abutment design dataset. Extensive experiments demonstrate that TCEAD achieves an Intersection over Union (IoU) improvement of 0.8%-12.85% over other mainstream methods, underscoring its potential in automated dental abutment design.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Necessity of Lengthy Chain-of-Thought in Vision-centric Reasoning Generalization</title>
<link>https://arxiv.org/abs/2511.22586</link>
<guid>https://arxiv.org/abs/2511.22586</guid>
<content:encoded><![CDATA[
arXiv:2511.22586v1 Announce Type: new 
Abstract: We study how different Chain-of-Thought (CoT) designs affect the acquisition of the generalizable visual reasoning ability in vision-language models (VLMs). While CoT data, especially long or visual CoT such as "think with image", has been widely used to supervise intermediate reasoning, it remains unclear why specific CoT designs help and which ones truly support generalizable reasoning. To systematically evaluate this, we focus on a controlled maze-solving benchmark where reasoning rules are fully visual, difficulty can be tuned by grid size, and all the intermediate steps can be automatically generated. Using Qwen2.5-VL-7B under a standard SFT-then-RL pipeline, we compare three representative CoT formats: Language CoT, Grounding CoT (with spatial coordinate trajectories), and Visual CoT (with image manipulations). Our experiments reveal that visual and longer CoT mainly accelerate convergence but do not lift the final performance ceiling; concise CoT containing only essential grounding steps outperforms longer traces; and, strikingly, CoT retaining only the minimal grounding results generalizes best across different maze sizes. We further validate these insights on other vision-centric tasks. These findings highlight a "short is long" effect and provide practical guidance for constructing more generalizable SFT datasets for visual reasoning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HarmoCLIP: Harmonizing Global and Regional Representations in Contrastive Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22594</link>
<guid>https://arxiv.org/abs/2511.22594</guid>
<content:encoded><![CDATA[
arXiv:2511.22594v1 Announce Type: new 
Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated remarkable generalization ability and strong performance across a wide range of vision-language tasks. However, due to the lack of region-level supervision, CLIP exhibits limited fine-grained semantic understanding. Although several methods attempt to mitigate this issue, they unintentionally disrupt the global alignment, resulting in a persistent trade-off where improving local perception simultaneously degrades global coherence. In this paper, we propose HarmoCLIP, a novel framework designed to harmonize global and region representations within CLIP. We first identify that the absence of direct alignment between local textual and visual semantics is the fundamental cause of the trade-off. To address this, HarmoCLIP introduces an explicit fine-grained semantic supervision term that directly aligns textual segments with their corresponding visual regions, effectively bridging the image region space and the textual space. To further strengthen the representation capability at the local level, our method introduces a novel Region-Language Alignment supervision strategy that promotes fine-grained semantic learning without compromising global semantic consistency. Extensive experiments demonstrate that HarmoCLIP achieves state-of-the-art (improvement up to 69.78%) performance on the global task of retrieval and yields a substantial 3.2% improvement in Top-1 accuracy on the region task of bounding-box classification, consistently outperforming prior approaches while providing a balanced, efficient, and plug-and-play solution to the global-local trade-off in CLIP. Code is available at https://github.com/Erosist/HarmoCLIP.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnoRefiner: Anomaly-Aware Group-Wise Refinement for Zero-Shot Industrial Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.22595</link>
<guid>https://arxiv.org/abs/2511.22595</guid>
<content:encoded><![CDATA[
arXiv:2511.22595v1 Announce Type: new 
Abstract: Zero-shot industrial anomaly detection (ZSAD) methods typically yield coarse anomaly maps as vision transformers (ViTs) extract patch-level features only. To solve this, recent solutions attempt to predict finer anomalies using features from ZSAD, but they still struggle to recover fine-grained anomalies without missed detections, mainly due to the gap between randomly synthesized training anomalies and real ones. We observe that anomaly score maps exactly provide complementary spatial cues that are largely absent from ZSAD's image features, a fact overlooked before.
  Inspired by this, we propose an anomaly-aware refiner (AnoRefiner) that can be plugged into most ZSAD models and improve patch-level anomaly maps to the pixel level. First, we design an anomaly refinement decoder (ARD) that progressively enhances image features using anomaly score maps, reducing the reliance on synthetic anomaly data. Second, motivated by the mass production paradigm, we propose a progressive group-wise test-time training (PGT) strategy that trains ARD in each product group for the refinement process in the next group, while staying compatible with any ZSAD method.
  Experiments on the MVTec AD and VisA datasets show that AnoRefiner boosts various ZSAD models by up to a 5.2\% gain in pixel-AP metrics, which can also be directly observed in many visualizations. The code will be available at https://github.com/HUST-SLOW/AnoRefiner.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GazeTrack: High-Precision Eye Tracking Based on Regularization and Spatial Computing</title>
<link>https://arxiv.org/abs/2511.22607</link>
<guid>https://arxiv.org/abs/2511.22607</guid>
<content:encoded><![CDATA[
arXiv:2511.22607v1 Announce Type: new 
Abstract: Eye tracking has become increasingly important in virtual and augmented reality applications; however, the current gaze accuracy falls short of meeting the requirements for spatial computing. We designed a gaze collection framework and utilized high-precision equipment to gather the first precise benchmark dataset, GazeTrack, encompassing diverse ethnicities, ages, and visual acuity conditions for pupil localization and gaze tracking. We propose a novel shape error regularization method to constrain pupil ellipse fitting and train on open-source datasets, enhancing semantic segmentation and pupil position prediction accuracy. Additionally, we invent a novel coordinate transformation method similar to paper unfolding to accurately predict gaze vectors on the GazeTrack dataset. Finally, we built a gaze vector generation model that achieves reduced gaze angle error with lower computational complexity compared to other methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MG-Nav: Dual-Scale Visual Navigation via Sparse Spatial Memory</title>
<link>https://arxiv.org/abs/2511.22609</link>
<guid>https://arxiv.org/abs/2511.22609</guid>
<content:encoded><![CDATA[
arXiv:2511.22609v1 Announce Type: new 
Abstract: We present MG-Nav (Memory-Guided Navigation), a dual-scale framework for zero-shot visual navigation that unifies global memory-guided planning with local geometry-enhanced control. At its core is the Sparse Spatial Memory Graph (SMG), a compact, region-centric memory where each node aggregates multi-view keyframe and object semantics, capturing both appearance and spatial structure while preserving viewpoint diversity. At the global level, the agent is localized on SMG and a goal-conditioned node path is planned via an image-to-instance hybrid retrieval, producing a sequence of reachable waypoints for long-horizon guidance. At the local level, a navigation foundation policy executes these waypoints in point-goal mode with obstacle-aware control, and switches to image-goal mode when navigating from the final node towards the visual target. To further enhance viewpoint alignment and goal recognition, we introduce VGGT-adapter, a lightweight geometric module built on the pre-trained VGGT model, which aligns observation and goal features in a shared 3D-aware space. MG-Nav operates global planning and local control at different frequencies, using periodic re-localization to correct errors. Experiments on HM3D Instance-Image-Goal and MP3D Image-Goal benchmarks demonstrate that MG-Nav achieves state-of-the-art zero-shot performance and remains robust under dynamic rearrangements and unseen scene conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stable-Drift: A Patient-Aware Latent Drift Replay Method for Stabilizing Representations in Continual Learning</title>
<link>https://arxiv.org/abs/2511.22615</link>
<guid>https://arxiv.org/abs/2511.22615</guid>
<content:encoded><![CDATA[
arXiv:2511.22615v1 Announce Type: new 
Abstract: When deep learning models are sequentially trained on new data, they tend to abruptly lose performance on previously learned tasks, a critical failure known as catastrophic forgetting. This challenge severely limits the deployment of AI in medical imaging, where models must continually adapt to data from new hospitals without compromising established diagnostic knowledge. To address this, we introduce a latent drift-guided replay method that identifies and replays samples with high representational instability. Specifically, our method quantifies this instability via latent drift, the change in a sample internal feature representation after naive domain adaptation. To ensure diversity and clinical relevance, we aggregate drift at the patient level, our memory buffer stores the per patient slices exhibiting the greatest multi-layer representation shift. Evaluated on a cross-hospital COVID-19 CT classification task using state-of-the-art CNN and Vision Transformer backbones, our method substantially reduces forgetting compared to naive fine-tuning and random replay. This work highlights latent drift as a practical and interpretable replay signal for advancing robust continual learning in real world medical settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REASONEDIT: Towards Reasoning-Enhanced Image Editing Models</title>
<link>https://arxiv.org/abs/2511.22625</link>
<guid>https://arxiv.org/abs/2511.22625</guid>
<content:encoded><![CDATA[
arXiv:2511.22625v1 Announce Type: new 
Abstract: Recent advances in image editing models have shown remarkable progress. A common architectural design couples a multimodal large language model (MLLM) encoder with a diffusion decoder, as seen in systems such as Step1X-Edit and Qwen-Image-Edit, where the MLLM encodes both the reference image and the instruction but remains frozen during training. In this work, we demonstrate that unlocking the reasoning capabilities of MLLM can further push the boundaries of editing models. Specifically, we explore two reasoning mechanisms, thinking and reflection, which enhance instruction understanding and editing accuracy. Based on that, our proposed framework enables image editing in a thinking-editing-reflection loop: the thinking mechanism leverages the world knowledge of MLLM to interpret abstract instructions, while the reflection reviews editing results, automatically corrects unintended manipulations, and identifies the stopping round. Extensive experiments demonstrate that our reasoning approach achieves significant performance gains, with improvements of ImgEdit (+4.3%), GEdit (+4.7%), and Kris (+8.2%) when initializing our DiT from the Step1X-Edit (ReasonEdit-S), and also outperforms previous open-source methods on both GEdit and Kris when integrated with Qwen-Image-Edit (ReasonEdit-Q).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoZero: Incentivizing Reasoning from Scratch on Geospatial Scenes</title>
<link>https://arxiv.org/abs/2511.22645</link>
<guid>https://arxiv.org/abs/2511.22645</guid>
<content:encoded><![CDATA[
arXiv:2511.22645v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have undergone rapid development in advancing geospatial scene understanding. Recent studies have sought to enhance the reasoning capabilities of remote sensing MLLMs, typically through cold-start training with elaborately curated chain-of-thought (CoT) data. However, this approach not only incurs substantial annotation costs but also introduces human biases that may limit the diversity of model reasoning. To address these challenges, we propose GeoZero, a framework that enables MLLMs to perform geospatial reasoning without any predefined CoT supervision. Specifically, we construct two datasets, GeoZero-Instruct and GeoZero-Hard. GeoZero-Instruct allows the model to acquire preliminary geospatial knowledge through supervised fine-tuning, while GeoZero-Hard stimulates deep reasoning during the subsequent reinforcement learning stage. Furthermore, we introduce Answer-Anchored Group Relative Policy Optimization (A$^2$GRPO), where the reasoning process is regularized by the model's own answers, encouraging diverse yet accurate thinking. Extensive experiments on multiple remote sensing vision-language benchmarks demonstrate that GeoZero not only surpasses existing state-of-the-art methods but also fosters universal emergent reasoning capabilities across diverse geospatial tasks. Code,data,and models will be publicly available at https://github.com/MiliLab/GeoZero.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architecture Decoupling Is Not All You Need For Unified Multimodal Model</title>
<link>https://arxiv.org/abs/2511.22663</link>
<guid>https://arxiv.org/abs/2511.22663</guid>
<content:encoded><![CDATA[
arXiv:2511.22663v1 Announce Type: new 
Abstract: Unified multimodal models for image generation and understanding represent a significant step toward AGI and have attracted widespread attention from researchers. The main challenge of this task lies in the difficulty in establishing an optimal training paradigm due to inherent conflicting targets in understanding and generation tasks. To alleviate these conflicts and pursue higher performance, many researchers adopt varying degrees of model decoupling (e.g., Double image encoders, MOE/MOT architecture, or frozen MLLM). However, excessive model decoupling can lead to the loss of interleave generation ability, undermining the original intent of unified models. In this work, we aim to explore how to mitigate task conflicts without resorting to model decoupling. Firstly, we analyze why decoupling alleviates conflicts by studying the cross-modal attention behavior of models. We observe that model decoupling essentially drives models toward task-specific multimodal interaction patterns, as seen in Qwen-VL and HunyuanImage, and that the more thorough the decoupling, the more consistent the behavior becomes. Motivated by this observation, we propose Attention Interaction Alignment (AIA) loss, which explicitly learns Task-Specific multimodal interaction patterns during training. To demonstrate the generalizability of our AIA loss, we apply it to Emu3 and Janus-Pro during SFT and post-training stage respectively. Without bells and whistles, AIA not only refines cross-modal attention patterns, but also boosts both generation and understanding performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22664</link>
<guid>https://arxiv.org/abs/2511.22664</guid>
<content:encoded><![CDATA[
arXiv:2511.22664v1 Announce Type: new 
Abstract: Vision-language models (VLMs), such as CLIP, have shown strong generalization under zero-shot settings, yet adapting them to downstream tasks with limited supervision remains a significant challenge. Existing multi-modal prompt learning methods typically rely on fixed, shared prompts and deterministic parameters, which limits their ability to capture instance-level variation or model uncertainty across diverse tasks and domains. To tackle this issue, we propose a novel Variational Multi-Modal Prompt Learning (VaMP) framework that enables sample-specific, uncertainty-aware prompt tuning in multi-modal representation learning. VaMP generates instance-conditioned prompts by sampling from a learned posterior distribution, allowing the model to personalize its behavior based on input content. To further enhance the integration of local and global semantics, we introduce a class-aware prior derived from the instance representation and class prototype. Building upon these, we formulate prompt tuning as variational inference over latent prompt representations and train the entire framework end-to-end through reparameterized sampling. Experiments on few-shot and domain generalization benchmarks show that VaMP achieves state-of-the-art performance, highlighting the benefits of modeling both uncertainty and task structure in our method. Project page: https://visual-ai.github.io/vamp
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A deep learning perspective on Rubens' attribution</title>
<link>https://arxiv.org/abs/2511.22667</link>
<guid>https://arxiv.org/abs/2511.22667</guid>
<content:encoded><![CDATA[
arXiv:2511.22667v1 Announce Type: new 
Abstract: This study explores the use of deep learning for the authentication and attribution of paintings, focusing on the complex case of Peter Paul Rubens and his workshop. A convolutional neural network was trained on a curated dataset of verified and comparative artworks to identify micro-level stylistic features characteristic of the master s hand. The model achieved high classification accuracy and demonstrated the potential of computational analysis to complement traditional art historical expertise, offering new insights into authorship and workshop collaboration.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield</title>
<link>https://arxiv.org/abs/2511.22677</link>
<guid>https://arxiv.org/abs/2511.22677</guid>
<content:encoded><![CDATA[
arXiv:2511.22677v1 Announce Type: new 
Abstract: Diffusion model distillation has emerged as a powerful technique for creating efficient few-step and single-step generators. Among these, Distribution Matching Distillation (DMD) and its variants stand out for their impressive performance, which is widely attributed to their core mechanism of matching the student's output distribution to that of a pre-trained teacher model. In this work, we challenge this conventional understanding. Through a rigorous decomposition of the DMD training objective, we reveal that in complex tasks like text-to-image generation, where CFG is typically required for desirable few-step performance, the primary driver of few-step distillation is not distribution matching, but a previously overlooked component we identify as CFG Augmentation (CA). We demonstrate that this term acts as the core ``engine'' of distillation, while the Distribution Matching (DM) term functions as a ``regularizer'' that ensures training stability and mitigates artifacts. We further validate this decoupling by demonstrating that while the DM term is a highly effective regularizer, it is not unique; simpler non-parametric constraints or GAN-based objectives can serve the same stabilizing function, albeit with different trade-offs. This decoupling of labor motivates a more principled analysis of the properties of both terms, leading to a more systematic and in-depth understanding. This new understanding further enables us to propose principled modifications to the distillation process, such as decoupling the noise schedules for the engine and the regularizer, leading to further performance gains. Notably, our method has been adopted by the Z-Image ( https://github.com/Tongyi-MAI/Z-Image ) project to develop a top-tier 8-step image generation model, empirically validating the generalization and robustness of our findings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Extreme-View Geometry in 3D Foundation Models</title>
<link>https://arxiv.org/abs/2511.22686</link>
<guid>https://arxiv.org/abs/2511.22686</guid>
<content:encoded><![CDATA[
arXiv:2511.22686v1 Announce Type: new 
Abstract: 3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ar2Can: An Architect and an Artist Leveraging a Canvas for Multi-Human Generation</title>
<link>https://arxiv.org/abs/2511.22690</link>
<guid>https://arxiv.org/abs/2511.22690</guid>
<content:encoded><![CDATA[
arXiv:2511.22690v1 Announce Type: new 
Abstract: Despite recent advances in text-to-image generation, existing models consistently fail to produce reliable multi-human scenes, often duplicating faces, merging identities, or miscounting individuals. We present Ar2Can, a novel two-stage framework that disentangles spatial planning from identity rendering for multi-human generation. The Architect module predicts structured layouts, specifying where each person should appear. The Artist module then synthesizes photorealistic images, guided by a spatially-grounded face matching reward that combines Hungarian spatial alignment with ArcFace identity similarity. This approach ensures faces are rendered at correct locations and faithfully preserve reference identities. We develop two Architect variants, seamlessly integrated with our diffusion-based Artist model and optimized via Group Relative Policy Optimization (GRPO) using compositional rewards for count accuracy, image quality, and identity matching. Evaluated on the MultiHuman-Testbench, Ar2Can achieves substantial improvements in both count accuracy and identity preservation, while maintaining high perceptual quality. Notably, our method achieves these results using primarily synthetic data, without requiring real multi-human images.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Z-Image: An Efficient Image Generation Foundation Model with Single-Stream Diffusion Transformer</title>
<link>https://arxiv.org/abs/2511.22699</link>
<guid>https://arxiv.org/abs/2511.22699</guid>
<content:encoded><![CDATA[
arXiv:2511.22699v1 Announce Type: new 
Abstract: The landscape of high-performance image generation models is currently dominated by proprietary systems, such as Nano Banana Pro and Seedream 4.0. Leading open-source alternatives, including Qwen-Image, Hunyuan-Image-3.0 and FLUX.2, are characterized by massive parameter counts (20B to 80B), making them impractical for inference, and fine-tuning on consumer-grade hardware. To address this gap, we propose Z-Image, an efficient 6B-parameter foundation generative model built upon a Scalable Single-Stream Diffusion Transformer (S3-DiT) architecture that challenges the "scale-at-all-costs" paradigm. By systematically optimizing the entire model lifecycle -- from a curated data infrastructure to a streamlined training curriculum -- we complete the full training workflow in just 314K H800 GPU hours (approx. $630K). Our few-step distillation scheme with reward post-training further yields Z-Image-Turbo, offering both sub-second inference latency on an enterprise-grade H800 GPU and compatibility with consumer-grade hardware (<16GB VRAM). Additionally, our omni-pre-training paradigm also enables efficient training of Z-Image-Edit, an editing model with impressive instruction-following capabilities. Both qualitative and quantitative experiments demonstrate that our model achieves performance comparable to or surpassing that of leading competitors across various dimensions. Most notably, Z-Image exhibits exceptional capabilities in photorealistic image generation and bilingual text rendering, delivering results that rival top-tier commercial models, thereby demonstrating that state-of-the-art results are achievable with significantly reduced computational overhead. We publicly release our code, weights, and online demo to foster the development of accessible, budget-friendly, yet state-of-the-art generative models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splat-SAP: Feed-Forward Gaussian Splatting for Human-Centered Scene with Scale-Aware Point Map Reconstruction</title>
<link>https://arxiv.org/abs/2511.22704</link>
<guid>https://arxiv.org/abs/2511.22704</guid>
<content:encoded><![CDATA[
arXiv:2511.22704v1 Announce Type: new 
Abstract: We present Splat-SAP, a feed-forward approach to render novel views of human-centered scenes from binocular cameras with large sparsity. Gaussian Splatting has shown its promising potential in rendering tasks, but it typically necessitates per-scene optimization with dense input views. Although some recent approaches achieve feed-forward Gaussian Splatting rendering through geometry priors obtained by multi-view stereo, such approaches still require largely overlapped input views to establish the geometry prior. To bridge this gap, we leverage pixel-wise point map reconstruction to represent geometry which is robust to large sparsity for its independent view modeling. In general, we propose a two-stage learning strategy. In stage 1, we transform the point map into real space via an iterative affinity learning process, which facilitates camera control in the following. In stage 2, we project point maps of two input views onto the target view plane and refine such geometry via stereo matching. Furthermore, we anchor Gaussian primitives on this refined plane in order to render high-quality images. As a metric representation, the scale-aware point map in stage 1 is trained in a self-supervised manner without 3D supervision and stage 2 is supervised with photo-metric loss. We collect multi-view human-centered data and demonstrate that our method improves both the stability of point map reconstruction and the visual quality of free-viewpoint rendering.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReAG: Reasoning-Augmented Generation for Knowledge-based Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.22715</link>
<guid>https://arxiv.org/abs/2511.22715</guid>
<content:encoded><![CDATA[
arXiv:2511.22715v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) have shown impressive capabilities in jointly understanding text, images, and videos, often evaluated via Visual Question Answering (VQA). However, even state-of-the-art MLLMs struggle with domain-specific or knowledge-intensive queries, where relevant information is underrepresented in pre-training data. Knowledge-based VQA (KB-VQA) addresses this by retrieving external documents to condition answer generation, but current retrieval-augmented approaches suffer from low precision, noisy passages, and limited reasoning. To address this, we propose ReAG, a novel Reasoning-Augmented Multimodal RAG approach that combines coarse- and fine-grained retrieval with a critic model that filters irrelevant passages, ensuring high-quality additional context. The model follows a multi-stage training strategy leveraging reinforcement learning to enhance reasoning over retrieved content, while supervised fine-tuning serves only as a cold start. Extensive experiments on Encyclopedic-VQA and InfoSeek demonstrate that ReAG significantly outperforms prior methods, improving answer accuracy and providing interpretable reasoning grounded in retrieved evidence. Our source code is publicly available at: https://github.com/aimagelab/ReAG.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All Centers Are at most a Few Tokens Apart: Knowledge Distillation with Domain Invariant Prompt Tuning</title>
<link>https://arxiv.org/abs/2511.22739</link>
<guid>https://arxiv.org/abs/2511.22739</guid>
<content:encoded><![CDATA[
arXiv:2511.22739v1 Announce Type: new 
Abstract: Domain generalization is critical in computational pathology (CPath) due to inherent domain shifts caused by variations in staining protocols, scanner devices, and imaging settings across clinical centers. Vision-language models (VLMs), such as PLIP-a pathology-tuned CLIP-trained on image-text pairs across diverse domains, serve as strong knowledge distillation sources. However, their zero-shot performance with predefined prompts remains limited due to sensitivity to prompt variations. Moreover, unlike natural images, histopathology centers lack semantic descriptors (e.g., 'sketch'), making it difficult to define domain-specific prompts for clinical centers. This requires a data-driven approach for learning domain-specific and ultimately class-generic continuous prompts. We propose Domain Invariant Prompt Tuning (DIPT) for knowledge distillation process, a novel step that learns multiple input tokens for each domain. These tokens are trained separately for each domain and are averaged across domains, leading to domain-invariant prompts. Our student model then distills knowledge from PLIP's text encoder by leveraging the prompts learned by DIPT. This leads to alignment of visual features with domain-invariant embeddings, enhancing generalization by training on multiple domains. Our method adds a significant improvement in average F1-score to existing state-of-the-art (SOTA) knowledge distillation approaches in domain generalization with histopathology datasets. This work helps the way of deploying robust CPath models in real-world clinical problems with heterogeneous data sources.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MammoRGB: Dual-View Mammogram Synthesis Using Denoising Diffusion Probabilistic Models</title>
<link>https://arxiv.org/abs/2511.22759</link>
<guid>https://arxiv.org/abs/2511.22759</guid>
<content:encoded><![CDATA[
arXiv:2511.22759v1 Announce Type: new 
Abstract: Purpose: This study aims to develop and evaluate a three channel denoising diffusion probabilistic model (DDPM) for synthesizing single breast dual view mammograms and to assess the impact of channel representations on image fidelity and cross view consistency. Materials and Methods: A pretrained three channel DDPM, sourced from Hugging Face, was fine tuned on a private dataset of 11020 screening mammograms to generate paired craniocaudal (CC) and mediolateral oblique (MLO) views. Three third channel encodings of the CC and MLO views were evaluated: sum, absolute difference, and zero channel. Each model produced 500 synthetic image pairs. Quantitative assessment involved breast mask segmentation using Intersection over Union (IoU) and Dice Similarity Coefficient (DSC), with distributional comparisons against 2500 real pairs using Earth Movers Distance (EMD) and Kolmogorov Smirnov (KS) tests. Qualitative evaluation included a visual Turing test by a non expert radiologist to assess cross view consistency and artifacts. Results: Synthetic mammograms showed IoU and DSC distributions comparable to real images, with EMD and KS values (0.020 and 0.077 respectively). Models using sum or absolute difference encodings outperformed others in IoU and DSC (p < 0.001), though distributions remained broadly similar. Generated CC and MLO views maintained cross view consistency, with 6 to 8 percent of synthetic images exhibiting artifacts consistent with those in the training data. Conclusion: Three channel DDPMs can generate realistic and anatomically consistent dual view mammograms with promising applications in dataset augmentation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusion or Confusion? Assessing the impact of visible-thermal image fusion for automated wildlife detection</title>
<link>https://arxiv.org/abs/2511.22768</link>
<guid>https://arxiv.org/abs/2511.22768</guid>
<content:encoded><![CDATA[
arXiv:2511.22768v1 Announce Type: new 
Abstract: Efficient wildlife monitoring methods are necessary for biodiversity conservation and management. The combination of remote sensing, aerial imagery and deep learning offer promising opportunities to renew or improve existing survey methods. The complementary use of visible (VIS) and thermal infrared (TIR) imagery can add information compared to a single-source image and improve results in an automated detection context. However, the alignment and fusion process can be challenging, especially since visible and thermal images usually have different fields of view (FOV) and spatial resolutions. This research presents a case study on the great blue heron (Ardea herodias) to evaluate the performances of synchronous aerial VIS and TIR imagery to automatically detect individuals and nests using a YOLO11n model. Two VIS-TIR fusion methods were tested and compared: an early fusion approach and a late fusion approach, to determine if the addition of the TIR image gives any added value compared to a VIS-only model. VIS and TIR images were automatically aligned using a deep learning model. A principal component analysis fusion method was applied to VIS-TIR image pairs to form the early fusion dataset. A classification and regression tree was used to process the late fusion dataset, based on the detection from the VIS-only and TIR-only trained models. Across all classes, both late and early fusion improved the F1 score compared to the VIS-only model. For the main class, occupied nest, the late fusion improved the F1 score from 90.2 (VIS-only) to 93.0%. This model was also able to identify false positives from both sources with 90% recall. Although fusion methods seem to give better results, this approach comes with a limiting TIR FOV and alignment constraints that eliminate data. Using an aircraft-mounted very high-resolution visible sensor could be an interesting option for operationalizing surveys.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alzheimer's Disease Prediction Using EffNetViTLoRA and BiLSTM with Multimodal Longitudinal MRI Data</title>
<link>https://arxiv.org/abs/2511.22774</link>
<guid>https://arxiv.org/abs/2511.22774</guid>
<content:encoded><![CDATA[
arXiv:2511.22774v1 Announce Type: new 
Abstract: Alzheimer's disease (AD) is a prevalent neurodegenerative disorder that progressively impairs memory, decision-making, and overall cognitive function. As AD is irreversible, early prediction is critical for timely intervention and management. Mild Cognitive Impairment (MCI), a transitional stage between cognitively normal (CN) aging and AD, plays a significant role in early AD diagnosis. However, predicting MCI progression remains a significant challenge, as not all individuals with MCI convert to AD. MCI subjects are categorized into stable MCI (sMCI) and progressive MCI (pMCI) based on conversion status. In this study, we propose a generalized, end-to-end deep learning model for AD prediction using MCI cases from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our hybrid architecture integrates Convolutional Neural Networks and Vision Transformers to capture both local spatial features and global contextual dependencies from Magnetic Resonance Imaging (MRI) scans. To incorporate temporal progression, we further employ Bidirectional Long Short-Term Memory (BiLSTM) networks to process features extracted from four consecutive MRI timepoints along with some other non-image biomarkers, predicting each subject's cognitive status at month 48. Our multimodal model achieved an average progression prediction accuracy of 95.05\% between sMCI and pMCI, outperforming existing studies in AD prediction. This work demonstrates state-of-the-art performance in longitudinal AD prediction and highlights the effectiveness of combining spatial and temporal modeling for the early detection of Alzheimer's disease.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World in a Frame: Understanding Culture Mixing as a New Challenge for Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.22787</link>
<guid>https://arxiv.org/abs/2511.22787</guid>
<content:encoded><![CDATA[
arXiv:2511.22787v1 Announce Type: new 
Abstract: In a globalized world, cultural elements from diverse origins frequently appear together within a single visual scene. We refer to these as culture mixing scenarios, yet how Large Vision-Language Models (LVLMs) perceive them remains underexplored. We investigate culture mixing as a critical challenge for LVLMs and examine how current models behave when cultural items from multiple regions appear together. To systematically analyze these behaviors, we construct CultureMix, a food Visual Question Answering (VQA) benchmark with 23k diffusion-generated, human-verified culture mixing images across four subtasks: (1) food-only, (2) food+food, (3) food+background, and (4) food+food+background. Evaluating 10 LVLMs, we find consistent failures to preserve individual cultural identities in mixed settings. Models show strong background reliance, with accuracy dropping 14% when cultural backgrounds are added to food-only baselines, and they produce inconsistent predictions for identical foods across different contexts. To address these limitations, we explore three robustness strategies. We find supervised fine-tuning using a diverse culture mixing dataset substantially improve model consistency and reduce background sensitivity. We call for increased attention to culture mixing scenarios as a critical step toward developing LVLMs capable of operating reliably in culturally diverse real-world environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images</title>
<link>https://arxiv.org/abs/2511.22805</link>
<guid>https://arxiv.org/abs/2511.22805</guid>
<content:encoded><![CDATA[
arXiv:2511.22805v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LC4-DViT: Land-cover Creation for Land-cover Classification with Deformable Vision Transformer</title>
<link>https://arxiv.org/abs/2511.22812</link>
<guid>https://arxiv.org/abs/2511.22812</guid>
<content:encoded><![CDATA[
arXiv:2511.22812v1 Announce Type: new 
Abstract: Land-cover underpins ecosystem services, hydrologic regulation, disaster-risk reduction, and evidence-based land planning; timely, accurate land-cover maps are therefore critical for environmental stewardship. Remote sensing-based land-cover classification offers a scalable route to such maps but is hindered by scarce and imbalanced annotations and by geometric distortions in high-resolution scenes. We propose LC4-DViT (Land-cover Creation for Land-cover Classification with Deformable Vision Transformer), a framework that combines generative data creation with a deformation-aware Vision Transformer. A text-guided diffusion pipeline uses GPT-4o-generated scene descriptions and super-resolved exemplars to synthesize class-balanced, high-fidelity training images, while DViT couples a DCNv4 deformable convolutional backbone with a Vision Transformer encoder to jointly capture fine-scale geometry and global context. On eight classes from the Aerial Image Dataset (AID)-Beach, Bridge, Desert, Forest, Mountain, Pond, Port, and River-DViT achieves 0.9572 overall accuracy, 0.9576 macro F1-score, and 0.9510 Cohen' s Kappa, improving over a vanilla ViT baseline (0.9274 OA, 0.9300 macro F1, 0.9169 Kappa) and outperforming ResNet50, MobileNetV2, and FlashInternImage. Cross-dataset experiments on a three-class SIRI-WHU subset (Harbor, Pond, River) yield 0.9333 overall accuracy, 0.9316 macro F1, and 0.8989 Kappa, indicating good transferability. An LLM-based judge using GPT-4o to score Grad-CAM heatmaps further shows that DViT' s attention aligns best with hydrologically meaningful structures. These results suggest that description-driven generative augmentation combined with deformation-aware transformers is a promising approach for high-resolution land-cover mapping.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Captain Safari: A World Engine</title>
<link>https://arxiv.org/abs/2511.22815</link>
<guid>https://arxiv.org/abs/2511.22815</guid>
<content:encoded><![CDATA[
arXiv:2511.22815v1 Announce Type: new 
Abstract: World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Some Modalities are More Equal Than Others: Decoding and Architecting Multimodal Integration in MLLMs</title>
<link>https://arxiv.org/abs/2511.22826</link>
<guid>https://arxiv.org/abs/2511.22826</guid>
<content:encoded><![CDATA[
arXiv:2511.22826v1 Announce Type: new 
Abstract: Despite remarkable advancements in Multimodal Large Language Models (MLLMs), a fundamental question remains: are MLLMs robust to contradicting modalities? To rigorously study this, we introduce MMA-Bench comprising videos and tasks that probe a model's reliance on specific modalities. Using black-box and white-box interpretability techniques, we provide a critical analysis of the brittleness of both open- and closed-sourced MLLMs. We show that current MLLMs struggle under misaligned audio-visual pairs and simple misleading text, thereby lacking robust multi-modal reasoning. Building on these findings, we propose a modality alignment tuning strategy to teach the model when to prioritize, leverage, or ignore specific modality cues. Through extensive experiments and analysis, we show that our alignment tuning yields demonstrably stronger multimodal grounding. This work provides both interpretability tools and a clear path toward developing MLLMs with intrinsically reliable cross-modal reasoning. Code and dataset will be publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Visual Shortcuts in Multimodal Knowledge-Based Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.22843</link>
<guid>https://arxiv.org/abs/2511.22843</guid>
<content:encoded><![CDATA[
arXiv:2511.22843v1 Announce Type: new 
Abstract: Existing Multimodal Knowledge-Based Visual Question Answering (MKB-VQA) benchmarks suffer from "visual shortcuts", as the query image typically matches the primary subject entity of the target document. We demonstrate that models can exploit these shortcuts, achieving comparable results using visual cues alone. To address this, we introduce Relational Entity Text-Image kNowledge Augmented (RETINA) benchmark, automatically constructed using an LLM-driven pipeline, consisting of 120k training and 2k human-curated test set. RETINA contains queries referencing secondary subjects (i.e. related entities) and pairs them with images of these related entities, removing the visual shortcut. When evaluated on RETINA existing models show significantly degraded performance, confirming their reliance on the shortcut. Furthermore, we propose Multi-Image MultImodal Retriever (MIMIR), which enriches document embeddings by augmenting images of multiple related entities, effectively handling RETINA, unlike prior work that uses only a single image per document. Our experiments validate the limitations of existing benchmarks and demonstrate the effectiveness of RETINA and MIMIR. Our project is available at: Project Page.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Evidence Sparsity: Agentic Context Engineering for Long-Document Understanding</title>
<link>https://arxiv.org/abs/2511.22850</link>
<guid>https://arxiv.org/abs/2511.22850</guid>
<content:encoded><![CDATA[
arXiv:2511.22850v1 Announce Type: new 
Abstract: Document understanding is a long standing practical task. Vision Language Models (VLMs) have gradually become a primary approach in this domain, demonstrating effective performance on single page tasks. However, their effectiveness diminishes when handling long documents. In such scenarios, clues are often scattered across multiple pages and modalities, and redundancy from lengthy inputs can impair the models judgment. While retrieval augmented generation mitigates this issue by filtering for question relevant content, the retrieved results still contain substantial redundancy. To address these limitations, we propose SLEUTH, a multi agent framework. Concretely, SLEUTH orchestrates a retriever and four collaborative agents in a coarse to fine process. The framework identifies key textual and visual clues within the retrieved pages, filters for salient visual evidence such as tables and charts, and analyzes the query to devise a reasoning strategy. It ultimately synthesizes a distilled, evidence dense multimodal context to generate the final prediction. SLEUTH is model agnostic and scalable. When paired with advanced VLM backbones, it consistently improves performance on multiple long document benchmarks, achieving state of the art results. Ablation studies verify each modules effectiveness and confirm the benefits of our hierarchical refinement paradigm.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOW: Global Illumination-Aware Inverse Rendering of Indoor Scenes Captured with Dynamic Co-Located Light &amp; Camera</title>
<link>https://arxiv.org/abs/2511.22857</link>
<guid>https://arxiv.org/abs/2511.22857</guid>
<content:encoded><![CDATA[
arXiv:2511.22857v1 Announce Type: new 
Abstract: Inverse rendering of indoor scenes remains challenging due to the ambiguity between reflectance and lighting, exacerbated by inter-reflections among multiple objects. While natural illumination-based methods struggle to resolve this ambiguity, co-located light-camera setups offer better disentanglement as lighting can be easily calibrated via Structure-from-Motion. However, such setups introduce additional complexities like strong inter-reflections, dynamic shadows, near-field lighting, and moving specular highlights, which existing approaches fail to handle. We present GLOW, a Global Illumination-aware Inverse Rendering framework designed to address these challenges. GLOW integrates a neural implicit surface representation with a neural radiance cache to approximate global illumination, jointly optimizing geometry and reflectance through carefully designed regularization and initialization. We then introduce a dynamic radiance cache that adapts to sharp lighting discontinuities from near-field motion, and a surface-angle-weighted radiometric loss to suppress specular artifacts common in flashlight captures. Experiments show that GLOW substantially outperforms prior methods in material reflectance estimation under both natural and co-located illumination.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoordSpeaker: Exploiting Gesture Captioning for Coordinated Caption-Empowered Co-Speech Gesture Generation</title>
<link>https://arxiv.org/abs/2511.22863</link>
<guid>https://arxiv.org/abs/2511.22863</guid>
<content:encoded><![CDATA[
arXiv:2511.22863v1 Announce Type: new 
Abstract: Co-speech gesture generation has significantly advanced human-computer interaction, yet speaker movements remain constrained due to the omission of text-driven non-spontaneous gestures (e.g., bowing while talking). Existing methods face two key challenges: 1) the semantic prior gap due to the lack of descriptive text annotations in gesture datasets, and 2) the difficulty in achieving coordinated multimodal control over gesture generation. To address these challenges, this paper introduces CoordSpeaker, a comprehensive framework that enables coordinated caption-empowered co-speech gesture synthesis. Our approach first bridges the semantic prior gap through a novel gesture captioning framework, leveraging a motion-language model to generate descriptive captions at multiple granularities. Building upon this, we propose a conditional latent diffusion model with unified cross-dataset motion representation and a hierarchically controlled denoiser to achieve highly controlled, coordinated gesture generation. CoordSpeaker pioneers the first exploration of gesture understanding and captioning to tackle the semantic gap in gesture generation while offering a novel perspective of bidirectional gesture-text mapping. Extensive experiments demonstrate that our method produces high-quality gestures that are both rhythmically synchronized with speeches and semantically coherent with arbitrary captions, achieving superior performance with higher efficiency compared to existing approaches.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Diffusion Transformer for Conditional 4D fMRI Synthesis</title>
<link>https://arxiv.org/abs/2511.22870</link>
<guid>https://arxiv.org/abs/2511.22870</guid>
<content:encoded><![CDATA[
arXiv:2511.22870v1 Announce Type: new 
Abstract: Generating whole-brain 4D fMRI sequences conditioned on cognitive tasks remains challenging due to the high-dimensional, heterogeneous BOLD dynamics across subjects/acquisitions and the lack of neuroscience-grounded validation. We introduce the first diffusion transformer for voxelwise 4D fMRI conditional generation, combining 3D VQ-GAN latent compression with a CNN-Transformer backbone and strong task conditioning via AdaLN-Zero and cross-attention. On HCP task fMRI, our model reproduces task-evoked activation maps, preserves the inter-task representational structure observed in real data (RSA), achieves perfect condition specificity, and aligns ROI time-courses with canonical hemodynamic responses. Performance improves predictably with scale, reaching task-evoked map correlation of 0.83 and RSA of 0.98, consistently surpassing a U-Net baseline on all metrics. By coupling latent diffusion with a scalable backbone and strong conditioning, this work establishes a practical path to conditional 4D fMRI synthesis, paving the way for future applications such as virtual experiments, cross-site harmonization, and principled augmentation for downstream neuroimaging models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-Based Framework for Pedestrian Age and Gender Classification Using Far-View Surveillance in Mixed-Traffic Intersections</title>
<link>https://arxiv.org/abs/2511.22873</link>
<guid>https://arxiv.org/abs/2511.22873</guid>
<content:encoded><![CDATA[
arXiv:2511.22873v1 Announce Type: new 
Abstract: Pedestrian safety remains a pressing concern in congested urban intersections, particularly in low- and middle-income countries where traffic is multimodal, and infrastructure often lacks formal control. Demographic factors like age and gender significantly influence pedestrian vulnerability, yet real-time monitoring systems rarely capture this information. To address this gap, this study proposes a deep learning framework that classifies pedestrian age group and gender from far-view intersection footage using convolutional neural networks (CNNs), without relying on facial recognition or high-resolution imagery. The classification is structured as a unified six-class problem, distinguishing adult, teenager, and child pedestrians for both males and females, based on full-body visual cues. Video data was collected from three high-risk intersections in Dhaka, Bangladesh. Two CNN architectures were implemented: ResNet50, a deep convolutional neural network pretrained on ImageNet, and a custom lightweight CNN optimized for computational efficiency. Eight model variants explored combinations of pooling strategies and optimizers. ResNet50 with Max Pooling and SGD achieved the highest accuracy (86.19%), while the custom CNN performed comparably (84.15%) with fewer parameters and faster training. The model's efficient design enables real-time inference on standard surveillance feeds. For practitioners, this system provides a scalable, cost-effective tool to monitor pedestrian demographics at intersections using existing camera infrastructure. Its outputs can shape intersection design, optimize signal timing, and enable targeted safety interventions for vulnerable groups such as children or the elderly. By offering demographic insights often missing in conventional traffic data, the framework supports more inclusive, data-driven planning in mixed-traffic environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ClearGCD: Mitigating Shortcut Learning For Robust Generalized Category Discovery</title>
<link>https://arxiv.org/abs/2511.22892</link>
<guid>https://arxiv.org/abs/2511.22892</guid>
<content:encoded><![CDATA[
arXiv:2511.22892v1 Announce Type: new 
Abstract: In open-world scenarios, Generalized Category Discovery (GCD) requires identifying both known and novel categories within unlabeled data. However, existing methods often suffer from prototype confusion caused by shortcut learning, which undermines generalization and leads to forgetting of known classes. We propose ClearGCD, a framework designed to mitigate reliance on non-semantic cues through two complementary mechanisms. First, Semantic View Alignment (SVA) generates strong augmentations via cross-class patch replacement and enforces semantic consistency using weak augmentations. Second, Shortcut Suppression Regularization (SSR) maintains an adaptive prototype bank that aligns known classes while encouraging separation of potential novel ones. ClearGCD can be seamlessly integrated into parametric GCD approaches and consistently outperforms state-of-the-art methods across multiple benchmarks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking</title>
<link>https://arxiv.org/abs/2511.22896</link>
<guid>https://arxiv.org/abs/2511.22896</guid>
<content:encoded><![CDATA[
arXiv:2511.22896v1 Announce Type: new 
Abstract: Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Points to Clouds: Learning Robust Semantic Distributions for Multi-modal Prompts</title>
<link>https://arxiv.org/abs/2511.22897</link>
<guid>https://arxiv.org/abs/2511.22897</guid>
<content:encoded><![CDATA[
arXiv:2511.22897v1 Announce Type: new 
Abstract: Multimodal Prompt Learning (MPL) has emerged as a pivotal technique for adapting large-scale Visual Language Models (VLMs). However, current MPL methods are fundamentally limited by their optimization of a single, static point representation. This paradigm is inherently brittle, leads to overfitting on base classes, and generalizes poorly to novel or ambiguous categories. We challenge this point paradigm, proposing that robust generalization requires learning a semantic cloud (i.e., a distribution over the embedding space). To achieve this, we introduce Points-to-Clouds (P2C), a novel framework inspired by diffusion models that reframes prompt learning as a dynamic denoising task. At the core of P2C is a dual denoising mechanism: a Dynamic Prompt Denoising (DPD) mechanism perturbs text prompts with sophisticated, annealed noise to learn a smoother semantic landscape, while an auxiliary V-L Mapper denoising loss re-tasks the mapper as a denoising autoencoder. This forces the mapper to reconstruct clean visual prompts from noisy text inputs, ensuring robust cross-modal alignment. Extensive experiments across 11 datasets demonstrate that P2C consistently outperforms strong baselines. On the base-to-novel generalization benchmark, our method achieves a Harmonic Mean of 79.7%, representing a relative improvement of 1.4% over the baseline. The code and models are available at https://vranlee.github.io/P2C/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Textual Compositional Reasoning for Robust Change Captioning</title>
<link>https://arxiv.org/abs/2511.22903</link>
<guid>https://arxiv.org/abs/2511.22903</guid>
<content:encoded><![CDATA[
arXiv:2511.22903v1 Announce Type: new 
Abstract: Change captioning aims to describe changes between a pair of images. However, existing works rely on visual features alone, which often fail to capture subtle but meaningful changes because they lack the ability to represent explicitly structured information such as object relationships and compositional semantics. To alleviate this, we present CORTEX (COmpositional Reasoning-aware TEXt-guided), a novel framework that integrates complementary textual cues to enhance change understanding. In addition to capturing cues from pixel-level differences, CORTEX utilizes scene-level textual knowledge provided by Vision Language Models (VLMs) to extract richer image text signals that reveal underlying compositional reasoning. CORTEX consists of three key modules: (i) an Image-level Change Detector that identifies low-level visual differences between paired images, (ii) a Reasoning-aware Text Extraction (RTE) module that use VLMs to generate compositional reasoning descriptions implicit in visual features, and (iii) an Image-Text Dual Alignment (ITDA) module that aligns visual and textual features for fine-grained relational reasoning. This enables CORTEX to reason over visual and textual features and capture changes that are otherwise ambiguous in visual features alone.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Rank, and Filter: Important Word-Aware Clip Filtering via Scene Understanding for Moment Retrieval and Highlight Detection</title>
<link>https://arxiv.org/abs/2511.22906</link>
<guid>https://arxiv.org/abs/2511.22906</guid>
<content:encoded><![CDATA[
arXiv:2511.22906v1 Announce Type: new 
Abstract: Video moment retrieval (MR) and highlight detection (HD) with natural language queries aim to localize relevant moments and key highlights in a video clips. However, existing methods overlook the importance of individual words, treating the entire text query and video clips as a black-box, which hinders contextual understanding. In this paper, we propose a novel approach that enables fine-grained clip filtering by identifying and prioritizing important words in the query. Our method integrates image-text scene understanding through Multimodal Large Language Models (MLLMs) and enhances the semantic understanding of video clips. We introduce a feature enhancement module (FEM) to capture important words from the query and a ranking-based filtering module (RFM) to iteratively refine video clips based on their relevance to these important words. Extensive experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods, achieving superior performance in both MR and HD tasks. Our code is available at: https://github.com/VisualAIKHU/SRF.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViGG: Robust RGB-D Point Cloud Registration using Visual-Geometric Mutual Guidance</title>
<link>https://arxiv.org/abs/2511.22908</link>
<guid>https://arxiv.org/abs/2511.22908</guid>
<content:encoded><![CDATA[
arXiv:2511.22908v1 Announce Type: new 
Abstract: Point cloud registration is a fundamental task in 3D vision. Most existing methods only use geometric information for registration. Recently proposed RGB-D registration methods primarily focus on feature fusion or improving feature learning, which limits their ability to exploit image information and hinders their practical applicability. In this paper, we propose ViGG, a robust RGB-D registration method using mutual guidance. First, we solve clique alignment in a visual-geometric combination form, employing a geometric guidance design to suppress ambiguous cliques. Second, to mitigate accuracy degradation caused by noise in visual matches, we propose a visual-guided geometric matching method that utilizes visual priors to determine the search space, enabling the extraction of high-quality, noise-insensitive correspondences. This mutual guidance strategy brings our method superior robustness, making it applicable for various RGB-D registration tasks. The experiments on 3DMatch, ScanNet and KITTI datasets show that our method outperforms recent state-of-the-art methods in both learning-free and learning-based settings. Code is available at https://github.com/ccjccjccj/ViGG.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artwork Interpretation with Vision Language Models: A Case Study on Emotions and Emotion Symbols</title>
<link>https://arxiv.org/abs/2511.22929</link>
<guid>https://arxiv.org/abs/2511.22929</guid>
<content:encoded><![CDATA[
arXiv:2511.22929v1 Announce Type: new 
Abstract: Emotions are a fundamental aspect of artistic expression. Due to their abstract nature, there is a broad spectrum of emotion realization in artworks. These are subject to historical change and their analysis requires expertise in art history. In this article, we investigate which aspects of emotional expression can be detected by current (2025) vision language models (VLMs). We present a case study of three VLMs (Llava-Llama and two Qwen models) in which we ask these models four sets of questions of increasing complexity about artworks (general content, emotional content, expression of emotions, and emotion symbols) and carry out a qualitative expert evaluation. We find that the VLMs recognize the content of the images surprisingly well and often also which emotions they depict and how they are expressed. The models perform best for concrete images but fail for highly abstract or highly symbolic images. Reliable recognition of symbols remains fundamentally difficult. Furthermore, the models continue to exhibit the well-known LLM weakness of providing inconsistent answers to related questions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuMatC: A General Neural Framework for Fast Parametric Matrix Operation</title>
<link>https://arxiv.org/abs/2511.22934</link>
<guid>https://arxiv.org/abs/2511.22934</guid>
<content:encoded><![CDATA[
arXiv:2511.22934v1 Announce Type: new 
Abstract: Matrix operations (e.g., inversion and singular value decomposition (SVD)) are fundamental in science and engineering. In many emerging real-world applications (such as wireless communication and signal processing), these operations must be performed repeatedly over matrices with parameters varying continuously. However, conventional methods tackle each matrix operation independently, underexploring the inherent low-rankness and continuity along the parameter dimension, resulting in significantly redundant computation. To address this challenge, we propose \textbf{\textit{Neural Matrix Computation Framework} (NeuMatC)}, which elegantly tackles general parametric matrix operation tasks by leveraging the underlying low-rankness and continuity along the parameter dimension. Specifically, NeuMatC unsupervisedly learns a low-rank and continuous mapping from parameters to their corresponding matrix operation results. Once trained, NeuMatC enables efficient computations at arbitrary parameters using only a few basic operations (e.g., matrix multiplications and nonlinear activations), significantly reducing redundant computations. Experimental results on both synthetic and real-world datasets demonstrate the promising performance of NeuMatC, exemplified by over $3\times$ speedup in parametric inversion and $10\times$ speedup in parametric SVD compared to the widely used NumPy baseline in wireless communication, while maintaining acceptable accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Image Self-Recovery against Tampering using Watermark Generation with Pixel Shuffling</title>
<link>https://arxiv.org/abs/2511.22936</link>
<guid>https://arxiv.org/abs/2511.22936</guid>
<content:encoded><![CDATA[
arXiv:2511.22936v1 Announce Type: new 
Abstract: The rapid growth of Artificial Intelligence-Generated Content (AIGC) raises concerns about the authenticity of digital media. In this context, image self-recovery, reconstructing original content from its manipulated version, offers a practical solution for understanding the attacker's intent and restoring trustworthy data. However, existing methods often fail to accurately recover tampered regions, falling short of the primary goal of self-recovery. To address this challenge, we propose ReImage, a neural watermarking-based self-recovery framework that embeds a shuffled version of the target image into itself as a watermark. We design a generator that produces watermarks optimized for neural watermarking and introduce an image enhancement module to refine the recovered image. We further analyze and resolve key limitations of shuffled watermarking, enabling its effective use in self-recovery. We demonstrate that ReImage achieves state-of-the-art performance across diverse tampering scenarios, consistently producing high-quality recovered images. The code and pretrained models will be released upon publication.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Barcode and QR Code Object Detection: An Experimental Study on YOLOv8 Models</title>
<link>https://arxiv.org/abs/2511.22937</link>
<guid>https://arxiv.org/abs/2511.22937</guid>
<content:encoded><![CDATA[
arXiv:2511.22937v1 Announce Type: new 
Abstract: This research work dives into an in-depth evaluation of the YOLOv8 (You Only Look Once) algorithm's efficiency in object detection, specially focusing on Barcode and QR code recognition. Utilizing the real-time detection abilities of YOLOv8, we performed a study aimed at enhancing its talent in swiftly and correctly figuring out objects. Through large training and high-quality-tuning on Kaggle datasets tailored for Barcode and QR code detection, our goal became to optimize YOLOv8's overall performance throughout numerous situations and environments. The look encompasses the assessment of YOLOv8 throughout special version iterations: Nano, Small, and Medium, with a meticulous attention on precision, recall, and F1 assessment metrics. The consequences exhibit large improvements in object detection accuracy with every subsequent model refinement. Specifically, we achieved an accuracy of 88.95% for the nano model, 97.10% for the small model, and 94.10% for the medium version, showcasing the incremental improvements finished via model scaling. Our findings highlight the big strides made through YOLOv8 in pushing the limits of computer vision, ensuring its function as a milestone within the subject of object detection. This study sheds light on how model scaling affects object recognition, increasing the concept of deep learning-based computer creative and prescient techniques.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenoiseGS: Gaussian Reconstruction Model for Burst Denoising</title>
<link>https://arxiv.org/abs/2511.22939</link>
<guid>https://arxiv.org/abs/2511.22939</guid>
<content:encoded><![CDATA[
arXiv:2511.22939v1 Announce Type: new 
Abstract: Burst denoising methods are crucial for enhancing images captured on handheld devices, but they often struggle with large motion or suffer from prohibitive computational costs. In this paper, we propose DenoiseGS, the first framework to leverage the efficiency of 3D Gaussian Splatting for burst denoising. Our approach addresses two key challenges when applying feedforward Gaussian reconsturction model to noisy inputs: the degradation of Gaussian point clouds and the loss of fine details. To this end, we propose a Gaussian self-consistency (GSC) loss, which regularizes the geometry predicted from noisy inputs with high-quality Gaussian point clouds. These point clouds are generated from clean inputs by the same model that we are training, thereby alleviating potential bias or domain gaps. Additionally, we introduce a log-weighted frequency (LWF) loss to strengthen supervision within the spectral domain, effectively preserving fine-grained details. The LWF loss adaptively weights frequency discrepancies in a logarithmic manner, emphasizing challenging high-frequency details. Extensive experiments demonstrate that DenoiseGS significantly exceeds the state-of-the-art NeRF-based methods on both burst denoising and novel view synthesis under noisy conditions, while achieving \textbf{250$\times$} faster inference speed. Code and models are released at https://github.com/yscheng04/DenoiseGS.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-to-All Animation: Alignment-Free Character Animation and Image Pose Transfe</title>
<link>https://arxiv.org/abs/2511.22940</link>
<guid>https://arxiv.org/abs/2511.22940</guid>
<content:encoded><![CDATA[
arXiv:2511.22940v1 Announce Type: new 
Abstract: Recent advances in diffusion models have greatly improved pose-driven character animation. However, existing methods are limited to spatially aligned reference-pose pairs with matched skeletal structures. Handling reference-pose misalignment remains unsolved. To address this, we present One-to-All Animation, a unified framework for high-fidelity character animation and image pose transfer for references with arbitrary layouts. First, to handle spatially misaligned reference, we reformulate training as a self-supervised outpainting task that transforms diverse-layout reference into a unified occluded-input format. Second, to process partially visible reference, we design a reference extractor for comprehensive identity feature extraction. Further, we integrate hybrid reference fusion attention to handle varying resolutions and dynamic sequence lengths. Finally, from the perspective of generation quality, we introduce identity-robust pose control that decouples appearance from skeletal structure to mitigate pose overfitting, and a token replace strategy for coherent long-video generation. Extensive experiments show that our method outperforms existing approaches. The code and model will be available at https://github.com/ssj9596/One-to-All-Animation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do We Need Perfect Data? Leveraging Noise for Domain Generalized Segmentation</title>
<link>https://arxiv.org/abs/2511.22948</link>
<guid>https://arxiv.org/abs/2511.22948</guid>
<content:encoded><![CDATA[
arXiv:2511.22948v1 Announce Type: new 
Abstract: Domain generalization in semantic segmentation faces challenges from domain shifts, particularly under adverse conditions. While diffusion-based data generation methods show promise, they introduce inherent misalignment between generated images and semantic masks. This paper presents FLEX-Seg (FLexible Edge eXploitation for Segmentation), a framework that transforms this limitation into an opportunity for robust learning. FLEX-Seg comprises three key components: (1) Granular Adaptive Prototypes that captures boundary characteristics across multiple scales, (2) Uncertainty Boundary Emphasis that dynamically adjusts learning emphasis based on prediction entropy, and (3) Hardness-Aware Sampling that progressively focuses on challenging examples. By leveraging inherent misalignment rather than enforcing strict alignment, FLEX-Seg learns robust representations while capturing rich stylistic variations. Experiments across five real-world datasets demonstrate consistent improvements over state-of-the-art methods, achieving 2.44% and 2.63% mIoU gains on ACDC and Dark Zurich. Our findings validate that adaptive strategies for handling imperfect synthetic data lead to superior domain generalization. Code is available at https://github.com/VisualScienceLab-KHU/FLEX-Seg.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video</title>
<link>https://arxiv.org/abs/2511.22950</link>
<guid>https://arxiv.org/abs/2511.22950</guid>
<content:encoded><![CDATA[
arXiv:2511.22950v1 Announce Type: new 
Abstract: Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Heliophysical Image Pretraining for Solar Dynamics Observatory Records</title>
<link>https://arxiv.org/abs/2511.22958</link>
<guid>https://arxiv.org/abs/2511.22958</guid>
<content:encoded><![CDATA[
arXiv:2511.22958v1 Announce Type: new 
Abstract: Deep learning has revolutionized solar image analysis, yet most approaches train task-specific encoders from scratch or rely on natural-image pretraining that ignores the unique characteristics of Solar Dynamics Observatory (SDO) data. We introduce SolarCHIP, a family of contrastively pretrained visual backbones tailored to multi-instrument SDO observations. SolarCHIP addresses three key challenges in solar imaging: multimodal sensing across AIA and HMI instruments, weak inter-class separability due to slow temporal evolution, and strong intra-class variability with sparse activity signals. Our pretraining framework employs a multi-granularity contrastive objective that jointly aligns (1) global class tokens across co-temporal AIA-HMI pairs to enhance temporal discrimination, (2) local patch tokens at fixed spatial indices to enforce position-consistent, modality-invariant features, and (3) intra-sample patches across different spatial locations to preserve fine-grained spatial structure. We train both CNN- and Vision Transformer-based autoencoders and demonstrate their effectiveness on two downstream tasks: cross-modal translation between HMI and AIA passbands via ControlNet, and full-disk flare classification. Experimental results show that SolarCHIP achieves state-of-the-art performance across both tasks, with particularly strong gains in low-resource settings where labeled data is limited. Ablation studies confirm that each contrastive component contributes essential discriminative capacity at different granularities. By publicly releasing pretrained weights and training code, we provide the heliophysics community with a practical, plug-and-play feature extractor that reduces computational requirements, improves label efficiency, and establishes a reusable foundation for diverse solar imaging applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model</title>
<link>https://arxiv.org/abs/2511.22961</link>
<guid>https://arxiv.org/abs/2511.22961</guid>
<content:encoded><![CDATA[
arXiv:2511.22961v1 Announce Type: new 
Abstract: Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM's embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&amp;A and general 3D Q&amp;A benchmarks demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM</title>
<link>https://arxiv.org/abs/2511.22968</link>
<guid>https://arxiv.org/abs/2511.22968</guid>
<content:encoded><![CDATA[
arXiv:2511.22968v1 Announce Type: new 
Abstract: Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems. To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs. First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene's intrinsic properties, such as albedo, from transient lighting. By learning a standardized, illumination-invariant appearance model, it assigns a stable and consistent color representation to each Gaussian primitive. Second, the Dynamic Radiance Balancing Loss (DRB-Loss) reactively handles frames with extreme exposure. It activates only when an image's exposure is poor, operating directly on the radiance field to guide targeted optimization. This prevents error accumulation from extreme lighting without compromising performance under normal conditions. The synergy between IAN's proactive invariance and DRB-Loss's reactive correction endows our system with unprecedented robustness. Evaluations on public datasets demonstrate state-of-the-art performance in camera tracking, map quality, and semantic and geometric accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlockVid: Block Diffusion for High-Quality and Consistent Minute-Long Video Generation</title>
<link>https://arxiv.org/abs/2511.22973</link>
<guid>https://arxiv.org/abs/2511.22973</guid>
<content:encoded><![CDATA[
arXiv:2511.22973v1 Announce Type: new 
Abstract: Generating minute-long videos is a critical step toward developing world models, providing a foundation for realistic extended scenes and advanced AI simulators. The emerging semi-autoregressive (block diffusion) paradigm integrates the strengths of diffusion and autoregressive models, enabling arbitrary-length video generation and improving inference efficiency through KV caching and parallel sampling. However, it yet faces two enduring challenges: (i) KV-cache-induced long-horizon error accumulation, and (ii) the lack of fine-grained long-video benchmarks and coherence-aware metrics. To overcome these limitations, we propose BlockVid, a novel block diffusion framework equipped with semantic-aware sparse KV cache, an effective training strategy called Block Forcing, and dedicated chunk-wise noise scheduling and shuffling to reduce error propagation and enhance temporal consistency. We further introduce LV-Bench, a fine-grained benchmark for minute-long videos, complete with new metrics evaluating long-range coherence. Extensive experiments on VBench and LV-Bench demonstrate that BlockVid consistently outperforms existing methods in generating high-quality, coherent minute-long videos. In particular, it achieves a 22.2% improvement on VDE Subject and a 19.4% improvement on VDE Clarity in LV-Bench over the state of the art approaches. Project website: https://ziplab.co/BlockVid. Inferix (Code): https://github.com/alibaba-damo-academy/Inferix.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>McSc: Motion-Corrective Preference Alignment for Video Generation with Self-Critic Hierarchical Reasoning</title>
<link>https://arxiv.org/abs/2511.22974</link>
<guid>https://arxiv.org/abs/2511.22974</guid>
<content:encoded><![CDATA[
arXiv:2511.22974v1 Announce Type: new 
Abstract: Text-to-video (T2V) generation has achieved remarkable progress in producing high-quality videos aligned with textual prompts. However, aligning synthesized videos with nuanced human preference remains challenging due to the subjective and multifaceted nature of human judgment. Existing video preference alignment methods rely on costly human annotations or utilize proxy metrics to predict preference, which lacks the understanding of human preference logic. Moreover, they usually directly align T2V models with the overall preference distribution, ignoring potential conflict dimensions like motion dynamics and visual quality, which may bias models towards low-motion content. To address these issues, we present Motion-corrective alignment with Self-critic hierarchical Reasoning (McSc), a three-stage reinforcement learning framework for robust preference modeling and alignment. Firstly, Self-critic Dimensional Reasoning (ScDR) trains a generative reward model (RM) to decompose preferences into per-dimension assessments, using self-critic reasoning chains for reliable learning. Secondly, to achieve holistic video comparison, we introduce Hierarchical Comparative Reasoning (HCR) for structural multi-dimensional reasoning with hierarchical reward supervision. Finally, using RM-preferred videos, we propose Motion-corrective Direct Preference Optimization (McDPO) to optimize T2V models, while dynamically re-weighting alignment objective to mitigate bias towards low-motion content. Experiments show that McSc achieves superior performance in human preference alignment and generates videos with high-motion dynamic.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ovis-Image Technical Report</title>
<link>https://arxiv.org/abs/2511.22982</link>
<guid>https://arxiv.org/abs/2511.22982</guid>
<content:encoded><![CDATA[
arXiv:2511.22982v1 Announce Type: new 
Abstract: We introduce $\textbf{Ovis-Image}$, a 7B text-to-image model specifically optimized for high-quality text rendering, designed to operate efficiently under stringent computational constraints. Built upon our previous Ovis-U1 framework, Ovis-Image integrates a diffusion-based visual decoder with the stronger Ovis 2.5 multimodal backbone, leveraging a text-centric training pipeline that combines large-scale pre-training with carefully tailored post-training refinements. Despite its compact architecture, Ovis-Image achieves text rendering performance on par with significantly larger open models such as Qwen-Image and approaches closed-source systems like Seedream and GPT4o. Crucially, the model remains deployable on a single high-end GPU with moderate memory, narrowing the gap between frontier-level text rendering and practical deployment. Our results indicate that combining a strong multimodal backbone with a carefully designed, text-focused training recipe is sufficient to achieve reliable bilingual text rendering without resorting to oversized or proprietary models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolutional Feature Noise Reduction for 2D Cardiac MR Image Segmentation</title>
<link>https://arxiv.org/abs/2511.22983</link>
<guid>https://arxiv.org/abs/2511.22983</guid>
<content:encoded><![CDATA[
arXiv:2511.22983v1 Announce Type: new 
Abstract: Noise reduction constitutes a crucial operation within Digital Signal Processing. Regrettably, it frequently remains neglected when dealing with the processing of convolutional features in segmentation networks. This oversight could trigger the butterfly effect, impairing the subsequent outcomes within the entire feature system. To complete this void, we consider convolutional features following Gaussian distributions as feature signal matrices and then present a simple and effective feature filter in this study. The proposed filter is fundamentally a low-amplitude pass filter primarily aimed at minimizing noise in feature signal inputs and is named Convolutional Feature Filter (CFF). We conducted experiments on two established 2D segmentation networks and two public cardiac MR image datasets to validate the effectiveness of the CFF, and the experimental findings demonstrated a decrease in noise within the feature signal matrices. To enable a numerical observation and analysis of this reduction, we developed a binarization equation to calculate the information entropy of feature signals.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiBanana: A Challenging Benchmark for Multi-Reference Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2511.22989</link>
<guid>https://arxiv.org/abs/2511.22989</guid>
<content:encoded><![CDATA[
arXiv:2511.22989v1 Announce Type: new 
Abstract: Recent text-to-image generation models have acquired the ability of multi-reference generation and editing; the ability to inherit the appearance of subjects from multiple reference images and re-render them under new contexts. However, the existing benchmark datasets often focus on the generation with single or a few reference images, which prevents us from measuring the progress on how model performance advances or pointing out their weaknesses, under different multi-reference conditions. In addition, their task definitions are still vague, typically limited to axes such as "what to edit" or "how many references are given", and therefore fail to capture the intrinsic difficulty of multi-reference settings. To address this gap, we introduce $\textbf{MultiBanana}$, which is carefully designed to assesses the edge of model capabilities by widely covering multi-reference-specific problems at scale: (1) varying the number of references, (2) domain mismatch among references (e.g., photo vs. anime), (3) scale mismatch between reference and target scenes, (4) references containing rare concepts (e.g., a red banana), and (5) multilingual textual references for rendering. Our analysis among a variety of text-to-image models reveals their superior performances, typical failure modes, and areas for improvement. MultiBanana will be released as an open benchmark to push the boundaries and establish a standardized basis for fair comparison in multi-reference image generation. Our data and code are available at https://github.com/matsuolab/multibanana .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIMM-X: Disentangling Spurious Correlations for Medical Image Analysis</title>
<link>https://arxiv.org/abs/2511.22990</link>
<guid>https://arxiv.org/abs/2511.22990</guid>
<content:encoded><![CDATA[
arXiv:2511.22990v1 Announce Type: new 
Abstract: Deep learning models can excel on medical tasks, yet often experience spurious correlations, known as shortcut learning, leading to poor generalization in new environments. Particularly in medical imaging, where multiple spurious correlations can coexist, misclassifications can have severe consequences. We propose MIMM-X, a framework that disentangles causal features from multiple spurious correlations by minimizing their mutual information. It enables predictions based on true underlying causal relationships rather than dataset-specific shortcuts. We evaluate MIMM-X on three datasets (UK Biobank, NAKO, CheXpert) across two imaging modalities (MRI and X-ray). Results demonstrate that MIMM-X effectively mitigates shortcut learning of multiple spurious correlations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Visual Autoregressive Models through Spectrum Weakening</title>
<link>https://arxiv.org/abs/2511.22991</link>
<guid>https://arxiv.org/abs/2511.22991</guid>
<content:encoded><![CDATA[
arXiv:2511.22991v1 Announce Type: new 
Abstract: Classifier-free guidance (CFG) has become a widely adopted and practical approach for enhancing generation quality and improving condition alignment. Recent studies have explored guidance mechanisms for unconditional generation, yet these approaches remain fundamentally tied to assumptions specific to diffusion models. In this work, we propose a spectrum-weakening framework for visual autoregressive (AR) models. This method works without the need for re-training, specific conditions, or any architectural modifications. It achieves this by constructing a controllable weak model in the spectral domain. We theoretically show that invertible spectral transformations preserve information, while selectively retaining only a subset of spectrum introduces controlled information reduction. Based on this insight, we perform spectrum selection along the channel dimension of internal representations, which avoids the structural constraints imposed by diffusion models. We further introduce two spectrum renormalization strategies that ensures numerical stability during the weakening process. Extensive experiments were conducted on both discrete and continuous AR models, with text or class conditioning. The results demonstrate that our method enables high-quality unconditional generation while maintaining strong prompt alignment for conditional generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizer Sensitivity In Vision Transformerbased Iris Recognition: Adamw Vs Sgd Vs Rmsprop</title>
<link>https://arxiv.org/abs/2511.22994</link>
<guid>https://arxiv.org/abs/2511.22994</guid>
<content:encoded><![CDATA[
arXiv:2511.22994v1 Announce Type: new 
Abstract: The security of biometric authentication is increasingly critical as digital identity systems expand. Iris recognition offers high reliability due to its distinctive and stable texture patterns. Recent progress in deep learning, especially Vision Transformers ViT, has improved visual recognition performance. Yet, the effect of optimizer choice on ViT-based biometric systems remains understudied. This work evaluates how different optimizers influence the accuracy and stability of ViT for iris recognition, providing insights to enhance the robustness of biometric identification models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MrGS: Multi-modal Radiance Fields with 3D Gaussian Splatting for RGB-Thermal Novel View Synthesis</title>
<link>https://arxiv.org/abs/2511.22997</link>
<guid>https://arxiv.org/abs/2511.22997</guid>
<content:encoded><![CDATA[
arXiv:2511.22997v1 Announce Type: new 
Abstract: Recent advances in Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved considerable performance in RGB scene reconstruction. However, multi-modal rendering that incorporates thermal infrared imagery remains largely underexplored. Existing approaches tend to neglect distinctive thermal characteristics, such as heat conduction and the Lambertian property. In this study, we introduce MrGS, a multi-modal radiance field based on 3DGS that simultaneously reconstructs both RGB and thermal 3D scenes. Specifically, MrGS derives RGB- and thermal-related information from a single appearance feature through orthogonal feature extraction and employs view-dependent or view-independent embedding strategies depending on the degree of Lambertian reflectance exhibited by each modality. Furthermore, we leverage two physics-based principles to effectively model thermal-domain phenomena. First, we integrate Fourier's law of heat conduction prior to alpha blending to model intensity interpolation caused by thermal conduction between neighboring Gaussians. Second, we apply the Stefan-Boltzmann law and the inverse-square law to formulate a depth-aware thermal radiation map that imposes additional geometric constraints on thermal rendering. Experimental results demonstrate that the proposed MrGS achieves high-fidelity RGB-T scene reconstruction while reducing the number of Gaussians.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JarvisEvo: Towards a Self-Evolving Photo Editing Agent with Synergistic Editor-Evaluator Optimization</title>
<link>https://arxiv.org/abs/2511.23002</link>
<guid>https://arxiv.org/abs/2511.23002</guid>
<content:encoded><![CDATA[
arXiv:2511.23002v1 Announce Type: new 
Abstract: Agent-based editing models have substantially advanced interactive experiences, processing quality, and creative flexibility. However, two critical challenges persist: (1) instruction hallucination, text-only chain-of-thought (CoT) reasoning cannot fully prevent factual errors due to inherent information bottlenecks; (2) reward hacking, dynamic policy optimization against static reward models allows agents to exploit flaws in reward functions. To address these issues, we propose JarvisEvo, a unified image editing agent that emulates an expert human designer by iteratively editing, selecting appropriate tools, evaluating results, and reflecting on its own decisions to refine outcomes. JarvisEvo offers three key advantages: (1) an interleaved multimodal chain-of-thought (iMCoT) reasoning mechanism that enhances instruction following and editing quality; (2) a synergistic editor-evaluator policy optimization (SEPO) framework that enables self-improvement without external rewards, effectively mitigating reward hacking; and (3) support for both global and local fine-grained editing through seamless integration of Adobe Lightroom. On ArtEdit-Bench, JarvisEvo outperforms Nano-Banana by an average of 18.95% on preservative editing metrics, including a substantial 44.96% improvement in pixel-level content fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2511.23031</link>
<guid>https://arxiv.org/abs/2511.23031</guid>
<content:encoded><![CDATA[
arXiv:2511.23031v1 Announce Type: new 
Abstract: Recent advances in vision-language reasoning underscore the importance of thinking with images, where models actively ground their reasoning in visual evidence. Yet, prevailing frameworks treat visual actions as optional tools, boosting metrics but leaving reasoning ungrounded and crops ineffective. This gap gives rise to the illusion of thinking with images: models seem visually grounded but rely on context-agnostic actions that neither refine perception nor guide reasoning toward correct answers. We address this problem by reframing visual actions as core reasoning primitives rather than optional tools, which we term visual rationalization, the visual analogue of textual Chain-of-Thought. Building on this insight, we propose Visual Rationale Learning (ViRL), an end-to-end paradigm that grounds training in the visual rationale itself. ViRL integrates (1) Process Supervision with ground-truth rationales, (2) Objective Alignment via step-level reward shaping, and (3) Fine-Grained Credit Assignment to distinguish correct, redundant, and erroneous actions. By ensuring each action contributes meaningfully to the reasoning chain, ViRL enables models to "get the right answer for the right visual reason". Trained purely with end-to-end RL, ViRL achieves state-of-the-art results across benchmarks spanning perception, hallucination, and reasoning. This work establishes visual rationalization as a task-agnostic, process-grounded paradigm for building transparent, verifiable, and trustworthy vision-language models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry-Consistent 4D Gaussian Splatting for Sparse-Input Dynamic View Synthesis</title>
<link>https://arxiv.org/abs/2511.23044</link>
<guid>https://arxiv.org/abs/2511.23044</guid>
<content:encoded><![CDATA[
arXiv:2511.23044v1 Announce Type: new 
Abstract: Gaussian Splatting has been considered as a novel way for view synthesis of dynamic scenes, which shows great potential in AIoT applications such as digital twins. However, recent dynamic Gaussian Splatting methods significantly degrade when only sparse input views are available, limiting their applicability in practice. The issue arises from the incoherent learning of 4D geometry as input views decrease. This paper presents GC-4DGS, a novel framework that infuses geometric consistency into 4D Gaussian Splatting (4DGS), offering real-time and high-quality dynamic scene rendering from sparse input views. While learning-based Multi-View Stereo (MVS) and monocular depth estimators (MDEs) provide geometry priors, directly integrating these with 4DGS yields suboptimal results due to the ill-posed nature of sparse-input 4D geometric optimization. To address these problems, we introduce a dynamic consistency checking strategy to reduce estimation uncertainties of MVS across spacetime. Furthermore, we propose a global-local depth regularization approach to distill spatiotemporal-consistent geometric information from monocular depths, thereby enhancing the coherent geometry and appearance learning within the 4D volume. Extensive experiments on the popular N3DV and Technicolor datasets validate the effectiveness of GC-4DGS in rendering quality without sacrificing efficiency. Notably, our method outperforms RF-DeRF, the latest dynamic radiance field tailored for sparse-input dynamic view synthesis, and the original 4DGS by 2.62dB and 1.58dB in PSNR, respectively, with seamless deployability on resource-constrained IoT edge devices.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GOATex: Geometry &amp; Occlusion-Aware Texturing</title>
<link>https://arxiv.org/abs/2511.23051</link>
<guid>https://arxiv.org/abs/2511.23051</guid>
<content:encoded><![CDATA[
arXiv:2511.23051v1 Announce Type: new 
Abstract: We present GOATex, a diffusion-based method for 3D mesh texturing that generates high-quality textures for both exterior and interior surfaces. While existing methods perform well on visible regions, they inherently lack mechanisms to handle occluded interiors, resulting in incomplete textures and visible seams. To address this, we introduce an occlusion-aware texturing framework based on the concept of hit levels, which quantify the relative depth of mesh faces via multi-view ray casting. This allows us to partition mesh faces into ordered visibility layers, from outermost to innermost. We then apply a two-stage visibility control strategy that progressively reveals interior regions with structural coherence, followed by texturing each layer using a pretrained diffusion model. To seamlessly merge textures obtained across layers, we propose a soft UV-space blending technique that weighs each texture's contribution based on view-dependent visibility confidence. Empirical results demonstrate that GOATex consistently outperforms existing methods, producing seamless, high-fidelity textures across both visible and occluded surfaces. Unlike prior works, GOATex operates entirely without costly fine-tuning of a pretrained diffusion model and allows separate prompting for exterior and interior mesh regions, enabling fine-grained control over layered appearances. For more qualitative results, please visit our project page: https://goatex3d.github.io/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Valuation in NeRF-based 3D reconstruction</title>
<link>https://arxiv.org/abs/2511.23052</link>
<guid>https://arxiv.org/abs/2511.23052</guid>
<content:encoded><![CDATA[
arXiv:2511.23052v1 Announce Type: new 
Abstract: Data valuation and monetization are becoming increasingly important across domains such as eXtended Reality (XR) and digital media. In the context of 3D scene reconstruction from a set of images -- whether casually or professionally captured -- not all inputs contribute equally to the final output. Neural Radiance Fields (NeRFs) enable photorealistic 3D reconstruction of scenes by optimizing a volumetric radiance field given a set of images. However, in-the-wild scenes often include image captures of varying quality, occlusions, and transient objects, resulting in uneven utility across inputs. In this paper we propose a method to quantify the individual contribution of each image to NeRF-based reconstructions of in-the-wild image sets. Contribution is assessed through reconstruction quality metrics based on PSNR and MSE. We validate our approach by removing low-contributing images during training and measuring the resulting impact on reconstruction fidelity.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Clinical Impact of Generative Inpainting on Bone Age Estimation</title>
<link>https://arxiv.org/abs/2511.23066</link>
<guid>https://arxiv.org/abs/2511.23066</guid>
<content:encoded><![CDATA[
arXiv:2511.23066v1 Announce Type: new 
Abstract: Generative foundation models can remove visual artifacts through realistic image inpainting, but their impact on medical AI performance remains uncertain. Pediatric hand radiographs often contain non-anatomical markers, and it is unclear whether inpainting these regions preserves features needed for bone age and gender prediction. To evaluate the clinical reliability of generative model-based inpainting for artifact removal, we used the RSNA Bone Age Challenge dataset, selecting 200 original radiographs and generating 600 inpainted versions with gpt-image-1 using natural language prompts to target non-anatomical artifacts. Downstream performance was assessed with deep learning ensembles for bone age estimation and gender classification, using mean absolute error (MAE) and area under the ROC curve (AUC) as metrics, and pixel intensity distributions to detect structural alterations. Inpainting markedly degraded model performance: bone age MAE increased from 6.26 to 30.11 months, and gender classification AUC decreased from 0.955 to 0.704. Inpainted images displayed pixel-intensity shifts and inconsistencies, indicating structural modifications not corrected by simple calibration. These findings show that, although visually realistic, foundation model-based inpainting can obscure subtle but clinically relevant features and introduce latent bias even when edits are confined to non-diagnostic regions, underscoring the need for rigorous, task-specific validation before integrating such generative tools into clinical AI workflows.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Buffer replay enhances the robustness of multimodal learning under missing-modality</title>
<link>https://arxiv.org/abs/2511.23070</link>
<guid>https://arxiv.org/abs/2511.23070</guid>
<content:encoded><![CDATA[
arXiv:2511.23070v1 Announce Type: new 
Abstract: Missing modalities consistently lead to significant performance degradation in multimodal models. Existing approaches either synthesize missing modalities at high computational cost or apply prompt-based fine-tuning that relies only on adjacent-layer features and overlooks long-distance contextual information, which may offer additional tolerance to errors when one or more modalities are missing. To address this, we introduce REplay Prompting (REP): (1) construct modality-wise feature buffers via a residual bypass to cache early-layer representations and replay them in deeper layers, mitigating information loss as network depth increases; (2) employ a private-shared feature decoupling strategy, where private buffers preserve modality-specific signals and shared buffers encode cross-modal semantics; and (3) design a task-aware dynamic initialization mechanism to configure these buffers differently, improving stability and generalization under diverse missing-modality conditions. Experiments on vision-language, vision-language-audio, and temporal multimodal benchmarks demonstrate that REP consistently outperforms prior methods under both single- and multi-modality missing scenarios, while introducing only negligible parameter overhead. These results establish REP as a lightweight and effective paradigm for robust multimodal learning in challenging missing-modality environments.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bharat Scene Text: A Novel Comprehensive Dataset and Benchmark for Indian Language Scene Text Understanding</title>
<link>https://arxiv.org/abs/2511.23071</link>
<guid>https://arxiv.org/abs/2511.23071</guid>
<content:encoded><![CDATA[
arXiv:2511.23071v1 Announce Type: new 
Abstract: Reading scene text, that is, text appearing in images, has numerous application areas, including assistive technology, search, and e-commerce. Although scene text recognition in English has advanced significantly and is often considered nearly a solved problem, Indian language scene text recognition remains an open challenge. This is due to script diversity, non-standard fonts, and varying writing styles, and, more importantly, the lack of high-quality datasets and open-source models. To address these gaps, we introduce the Bharat Scene Text Dataset (BSTD) - a large-scale and comprehensive benchmark for studying Indian Language Scene Text Recognition. It comprises more than 100K words that span 11 Indian languages and English, sourced from over 6,500 scene images captured across various linguistic regions of India. The dataset is meticulously annotated and supports multiple scene text tasks, including: (i) Scene Text Detection, (ii) Script Identification, (iii) Cropped Word Recognition, and (iv) End-to-End Scene Text Recognition. We evaluated state-of-the-art models originally developed for English by adapting (fine-tuning) them for Indian languages. Our results highlight the challenges and opportunities in Indian language scene text recognition. We believe that this dataset represents a significant step toward advancing research in this domain. All our models and data are open source.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpaceMind: Camera-Guided Modality Fusion for Spatial Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.23075</link>
<guid>https://arxiv.org/abs/2511.23075</guid>
<content:encoded><![CDATA[
arXiv:2511.23075v1 Announce Type: new 
Abstract: Large vision-language models (VLMs) show strong multimodal understanding but still struggle with 3D spatial reasoning, such as distance estimation, size comparison, and cross-view consistency. Existing 3D-aware methods either depend on auxiliary 3D information or enhance RGB-only VLMs with geometry encoders through shallow feature fusion. We propose SpaceMind, a multimodal large language model explicitly designed for spatial reasoning solely from RGB inputs. The model adopts a dual-encoder architecture, integrating VGGT as a spatial understanding encoder and InternViT as a 2D visual encoder. The key idea is to treat the camera representation as an active guiding modality rather than passive metadata. Specifically, SpaceMind introduces a lightweight Camera-Guided Modality Fusion module before the language model to replace shallow fusion. It applies camera-conditioned biasing to spatial tokens, assigns query-independent weights reflecting their geometric importance, and uses the camera embedding to gate the fused representation. Empirically, SpaceMind establishes new state-of-the-art results on VSI-Bench, SQA3D and SPBench, surpassing both open and proprietary systems on VSI-Bench and SPBench by large margins and achieving state-of-the-art performance on SQA3D. These results demonstrate that camera-guided modality fusion is an effective and practical inductive bias for equipping VLMs with genuinely spatially grounded intelligence. We will release code and model checkpoints to support future research.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implementation of a Skin Lesion Detection System for Managing Children with Atopic Dermatitis Based on Ensemble Learning</title>
<link>https://arxiv.org/abs/2511.23082</link>
<guid>https://arxiv.org/abs/2511.23082</guid>
<content:encoded><![CDATA[
arXiv:2511.23082v1 Announce Type: new 
Abstract: The amendments made to the Data 3 Act and impact of COVID-19 have fostered the growth of digital healthcare market and promoted the use of medical data in artificial intelligence in South Korea. Atopic dermatitis, a chronic inflammatory skin disease, is diagnosed via subjective evaluations without using objective diagnostic methods, thereby increasing the risk of misdiagnosis. It is also similar to psoriasis in appearance, further complicating its accurate diagnosis. Existing studies on skin diseases have used high-quality dermoscopic image datasets, but such high-quality images cannot be obtained in actual clinical settings. Moreover, existing systems must ensure accuracy and fast response times. To this end, an ensemble learning-based skin lesion detection system (ENSEL) was proposed herein. ENSEL enhanced diagnostic accuracy by integrating various deep learning models via an ensemble approach. Its performance was verified by conducting skin lesion detection experiments using images of skin lesions taken by actual users. Its accuracy and response time were measured using randomly sampled skin disease images. Results revealed that ENSEL achieved high recall in most images and less than 1s s processing speed. This study contributes to the objective diagnosis of skin lesions and promotes the advancement of digital healthcare.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NumeriKontrol: Adding Numeric Control to Diffusion Transformers for Instruction-based Image Editing</title>
<link>https://arxiv.org/abs/2511.23105</link>
<guid>https://arxiv.org/abs/2511.23105</guid>
<content:encoded><![CDATA[
arXiv:2511.23105v1 Announce Type: new 
Abstract: Instruction-based image editing enables intuitive manipulation through natural language commands. However, text instructions alone often lack the precision required for fine-grained control over edit intensity. We introduce NumeriKontrol, a framework that allows users to precisely adjust image attributes using continuous scalar values with common units. NumeriKontrol encodes numeric editing scales via an effective Numeric Adapter and injects them into diffusion models in a plug-and-play manner. Thanks to a task-separated design, our approach supports zero-shot multi-condition editing, allowing users to specify multiple instructions in any order. To provide high-quality supervision, we synthesize precise training data from reliable sources, including high-fidelity rendering engines and DSLR cameras. Our Common Attribute Transform (CAT) dataset covers diverse attribute manipulations with accurate ground-truth scales, enabling NumeriKontrol to function as a simple yet powerful interactive editing studio. Extensive experiments show that NumeriKontrol delivers accurate, continuous, and stable scale control across a wide range of attribute editing scenarios. These contributions advance instruction-based image editing by enabling precise, scalable, and user-controllable image manipulation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?</title>
<link>https://arxiv.org/abs/2511.23112</link>
<guid>https://arxiv.org/abs/2511.23112</guid>
<content:encoded><![CDATA[
arXiv:2511.23112v1 Announce Type: new 
Abstract: Recent advances in Vision-Language Models (VLMs) have achieved impressive progress in multimodal mathematical reasoning. Yet, how much visual information truly contributes to reasoning remains unclear. Existing benchmarks report strong overall performance but seldom isolate the role of the image modality, leaving open whether VLMs genuinely leverage visual understanding or merely depend on linguistic priors. To address this, we present MathSight, a university-level multimodal mathematical reasoning benchmark designed to disentangle and quantify the effect of visual input. Each problem includes multiple visual variants -- original, hand-drawn, photo-captured -- and a text-only condition for controlled comparison. Experiments on state-of-the-art VLMs reveal a consistent trend: the contribution of visual information diminishes with increasing problem difficulty. Remarkably, Qwen3-VL without any image input surpasses both its multimodal variants and GPT-5, underscoring the need for benchmarks like MathSight to advance genuine vision-grounded reasoning in future models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism</title>
<link>https://arxiv.org/abs/2511.23113</link>
<guid>https://arxiv.org/abs/2511.23113</guid>
<content:encoded><![CDATA[
arXiv:2511.23113v1 Announce Type: new 
Abstract: Scaling Diffusion Transformer (DiT) inference via sequence parallelism is critical for reducing latency in visual generation, but is severely hampered by workload imbalance when applied to models employing block-wise sparse attention. The imbalance stems from the inherent variation in sparsity across attention heads and the irregular distribution of dense blocks within the sparse mask, when sequence parallelism is applied along the head dimension (as in Ulysses) or the block dimension (as in Ring Attention). In this paper, we formalize a sparse imbalance ratio to quantify the imbalance, and propose db-SP, a sparsity-aware sequence parallelism technique that tackles the challenge. db-SP contains a dual-level partitioning approach that achieves near-perfect workload balance at both the head and block levels with negligible overhead. Furthermore, to handle the evolving sparsity patterns across denoising steps and layers, db-SP dynamically determines the parallel degrees for the head and block dimensions at runtime. Experimental results demonstrate that db-SP delivers an end-to-end speedup of 1.25x and an attention-specific speedup of 1.40x over state-of-the-art sequence parallel methods on average. Code is available at https://github.com/thu-nics/db-SP.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Image Beyond Visual Aspect: Image Emotion Classification via Multiple-Affective Captioning</title>
<link>https://arxiv.org/abs/2511.23115</link>
<guid>https://arxiv.org/abs/2511.23115</guid>
<content:encoded><![CDATA[
arXiv:2511.23115v1 Announce Type: new 
Abstract: Image emotion classification (IEC) is a longstanding research field that has received increasing attention with the rapid progress of deep learning. Although recent advances have leveraged the knowledge encoded in pre-trained visual models, their effectiveness is constrained by the "affective gap" , limits the applicability of pre-training knowledge for IEC tasks. It has been demonstrated in psychology that language exhibits high variability, encompasses diverse and abundant information, and can effectively eliminate the "affective gap". Inspired by this, we propose a novel Affective Captioning for Image Emotion Classification (ACIEC) to classify image emotion based on pure texts, which effectively capture the affective information in the image. In our method, a hierarchical multi-level contrastive loss is designed for detecting emotional concepts from images, while an emotional attribute chain-of-thought reasoning is proposed to generate affective sentences. Then, a pre-trained language model is leveraged to synthesize emotional concepts and affective sentences to conduct IEC. Additionally, a contrastive loss based on semantic similarity sampling is designed to solve the problem of large intra-class differences and small inter-class differences in affective datasets. Moreover, we also take the images with embedded texts into consideration, which were ignored by previous studies. Extensive experiments illustrate that our method can effectively bridge the affective gap and achieve superior results on multiple benchmarks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DNA-Prior: Unsupervised Denoise Anything via Dual-Domain Prior</title>
<link>https://arxiv.org/abs/2511.23124</link>
<guid>https://arxiv.org/abs/2511.23124</guid>
<content:encoded><![CDATA[
arXiv:2511.23124v1 Announce Type: new 
Abstract: Medical imaging pipelines critically rely on robust denoising to stabilise downstream tasks such as segmentation and reconstruction. However, many existing denoisers depend on large annotated datasets or supervised learning, which restricts their usability in clinical environments with heterogeneous modalities and limited ground-truth data. To address this limitation, we introduce DNA-Prior, a universal unsupervised denoising framework that reconstructs clean images directly from corrupted observations through a mathematically principled hybrid prior. DNA-Prior integrates (i) an implicit architectural prior, enforced through a deep network parameterisation, with (ii) an explicit spectral-spatial prior composed of a frequency-domain fidelity term and a spatial regularisation functional. This dual-domain formulation yields a well-structured optimisation problem that jointly preserves global frequency characteristics and local anatomical structure, without requiring any external training data or modality-specific tuning. Experiments across multiple modalities show that DNA achieves consistent noise suppression and structural preservation under diverse noise conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DualCamCtrl: Dual-Branch Diffusion Model for Geometry-Aware Camera-Controlled Video Generation</title>
<link>https://arxiv.org/abs/2511.23127</link>
<guid>https://arxiv.org/abs/2511.23127</guid>
<content:encoded><![CDATA[
arXiv:2511.23127v1 Announce Type: new 
Abstract: This paper presents DualCamCtrl, a novel end-to-end diffusion model for camera-controlled video generation. Recent works have advanced this field by representing camera poses as ray-based conditions, yet they often lack sufficient scene understanding and geometric awareness. DualCamCtrl specifically targets this limitation by introducing a dual-branch framework that mutually generates camera-consistent RGB and depth sequences. To harmonize these two modalities, we further propose the Semantic Guided Mutual Alignment (SIGMA) mechanism, which performs RGB-depth fusion in a semantics-guided and mutually reinforced manner. These designs collectively enable DualCamCtrl to better disentangle appearance and geometry modeling, generating videos that more faithfully adhere to the specified camera trajectories. Additionally, we analyze and reveal the distinct influence of depth and camera poses across denoising stages and further demonstrate that early and late stages play complementary roles in forming global structure and refining local details. Extensive experiments demonstrate that DualCamCtrl achieves more consistent camera-controlled video generation, with over 40\% reduction in camera motion errors compared with prior methods. Our project page: https://soyouthinkyoucantell.github.io/dualcamctrl\-page/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstanceV: Instance-Level Video Generation</title>
<link>https://arxiv.org/abs/2511.23146</link>
<guid>https://arxiv.org/abs/2511.23146</guid>
<content:encoded><![CDATA[
arXiv:2511.23146v1 Announce Type: new 
Abstract: Recent advances in text-to-video diffusion models have enabled the generation of high-quality videos conditioned on textual descriptions. However, most existing text-to-video models rely solely on textual conditions, lacking general fine-grained controllability over video generation. To address this challenge, we propose InstanceV, a video generation framework that enables i) instance-level control and ii) global semantic consistency. Specifically, with the aid of proposed Instance-aware Masked Cross-Attention mechanism, InstanceV maximizes the utilization of additional instance-level grounding information to generate correctly attributed instances at designated spatial locations. To improve overall consistency, We introduce the Shared Timestep-Adaptive Prompt Enhancement module, which connects local instances with global semantics in a parameter-efficient manner. Furthermore, we incorporate Spatially-Aware Unconditional Guidance during both training and inference to alleviate the disappearance of small instances. Finally, we propose a new benchmark, named InstanceBench, which combines general video quality metrics with instance-aware metrics for more comprehensive evaluation on instance-level video generation. Extensive experiments demonstrate that InstanceV not only achieves remarkable instance-level controllability in video generation, but also outperforms existing state-of-the-art models in both general quality and instance-aware metrics across qualitative and quantitative evaluations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cascaded Robust Rectification for Arbitrary Document Images</title>
<link>https://arxiv.org/abs/2511.23150</link>
<guid>https://arxiv.org/abs/2511.23150</guid>
<content:encoded><![CDATA[
arXiv:2511.23150v1 Announce Type: new 
Abstract: Document rectification in real-world scenarios poses significant challenges due to extreme variations in camera perspectives and physical distortions. Driven by the insight that complex transformations can be decomposed and resolved progressively, we introduce a novel multi-stage framework that progressively reverses distinct distortion types in a coarse-to-fine manner. Specifically, our framework first performs a global affine transformation to correct perspective distortions arising from the camera's viewpoint, then rectifies geometric deformations resulting from physical paper curling and folding, and finally employs a content-aware iterative process to eliminate fine-grained content distortions. To address limitations in existing evaluation protocols, we also propose two enhanced metrics: layout-aligned OCR metrics (AED/ACER) for a stable assessment that decouples geometric rectification quality from the layout analysis errors of OCR engines, and masked AD/AAD (AD-M/AAD-M) tailored for accurately evaluating geometric distortions in documents with incomplete boundaries. Extensive experiments show that our method establishes new state-of-the-art performance on multiple challenging benchmarks, yielding a substantial reduction of 14.1\%--34.7\% in the AAD metric and demonstrating superior efficacy in real-world applications. The code will be publicly available at https://github.com/chaoyunwang/ArbDR.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding</title>
<link>https://arxiv.org/abs/2511.23151</link>
<guid>https://arxiv.org/abs/2511.23151</guid>
<content:encoded><![CDATA[
arXiv:2511.23151v1 Announce Type: new 
Abstract: Video Temporal Grounding (VTG) aims to localize a temporal segment in a video corresponding to a natural language query. However, existing VTG models assume that a relevant segment always exists, causing them to always predict a target segment even when the query is irrelevant to the video. While recent approaches attempt to handle irrelevant queries, they can only reject those that are entirely unrelated to the video and still fail to handle hard-irrelevant queries that are semantically similar but not actually relevant. To address this, we propose Refusal-Aware Reinforcement Fine-Tuning (RA-RFT) to effectively refuse hard-irrelevant queries in VTG. Our method is based on the Group Relative Policy Optimization (GRPO) framework and integrates four reward objectives-format, refuse-IoU, explain, and query correction-to improve both relevance discrimination and fine-grained semantic reasoning. In addition, to effectively support RA-RFT, we construct a Hard-Irrelevant VTG (HI-VTG) dataset, which includes hard-irrelevant queries and their refusal answers. We demonstrate the effectiveness of our method across various relevance-aware VTG scenarios, including hard-irrelevant VTG, simply-shuffled RA-VTG, and human-annotated RA-VTG settings. We also show that the proposed method is scalable by applying it to various LVLM-based VTG models. Our code is available at https://github.com/JINSUBY/RA-RFT.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REVEAL: Reasoning-enhanced Forensic Evidence Analysis for Explainable AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2511.23158</link>
<guid>https://arxiv.org/abs/2511.23158</guid>
<content:encoded><![CDATA[
arXiv:2511.23158v1 Announce Type: new 
Abstract: With the rapid advancement of generative models, visually realistic AI-generated images have become increasingly difficult to distinguish from authentic ones, posing severe threats to social trust and information integrity. Consequently, there is an urgent need for efficient and truly explainable image forensic methods. Recent detection paradigms have shifted towards explainable forensics. However, state-of-the-art approaches primarily rely on post-hoc rationalizations or visual discrimination, lacking a verifiable chain of evidence. This reliance on surface-level pattern matching limits the generation of causally grounded explanations and often results in poor generalization. To bridge this critical gap, we introduce \textbf{REVEAL-Bench}, the first reasoning-enhanced multimodal benchmark for AI-generated image detection that is explicitly structured around a chain-of-evidence derived from multiple lightweight expert models, then records step-by-step reasoning traces and evidential justifications. Building upon this dataset, we propose \textbf{REVEAL} (\underline{R}easoning-\underline{e}nhanced Forensic E\underline{v}id\underline{e}nce \underline{A}na\underline{l}ysis), an effective and explainable forensic framework that integrates detection with a novel expert-grounded reinforcement learning. Our reward mechanism is specially tailored to jointly optimize detection accuracy, explanation fidelity, and logical coherence grounded in explicit forensic evidence, enabling REVEAL to produce fine-grained, interpretable, and verifiable reasoning chains alongside its detection outcomes. Extensive experimental results demonstrate that REVEAL significantly enhances detection accuracy, explanation fidelity, and robust cross-model generalization, benchmarking a new state of the art for explainable image forensics.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PowerCLIP: Powerset Alignment for Contrastive Pre-Training</title>
<link>https://arxiv.org/abs/2511.23170</link>
<guid>https://arxiv.org/abs/2511.23170</guid>
<content:encoded><![CDATA[
arXiv:2511.23170v1 Announce Type: new 
Abstract: Contrastive vision-language pre-training frameworks such as CLIP have demonstrated impressive zero-shot performance across a range of vision-language tasks. Recent studies have shown that aligning individual text tokens with specific image patches or regions enhances fine-grained compositional understanding. However, it remains challenging to capture compositional semantics that span multiple image regions. To address this limitation, we propose PowerCLIP, a novel contrastive pre-training framework enhanced by powerset alignment, which exhaustively optimizes region-to-phrase alignments by minimizing the loss defined between powersets of image regions and textual parse trees. Since the naive powerset construction incurs exponential computational cost due to the combinatorial explosion in the number of region subsets, we introduce efficient non-linear aggregators (NLAs) that reduce complexity from O(2^M) to O(M) with respect to the number of regions M, while approximating the exact loss value with arbitrary precision. Our extensive experiments demonstrate that PowerCLIP outperforms state-of-the-art methods in zero-shot classification and retrieval tasks, underscoring the compositionality and robustness of our approach. Our code will be made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Multi-view Consistent 3D Editing with Video Priors</title>
<link>https://arxiv.org/abs/2511.23172</link>
<guid>https://arxiv.org/abs/2511.23172</guid>
<content:encoded><![CDATA[
arXiv:2511.23172v1 Announce Type: new 
Abstract: Text-driven 3D editing enables user-friendly 3D object or scene editing with text instructions. Due to the lack of multi-view consistency priors, existing methods typically resort to employing 2D generation or editing models to process each view individually, followed by iterative 2D-3D-2D updating. However, these methods are not only time-consuming but also prone to over-smoothed results because the different editing signals gathered from different views are averaged during the iterative process. In this paper, we propose generative Video Prior based 3D Editing (ViP3DE) to employ the temporal consistency priors from pre-trained video generation models for multi-view consistent 3D editing in a single forward pass. Our key insight is to condition the video generation model on a single edited view to generate other consistent edited views for 3D updating directly, thereby bypassing the iterative editing paradigm. Since 3D updating requires edited views to be paired with specific camera poses, we propose motion-preserved noise blending for the video model to generate edited views at predefined camera poses. In addition, we introduce geometry-aware denoising to further enhance multi-view consistency by integrating 3D geometric priors into video models. Extensive experiments demonstrate that our proposed ViP3DE can achieve high-quality 3D editing results even within a single forward pass, significantly outperforming existing methods in both editing quality and speed.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoWorld: Unlocking the Potential of Geometry Models to Facilitate High-Fidelity 3D Scene Generation</title>
<link>https://arxiv.org/abs/2511.23191</link>
<guid>https://arxiv.org/abs/2511.23191</guid>
<content:encoded><![CDATA[
arXiv:2511.23191v1 Announce Type: new 
Abstract: Previous works leveraging video models for image-to-3D scene generation tend to suffer from geometric distortions and blurry content. In this paper, we renovate the pipeline of image-to-3D scene generation by unlocking the potential of geometry models and present our GeoWorld. Instead of exploiting geometric information obtained from a single-frame input, we propose to first generate consecutive video frames and then take advantage of the geometry model to provide full-frame geometry features, which contain richer information than single-frame depth maps or camera embeddings used in previous methods, and use these geometry features as geometrical conditions to aid the video generation model. To enhance the consistency of geometric structures, we further propose a geometry alignment loss to provide the model with real-world geometric constraints and a geometry adaptation module to ensure the effective utilization of geometry features. Extensive experiments show that our GeoWorld can generate high-fidelity 3D scenes from a single image and a given camera trajectory, outperforming prior methods both qualitatively and quantitatively. Project Page: https://peaes.github.io/GeoWorld/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Bridge Transformer at Scale</title>
<link>https://arxiv.org/abs/2511.23199</link>
<guid>https://arxiv.org/abs/2511.23199</guid>
<content:encoded><![CDATA[
arXiv:2511.23199v1 Announce Type: new 
Abstract: We introduce Vision Bridge Transformer (ViBT), a large-scale instantiation of Brownian Bridge Models designed for conditional generation. Unlike traditional diffusion models that transform noise into data, Bridge Models directly model the trajectory between inputs and outputs, creating an efficient data-to-data translation paradigm. By scaling these models to 20B and 1.3B parameters, we demonstrate their effectiveness for image and video translation tasks. To support this scale, we adopt a Transformer architecture and propose a variance-stabilized velocity-matching objective for robust training. Together, these advances highlight the power of scaling Bridge Models for instruction-based image editing and complex video translation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pathryoshka: Compressing Pathology Foundation Models via Multi-Teacher Knowledge Distillation with Nested Embeddings</title>
<link>https://arxiv.org/abs/2511.23204</link>
<guid>https://arxiv.org/abs/2511.23204</guid>
<content:encoded><![CDATA[
arXiv:2511.23204v1 Announce Type: new 
Abstract: Pathology foundation models (FMs) have driven significant progress in computational pathology. However, these high-performing models can easily exceed a billion parameters and produce high-dimensional embeddings, thus limiting their applicability for research or clinical use when computing resources are tight. Here, we introduce Pathryoshka, a multi-teacher distillation framework inspired by RADIO distillation and Matryoshka Representation Learning to reduce pathology FM sizes while allowing for adaptable embedding dimensions. We evaluate our framework with a distilled model on ten public pathology benchmarks with varying downstream tasks. Compared to its much larger teachers, Pathryoshka reduces the model size by 86-92% at on-par performance. It outperforms state-of-the-art single-teacher distillation models of comparable size by a median margin of 7.0 in accuracy. By enabling efficient local deployment without sacrificing accuracy or representational richness, Pathryoshka democratizes access to state-of-the-art pathology FMs for the broader research and clinical community.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Multi-Criteria Visual Quality Inspection for Semi-Controlled Industrial Environments via Real-Time 3D Digital Twin Simulation</title>
<link>https://arxiv.org/abs/2511.23214</link>
<guid>https://arxiv.org/abs/2511.23214</guid>
<content:encoded><![CDATA[
arXiv:2511.23214v1 Announce Type: new 
Abstract: Early-stage visual quality inspection is vital for achieving Zero-Defect Manufacturing and minimizing production waste in modern industrial environments. However, the complexity of robust visual inspection systems and their extensive data requirements hinder widespread adoption in semi-controlled industrial settings. In this context, we propose a pose-agnostic, zero-shot quality inspection framework that compares real scenes against real-time Digital Twins (DT) in the RGB-D space. Our approach enables efficient real-time DT rendering by semantically describing industrial scenes through object detection and pose estimation of known Computer-Aided Design models. We benchmark tools for real-time, multimodal RGB-D DT creation while tracking consumption of computational resources. Additionally, we provide an extensible and hierarchical annotation strategy for multi-criteria defect detection, unifying pose labelling with logical and structural defect annotations. Based on an automotive use case featuring the quality inspection of an axial flux motor, we demonstrate the effectiveness of our framework. Our results demonstrate detection performace, achieving intersection-over-union (IoU) scores of up to 63.3% compared to ground-truth masks, even if using simple distance measurements under semi-controlled industrial conditions. Our findings lay the groundwork for future research on generalizable, low-data defect detection methods in dynamic manufacturing settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day</title>
<link>https://arxiv.org/abs/2511.23220</link>
<guid>https://arxiv.org/abs/2511.23220</guid>
<content:encoded><![CDATA[
arXiv:2511.23220v1 Announce Type: new 
Abstract: Tabular instruction tuning has emerged as a promising research direction for improving LLMs understanding of tabular data. However, the majority of existing works only consider question-answering and reasoning tasks over tabular data, leaving tabular data generation largely unnoticed. In this work, for the first time, we explore the efficacy of instruction tuning in improving LLMs tabular data generation capabilities. More specifically, given the high data and computation requirements of tabular instruction tuning, we aim to address the possibility of instruction tuning for tabular data generation with limited data and computational resources. To achieve this, we first create a high-quality instruction dataset for tabular data, enabling efficient LLM comprehension. We then instruction-tune an open-source LLM (Llama3.1-8B-Instruct) on the training set of this dataset to improve its tabular data generation performance. Our experimental results show that by using our high-quality dataset and instruction-tuning on only 7K instructions with an A100 GPU, for less than 6 hours, we achieve tabular data generation performance on par with the most capable commercial LLM, GPT-4o.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust 3DGS-based SLAM via Adaptive Kernel Smoothing</title>
<link>https://arxiv.org/abs/2511.23221</link>
<guid>https://arxiv.org/abs/2511.23221</guid>
<content:encoded><![CDATA[
arXiv:2511.23221v1 Announce Type: new 
Abstract: In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy. We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking. To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM. Unlike conventional methods that focus solely on minimizing rendering error, our core insight is to make the rasterization process more resilient to imperfections in the 3DGS parameters. We hypothesize that by allowing each Gaussian to influence a smoother, wider distribution of pixels during rendering, we can mitigate the detrimental effects of parameter noise from outlier Gaussians. This approach intentionally introduces a controlled blur to the rendered image, which acts as a regularization term, stabilizing the subsequent pose optimization. While a complete redesign of the rasterization pipeline is an ideal solution, we propose a practical and effective alternative that is readily integrated into existing 3DGS frameworks. Our method, termed Corrective Blurry KNN (CB-KNN), adaptively modifies the RGB values and locations of the K-nearest neighboring Gaussians within a local region. This dynamic adjustment generates a smoother local rendering, reducing the impact of erroneous GS parameters on the overall image. Experimental results demonstrate that our approach, while maintaining the overall quality of the scene reconstruction (mapping), significantly improves the robustness and accuracy of camera pose tracking.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAONet-YOLOv8: An Occlusion-Aware Dual-Attention Network for Tea Leaf Pest and Disease Detection</title>
<link>https://arxiv.org/abs/2511.23222</link>
<guid>https://arxiv.org/abs/2511.23222</guid>
<content:encoded><![CDATA[
arXiv:2511.23222v1 Announce Type: new 
Abstract: Accurate detection of tea leaf pests and diseases in real plantations remains challenging due to complex backgrounds, variable illumination, and frequent occlusions among dense branches and leaves. Existing detectors often suffer from missed detections and false positives in such scenarios. To address these issues, we propose DAONet-YOLOv8, an enhanced YOLOv8 variant with three key improvements: (1) a Dual-Attention Fusion Module (DAFM) that combines convolutional local feature extraction with self-attention based global context modeling to focus on subtle lesion regions while suppressing background noise; (2) an occlusion-aware detection head (Detect-OAHead) that learns the relationship between visible and occluded parts to compensate for missing lesion features; and (3) a C2f-DSConv module employing dynamic synthesis convolutions with multiple kernel shapes to better capture irregular lesion boundaries. Experiments on our real-world tea plantation dataset containing six pest and disease categories demonstrate that DAONet-YOLOv8 achieves 92.97% precision, 92.80% recall, 97.10% mAP@50 and 76.90% mAP@50:95, outperforming the YOLOv8n baseline by 2.34, 4.68, 1.40 and 1.80 percentage points respectively, while reducing parameters by 16.7%. Comparative experiments further confirm that DAONet-YOLOv8 achieves superior performance over mainstream detection models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointCNN++: Performant Convolution on Native Points</title>
<link>https://arxiv.org/abs/2511.23227</link>
<guid>https://arxiv.org/abs/2511.23227</guid>
<content:encoded><![CDATA[
arXiv:2511.23227v1 Announce Type: new 
Abstract: Existing convolutional learning methods for 3D point cloud data are divided into two paradigms: point-based methods that preserve geometric precision but often face performance challenges, and voxel-based methods that achieve high efficiency through quantization at the cost of geometric fidelity. This loss of precision is a critical bottleneck for tasks such as point cloud registration. We propose PointCNN++, a novel architectural design that fundamentally mitigates this precision-performance trade-off. It \textbf{generalizes sparse convolution from voxels to points}, treating voxel-based convolution as a specialized, degraded case of our more general point-based convolution. First, we introduce a point-centric convolution where the receptive field is centered on the original, high-precision point coordinates. Second, to make this high-fidelity operation performant, we design a computational strategy that operates \textbf{natively} on points. We formulate the convolution on native points as a Matrix-Vector Multiplication and Reduction (MVMR) problem, for which we develop a dedicated, highly-optimized GPU kernel. Experiments demonstrate that PointCNN++ \textbf{uses an order of magnitude less memory and is several times faster} than representative point-based methods. Furthermore, when used as a simple replacement for the voxel-based backbones it generalizes, it \textbf{significantly improves point cloud registration accuracies while proving both more memory-efficient and faster}. PointCNN++ shows that preserving geometric detail and achieving high performance are not mutually exclusive, paving the way for a new class of 3D learning with high fidelity and efficiency. Our code will be open sourced.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language-guided 3D scene synthesis for fine-grained functionality understanding</title>
<link>https://arxiv.org/abs/2511.23230</link>
<guid>https://arxiv.org/abs/2511.23230</guid>
<content:encoded><![CDATA[
arXiv:2511.23230v1 Announce Type: new 
Abstract: Functionality understanding in 3D, which aims to identify the functional element in a 3D scene to complete an action (e.g., the correct handle to "Open the second drawer of the cabinet near the bed"), is hindered by the scarcity of real-world data due to the substantial effort needed for its collection and annotation. To address this, we introduce SynthFun3D, the first method for task-based 3D scene synthesis. Given the action description, SynthFun3D generates a 3D indoor environment using a furniture asset database with part-level annotation, ensuring the action can be accomplished. It reasons about the action to automatically identify and retrieve the 3D mask of the correct functional element, enabling the inexpensive and large-scale generation of high-quality annotated data. We validate SynthFun3D through user studies, which demonstrate improved scene-prompt coherence compared to other approaches. Our quantitative results further show that the generated data can either replace real data with minor performance loss or supplement real data for improved performance, thereby providing an inexpensive and scalable solution for data-hungry 3D applications. Project page: github.com/tev-fbk/synthfun3d.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering</title>
<link>https://arxiv.org/abs/2511.23231</link>
<guid>https://arxiv.org/abs/2511.23231</guid>
<content:encoded><![CDATA[
arXiv:2511.23231v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) demonstrate strong reasoning capabilities, yet their performance in English significantly outperforms that in low-resource languages, raising fairness concerns in multilingual applications. Existing approaches either rely on costly multilingual training or employ prompting with external translation tools, both of which are resource-intensive and sensitive to translation quality. To address these limitations, we propose a training-free inference-time method to enhance Multilingual Reasoning capabilities via Representation Engineering (MRRE) without using any additional training data or tools. MRRE sequentially injects two precomputed vectors at specific layers during inference processing: cross-lingual reasoning enhancement vectors, which steer non-English reasoning representations toward English space to unlock multilingual reasoning, and target-language output anchoring vectors, which restore the distribution of the target language to preserve input-output language consistency. Comprehensive experiments across six advanced LLMs and LVLMs on four reasoning benchmarks demonstrate that MRRE consistently enhances non-English reasoning by an average gain of 5.48% and up to 7.54% in low-resource languages (Thai and Swahili), while improving input-output language consistency by 3.78%.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods</title>
<link>https://arxiv.org/abs/2511.23241</link>
<guid>https://arxiv.org/abs/2511.23241</guid>
<content:encoded><![CDATA[
arXiv:2511.23241v1 Announce Type: new 
Abstract: Reducing the burden of data generation and annotation remains a major challenge for the cost-effective deployment of machine learning in industrial and robotics settings. While synthetic rendering is a promising solution, bridging the sim-to-real gap often requires expert intervention. In this work, we benchmark a range of domain randomization (DR) and domain adaptation (DA) techniques, including feature-based methods, generative AI (GenAI), and classical rendering approaches, for creating contextualized synthetic data without manual annotation. Our evaluation focuses on the effectiveness and efficiency of low-level and high-level feature alignment, as well as a controlled diffusion-based DA method guided by prompts generated from real-world contexts. We validate our methods on two datasets: a proprietary industrial dataset (automotive and logistics) and a public robotics dataset. Results show that if render-based data with enough variability is available as seed, simpler feature-based methods, such as brightness-based and perceptual hashing filtering, outperform more complex GenAI-based approaches in both accuracy and resource efficiency. Perceptual hashing consistently achieves the highest performance, with mAP50 scores of 98% and 67% on the industrial and robotics datasets, respectively. Additionally, GenAI methods present significant time overhead for data generation at no apparent improvement of sim-to-real mAP values compared to simpler methods. Our findings offer actionable insights for efficiently bridging the sim-to-real gap, enabling high real-world performance from models trained exclusively on synthetic data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Predict Aboveground Biomass from RGB Images with 3D Synthetic Scenes</title>
<link>https://arxiv.org/abs/2511.23249</link>
<guid>https://arxiv.org/abs/2511.23249</guid>
<content:encoded><![CDATA[
arXiv:2511.23249v1 Announce Type: new 
Abstract: Forests play a critical role in global ecosystems by supporting biodiversity and mitigating climate change via carbon sequestration. Accurate aboveground biomass (AGB) estimation is essential for assessing carbon storage and wildfire fuel loads, yet traditional methods rely on labor-intensive field measurements or remote sensing approaches with significant limitations in dense vegetation. In this work, we propose a novel learning-based method for estimating AGB from a single ground-based RGB image. We frame this as a dense prediction task, introducing AGB density maps, where each pixel represents tree biomass normalized by the plot area and each tree's image area. We leverage the recently introduced synthetic 3D SPREAD dataset, which provides realistic forest scenes with per-image tree attributes (height, trunk and canopy diameter) and instance segmentation masks. Using these assets, we compute AGB via allometric equations and train a model to predict AGB density maps, integrating them to recover the AGB estimate for the captured scene. Our approach achieves a median AGB estimation error of 1.22 kg/m^2 on held-out SPREAD data and 1.94 kg/m^2 on a real-image dataset. To our knowledge, this is the first method to estimate aboveground biomass directly from a single RGB image, opening up the possibility for a scalable, interpretable, and cost-effective solution for forest monitoring, while also enabling broader participation through citizen science initiatives.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simultaneous Image Quality Improvement and Artefacts Correction in Accelerated MRI</title>
<link>https://arxiv.org/abs/2511.23274</link>
<guid>https://arxiv.org/abs/2511.23274</guid>
<content:encoded><![CDATA[
arXiv:2511.23274v1 Announce Type: new 
Abstract: MR data are acquired in the frequency domain, known as k-space. Acquiring high-quality and high-resolution MR images can be time-consuming, posing a significant challenge when multiple sequences providing complementary contrast information are needed or when the patient is unable to remain in the scanner for an extended period of time. Reducing k-space measurements is a strategy to speed up acquisition, but often leads to reduced quality in reconstructed images. Additionally, in real-world MRI, both under-sampled and full-sampled images are prone to artefacts, and correcting these artefacts is crucial for maintaining diagnostic accuracy. Deep learning methods have been proposed to restore image quality from under-sampled data, while others focused on the correction of artefacts that result from the noise or motion. No approach has however been proposed so far that addresses both acceleration and artefacts correction, limiting the performance of these models when these degradation factors occur simultaneously. To address this gap, we present a method for recovering high-quality images from under-sampled data with simultaneously correction for noise and motion artefact called USArt (Under-Sampling and Artifact correction model). Customized for 2D brain anatomical images acquired with Cartesian sampling, USArt employs a dual sub-model approach. The results demonstrate remarkable increase of signal-to-noise ratio (SNR) and contrast in the images restored. Various under-sampling strategies and degradation levels were explored, with the gradient under-sampling strategy yielding the best outcomes. We achieved up to 5x acceleration and simultaneously artefacts correction without significant degradation, showcasing the model's robustness in real-world settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FACT-GS: Frequency-Aligned Complexity-Aware Texture Reparameterization for 2D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.23292</link>
<guid>https://arxiv.org/abs/2511.23292</guid>
<content:encoded><![CDATA[
arXiv:2511.23292v1 Announce Type: new 
Abstract: Realistic scene appearance modeling has advanced rapidly with Gaussian Splatting, which enables real-time, high-quality rendering. Recent advances introduced per-primitive textures that incorporate spatial color variations within each Gaussian, improving their expressiveness. However, texture-based Gaussians parameterize appearance with a uniform per-Gaussian sampling grid, allocating equal sampling density regardless of local visual complexity. This leads to inefficient texture space utilization, where high-frequency regions are under-sampled and smooth regions waste capacity, causing blurred appearance and loss of fine structural detail. We introduce FACT-GS, a Frequency-Aligned Complexity-aware Texture Gaussian Splatting framework that allocates texture sampling density according to local visual frequency. Grounded in adaptive sampling theory, FACT-GS reformulates texture parameterization as a differentiable sampling-density allocation problem, replacing the uniform textures with a learnable frequency-aware allocation strategy implemented via a deformation field whose Jacobian modulates local sampling density. Built on 2D Gaussian Splatting, FACT-GS performs non-uniform sampling on fixed-resolution texture grids, preserving real-time performance while recovering sharper high-frequency details under the same parameter budget.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach</title>
<link>https://arxiv.org/abs/2511.23311</link>
<guid>https://arxiv.org/abs/2511.23311</guid>
<content:encoded><![CDATA[
arXiv:2511.23311v1 Announce Type: new 
Abstract: Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Perceptually Inspired Variational Framework for Color Enhancement</title>
<link>https://arxiv.org/abs/2511.23329</link>
<guid>https://arxiv.org/abs/2511.23329</guid>
<content:encoded><![CDATA[
arXiv:2511.23329v1 Announce Type: new 
Abstract: Basic phenomenology of human color vision has been widely taken as an inspiration to devise explicit color correction algorithms. The behavior of these models in terms of significative image features (such as contrast and dispersion) can be difficult to characterize. To cope with this, we propose to use a variational formulation of color contrast enhancement that is inspired by the basic phenomenology of color perception. In particular, we devise a set of basic requirements to be fulfilled by an energy to be considered as `perceptually inspired', showing that there is an explicit class of functionals satisfying all of them. We single out three explicit functionals that we consider of basic interest, showing similarities and differences with existing models. The minima of such functionals is computed using a gradient descent approach. We also present a general methodology to reduce the computational cost of the algorithms under analysis from ${\cal O}(N^2)$ to ${\cal O}(N\log N)$, being $N$ the number of input pixels.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniGeoSeg: Towards Unified Open-World Segmentation for Geospatial Scenes</title>
<link>https://arxiv.org/abs/2511.23332</link>
<guid>https://arxiv.org/abs/2511.23332</guid>
<content:encoded><![CDATA[
arXiv:2511.23332v1 Announce Type: new 
Abstract: Instruction-driven segmentation in remote sensing generates masks from guidance, offering great potential for accessible and generalizable applications. However, existing methods suffer from fragmented task formulations and limited instruction data, hindering effective understanding and generalization. To address these issues, we introduce GeoSeg-1M, the first million-scale dataset for remote sensing instruction-driven segmentation, constructed via an automatic mask filtering and instruction generation pipeline that synthesizes referring, interactive, and reasoning segmentation instructions from multiple public datasets. GeoSeg-1M contains 590K images, 117 categories, and 1.1M image-mask-instruction triplets. Building upon this foundation, we further curate GeoSeg-Bench, a challenging benchmark designed to evaluate contextual understanding and reasoning capabilities across diverse instruction-driven tasks and complex geospatial scenes. Furthermore, we present UniGeoSeg, a unified framework that serves as a strong baseline, incorporating task-aware text enhancement, latent knowledge memory, and a progressive training strategy to facilitate multi-task learning. Extensive experiments demonstrate the state-of-the-art performance of UniGeoSeg across GeoSeg-Bench and diverse public benchmarks, while exhibiting strong zero-shot generalization. Datasets and source code were released at https://github.com/MiliLab/UniGeoSeg.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Markovian Scale Prediction: A New Era of Visual Autoregressive Generation</title>
<link>https://arxiv.org/abs/2511.23334</link>
<guid>https://arxiv.org/abs/2511.23334</guid>
<content:encoded><![CDATA[
arXiv:2511.23334v1 Announce Type: new 
Abstract: Visual AutoRegressive modeling (VAR) based on next-scale prediction has revitalized autoregressive visual generation. Although its full-context dependency, i.e., modeling all previous scales for next-scale prediction, facilitates more stable and comprehensive representation learning by leveraging complete information flow, the resulting computational inefficiency and substantial overhead severely hinder VAR's practicality and scalability. This motivates us to develop a new VAR model with better performance and efficiency without full-context dependency. To address this, we reformulate VAR as a non-full-context Markov process, proposing Markov-VAR. It is achieved via Markovian Scale Prediction: we treat each scale as a Markov state and introduce a sliding window that compresses certain previous scales into a compact history vector to compensate for historical information loss owing to non-full-context dependency. Integrating the history vector with the Markov state yields a representative dynamic state that evolves under a Markov process. Extensive experiments demonstrate that Markov-VAR is extremely simple yet highly effective: Compared to VAR on ImageNet, Markov-VAR reduces FID by 10.5% (256 $\times$ 256) and decreases peak memory consumption by 83.8% (1024 $\times$ 1024). We believe that Markov-VAR can serve as a foundation for future research on visual autoregressive generation and other downstream tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Straighter and Faster: Efficient One-Step Generative Modeling via MeanFlow on Rectified Trajectories</title>
<link>https://arxiv.org/abs/2511.23342</link>
<guid>https://arxiv.org/abs/2511.23342</guid>
<content:encoded><![CDATA[
arXiv:2511.23342v1 Announce Type: new 
Abstract: Flow-based generative models have recently demonstrated strong performance, yet sampling typically relies on expensive numerical integration of ordinary differential equations (ODEs). Rectified Flow enables one-step sampling by learning nearly straight probability paths, but achieving such straightness requires multiple computationally intensive reflow iterations. MeanFlow achieves one-step generation by directly modeling the average velocity over time; however, when trained on highly curved flows, it suffers from slow convergence and noisy supervision. To address these limitations, we propose Rectified MeanFlow, a framework that models the mean velocity field along the rectified trajectory using only a single reflow step. This eliminates the need for perfectly straightened trajectories while enabling efficient training. Furthermore, we introduce a simple yet effective truncation heuristic that aims to reduce residual curvature and further improve performance. Extensive experiments on ImageNet at 64, 256, and 512 resolutions show that Re-MeanFlow consistently outperforms prior one-step flow distillation and Rectified Flow methods in both sample quality and training efficiency. Code is available at https://github.com/Xinxi-Zhang/Re-MeanFlow.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hierarchical Computer Vision Pipeline for Physiological Data Extraction from Bedside Monitors</title>
<link>https://arxiv.org/abs/2511.23355</link>
<guid>https://arxiv.org/abs/2511.23355</guid>
<content:encoded><![CDATA[
arXiv:2511.23355v1 Announce Type: new 
Abstract: In many low-resource healthcare settings, bedside monitors remain standalone legacy devices without network connectivity, creating a persistent interoperability gap that prevents seamless integration of physiological data into electronic health record (EHR) systems. To address this challenge without requiring costly hardware replacement, we present a computer vision-based pipeline for the automated capture and digitisation of vital sign data directly from bedside monitor screens. Our method employs a hierarchical detection framework combining YOLOv11 for accurate monitor and region of interest (ROI) localisation with PaddleOCR for robust text extraction. To enhance reliability across variable camera angles and lighting conditions, a geometric rectification module standardizes the screen perspective before character recognition. We evaluated the system on a dataset of 6,498 images collected from open-source corpora and real-world intensive care units in Vietnam. The model achieved a mean Average Precision (mAP@50-95) of 99.5% for monitor detection and 91.5% for vital sign ROI localisation. The end-to-end extraction accuracy exceeded 98.9% for core physiological parameters, including heart rate, oxygen saturation SpO2, and arterial blood pressure. These results demonstrate that a lightweight, camera-based approach can reliably transform unstructured information from screen captures into structured digital data, providing a practical and scalable pathway to improve information accessibility and clinical documentation in low-resource settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimScale: Learning to Drive via Real-World Simulation at Scale</title>
<link>https://arxiv.org/abs/2511.23369</link>
<guid>https://arxiv.org/abs/2511.23369</guid>
<content:encoded><![CDATA[
arXiv:2511.23369v1 Announce Type: new 
Abstract: Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline</title>
<link>https://arxiv.org/abs/2511.23377</link>
<guid>https://arxiv.org/abs/2511.23377</guid>
<content:encoded><![CDATA[
arXiv:2511.23377v1 Announce Type: new 
Abstract: Diffusion-based image editing has made semantic level image manipulation easy for general users, but it also enables realistic local forgeries that are hard to localize. Existing benchmarks mainly focus on the binary detection of generated images or the localization of manually edited regions and do not reflect the properties of diffusion-based edits, which often blend smoothly into the original content. We present Diffusion-Based Image Editing Area Localization Dataset (DEAL-300K), a large scale dataset for diffusion-based image manipulation localization (DIML) with more than 300,000 annotated images. We build DEAL-300K by using a multi-modal large language model to generate editing instructions, a mask-free diffusion editor to produce manipulated images, and an active-learning change detection pipeline to obtain pixel-level annotations. On top of this dataset, we propose a localization framework that uses a frozen Visual Foundation Model (VFM) together with Multi Frequency Prompt Tuning (MFPT) to capture both semantic and frequency-domain cues of edited regions. Trained on DEAL-300K, our method reaches a pixel-level F1 score of 82.56% on our test split and 80.97% on the external CoCoGlide benchmark, providing strong baselines and a practical foundation for future DIML research.The dataset can be accessed via https://github.com/ymhzyj/DEAL-300K.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</title>
<link>https://arxiv.org/abs/2511.23386</link>
<guid>https://arxiv.org/abs/2511.23386</guid>
<content:encoded><![CDATA[
arXiv:2511.23386v1 Announce Type: new 
Abstract: Unifying multimodal understanding, generation and reconstruction representation in a single tokenizer remains a key challenge in building unified models. Previous research predominantly attempts to address this in a dual encoder paradigm, e.g., utilizing the separate encoders for understanding and generation respectively or balancing semantic representations and low-level features with contrastive loss. In this paper, we propose VQRAE, a Vector Quantization version of Representation AutoEncoders, which pioneers the first exploration in unified representation to produce Continuous semantic features for image understanding and Discrete tokens for visual generation within a unified tokenizer. Specifically, we build upon pretrained vision foundation models with a symmetric ViT decoder and adopt a two-stage training strategy: first, it freezes the encoder and learns a high-dimensional semantic VQ codebook with pixel reconstruction objective; then jointly optimizes the encoder with self-distillation constraints. This design enables negligible semantic information for maintaining the ability of multimodal understanding, discrete tokens that are compatible for generation and fine-grained reconstruction. Besides, we identify the intriguing property in quantizing semantic encoders that rely on high-dimensional codebook in contrast to the previous common practice of low-dimensional codebook in image reconstruction. The semantic VQ codebook can achieve a 100% utilization ratio at a dimension of 1536. VQRAE presents competitive performance on several benchmarks of visual understanding, generation and reconstruction with promising scaling property in the autoregressive paradigm for its discrete merits.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MANTA: Physics-Informed Generalized Underwater Object Tracking</title>
<link>https://arxiv.org/abs/2511.23405</link>
<guid>https://arxiv.org/abs/2511.23405</guid>
<content:encoded><![CDATA[
arXiv:2511.23405v1 Announce Type: new 
Abstract: Underwater object tracking is challenging due to wavelength dependent attenuation and scattering, which severely distort appearance across depths and water conditions. Existing trackers trained on terrestrial data fail to generalize to these physics-driven degradations. We present MANTA, a physics-informed framework integrating representation learning with tracking design for underwater scenarios. We propose a dual-positive contrastive learning strategy coupling temporal consistency with Beer-Lambert augmentations to yield features robust to both temporal and underwater distortions. We further introduce a multi-stage pipeline augmenting motion-based tracking with a physics-informed secondary association algorithm that integrates geometric consistency and appearance similarity for re-identification under occlusion and drift. To complement standard IoU metrics, we propose Center-Scale Consistency (CSC) and Geometric Alignment Score (GAS) to assess geometric fidelity. Experiments on four underwater benchmarks (WebUOT-1M, UOT32, UTB180, UWCOT220) show that MANTA achieves state-of-the-art performance, improving Success AUC by up to 6 percent, while ensuring stable long-term generalized underwater tracking and efficient runtime.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DisMo: Disentangled Motion Representations for Open-World Motion Transfer</title>
<link>https://arxiv.org/abs/2511.23428</link>
<guid>https://arxiv.org/abs/2511.23428</guid>
<content:encoded><![CDATA[
arXiv:2511.23428v1 Announce Type: new 
Abstract: Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model</title>
<link>https://arxiv.org/abs/2511.23429</link>
<guid>https://arxiv.org/abs/2511.23429</guid>
<content:encoded><![CDATA[
arXiv:2511.23429v1 Announce Type: new 
Abstract: Recent advances in generative world models have enabled remarkable progress in creating open-ended game environments, evolving from static scene synthesis toward dynamic, interactive simulation. However, current approaches remain limited by rigid action schemas and high annotation costs, restricting their ability to model diverse in-game interactions and player-driven dynamics. To address these challenges, we introduce Hunyuan-GameCraft-2, a new paradigm of instruction-driven interaction for generative game world modeling. Instead of relying on fixed keyboard inputs, our model allows users to control game video contents through natural language prompts, keyboard, or mouse signals, enabling flexible and semantically rich interaction within generated worlds. We formally defined the concept of interactive video data and developed an automated process to transform large-scale, unstructured text-video pairs into causally aligned interactive datasets. Built upon a 14B image-to-video Mixture-of-Experts(MoE) foundation model, our model incorporates a text-driven interaction injection mechanism for fine-grained control over camera motion, character behavior, and environment dynamics. We introduce an interaction-focused benchmark, InterBench, to evaluate interaction performance comprehensively. Extensive experiments demonstrate that our model generates temporally coherent and causally grounded interactive game videos that faithfully respond to diverse and free-form user instructions such as "open the door", "draw a torch", or "trigger an explosion".
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-Centric Data Synthesis for Category-level Object Detection</title>
<link>https://arxiv.org/abs/2511.23450</link>
<guid>https://arxiv.org/abs/2511.23450</guid>
<content:encoded><![CDATA[
arXiv:2511.23450v1 Announce Type: new 
Abstract: Deep learning approaches to object detection have achieved reliable detection of specific object classes in images. However, extending a model's detection capability to new object classes requires large amounts of annotated training data, which is costly and time-consuming to acquire, especially for long-tailed classes with insufficient representation in existing datasets. Here, we introduce the object-centric data setting, when limited data is available in the form of object-centric data (multi-view images or 3D models), and systematically evaluate the performance of four different data synthesis methods to finetune object detection models on novel object categories in this setting. The approaches are based on simple image processing techniques, 3D rendering, and image diffusion models, and use object-centric data to synthesize realistic, cluttered images with varying contextual coherence and complexity. We assess how these methods enable models to achieve category-level generalization in real-world data, and demonstrate significant performance boosts within this data-constrained experimental setting.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Generation Tuning</title>
<link>https://arxiv.org/abs/2511.23469</link>
<guid>https://arxiv.org/abs/2511.23469</guid>
<content:encoded><![CDATA[
arXiv:2511.23469v1 Announce Type: new 
Abstract: Large Vision Language Models (VLMs) effectively bridge the modality gap through extensive pretraining, acquiring sophisticated visual representations aligned with language. However, it remains underexplored whether these representations, optimized for multimodal understanding tasks, harbor an inherent potential for visual generation. In this paper, we propose VGT, Visual Generation Tuning, a novel paradigm designed to stimulate the underlying capabilities of visual generation within any vision language models. By performing efficient visual generation tuning on well-pretrained VLMs, we significantly mitigate the alignment costs and accelerate the convergence of autoregressive modeling in the continuous space (20x speedup). Specifically, we dismiss the entangled pixel-level VAEs designed for diffusion transformers and formulate VGT-AE through aligning the semantic encoders from pretrained VLMs with the latent representations of pixel decoders. In image reconstruction tasks, we achieve 26.67 PSNR and 0.50 rFID at a 28x compression ratio, outperforming specialized VAEs; in visual generation tasks, we achieve state-of-the-art outcomes among autoregressive models, 0.77 on GenEval and 78.73 on DPG-Bench. Furthermore, our proposed VGT showcases significant scaling promise and is versatile for endowing any VLMs trained for multimodal understanding with the capabilities of visual generation, which paves the new avenue to explore next-generation unified multimodal foundation models. Models and codes are available at https://github.com/hustvl/VGT.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnyTalker: Scaling Multi-Person Talking Video Generation with Interactivity Refinement</title>
<link>https://arxiv.org/abs/2511.23475</link>
<guid>https://arxiv.org/abs/2511.23475</guid>
<content:encoded><![CDATA[
arXiv:2511.23475v1 Announce Type: new 
Abstract: Recently, multi-person video generation has started to gain prominence. While a few preliminary works have explored audio-driven multi-person talking video generation, they often face challenges due to the high costs of diverse multi-person data collection and the difficulty of driving multiple identities with coherent interactivity. To address these challenges, we propose AnyTalker, a multi-person generation framework that features an extensible multi-stream processing architecture. Specifically, we extend Diffusion Transformer's attention block with a novel identity-aware attention mechanism that iteratively processes identity-audio pairs, allowing arbitrary scaling of drivable identities. Besides, training multi-person generative models demands massive multi-person data. Our proposed training pipeline depends solely on single-person videos to learn multi-person speaking patterns and refines interactivity with only a few real multi-person clips. Furthermore, we contribute a targeted metric and dataset designed to evaluate the naturalness and interactivity of the generated multi-person videos. Extensive experiments demonstrate that AnyTalker achieves remarkable lip synchronization, visual quality, and natural interactivity, striking a favorable balance between data costs and identity scalability.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-CoM: Interactive Video Reasoning via Chain of Manipulations</title>
<link>https://arxiv.org/abs/2511.23477</link>
<guid>https://arxiv.org/abs/2511.23477</guid>
<content:encoded><![CDATA[
arXiv:2511.23477v1 Announce Type: new 
Abstract: Recent multimodal large language models (MLLMs) have advanced video understanding, yet most still "think about videos" ie once a video is encoded, reasoning unfolds entirely in text, treating visual input as a static context. This passive paradigm creates a semantic bottleneck: models cannot rewatch, refocus, or verify evidence, leading to shallow visual reasoning on tasks requiring fine grained spatio temporal understanding. In this work, we introduce Interactive Video Reasoning, a new paradigm that transforms video into an active cognitive workspace, enabling models to "think with videos". Our model, Video CoM, reasons through a Chain of Manipulations (CoM), performing iterative visual actions to gather and refine evidence. To support this behavior, we construct Video CoM Instruct, an 18K instruction tuning dataset curated for multi step manipulation reasoning. Beyond supervised learning, we further optimize the manipulation policy via reinforcement learning with reasoning aware Group Relative Policy Optimization (GRPO). Unlike prior work that relies solely on sparse answer rewards, our method introduces step level reasoning rewards, guiding the model toward grounded and consistent reasoning. Video CoM achieves strong results across nine video reasoning benchmarks, improving average performance by 3.6 percent over recent state of the art models, while training on only 25K SFT and 3K GRPO video samples, significantly fewer than comparable large scale models. Ablation studies demonstrate that reasoning aware rewards improve both accuracy and interpretability. Code: https://github.com/mbzuai-oryx/Video-CoM
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models</title>
<link>https://arxiv.org/abs/2511.23478</link>
<guid>https://arxiv.org/abs/2511.23478</guid>
<content:encoded><![CDATA[
arXiv:2511.23478v1 Announce Type: new 
Abstract: Reasoning over dynamic visual content remains a central challenge for multimodal large language models. Recent thinking models generate explicit reasoning traces for interpretability; however, their reasoning often appears convincing while being logically inconsistent or weakly grounded in visual evidence. We identify and formalize these issues through two diagnostic metrics: Think Answer Consistency (TAC), which measures the alignment between reasoning and answers, and Video Attention Score (VAS), which captures the extent to which reasoning depends on visual versus textual cues. Analysis across 11 video reasoning benchmarks shows that current models rely heavily on linguistic priors rather than visual content. To address this, we propose a reinforcement learning approach that enhances both temporal precision and reasoning consistency. Our approach combines timestamp aware supervised fine tuning with Group Relative Policy Optimization (GRPO) guided by a novel Temporal Alignment Reward (TAR). This dual step post training stage encourages temporally aligned and causally coherent video reasoning. The resulting model, Video R2, achieves consistently higher TAC, VAS, and accuracy across multiple benchmarks, demonstrating that improvements in temporal alignment and reasoning coherence lead to more accurate and trustworthy video understanding. Our code, dataset, and model will be open sourced.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion</title>
<link>https://arxiv.org/abs/2511.21542</link>
<guid>https://arxiv.org/abs/2511.21542</guid>
<content:encoded><![CDATA[
arXiv:2511.21542v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Insight-A: Attribution-aware for Multimodal Misinformation Detection</title>
<link>https://arxiv.org/abs/2511.21705</link>
<guid>https://arxiv.org/abs/2511.21705</guid>
<content:encoded><![CDATA[
arXiv:2511.21705v1 Announce Type: cross 
Abstract: AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution</title>
<link>https://arxiv.org/abs/2511.21717</link>
<guid>https://arxiv.org/abs/2511.21717</guid>
<content:encoded><![CDATA[
arXiv:2511.21717v1 Announce Type: cross 
Abstract: Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting</title>
<link>https://arxiv.org/abs/2511.21735</link>
<guid>https://arxiv.org/abs/2511.21735</guid>
<content:encoded><![CDATA[
arXiv:2511.21735v1 Announce Type: cross 
Abstract: AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&amp;T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&amp;T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&amp;T-related elements. A novel L&amp;T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAYER: A Quantitative Explainable AI Framework for Decoding Tissue-Layer Drivers of Myofascial Low Back Pain</title>
<link>https://arxiv.org/abs/2511.21767</link>
<guid>https://arxiv.org/abs/2511.21767</guid>
<content:encoded><![CDATA[
arXiv:2511.21767v1 Announce Type: cross 
Abstract: Myofascial pain (MP) is a leading cause of chronic low back pain, yet its tissue-level drivers remain poorly defined and lack reliable image biomarkers. Existing studies focus predominantly on muscle while neglecting fascia, fat, and other soft tissues that play integral biomechanical roles. We developed an anatomically grounded explainable artificial intelligence (AI) framework, LAYER (Layer-wise Analysis for Yielding Explainable Relevance Tissue), that analyses six tissue layers in three-dimensional (3D) ultrasound and quantifies their contribution to MP prediction. By utilizing the largest multi-model 3D ultrasound cohort consisting of over 4,000 scans, LAYER reveals that non-muscle tissues contribute substantially to pain prediction. In B-mode imaging, the deep fascial membrane (DFM) showed the highest saliency (0.420), while in combined B-mode and shear-wave images, the collective saliency of non-muscle layers (0.316) nearly matches that of muscle (0.317), challenging the conventional muscle-centric paradigm in MP research and potentially affecting the therapy methods. LAYER establishes a quantitative, interpretable framework for linking layer-specific anatomy to pain physiology, uncovering new tissue targets and providing a generalizable approach for explainable analysis of soft-tissue imaging.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Strategies for Synthesizing Clinical Notes for Medical Multimodal AI</title>
<link>https://arxiv.org/abs/2511.21827</link>
<guid>https://arxiv.org/abs/2511.21827</guid>
<content:encoded><![CDATA[
arXiv:2511.21827v1 Announce Type: cross 
Abstract: Multimodal (MM) learning is emerging as a promising paradigm in biomedical artificial intelligence (AI) applications, integrating complementary modality, which highlight different aspects of patient health. The scarcity of large heterogeneous biomedical MM data has restrained the development of robust models for medical AI applications. In the dermatology domain, for instance, skin lesion datasets typically include only images linked to minimal metadata describing the condition, thereby limiting the benefits of MM data integration for reliable and generalizable predictions. Recent advances in Large Language Models (LLMs) enable the synthesis of textual description of image findings, potentially allowing the combination of image and text representations. However, LLMs are not specifically trained for use in the medical domain, and their naive inclusion has raised concerns about the risk of hallucinations in clinically relevant contexts. This work investigates strategies for generating synthetic textual clinical notes, in terms of prompt design and medical metadata inclusion, and evaluates their impact on MM architectures toward enhancing performance in classification and cross-modal retrieval tasks. Experiments across several heterogeneous dermatology datasets demonstrate that synthetic clinical notes not only enhance classification performance, particularly under domain shift, but also unlock cross-modal retrieval capabilities, a downstream task that is not explicitly optimized during training.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Loop Transformers: Autoregressive Modeling as Iterative Latent Equilibrium</title>
<link>https://arxiv.org/abs/2511.21882</link>
<guid>https://arxiv.org/abs/2511.21882</guid>
<content:encoded><![CDATA[
arXiv:2511.21882v1 Announce Type: cross 
Abstract: Contemporary autoregressive transformers operate in open loop: each hidden state is computed in a single forward pass and never revised, causing errors to propagate uncorrected through the sequence. We identify this open-loop bottleneck as a fundamental architectural limitation underlying well-documented failures in long-range reasoning, factual consistency, and multi-step planning. To address this limitation, we introduce the closed-loop prediction principle, which requires that models iteratively refine latent representations until reaching a self-consistent equilibrium before committing to each token. We instantiate this principle as Equilibrium Transformers (EqT), which augment standard transformer layers with an Equilibrium Refinement Module that minimizes a learned energy function via gradient descent in latent space. The energy function enforces bidirectional prediction consistency, episodic memory coherence, and output confidence, all computed without external supervision. Theoretically, we prove that EqT performs approximate MAP inference in a latent energy-based model, establish linear convergence guarantees, and show that refinement improves predictions precisely on hard instances where one-shot inference is suboptimal. The framework unifies deep equilibrium models, diffusion language models, and test-time training as special cases. Preliminary experiments on the binary parity task demonstrate +3.28% average improvement on challenging sequences, with gains reaching +8.07% where standard transformers approach random performance, validating that the benefit of deliberation scales with task difficulty. Just as attention mechanisms resolved the sequential bottleneck of recurrent networks, we propose that closed-loop equilibrium may resolve the commitment bottleneck of open-loop autoregression, representing a foundational step toward language models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing SAM 2 and SAM 3 for Zero-Shot Segmentation of 3D Medical Data</title>
<link>https://arxiv.org/abs/2511.21926</link>
<guid>https://arxiv.org/abs/2511.21926</guid>
<content:encoded><![CDATA[
arXiv:2511.21926v1 Announce Type: cross 
Abstract: Foundation models for promptable segmentation, including SAM, SAM 2, and the recently released SAM 3, have renewed interest in zero-shot segmentation of medical imaging. Although these models perform strongly on natural images, their behavior on medical data remains insufficiently characterized. While SAM 2 is widely used for annotation in 3D medical workflows, SAM 3 introduces a new perception backbone, detector-tracker pipeline, and concept-level prompting that may alter its behavior under spatial prompts. We present the first controlled comparison of SAM 2 and SAM 3 for zero-shot segmentation of 3D medical volumes and videos under purely visual prompting, with concept mechanisms disabled. We assess whether SAM 3 can serve as an out-of-the-box replacement for SAM 2 without customization. We benchmark both models on 16 public datasets (CT, MRI, 3D and cine ultrasound, endoscopy) covering 54 anatomical structures, pathologies, and surgical instruments. Prompts are restricted to the first frame and use four modes: single-click, multi-click, bounding box, and dense mask. This design standardizes preprocessing, prompt placement, propagation rules, and metric computation to disentangle prompt interpretation from propagation. Prompt-frame analysis shows that SAM 3 provides substantially stronger initialization than SAM 2 for click prompting across most structures. In full-volume analysis, SAM 3 retains this advantage for complex, vascular, and soft-tissue anatomies, emerging as the more versatile general-purpose segmenter. While SAM 2 remains competitive for compact, rigid organs under strong spatial guidance, it frequently fails on challenging targets where SAM 3 succeeds. Overall, our results suggest that SAM 3 is the superior default choice for most medical segmentation tasks, particularly those involving sparse user interaction or complex anatomical topology.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Elevation Model Estimation from RGB Satellite Imagery using Generative Deep Learning</title>
<link>https://arxiv.org/abs/2511.21985</link>
<guid>https://arxiv.org/abs/2511.21985</guid>
<content:encoded><![CDATA[
arXiv:2511.21985v1 Announce Type: cross 
Abstract: Digital Elevation Models (DEMs) are vital datasets for geospatial applications such as hydrological modeling and environmental monitoring. However, conventional methods to generate DEM, such as using LiDAR and photogrammetry, require specific types of data that are often inaccessible in resource-constrained settings. To alleviate this problem, this study proposes an approach to generate DEM from freely available RGB satellite imagery using generative deep learning, particularly based on a conditional Generative Adversarial Network (GAN). We first developed a global dataset consisting of 12K RGB-DEM pairs using Landsat satellite imagery and NASA's SRTM digital elevation data, both from the year 2000. A unique preprocessing pipeline was implemented to select high-quality, cloud-free regions and aggregate normalized RGB composites from Landsat imagery. Additionally, the model was trained in a two-stage process, where it was first trained on the complete dataset and then fine-tuned on high-quality samples filtered by Structural Similarity Index Measure (SSIM) values to improve performance on challenging terrains. The results demonstrate promising performance in mountainous regions, achieving an overall mean root-mean-square error (RMSE) of 0.4671 and a mean SSIM score of 0.2065 (scale -1 to 1), while highlighting limitations in lowland and residential areas. This study underscores the importance of meticulous preprocessing and iterative refinement in generative modeling for DEM generation, offering a cost-effective and adaptive alternative to conventional methods while emphasizing the challenge of generalization across diverse terrains worldwide.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Do Domain-Specific Foundation Models Justify Their Cost? A Systematic Evaluation Across Retinal Imaging Tasks</title>
<link>https://arxiv.org/abs/2511.22001</link>
<guid>https://arxiv.org/abs/2511.22001</guid>
<content:encoded><![CDATA[
arXiv:2511.22001v1 Announce Type: cross 
Abstract: Large vision foundation models have been widely adopted for retinal disease classification without systematic evidence justifying their parameter requirements. In the present work we address two critical questions: First, are large domain-specific foundation models essential, or do compact general-purpose architectures suffice? Second, does specialized retinal pretraining justify its computational cost? To answer this, we benchmark initialization strategies across four retinal imaging classification tasks spanning Optical Coherence Tomography (OCT) and Color Fundus Photography (CFP) modalities: 8-class OCT classification, 3-class diabetic macular edema (DME), 5-class diabetic retinopathy (DR), and 3-class glaucoma (GL) detection. We evaluate 12-13 model configurations per task, including vision transformers (22.8M-86.6M parameters), Swin Transformers (27.6M-28.3M), ConvNeXt (28.6M), and the domain-specific RETFound models (303M), under identical training conditions. Our results challenge prevailing assumptions: First, we demonstrate that pretraining provides universal benefits (5.18-18.41% improvement), scaling with task difficulty. Second, compact architectures (27-29M) dominate Pareto frontiers; SwinV2-tiny achieves top-1 performance on three datasets. Third, RETFound (303M) justifies its computational cost only for challenging DR grading (accuracy of 71.15%), while ImageNet pretraining proves to be sufficient with all other tasks (DME accuracy: 99.24%, OCT accuracy: 97.96%). CFP tasks show larger pretraining accuracy gains (9.13-18.41%) than OCT (5.18%). Thus, the evidence suggests that compact general-purpose models deliver near-optimal performance for most retinal classification tasks; specialized foundation models warranted only for fine-grained discrimination under extreme class imbalance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GACELLE: GPU-accelerated tools for model parameter estimation and image reconstruction</title>
<link>https://arxiv.org/abs/2511.22094</link>
<guid>https://arxiv.org/abs/2511.22094</guid>
<content:encoded><![CDATA[
arXiv:2511.22094v1 Announce Type: cross 
Abstract: Quantitative MRI (qMRI) offers tissue-specific biomarkers that can be tracked over time or compared across populations; however, its adoption in clinical research is hindered by significant computational demands of parameter estimation. Images acquired at high spatial resolution or requiring fitting multiple parameters often require lengthy processing time, constraining their use in routine pipelines and slowing methodological innovation and clinical translation.
  We present GACELLE, an open source, GPU-accelerated framework for high-throughput qMRI analysis. GACELLE provides a stochastic gradient descent optimiser and a stochastic sampler in MATLAB, enabling fast parameter mapping, improved estimation robustness via spatial regularisation, and uncertainty quantification. GACELLE prioritises accessibility: users only need to provide a forward signal model, while GACELLE's backend manages computational parallelisation, automatic parameter updates, and memory-batching. The stochastic solver performs fully vectorised Markov chain Monte Carlo with identical likelihood on CPU and GPU, ensuring reproducibility across hardware.
  Benchmarking demonstrates up to 451-fold acceleration for the stochastic gradient descent solver and 14,380-fold acceleration for stochastic sampling compared to CPU-based estimation, without compromising accuracy. We demonstrated GACELLE's versatility on three representative qMRI models and on an image reconstruction task. Across these applications, GACELLE improves parameter precision, enhances test-retest reproducibility, and reduces noise in quantitative maps.
  By combining speed, usability and flexibility, GACELLE provides a generalisable optimisation framework for medical image analysis. It lowers the computational barrier for qMRI, paving the way for reproducible biomarker development, large-scale imaging studies, and clinical translation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Instance-Level Sampling Schedules via REINFORCE with James-Stein Shrinkage</title>
<link>https://arxiv.org/abs/2511.22177</link>
<guid>https://arxiv.org/abs/2511.22177</guid>
<content:encoded><![CDATA[
arXiv:2511.22177v1 Announce Type: cross 
Abstract: Most post-training methods for text-to-image samplers focus on model weights: either fine-tuning the backbone for alignment or distilling it for few-step efficiency. We take a different route: rescheduling the sampling timeline of a frozen sampler. Instead of a fixed, global schedule, we learn instance-level (prompt- and noise-conditioned) schedules through a single-pass Dirichlet policy. To ensure accurate gradient estimates in high-dimensional policy learning, we introduce a novel reward baseline based on a principled James-Stein estimator; it provably achieves lower estimation errors than commonly used variants and leads to superior performance. Our rescheduled samplers consistently improve text-image alignment including text rendering and compositional control across modern Stable Diffusion and Flux model families. Additionally, a 5-step Flux-Dev sampler with our schedules can attain generation quality comparable to deliberately distilled samplers like Flux-Schnell. We thus position our scheduling framework as an emerging model-agnostic post-training lever that unlocks additional generative potential in pretrained samplers.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIGROTD: A Friendly-to-Handle Dataset for Image Guided Retrieval with Optional Text</title>
<link>https://arxiv.org/abs/2511.22247</link>
<guid>https://arxiv.org/abs/2511.22247</guid>
<content:encoded><![CDATA[
arXiv:2511.22247v1 Announce Type: cross 
Abstract: Image-Guided Retrieval with Optional Text (IGROT) unifies visual retrieval (without text) and composed retrieval (with text). Despite its relevance in applications like Google Image and Bing, progress has been limited by the lack of an accessible benchmark and methods that balance performance across subtasks. Large-scale datasets such as MagicLens are comprehensive but computationally prohibitive, while existing models often favor either visual or compositional queries. We introduce FIGROTD, a lightweight yet high-quality IGROT dataset with 16,474 training triplets and 1,262 test triplets across CIR, SBIR, and CSTBIR. To reduce redundancy, we propose the Variance Guided Feature Mask (VaGFeM), which selectively enhances discriminative dimensions based on variance statistics. We further adopt a dual-loss design (InfoNCE + Triplet) to improve compositional reasoning. Trained on FIGROTD, VaGFeM achieves competitive results on nine benchmarks, reaching 34.8 mAP@10 on CIRCO and 75.7 mAP@200 on Sketchy, outperforming stronger baselines despite fewer triplets.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ColonAdapter: Geometry Estimation Through Foundation Model Adaptation for Colonoscopy</title>
<link>https://arxiv.org/abs/2511.22250</link>
<guid>https://arxiv.org/abs/2511.22250</guid>
<content:encoded><![CDATA[
arXiv:2511.22250v1 Announce Type: cross 
Abstract: Estimating 3D geometry from monocular colonoscopy images is challenging due to non-Lambertian surfaces, moving light sources, and large textureless regions. While recent 3D geometric foundation models eliminate the need for multi-stage pipelines, their performance deteriorates in clinical scenes. These models are primarily trained on natural scene datasets and struggle with specularity and homogeneous textures typical in colonoscopy, leading to inaccurate geometry estimation. In this paper, we present ColonAdapter, a self-supervised fine-tuning framework that adapts geometric foundation models for colonoscopy geometry estimation. Our method leverages pretrained geometric priors while tailoring them to clinical data. To improve performance in low-texture regions and ensure scale consistency, we introduce a Detail Restoration Module (DRM) and a geometry consistency loss. Furthermore, a confidence-weighted photometric loss enhances training stability in clinical environments. Experiments on both synthetic and real datasets demonstrate that our approach achieves state-of-the-art performance in camera pose estimation, monocular depth prediction, and dense 3D point map reconstruction, without requiring ground-truth intrinsic parameters.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNION: A Lightweight Target Representation for Efficient Zero-Shot Image-Guided Retrieval with Optional Textual Queries</title>
<link>https://arxiv.org/abs/2511.22253</link>
<guid>https://arxiv.org/abs/2511.22253</guid>
<content:encoded><![CDATA[
arXiv:2511.22253v1 Announce Type: cross 
Abstract: Image-Guided Retrieval with Optional Text (IGROT) is a general retrieval setting where a query consists of an anchor image, with or without accompanying text, aiming to retrieve semantically relevant target images. This formulation unifies two major tasks: Composed Image Retrieval (CIR) and Sketch-Based Image Retrieval (SBIR). In this work, we address IGROT under low-data supervision by introducing UNION, a lightweight and generalisable target representation that fuses the image embedding with a null-text prompt. Unlike traditional approaches that rely on fixed target features, UNION enhances semantic alignment with multimodal queries while requiring no architectural modifications to pretrained vision-language models. With only 5,000 training samples - from LlavaSCo for CIR and Training-Sketchy for SBIR - our method achieves competitive results across benchmarks, including CIRCO mAP@50 of 38.5 and Sketchy mAP@200 of 82.7, surpassing many heavily supervised baselines. This demonstrates the robustness and efficiency of UNION in bridging vision and language across diverse query types.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Content Adaptive Encoding For Interactive Game Streaming</title>
<link>https://arxiv.org/abs/2511.22327</link>
<guid>https://arxiv.org/abs/2511.22327</guid>
<content:encoded><![CDATA[
arXiv:2511.22327v1 Announce Type: cross 
Abstract: Video-on-demand streaming has benefitted from \textit{content-adaptive encoding} (CAE), i.e., adaptation of resolution and/or quantization parameters for each scene based on convex hull optimization. However, CAE is very challenging to develop and deploy for interactive game streaming (IGS). Commercial IGS services impose ultra-low latency encoding with no lookahead or buffering, and have extremely tight compute constraints for any CAE algorithm execution. We propose the first CAE approach for resolution adaptation in IGS based on compact encoding metadata from past frames. Specifically, we train a convolutional neural network (CNN) to infer the best resolution from the options available for the upcoming scene based on a running window of aggregated coding block statistics from the current scene. By deploying the trained CNN within a practical IGS setup based on HEVC encoding, our proposal: (i) improves over the default fixed-resolution ladder of HEVC by 2.3 Bj{\o}ntegaard Delta-VMAF points; (ii) infers using 1ms of a single CPU core per scene, thereby having no latency overhead.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEO-Detective: Unveiling Location Privacy Risks in Images with LLM Agents</title>
<link>https://arxiv.org/abs/2511.22441</link>
<guid>https://arxiv.org/abs/2511.22441</guid>
<content:encoded><![CDATA[
arXiv:2511.22441v1 Announce Type: cross 
Abstract: Images shared on social media often expose geographic cues. While early geolocation methods required expert effort and lacked generalization, the rise of Large Vision Language Models (LVLMs) now enables accurate geolocation even for ordinary users. However, existing approaches are not optimized for this task. To explore the full potential and associated privacy risks, we present Geo-Detective, an agent that mimics human reasoning and tool use for image geolocation inference. It follows a procedure with four steps that adaptively selects strategies based on image difficulty and is equipped with specialized tools such as visual reverse search, which emulates how humans gather external geographic clues. Experimental results show that GEO-Detective outperforms baseline large vision language models (LVLMs) overall, particularly on images lacking visible geographic features. In country level geolocation tasks, it achieves an improvement of over 11.1% compared to baseline LLMs, and even at finer grained levels, it still provides around a 5.2% performance gain. Meanwhile, when equipped with external clues, GEO-Detective becomes more likely to produce accurate predictions, reducing the "unknown" prediction rate by more than 50.6%. We further explore multiple defense strategies and find that Geo-Detective exhibits stronger robustness, highlighting the need for more effective privacy safeguards.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Is the Optimal Ranking Score Between Precision and Recall? We Can Always Find It and It Is Rarely $F_1$</title>
<link>https://arxiv.org/abs/2511.22442</link>
<guid>https://arxiv.org/abs/2511.22442</guid>
<content:encoded><![CDATA[
arXiv:2511.22442v1 Announce Type: cross 
Abstract: Ranking methods or models based on their performance is of prime importance but is tricky because performance is fundamentally multidimensional. In the case of classification, precision and recall are scores with probabilistic interpretations that are both important to consider and complementary. The rankings induced by these two scores are often in partial contradiction. In practice, therefore, it is extremely useful to establish a compromise between the two views to obtain a single, global ranking. Over the last fifty years or so,it has been proposed to take a weighted harmonic mean, known as the F-score, F-measure, or $F_\beta$. Generally speaking, by averaging basic scores, we obtain a score that is intermediate in terms of values. However, there is no guarantee that these scores lead to meaningful rankings and no guarantee that the rankings are good tradeoffs between these base scores. Given the ubiquity of $F_\beta$ scores in the literature, some clarification is in order. Concretely: (1) We establish that $F_\beta$-induced rankings are meaningful and define a shortest path between precision- and recall-induced rankings. (2) We frame the problem of finding a tradeoff between two scores as an optimization problem expressed with Kendall rank correlations. We show that $F_1$ and its skew-insensitive version are far from being optimal in that regard. (3) We provide theoretical tools and a closed-form expression to find the optimal value for $\beta$ for any distribution or set of performances, and we illustrate their use on six case studies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Flow Models</title>
<link>https://arxiv.org/abs/2511.22475</link>
<guid>https://arxiv.org/abs/2511.22475</guid>
<content:encoded><![CDATA[
arXiv:2511.22475v1 Announce Type: cross 
Abstract: We present adversarial flow models, a class of generative models that unifies adversarial models and flow models. Our method supports native one-step or multi-step generation and is trained using the adversarial objective. Unlike traditional GANs, where the generator learns an arbitrary transport plan between the noise and the data distributions, our generator learns a deterministic noise-to-data mapping, which is the same optimal transport as in flow-matching models. This significantly stabilizes adversarial training. Also, unlike consistency-based methods, our model directly learns one-step or few-step generation without needing to learn the intermediate timesteps of the probability flow for propagation. This saves model capacity, reduces training iterations, and avoids error accumulation. Under the same 1NFE setting on ImageNet-256px, our B/2 model approaches the performance of consistency-based XL/2 models, while our XL/2 model creates a new best FID of 2.38. We additionally show the possibility of end-to-end training of 56-layer and 112-layer models through depth repetition without any intermediate supervision, and achieve FIDs of 2.08 and 1.94 using a single forward pass, surpassing their 2NFE and 4NFE counterparts.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion</title>
<link>https://arxiv.org/abs/2511.22505</link>
<guid>https://arxiv.org/abs/2511.22505</guid>
<content:encoded><![CDATA[
arXiv:2511.22505v1 Announce Type: cross 
Abstract: Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hard Spatial Gating for Precision-Driven Brain Metastasis Segmentation: Addressing the Over-Segmentation Paradox in Deep Attention Networks</title>
<link>https://arxiv.org/abs/2511.22606</link>
<guid>https://arxiv.org/abs/2511.22606</guid>
<content:encoded><![CDATA[
arXiv:2511.22606v1 Announce Type: cross 
Abstract: Brain metastasis segmentation in MRI remains a formidable challenge due to diminutive lesion sizes (5-15 mm) and extreme class imbalance (less than 2% tumor volume). While soft-attention CNNs are widely used, we identify a critical failure mode termed the "over-segmentation paradox," where models achieve high sensitivity (recall > 0.88) but suffer from catastrophic precision collapse (precision < 0.23) and boundary errors exceeding 150 mm. This imprecision poses significant risks for stereotactic radiosurgery planning. To address this, we introduce the Spatial Gating Network (SG-Net), a precision-first architecture employing hard spatial gating mechanisms. Unlike traditional soft attention, SG-Net enforces strict feature selection to aggressively suppress background artifacts while preserving tumor features. Validated on the Brain-Mets-Lung-MRI dataset (n=92), SG-Net achieves a Dice Similarity Coefficient of 0.5578 +/- 0.0243 (95% CI: 0.45-0.67), statistically outperforming Attention U-Net (p < 0.001) and ResU-Net (p < 0.001). Most critically, SG-Net demonstrates a threefold improvement in boundary precision, achieving a 95% Hausdorff Distance of 56.13 mm compared to 157.52 mm for Attention U-Net, while maintaining robust recall (0.79) and superior precision (0.52 vs. 0.20). Furthermore, SG-Net requires only 0.67M parameters (8.8x fewer than Attention U-Net), facilitating deployment in resource-constrained environments. These findings establish hard spatial gating as a robust solution for precision-driven lesion detection, directly enhancing radiosurgery accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometrically-Constrained Agent for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2511.22659</link>
<guid>https://arxiv.org/abs/2511.22659</guid>
<content:encoded><![CDATA[
arXiv:2511.22659v1 Announce Type: cross 
Abstract: Vision Language Models (VLMs) exhibit a fundamental semantic-to-geometric gap in spatial reasoning: they excel at qualitative semantic inference but their reasoning operates within a lossy semantic space, misaligned with high-fidelity geometry. Current paradigms fail to bridge this gap. Training-based methods suffer from an ``oracle paradox,'' learning flawed spatial logic from imperfect oracles. Tool-integrated methods constrain the final computation but critically leave the VLM's planning process unconstrained, resulting in geometrically flawed plans. In this work, we propose Geometrically-Constrained Agent (GCA), a training-free agentic paradigm that resolves this gap by introducing a formal task constraint. Specifically, we strategically decouples the VLM's role into two stages. First, acting as a semantic analyst, the VLM translates the user's ambiguous query into the formal, verifiable task constraint, which defines the reference frame and objective. Second, acting as a task solver, the VLM generates and executes tool calls strictly within the deterministic bounds defined by the constraint. This geometrically-constrained reasoning strategy successfully resolve the semantic-to-geometric gap, yielding a robust and verifiable reasoning pathway for spatial reasoning. Comprehensive experiments demonstrate that GCA achieves SOTA performance on multiple spatial reasoning benchmarks, surpassing existing training-based and tool-integrated methods by ~27%. Please see our homepage at https://gca-spatial-reasoning.github.io.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Preserving Unpaired Image Translation to Photometrically Calibrate JunoCam with Hubble Data</title>
<link>https://arxiv.org/abs/2511.22668</link>
<guid>https://arxiv.org/abs/2511.22668</guid>
<content:encoded><![CDATA[
arXiv:2511.22668v1 Announce Type: cross 
Abstract: Insights into Jupiter's atmospheric dynamics are vital for understanding planetary meteorology and exoplanetary gas giant atmospheres. To study these dynamics, we require high-resolution, photometrically calibrated observations. Over the last 9 years, the Juno spacecraft's optical camera, JunoCam, has generated a unique dataset with high spatial resolution, wide coverage during perijove passes, and a long baseline. However, JunoCam lacks absolute photometric calibration, hindering quantitative analysis of the Jovian atmosphere. Using observations from the Hubble Space Telescope (HST) as a proxy for a calibrated sensor, we present a novel method for performing unpaired image-to-image translation (I2I) between JunoCam and HST, focusing on addressing the resolution discrepancy between the two sensors. Our structure-preserving I2I method, SP-I2I, incorporates explicit frequency-space constraints designed to preserve high-frequency features ensuring the retention of fine, small-scale spatial structures - essential for studying Jupiter's atmosphere. We demonstrate that state-of-the-art unpaired image-to-image translation methods are inadequate to address this problem, and, importantly, we show the broader impact of our proposed solution on relevant remote sensing data for the pansharpening task.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations</title>
<link>https://arxiv.org/abs/2511.22697</link>
<guid>https://arxiv.org/abs/2511.22697</guid>
<content:encoded><![CDATA[
arXiv:2511.22697v1 Announce Type: cross 
Abstract: Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distracted Robot: How Visual Clutter Undermine Robotic Manipulation</title>
<link>https://arxiv.org/abs/2511.22780</link>
<guid>https://arxiv.org/abs/2511.22780</guid>
<content:encoded><![CDATA[
arXiv:2511.22780v1 Announce Type: cross 
Abstract: In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARVO: Marine-Adaptive Radiance-aware Visual Odometry</title>
<link>https://arxiv.org/abs/2511.22860</link>
<guid>https://arxiv.org/abs/2511.22860</guid>
<content:encoded><![CDATA[
arXiv:2511.22860v1 Announce Type: cross 
Abstract: Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Modalities via Progressive Re-alignment for Multimodal Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2511.22862</link>
<guid>https://arxiv.org/abs/2511.22862</guid>
<content:encoded><![CDATA[
arXiv:2511.22862v1 Announce Type: cross 
Abstract: Test-time adaptation (TTA) enables online model adaptation using only unlabeled test data, aiming to bridge the gap between source and target distributions. However, in multimodal scenarios, varying degrees of distribution shift across different modalities give rise to a complex coupling effect of unimodal shallow feature shift and cross-modal high-level semantic misalignment, posing a major obstacle to extending existing TTA methods to the multimodal field. To address this challenge, we propose a novel multimodal test-time adaptation (MMTTA) framework, termed as Bridging Modalities via Progressive Re-alignment (BriMPR). BriMPR, consisting of two progressively enhanced modules, tackles the coupling effect with a divide-and-conquer strategy. Specifically, we first decompose MMTTA into multiple unimodal feature alignment sub-problems. By leveraging the strong function approximation ability of prompt tuning, we calibrate the unimodal global feature distributions to their respective source distributions, so as to achieve the initial semantic re-alignment across modalities. Subsequently, we assign the credible pseudo-labels to combinations of masked and complete modalities, and introduce inter-modal instance-wise contrastive learning to further enhance the information interaction among modalities and refine the alignment. Extensive experiments on MMTTA tasks, including both corruption-based and real-world domain shift benchmarks, demonstrate the superiority of our method. Our source code is available at [this URL](https://github.com/Luchicken/BriMPR).
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.22865</link>
<guid>https://arxiv.org/abs/2511.22865</guid>
<content:encoded><![CDATA[
arXiv:2511.22865v1 Announce Type: cross 
Abstract: End-to-End (E2E) planning has become a powerful paradigm for autonomous driving, yet current systems remain fundamentally uncertainty-blind. They assume perception outputs are fully reliable, even in ambiguous or poorly observed scenes, leaving the planner without an explicit measure of uncertainty. To address this limitation, we propose a camera-only E2E framework that estimates aleatoric uncertainty directly in BEV space and incorporates it into planning. Our method produces a dense, uncertainty-aware drivability map that captures both semantic structure and geometric layout at pixel-level resolution. To further promote safe and rule-compliant behavior, we introduce a lane-following regularization that encodes lane structure and traffic norms. This prior stabilizes trajectory planning under normal conditions while preserving the flexibility needed for maneuvers such as overtaking or lane changes. Together, these components enable robust and interpretable trajectory planning, even under challenging uncertainty conditions. Evaluated on the NAVSIM benchmark, our method achieves state-of-the-art performance, delivering substantial gains on both the challenging NAVHARD and NAVSAFE subsets. These results demonstrate that our principled aleatoric uncertainty modeling combined with driving priors significantly advances the safety and reliability of camera-only E2E autonomous driving.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MICCAI STS 2024 Challenge: Semi-Supervised Instance-Level Tooth Segmentation in Panoramic X-ray and CBCT Images</title>
<link>https://arxiv.org/abs/2511.22911</link>
<guid>https://arxiv.org/abs/2511.22911</guid>
<content:encoded><![CDATA[
arXiv:2511.22911v1 Announce Type: cross 
Abstract: Orthopantomogram (OPGs) and Cone-Beam Computed Tomography (CBCT) are vital for dentistry, but creating large datasets for automated tooth segmentation is hindered by the labor-intensive process of manual instance-level annotation. This research aimed to benchmark and advance semi-supervised learning (SSL) as a solution for this data scarcity problem. We organized the 2nd Semi-supervised Teeth Segmentation (STS 2024) Challenge at MICCAI 2024. We provided a large-scale dataset comprising over 90,000 2D images and 3D axial slices, which includes 2,380 OPG images and 330 CBCT scans, all featuring detailed instance-level FDI annotations on part of the data. The challenge attracted 114 (OPG) and 106 (CBCT) registered teams. To ensure algorithmic excellence and full transparency, we rigorously evaluated the valid, open-source submissions from the top 10 (OPG) and top 5 (CBCT) teams, respectively. All successful submissions were deep learning-based SSL methods. The winning semi-supervised models demonstrated impressive performance gains over a fully-supervised nnU-Net baseline trained only on the labeled data. For the 2D OPG track, the top method improved the Instance Affinity (IA) score by over 44 percentage points. For the 3D CBCT track, the winning approach boosted the Instance Dice score by 61 percentage points. This challenge confirms the substantial benefit of SSL for complex, instance-level medical image segmentation tasks where labeled data is scarce. The most effective approaches consistently leveraged hybrid semi-supervised frameworks that combined knowledge from foundational models like SAM with multi-stage, coarse-to-fine refinement pipelines. Both the challenge dataset and the participants' submitted code have been made publicly available on GitHub (https://github.com/ricoleehduu/STS-Challenge-2024), ensuring transparency and reproducibility.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework</title>
<link>https://arxiv.org/abs/2511.22943</link>
<guid>https://arxiv.org/abs/2511.22943</guid>
<content:encoded><![CDATA[
arXiv:2511.22943v1 Announce Type: cross 
Abstract: We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geodiffussr: Generative Terrain Texturing with Elevation Fidelity</title>
<link>https://arxiv.org/abs/2511.23029</link>
<guid>https://arxiv.org/abs/2511.23029</guid>
<content:encoded><![CDATA[
arXiv:2511.23029v1 Announce Type: cross 
Abstract: Large-scale terrain generation remains a labor-intensive task in computer graphics. We introduce Geodiffussr, a flow-matching pipeline that synthesizes text-guided texture maps while strictly adhering to a supplied Digital Elevation Map (DEM). The core mechanism is multi-scale content aggregation (MCA): DEM features from a pretrained encoder are injected into UNet blocks at multiple resolutions to enforce global-to-local elevation consistency. Compared with a non-MCA baseline, MCA markedly improves visual fidelity and strengthens height-appearance coupling (FID $\downarrow$ 49.16%, LPIPS $\downarrow$ 32.33%, $\Delta$dCor $\downarrow$ to 0.0016). To train and evaluate Geodiffussr, we assemble a globally distributed, biome- and climate-stratified corpus of triplets pairing SRTM-derived DEMs with Sentinel-2 imagery and vision-grounded natural-language captions that describe visible land cover. We position Geodiffussr as a strong baseline and step toward controllable 2.5D landscape generation for coarse-scale ideation and previz, complementary to physically based terrain and ecosystem simulators.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management</title>
<link>https://arxiv.org/abs/2511.23030</link>
<guid>https://arxiv.org/abs/2511.23030</guid>
<content:encoded><![CDATA[
arXiv:2511.23030v1 Announce Type: cross 
Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Obstruction reasoning for robotic grasping</title>
<link>https://arxiv.org/abs/2511.23186</link>
<guid>https://arxiv.org/abs/2511.23186</guid>
<content:encoded><![CDATA[
arXiv:2511.23186v1 Announce Type: cross 
Abstract: Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies</title>
<link>https://arxiv.org/abs/2511.23225</link>
<guid>https://arxiv.org/abs/2511.23225</guid>
<content:encoded><![CDATA[
arXiv:2511.23225v1 Announce Type: cross 
Abstract: Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Scientific Visualization: Ensemble Data Analysis</title>
<link>https://arxiv.org/abs/2511.23290</link>
<guid>https://arxiv.org/abs/2511.23290</guid>
<content:encoded><![CDATA[
arXiv:2511.23290v1 Announce Type: cross 
Abstract: Scientific simulations and experimental measurements produce vast amounts of spatio-temporal data, yet extracting meaningful insights remains challenging due to high dimensionality, complex structures, and missing information. Traditional analysis methods often struggle with these issues, motivating the need for more robust, data-driven approaches. This dissertation explores deep learning methodologies to improve the analysis and visualization of spatio-temporal scientific ensembles, focusing on dimensionality reduction, flow estimation, and temporal interpolation. First, we address high-dimensional data representation through autoencoder-based dimensionality reduction for scientific ensembles. We evaluate the stability of projection metrics under partial labeling and introduce a Pareto-efficient selection strategy to identify optimal autoencoder variants, ensuring expressive and reliable low-dimensional embeddings. Next, we present FLINT, a deep learning model for high-quality flow estimation and temporal interpolation in both flow-supervised and flow-unsupervised settings. FLINT reconstructs missing velocity fields and generates high-fidelity temporal interpolants for scalar fields across 2D+time and 3D+time ensembles without domain-specific assumptions or extensive finetuning. To further improve adaptability and generalization, we introduce HyperFLINT, a hypernetwork-based approach that conditions on simulation parameters to estimate flow fields and interpolate scalar data. This parameter-aware adaptation yields more accurate reconstructions across diverse scientific domains, even with sparse or incomplete data. Overall, this dissertation advances deep learning techniques for scientific visualization, providing scalable, adaptable, and high-quality solutions for interpreting complex spatio-temporal ensembles.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Multimodal Language Models through Attention-based Interpretability</title>
<link>https://arxiv.org/abs/2511.23375</link>
<guid>https://arxiv.org/abs/2511.23375</guid>
<content:encoded><![CDATA[
arXiv:2511.23375v1 Announce Type: cross 
Abstract: Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Thermophysical Property Retrieval</title>
<link>https://arxiv.org/abs/2511.23449</link>
<guid>https://arxiv.org/abs/2511.23449</guid>
<content:encoded><![CDATA[
arXiv:2511.23449v1 Announce Type: cross 
Abstract: Inverse heat problems refer to the estimation of material thermophysical properties given observed or known heat diffusion behaviour. Inverse heat problems have wide-ranging uses, but a critical application lies in quantifying how building facade renovation reduces thermal transmittance, a key determinant of building energy efficiency. However, solving inverse heat problems with non-invasive data collected in situ is error-prone due to environmental variability or deviations from theoretically assumed conditions. Hence, current methods for measuring thermal conductivity are either invasive, require lengthy observation periods, or are sensitive to environmental and experimental conditions. Here, we present a PINN-based iterative framework to estimate the thermal conductivity k of a wall from a set of thermographs; our framework alternates between estimating the forward heat problem with a PINN for a fixed k, and optimizing k by comparing the thermographs and surface temperatures predicted by the PINN, repeating until the estimated k's convergence. Using both environmental data captured by a weather station and data generated from Finite-Volume-Method software simulations, we accurately predict k across different environmental conditions and data collection sampling times, given the temperature profile of the wall at dawn is close to steady state. Although violating the steady-state assumption impacts the accuracy of k's estimation, we show that our proposed framework still only exhibits a maximum MAE of 4.0851. Our work demonstrates the potential of PINN-based methods for reliable estimation of material properties in situ and under realistic conditions, without lengthy measurement campaigns. Given the lack of research on using machine learning, and more specifically on PINNs, for solving in-situ inverse problems, we expect our work to be a starting point for more research on the topic.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Total Least Square Optimal Analytic Signal by Structure Tensor for N-D images</title>
<link>https://arxiv.org/abs/2005.08108</link>
<guid>https://arxiv.org/abs/2005.08108</guid>
<content:encoded><![CDATA[
arXiv:2005.08108v2 Announce Type: replace 
Abstract: We produce the analytic signal by using the Structure Tensor, which provides Total Least Squares optimal vectors for estimating orientation and scale locally. Together, these vectors represent N-D frequency components that determine adaptive, complex probing filters. The N-D analytic signal is obtained through scalar products of adaptive filters with image neighborhoods. It comprises orientation, scale, phase, and amplitude information of the neighborhood. The ST analytic signal $ f_A $ is continuous and isotropic, and its extension to N-D is straightforward. The phase gradient can be represented as a vector (instantaneous frequency) or as a tensor. Both are continuous and isotropic, while the tensor additionally preserves continuity of orientation and retains the same information as the vector representation. The tensor representation can also be used to detect singularities. Detection with known phase portraits has been demonstrated in 2-D with relevance to fringe pattern processing in wave physics, including optics and fingerprint measurements. To construct adaptive filters we have used Gabor filter family members as probing functions, but other function families can also be used to sample the spectrum, e.g., quadrature filters. A comparison to three baseline alternatives-in representation (Monogenic signal), enhancement (Monogenic signal combined with a spline-wavelet pyramid), and singularity detection (mindtct, a fingerprint minutia detector widely used in numerous studies)-is also reported using images with precisely known ground truths for location, orientation, singularity type (where applicable), and wave period.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Source-free Video Domain Adaptation by Learning from Noisy Labels</title>
<link>https://arxiv.org/abs/2311.18572</link>
<guid>https://arxiv.org/abs/2311.18572</guid>
<content:encoded><![CDATA[
arXiv:2311.18572v2 Announce Type: replace 
Abstract: Despite the progress seen in classification methods, current approaches for handling videos with distribution shifts in source and target domains remain source-dependent as they require access to the source data during the adaptation stage. In this paper, we present a self-training based source-free video domain adaptation approach to address this challenge by bridging the gap between the source and the target domains. We use the source pre-trained model to generate pseudo-labels for the target domain samples, which are inevitably noisy. Thus, we treat the problem of source-free video domain adaptation as learning from noisy labels and argue that the samples with correct pseudo-labels can help us in adaptation. To this end, we leverage the cross-entropy loss as an indicator of the correctness of the pseudo-labels and use the resulting small-loss samples from the target domain for fine-tuning the model. We further enhance the adaptation performance by implementing a teacher-student (TS) framework, in which the teacher, which is updated gradually, produces reliable pseudo-labels. Meanwhile, the student undergoes fine-tuning on the target domain videos using these generated pseudo-labels to improve its performance. Extensive experimental evaluations show that our methods, termed as CleanAdapt, CleanAdapt + TS, achieve state-of-the-art results, outperforming the existing approaches on various open datasets. Our source code is publicly available at https://avijit9.github.io/CleanAdapt.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding in Novel Domains</title>
<link>https://arxiv.org/abs/2311.18773</link>
<guid>https://arxiv.org/abs/2311.18773</guid>
<content:encoded><![CDATA[
arXiv:2311.18773v4 Announce Type: replace 
Abstract: Learning from (procedural) videos has increasingly served as a pathway for embodied agents to acquire skills from human demonstrations. To do this, video understanding models must be able to obtain structured understandings, such as the temporal segmentation of a demonstration into sequences of actions and skills, and to generalize the understandings to novel environments, tasks, and problem domains. In pursuit of this goal, we introduce Spacewalk-18, a benchmark containing two tasks: (1) step recognition and (2) video question answering, over a dataset of temporally segmented and labeled tasks in International Space Station spacewalk recordings. In tandem, the two tasks quantify a model's ability to: (1) generalize to novel domains; (2) utilize long temporal context and multimodal (e.g. visual and speech) information. Our extensive experimental analysis highlights the challenges of Spacewalk-18, but also suggests best practices for domain generalization and long-form understanding. Notably, we discover a promising adaptation via summarization technique that leads to significant performance improvement without model fine-tuning. The Spacewalk-18 benchmark is released at https://brown-palm.github.io/Spacewalk-18/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infrared and Visible Image Fusion with Language-Driven Loss in CLIP Embedding Space</title>
<link>https://arxiv.org/abs/2402.16267</link>
<guid>https://arxiv.org/abs/2402.16267</guid>
<content:encoded><![CDATA[
arXiv:2402.16267v3 Announce Type: replace 
Abstract: Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities. Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically. As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited. In this paper, we propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance. For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the multi-modal embedding space using CLIP. A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors representing the fusion objective and input image modalities. Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via supervised training. Experiments show that our method can obtain much better fusion results than existing techniques. The code is available at https://github.com/wyhlaowang/LDFusion.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Configurable Fairness: Direct Optimization of Parity Metrics via Vision-Language Models</title>
<link>https://arxiv.org/abs/2403.10624</link>
<guid>https://arxiv.org/abs/2403.10624</guid>
<content:encoded><![CDATA[
arXiv:2403.10624v3 Announce Type: replace 
Abstract: Performance disparities of image recognition across demographic groups are known to exist in deep learning-based models, due to imbalanced group representations or spurious correlation between group and target labels. Previous work has addressed such challenges without relying on expensive group labels, typically by upweighting high-loss samples or balancing discovered clusters. However, these heuristic strategies lack direct connection to specific fairness metrics and cannot guarantee optimization of parity-based criteria like equal opportunity, which ensures equal chance to receive positive outcomes across groups. In this work, we propose a novel paradigm that directly optimizes parity-based fairness metrics through specifically designed training objectives, without requiring group labels. We leverage vision-language models to analyze sensitive attribute relevancy for individual samples, then formulate loss functions that mathematically connect to each target fairness metric. This enables flexible optimization of different fairness criteria based on application needs. Experiments on multiple image classification datasets show that our metric-specific approach significantly improves parity-based fairness criteria and outperforms existing methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Personalized Content Synthesis with Diffusion Models</title>
<link>https://arxiv.org/abs/2405.05538</link>
<guid>https://arxiv.org/abs/2405.05538</guid>
<content:encoded><![CDATA[
arXiv:2405.05538v5 Announce Type: replace 
Abstract: Recent advancements in diffusion models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS). By utilizing a small set of user-provided examples featuring the same subject, PCS aims to tailor this subject to specific user-defined prompts. Over the past two years, more than 150 methods have been introduced in this area. However, existing surveys primarily focus on text-to-image generation, with few providing up-to-date summaries on PCS. This paper provides a comprehensive survey of PCS, introducing the general frameworks of PCS research, which can be categorized into test-time fine-tuning (TTF) and pre-trained adaptation (PTA) approaches. We analyze the strengths, limitations, and key techniques of these methodologies. Additionally, we explore specialized tasks within the field, such as object, face, and style personalization, while highlighting their unique challenges and innovations. Despite the promising progress, we also discuss ongoing challenges, including overfitting and the trade-off between subject fidelity and text alignment. Through this detailed overview and analysis, we propose future directions to further the development of PCS.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Descriptive Image Quality Assessment with A Large-scale Multi-modal Dataset</title>
<link>https://arxiv.org/abs/2405.18842</link>
<guid>https://arxiv.org/abs/2405.18842</guid>
<content:encoded><![CDATA[
arXiv:2405.18842v3 Announce Type: replace 
Abstract: With the rapid advancement of Vision Language Models (VLMs), VLM-based Image Quality Assessment (IQA) seeks to describe image quality linguistically to align with human expression and capture the multifaceted nature of IQA tasks. However, current methods are still far from practical usage. First, prior works focus narrowly on specific sub-tasks or settings, which do not align with diverse real-world applications. Second, their performance is sub-optimal due to limitations in dataset coverage, scale, and quality. To overcome these challenges, we introduce the enhanced Depicted image Quality Assessment model (DepictQA-Wild). Our method includes a multi-functional IQA task paradigm that encompasses both assessment and comparison tasks, brief and detailed responses, full-reference and non-reference scenarios. We introduce a ground-truth-informed dataset construction approach to enhance data quality, and scale up the dataset to 495K under the brief-detail joint framework. Consequently, we construct a comprehensive, large-scale, and high-quality dataset, named DQ-495K. We also retain image resolution during training to better handle resolution-related quality issues, and estimate a confidence score that is helpful to filter out low-quality responses. Experimental results demonstrate that DepictQA-Wild significantly outperforms traditional score-based methods, prior VLM-based IQA models, and proprietary GPT-4V in distortion identification, instant rating, and reasoning tasks. Our advantages are further confirmed by real-world applications including assessing the web-downloaded images and ranking model-processed images. Codes, datasets, and model weights have been released in https://depictqa.github.io/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Octahedral Field: Octahedral prior for simultaneous smoothing and sharp edge regularization</title>
<link>https://arxiv.org/abs/2408.00303</link>
<guid>https://arxiv.org/abs/2408.00303</guid>
<content:encoded><![CDATA[
arXiv:2408.00303v2 Announce Type: replace 
Abstract: Neural implicit representation, the parameterization of a continuous distance function as a Multi-Layer Perceptron (MLP), has emerged as a promising lead in tackling surface reconstruction from unoriented point clouds. In the presence of noise, however, its lack of explicit neighborhood connectivity makes sharp edges identification particularly challenging, hence preventing the separation of smoothing and sharpening operations, as is achievable with its discrete counterparts. In this work, we propose to tackle this challenge with an auxiliary field, the \emph{octahedral field}. We observe that both smoothness and sharp features in the distance field can be equivalently described by the smoothness in octahedral space. Therefore, by aligning and smoothing an octahedral field alongside the implicit geometry, our method behaves analogously to bilateral filtering, resulting in a smooth reconstruction while preserving sharp edges. Despite being operated purely pointwise, our method outperforms various traditional and neural implicit fitting approaches across extensive experiments, and is very competitive with methods that require normals and data priors. Code and data of our work are available at: https://github.com/Ankbzpx/frame-field.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event Stream-based Sign Language Translation: A High-Definition Benchmark Dataset and A Novel Baseline</title>
<link>https://arxiv.org/abs/2408.10488</link>
<guid>https://arxiv.org/abs/2408.10488</guid>
<content:encoded><![CDATA[
arXiv:2408.10488v2 Announce Type: replace 
Abstract: Sign Language Translation (SLT) is a core task in the field of AI-assisted disability. Traditional SLT methods are typically based on visible light videos, which are easily affected by factors such as lighting variations, rapid hand movements, and privacy concerns. This paper proposes the use of bio-inspired event cameras to alleviate the aforementioned issues. Specifically, we introduce a new high-definition event-based sign language dataset, termed Event-CSL, which effectively addresses the data scarcity in this research area. The dataset comprises 14,827 videos, 14,821 glosses, and 2,544 Chinese words in the text vocabulary. These samples are collected across diverse indoor and outdoor scenes, covering multiple viewpoints, lighting conditions, and camera motions. We have also benchmarked existing mainstream SLT methods on this dataset to facilitate fair comparisons in future research.Furthermore, we propose a novel event-based sign language translation framework, termed EvSLT. The framework first segments continuous video features into clips and employs a Mamba-based memory aggregation module to compress and aggregate spatial detail features at the clip level. Subsequently, these spatial features, along with temporal representations obtained from temporal convolution, are then fused by a graph-guided spatiotemporal fusion module. Extensive experiments on Event-CSL, as well as other publicly available datasets, demonstrate the superior performance of our method. The dataset and source code will be released on https://github.com/Event-AHU/OpenESL
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoseAdapt: Sustainable Human Pose Estimation via Continual Learning Benchmarks and Toolkit</title>
<link>https://arxiv.org/abs/2409.20469</link>
<guid>https://arxiv.org/abs/2409.20469</guid>
<content:encoded><![CDATA[
arXiv:2409.20469v2 Announce Type: replace 
Abstract: Human pose estimators are typically retrained from scratch or naively fine-tuned whenever keypoint sets, sensing modalities, or deployment domains change--an inefficient, compute-intensive practice that rarely matches field constraints. We present PoseAdapt, an open-source framework and benchmark suite for continual pose model adaptation. PoseAdapt defines domain-incremental and class-incremental tracks that simulate realistic changes in density, lighting, and sensing modality, as well as skeleton growth. The toolkit supports two workflows: (i) Strategy Benchmarking, which lets researchers implement continual learning (CL) methods as plugins and evaluate them under standardized protocols; and (ii) Model Adaptation, which allows practitioners to adapt strong pretrained models to new tasks with minimal supervision. We evaluate representative regularization-based methods in single-step and sequential settings. Benchmarks enforce a fixed lightweight backbone, no access to past data, and tight per-step budgets. This isolates adaptation strategy effects, highlighting the difficulty of maintaining accuracy under strict resource limits. PoseAdapt connects modern CL techniques with practical pose estimation needs, enabling adaptable models that improve over time without repeated full retraining.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraspDiffusion: Synthesizing Realistic Whole-body Hand-Object Interaction</title>
<link>https://arxiv.org/abs/2410.13911</link>
<guid>https://arxiv.org/abs/2410.13911</guid>
<content:encoded><![CDATA[
arXiv:2410.13911v3 Announce Type: replace 
Abstract: Recent generative models can synthesize high-quality images, but they often fail to generate humans interacting with objects using their hands. This arises mostly from the model's misunderstanding of such interactions and the hardships of synthesizing intricate regions of the body. In this paper, we propose \textbf{GraspDiffusion}, a novel generative method that creates realistic scenes of human-object interaction. Given a 3D object, GraspDiffusion constructs whole-body poses with control over the object's location relative to the human body, which is achieved by separately leveraging the generative priors for body and hand poses, optimizing them into a joint grasping pose. This pose guides the image synthesis to correctly reflect the intended interaction, creating realistic and diverse human-object interaction scenes. We demonstrate that GraspDiffusion can successfully tackle the relatively uninvestigated problem of generating full-bodied human-object interactions while outperforming previous methods. Our project page is available at https://yj7082126.github.io/graspdiffusion/
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FreeGaussian: Annotation-free Control of Articulated Objects via 3D Gaussian Splats with Flow Derivatives</title>
<link>https://arxiv.org/abs/2410.22070</link>
<guid>https://arxiv.org/abs/2410.22070</guid>
<content:encoded><![CDATA[
arXiv:2410.22070v3 Announce Type: replace 
Abstract: Reconstructing controllable Gaussian splats for articulated objects from monocular video is especially challenging due to its inherently insufficient constraints. Existing methods address this by relying on dense masks and manually defined control signals, limiting their real-world applications. In this paper, we propose an annotation-free method, FreeGaussian, which mathematically disentangles camera egomotion and articulated movements via flow derivatives. By establishing a connection between 2D flows and 3D Gaussian dynamic flow, our method enables optimization and continuity of dynamic Gaussian motions from flow priors without any control signals. Furthermore, we introduce a 3D spherical vector controlling scheme, which represents the state as a 3D Gaussian trajectory, thereby eliminating the need for complex 1D control signal calculations and simplifying controllable Gaussian modeling. Extensive experiments on articulated objects demonstrate the state-of-the-art visual performance and precise, part-aware controllability of our method. Code is available at: https://github.com/Tavish9/freegaussian.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual-Word Tokenizer: Beyond Fixed Sets of Tokens in Vision Transformers</title>
<link>https://arxiv.org/abs/2411.15397</link>
<guid>https://arxiv.org/abs/2411.15397</guid>
<content:encoded><![CDATA[
arXiv:2411.15397v4 Announce Type: replace 
Abstract: The cost of deploying vision transformers increasingly represents a barrier to wider industrial adoption. Existing compression techniques require additional end-to-end fine-tuning or incur a significant drawback to energy efficiency, making them ill-suited for online (real-time) inference, where a prediction is made on any new input as it comes in. We introduce the $\textbf{Visual-Word Tokenizer}$ (VWT), a training-free method for reducing energy costs while retaining performance. The VWT groups visual subwords (image patches) that are frequently used into visual words, while infrequent ones remain intact. To do so, $\textit{intra}$-image or $\textit{inter}$-image statistics are leveraged to identify similar visual concepts for sequence compression. Experimentally, we demonstrate a reduction in energy consumed of up to 47%. Comparative approaches of 8-bit quantization and token merging can lead to significantly increased energy costs (up to 500% or more). Our results indicate that VWTs are well-suited for efficient online inference with a marginal compromise on performance. The experimental code for our paper is also made publicly available.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-Foresight: Looking into the Future with DINO</title>
<link>https://arxiv.org/abs/2412.11673</link>
<guid>https://arxiv.org/abs/2412.11673</guid>
<content:encoded><![CDATA[
arXiv:2412.11673v2 Announce Type: replace 
Abstract: Predicting future dynamics is crucial for applications like autonomous driving and robotics, where understanding the environment is key. Existing pixel-level methods are computationally expensive and often focus on irrelevant details. To address these challenges, we introduce DINO-Foresight, a novel framework that operates in the semantic feature space of pretrained Vision Foundation Models (VFMs). Our approach trains a masked feature transformer in a self-supervised manner to predict the evolution of VFM features over time. By forecasting these features, we can apply off-the-shelf, task-specific heads for various scene understanding tasks. In this framework, VFM features are treated as a latent space, to which different heads attach to perform specific tasks for future-frame analysis. Extensive experiments show the very strong performance, robustness and scalability of our framework. Project page and code at https://dino-foresight.github.io/ .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple yet Effective Test-Time Adaptation for Zero-Shot Monocular Metric Depth Estimation</title>
<link>https://arxiv.org/abs/2412.14103</link>
<guid>https://arxiv.org/abs/2412.14103</guid>
<content:encoded><![CDATA[
arXiv:2412.14103v3 Announce Type: replace 
Abstract: The recent development of \emph{foundation models} for monocular depth estimation such as Depth Anything paved the way to zero-shot monocular depth estimation. Since it returns an affine-invariant disparity map, the favored technique to recover the metric depth consists in fine-tuning the model. However, this stage is not straightforward, it can be costly and time-consuming because of the training and the creation of the dataset. The latter must contain images captured by the camera that will be used at test time and the corresponding ground truth. Moreover, the fine-tuning may also degrade the generalizing capacity of the original model. Instead, we propose in this paper a new method to rescale Depth Anything predictions using 3D points provided by sensors or techniques such as low-resolution LiDAR or structure-from-motion with poses given by an IMU. This approach avoids fine-tuning and preserves the generalizing power of the original depth estimation model while being robust to the noise of the sparse depth, of the camera-LiDAR calibration or of the depth model. Our experiments highlight enhancements relative to zero-shot monocular metric depth estimation methods, competitive results compared to fine-tuned approaches and a better robustness than depth completion approaches. Code available at github.com/ENSTA-U2IS-AI/depth-rescaling.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers</title>
<link>https://arxiv.org/abs/2501.08303</link>
<guid>https://arxiv.org/abs/2501.08303</guid>
<content:encoded><![CDATA[
arXiv:2501.08303v2 Announce Type: replace 
Abstract: Semantic future prediction is important for autonomous systems navigating dynamic environments. This paper introduces FUTURIST, a method for multimodal future semantic prediction that uses a unified and efficient visual sequence transformer architecture. Our approach incorporates a multimodal masked visual modeling objective and a novel masking mechanism designed for multimodal training. This allows the model to effectively integrate visible information from various modalities, improving prediction accuracy. Additionally, we propose a VAE-free hierarchical tokenization process, which reduces computational complexity, streamlines the training pipeline, and enables end-to-end training with high-resolution, multimodal inputs. We validate FUTURIST on the Cityscapes dataset, demonstrating state-of-the-art performance in future semantic segmentation for both short- and mid-term forecasting. Project page and code at https://futurist-cvpr2025.github.io/ .
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Large Language Models to Regress Accurate Image Quality Scores using Score Distribution</title>
<link>https://arxiv.org/abs/2501.11561</link>
<guid>https://arxiv.org/abs/2501.11561</guid>
<content:encoded><![CDATA[
arXiv:2501.11561v3 Announce Type: replace 
Abstract: With the rapid advancement of Multi-modal Large Language Models (MLLMs), MLLM-based Image Quality Assessment (IQA) methods have shown promising performance in linguistic quality description. However, current methods still fall short in accurately scoring image quality. In this work, we aim to leverage MLLMs to regress accurate quality scores. A key challenge is that the quality score is inherently continuous, typically modeled as a Gaussian distribution, whereas MLLMs generate discrete token outputs. This mismatch necessitates score discretization. Previous approaches discretize the mean score into a one-hot label, resulting in information loss and failing to capture inter-image relationships. We propose a distribution-based approach that discretizes the score distribution into a soft label. This method preserves the characteristics of the score distribution, achieving high accuracy and maintaining inter-image relationships. Moreover, to address dataset variation, where different IQA datasets exhibit various distributions, we introduce a fidelity loss based on Thurstone's model. This loss captures intra-dataset relationships, facilitating co-training across multiple IQA datasets. With these designs, we develop the distribution-based Depicted image Quality Assessment model for Score regression (DeQA-Score). Experiments across multiple benchmarks show that DeQA-Score stably outperforms baselines in score regression. Also, DeQA-Score can predict the score distribution that closely aligns with human annotations. Codes and model weights have been released in https://depictqa.github.io/deqa-score/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Multimodal Learning Via Multi-Level Adaptive DeConfusion</title>
<link>https://arxiv.org/abs/2502.19674</link>
<guid>https://arxiv.org/abs/2502.19674</guid>
<content:encoded><![CDATA[
arXiv:2502.19674v2 Announce Type: replace 
Abstract: Multimodal learning enhances the performance of various machine learning tasks by leveraging complementary information across different modalities. However, existing methods often learn multimodal representations that retain substantial inter-class confusion, making it difficult to achieve high-confidence predictions, particularly in real-world scenarios with low-quality or noisy data. To address this challenge, we propose Multi-Level Adaptive DeConfusion (MLAD), which eliminates inter-class confusion in multimodal data at both global and sample levels, significantly enhancing the classification reliability of multimodal models. Specifically, MLAD first learns class-wise latent distributions with global-level confusion removed via dynamic-exit modality encoders that adapt to the varying discrimination difficulty of each class and a cross-class residual reconstruction mechanism. Subsequently, MLAD further removes sample-specific confusion through sample-adaptive cross-modality rectification guided by confusion-free modality priors. These priors are constructed from low-confusion modality features, identified by evaluating feature confusion using the learned class-wise latent distributions and selecting those with low confusion via a Gaussian mixture model. Experiments demonstrate that MLAD outperforms state-of-the-art methods across multiple benchmarks and exhibits superior reliability.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation</title>
<link>https://arxiv.org/abs/2503.09399</link>
<guid>https://arxiv.org/abs/2503.09399</guid>
<content:encoded><![CDATA[
arXiv:2503.09399v3 Announce Type: replace 
Abstract: Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases, such as center or size bias, that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation operation that addresses these challenges by explicitly imposing invariances into the training data, which are otherwise part of the neural network architecture. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds. This recombination step enables us to take fine-grained control over object position and size, as well as background selection. We demonstrate that using ForAug significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet, which translates to 7.3 p.p. on downstream tasks. Importantly, ForAug not only improves accuracy but also opens new ways to analyze model behavior and quantify biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that using ForAug during training substantially reduces these biases. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at https://github.com/tobna/ForAug.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Histomorphology-Guided Prototypical Multi-Instance Learning for Breast Cancer WSI Classification</title>
<link>https://arxiv.org/abs/2503.17983</link>
<guid>https://arxiv.org/abs/2503.17983</guid>
<content:encoded><![CDATA[
arXiv:2503.17983v2 Announce Type: replace 
Abstract: Histomorphology is crucial in cancer diagnosis. However, existing whole slide image (WSI) classification methods struggle to effectively incorporate histomorphology information, limiting their ability to capture key pathological features. Particularly when the number of instances within a bag is large and their features are complex, it becomes challenging to accurately identify instances decisive for the bag label, making these methods prone to interference from ambiguous instances. To address this limitation, we propose a novel Histomorphology-Guided Prototypical Multi-Instance Learning (HGPMIL) framework that explicitly learns histomorphology-guided prototypical representations by incorporating tumor cellularity, cellular morphology, and tissue architecture. Specifically, our approach consists of three key components: (1) estimating the importance of tumor-related histomorphology information at patch-level based on medical prior knowledge; (2) generating representative prototypes through histomorphology-prototypical clustering; and (3) enabling WSI classification through histomorphology-guided prototypical aggregation. HGPMIL adjusts the decision boundary by incorporating histomorphological importance to reduce instance label uncertainty, thereby reversely optimizing the bag-level boundary. Experimental results demonstrate its effectiveness, achieving high diagnostic accuracy for molecular subtyping, cancer subtyping and survival analysis. The code will be made available at https://github.com/Badgewho/HMDMIL.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IntrinsiX: High-Quality PBR Generation using Image Priors</title>
<link>https://arxiv.org/abs/2504.01008</link>
<guid>https://arxiv.org/abs/2504.01008</guid>
<content:encoded><![CDATA[
arXiv:2504.01008v2 Announce Type: replace 
Abstract: We introduce IntrinsiX, a novel method that generates high-quality intrinsic images from text description. In contrast to existing text-to-image models whose outputs contain baked-in scene lighting, our approach predicts physically-based rendering (PBR) maps. This enables the generated outputs to be used for content creation scenarios in core graphics applications that facilitate re-lighting, editing, and texture generation tasks. In order to train our generator, we exploit strong image priors, and pre-train separate models for each PBR material component (albedo, roughness, metallic, normals). We then align these models with a new cross-intrinsic attention formulation that concatenates key and value features in a consistent fashion. This allows us to exchange information between each output modality and to obtain semantically coherent PBR predictions. To ground each intrinsic component, we propose a rendering loss which provides image-space signals to constrain the model, thus facilitating sharp details also in the output BRDF properties. Our results demonstrate detailed intrinsic generation with strong generalization capabilities that outperforms existing intrinsic image decomposition methods used with generated images by a significant margin. Finally, we show a series of applications, including re-lighting, editing, and text-conditioned room-scale PBR texture generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models</title>
<link>https://arxiv.org/abs/2504.02821</link>
<guid>https://arxiv.org/abs/2504.02821</guid>
<content:encoded><![CDATA[
arXiv:2504.02821v3 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) have recently gained attention as a means to improve the interpretability and steerability of Large Language Models (LLMs), both of which are essential for AI safety. In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity at the neuron-level in visual representations. To ensure that our evaluation aligns with human perception, we propose a benchmark derived from a large-scale user study. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons, with sparsity and wide latents being the most influential factors. Further, we demonstrate that applying SAE interventions on CLIP's vision encoder directly steers multimodal LLM outputs (e.g., LLaVA), without any modifications to the underlying language model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised tool for enhancing both interpretability and control of VLMs. Code and benchmark data are available at https://github.com/ExplainableML/sae-for-vlm.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage</title>
<link>https://arxiv.org/abs/2504.08718</link>
<guid>https://arxiv.org/abs/2504.08718</guid>
<content:encoded><![CDATA[
arXiv:2504.08718v2 Announce Type: replace 
Abstract: Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate human pose, hand gesture, and facial expression from monocular images. Existing methods predominantly rely on Transformer-based architectures, which suffer from quadratic complexity in self-attention, leading to substantial computational overhead, especially in multi-person scenarios. Recently, Mamba has emerged as a promising alternative to Transformers due to its efficient global modeling capability. However, it remains limited in capturing fine-grained local dependencies, which are essential for precise EHPS. To address these issues, we propose EMO-X, the Efficient Multi-person One-stage model for multi-person EHPS. Specifically, we explore a Scan-based Global-Local Decoder (SGLD) that integrates global context with skeleton-aware local features to iteratively enhance human tokens. Our EMO-X leverages the superior global modeling capability of Mamba and designs a local bidirectional scan mechanism for skeleton-aware local refinement. Comprehensive experiments demonstrate that EMO-X strikes an excellent balance between efficiency and accuracy. Notably, it achieves a significant reduction in computational complexity, requiring 69.8% less inference time compared to state-of-the-art (SOTA) methods, while outperforming most of them in accuracy.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2504.10068</link>
<guid>https://arxiv.org/abs/2504.10068</guid>
<content:encoded><![CDATA[
arXiv:2504.10068v2 Announce Type: replace 
Abstract: Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose $\mathbf{Mavors}$, a novel framework that introduces $\mathbf{M}$ulti-gr$\mathbf{a}$nularity $\mathbf{v}$ide$\mathbf{o}$ $\mathbf{r}$epre$\mathbf{s}$entation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an Intra-chunk Vision Encoder (IVE) that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an Inter-chunk Feature Aggregator (IFA) that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy Rectifying Guidance for Diffusion and Flow Models</title>
<link>https://arxiv.org/abs/2504.13987</link>
<guid>https://arxiv.org/abs/2504.13987</guid>
<content:encoded><![CDATA[
arXiv:2504.13987v2 Announce Type: replace 
Abstract: Guidance techniques are commonly used in diffusion and flow models to improve image quality and input consistency for conditional generative tasks such as class-conditional and text-to-image generation. In particular, classifier-free guidance (CFG) is the most widely adopted guidance technique. It results, however, in trade-offs across quality, diversity and consistency: improving some at the expense of others. While recent work has shown that it is possible to disentangle these factors to some extent, such methods come with an overhead of requiring an additional (weaker) model, or require more forward passes per sampling step. In this paper, we propose Entropy Rectifying Guidance (ERG), a simple and effective guidance method based on inference-time changes in the attention mechanism of state-of-the-art diffusion transformer architectures, which allows for simultaneous improvements over image quality, diversity and prompt consistency. ERG is more general than CFG and similar guidance techniques, as it extends to unconditional sampling. We show that ERG results in significant improvements in various tasks, including text-to-image, class-conditional and unconditional image generation. We also show that ERG can be seamlessly combined with other recent guidance methods such as CADS and APG, further improving generation results.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DreamO: A Unified Framework for Image Customization</title>
<link>https://arxiv.org/abs/2504.16915</link>
<guid>https://arxiv.org/abs/2504.16915</guid>
<content:encoded><![CDATA[
arXiv:2504.16915v5 Announce Type: replace 
Abstract: Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>I-INR: Iterative Implicit Neural Representations</title>
<link>https://arxiv.org/abs/2504.17364</link>
<guid>https://arxiv.org/abs/2504.17364</guid>
<content:encoded><![CDATA[
arXiv:2504.17364v3 Announce Type: replace 
Abstract: Implicit Neural Representations (INRs) have revolutionized signal processing and computer vision by modeling signals as continuous, differentiable functions parameterized by neural networks. However, their inherent formulation as a regression problem makes them prone to regression to the mean, limiting their ability to capture fine details, retain high-frequency information, and handle noise effectively. To address these challenges, we propose Iterative Implicit Neural Representations (I-INRs) a novel plug-and-play framework that enhances signal reconstruction through an iterative refinement process. I-INRs effectively recover high-frequency details, improve robustness to noise, and achieve superior reconstruction quality. Our framework seamlessly integrates with existing INR architectures, delivering substantial performance gains across various tasks. Extensive experiments show that I-INRs outperform baseline methods, including WIRE, SIREN, and Gauss, in diverse computer vision applications such as image restoration, image denoising, and object occupancy prediction.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</title>
<link>https://arxiv.org/abs/2504.17457</link>
<guid>https://arxiv.org/abs/2504.17457</guid>
<content:encoded><![CDATA[
arXiv:2504.17457v2 Announce Type: replace 
Abstract: Expressive human pose and shape estimation (EHPS) is crucial for digital human generation, especially in applications like live streaming. While existing research primarily focuses on reducing estimation errors, it largely neglects robustness and security aspects, leaving these systems vulnerable to adversarial attacks. To address this significant challenge, we propose the \textbf{Tangible Attack (TBA)}, a novel framework designed to generate adversarial examples capable of effectively compromising any digital human generation model. Our approach introduces a \textbf{Dual Heterogeneous Noise Generator (DHNG)}, which leverages Variational Autoencoders (VAE) and ControlNet to produce diverse, targeted noise tailored to the original image features. Additionally, we design a custom \textbf{adversarial loss function} to optimize the noise, ensuring both high controllability and potent disruption. By iteratively refining the adversarial sample through multi-gradient signals from both the noise and the state-of-the-art EHPS model, TBA substantially improves the effectiveness of adversarial attacks. Extensive experiments demonstrate TBA's superiority, achieving a remarkable 41.0\% increase in estimation error, with an average improvement of approximately 17.0\%. These findings expose significant security vulnerabilities in current EHPS models and highlight the need for stronger defenses in digital human generation systems.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Partially Relevant Video Retrieval through Inter- and Intra-Sample Analysis with Coherence Prediction</title>
<link>https://arxiv.org/abs/2504.19637</link>
<guid>https://arxiv.org/abs/2504.19637</guid>
<content:encoded><![CDATA[
arXiv:2504.19637v5 Announce Type: replace 
Abstract: Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video that is partially relevant to the text query. The primary challenge in PRVR arises from the semantic asymmetry between textual and visual modalities, as videos often contain substantial content irrelevant to the query. Existing methods coarsely align paired videos and text queries to construct the semantic space, neglecting the critical cross-modal dual nature inherent in this task: inter-sample correlation and intra-sample redundancy. To this end, we propose a novel PRVR framework to systematically exploit these two characteristics. Our framework consists of three core modules. First, the Inter Correlation Enhancement (ICE) module captures inter-sample correlation by identifying semantically similar yet unpaired text queries and video moments, combining them to form pseudo-positive pairs for more robust semantic space construction. Second, the Intra Redundancy Mining (IRM) module mitigates intra-sample redundancy by mining redundant moment features and distinguishing them from query-relevant moments, encouraging the model to learn more discriminative representations. Finally, to reinforce these modules, we introduce the Temporal Coherence Prediction (TCP) module, enhancing discrimination of fine-grained moment-level semantics by training the model to predict the original temporal order of randomly shuffled video sequences. Extensive experiments demonstrate the superiority of our method, achieving state-of-the-art results.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated segmentation of pediatric neuroblastoma on multi-modal MRI: Results of the SPPIN challenge at MICCAI 2023</title>
<link>https://arxiv.org/abs/2505.00369</link>
<guid>https://arxiv.org/abs/2505.00369</guid>
<content:encoded><![CDATA[
arXiv:2505.00369v2 Announce Type: replace 
Abstract: Surgery plays an important role within the treatment for neuroblastoma, a common pediatric cancer. This requires careful planning, often via magnetic resonance imaging (MRI)-based anatomical 3D models. However, creating these models is often time-consuming and user dependent. We organized the Surgical Planning in Pediatric Neuroblastoma (SPPIN) challenge, to stimulate developments on this topic, and set a benchmark for fully automatic segmentation of neuroblastoma on multi-model MRI. The challenge started with a training phase, where teams received 78 sets of MRI scans from 34 patients, consisting of both diagnostic and post-chemotherapy MRI scans. The final test phase, consisting of 18 MRI sets from 9 patients, determined the ranking of the teams. Ranking was based on the Dice similarity coefficient (Dice score), the 95th percentile of the Hausdorff distance (HD95) and the volumetric similarity (VS). The SPPIN challenge was hosted at MICCAI 2023. The final leaderboard consisted of 9 teams. The highest-ranking team achieved a median Dice score 0.82, a median HD95 of 7.69 mm and a VS of 0.91, utilizing a large, pretrained network called STU-Net. A significant difference for the segmentation results between diagnostic and post-chemotherapy MRI scans was observed (Dice = 0.89 vs Dice = 0.59, P = 0.01) for the highest-ranking team. SPPIN is the first medical segmentation challenge in extracranial pediatric oncology. The highest-ranking team used a large pre-trained network, suggesting that pretraining can be of use in small, heterogenous datasets. Although the results of the highest-ranking team were high for most patients, segmentation especially in small, pre-treated tumors were insufficient. Therefore, more reliable segmentation methods are needed to create clinically applicable models to aid surgical planning in pediatric neuroblastoma.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering</title>
<link>https://arxiv.org/abs/2505.07073</link>
<guid>https://arxiv.org/abs/2505.07073</guid>
<content:encoded><![CDATA[
arXiv:2505.07073v2 Announce Type: replace 
Abstract: Concept-based explanations have emerged as an effective approach within Explainable Artificial Intelligence, enabling interpretable insights by aligning model decisions with human-understandable concepts. However, existing methods rely on computationally intensive procedures and struggle to efficiently capture complex, semantic concepts. This work introduces the Concept Directions via Latent Clustering (CDLC), which extracts global, class-specific concept directions by clustering latent difference vectors derived from factual and diffusion-generated counterfactual image pairs. CDLC reduces storage requirements by ~4.6% and accelerates concept discovery by ~5.3% compared to the baseline method, while requiring no GPU for clustering, thereby enabling efficient extraction of multidimensional semantic concepts across latent dimensions. This approach is validated on a real-world skin lesion dataset, demonstrating that the extracted concept directions align with clinically recognized dermoscopic features and, in some cases, reveal dataset-specific biases or unknown biomarkers. These results highlight that CDLC is interpretable, scalable, and applicable across high-stakes domains and diverse data modalities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAMChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Small Scale Remote Sensing</title>
<link>https://arxiv.org/abs/2505.07984</link>
<guid>https://arxiv.org/abs/2505.07984</guid>
<content:encoded><![CDATA[
arXiv:2505.07984v2 Announce Type: replace 
Abstract: Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those requiring resource-efficient and domain-specific adaptations-has remained limited. In this work, a lightweight multimodal language model termed SAMChat is introduced, specifically adapted to analyze remote sensing imagery in secluded areas, including challenging missile launch sites. A new dataset, SAMData, was compiled by verifying hundreds of aerial images through expert review, and subtle military installations were highlighted via detailed captions. Supervised fine-tuning on a 2B parameter open-source MLLM with chain-of-thought (CoT) reasoning annotations was performed, enabling more accurate and interpretable explanations. Additionally, Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's ability to detect critical domain-specific cues-such as defensive layouts and key military structures-while minimizing false positives on civilian scenes. Through empirical evaluations, it has been shown that SAMChat significantly outperforms both larger, general-purpose multimodal models and existing remote sensing adapted approaches on open-ended captioning and classification metrics. Over 80% recall and 98% precision were achieved on the newly proposed SAMData benchmark, underscoring the potency of targeted fine-tuning and reinforcement learning in specialized real-world applications.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning</title>
<link>https://arxiv.org/abs/2505.10999</link>
<guid>https://arxiv.org/abs/2505.10999</guid>
<content:encoded><![CDATA[
arXiv:2505.10999v2 Announce Type: replace 
Abstract: While diffusion models excel at image synthesis, useful representations have been shown to emerge from generative pre-training, suggesting a path towards unified generative and discriminative learning. However, suboptimal semantic flow within current architectures can hinder this potential: features encoding the richest high-level semantics are underutilized and diluted when propagating through decoding layers, impeding the formation of an explicit semantic bottleneck layer. To address this, we introduce self-conditioning, a lightweight mechanism that reshapes the model's layer-wise semantic hierarchy without external guidance. By aggregating and rerouting intermediate features to guide subsequent decoding layers, our method concentrates more high-level semantics, concurrently strengthening global generative guidance and forming more discriminative representations. This simple approach yields a dual-improvement trend across pixel-space UNet, UViT and latent-space DiT models with minimal overhead. Crucially, it creates an architectural semantic bridge that propagates discriminative improvements into generation and accommodates further techniques such as contrastive self-distillation. Experiments show that our enhanced models, especially self-conditioned DiT, are powerful dual learners that yield strong and transferable representations on image and dense classification tasks, surpassing various generative self-supervised models in linear probing while also improving or maintaining high generation quality.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParticleGS: Learning Neural Gaussian Particle Dynamics from Videos for Prior-free Physical Motion Extrapolation</title>
<link>https://arxiv.org/abs/2505.20270</link>
<guid>https://arxiv.org/abs/2505.20270</guid>
<content:encoded><![CDATA[
arXiv:2505.20270v2 Announce Type: replace 
Abstract: The ability to extrapolate dynamic 3D scenes beyond the observed timeframe is fundamental to advancing physical world understanding and predictive modeling. Existing dynamic 3D reconstruction methods have achieved high-fidelity rendering of temporal interpolation, but typically lack physical consistency in predicting the future. To overcome this issue, we propose ParticleGS, a physics-based framework that reformulates dynamic 3D scenes as physically grounded systems. ParticleGS comprises three key components: 1) an encoder that decomposes the scene into static properties and initial dynamic physical fields; 2) an evolver based on Neural Ordinary Differential Equations (Neural ODEs) that learns continuous-time dynamics for motion extrapolation; and 3) a decoder that reconstructs 3D Gaussians from evolved particle states for rendering. Through this design, ParticleGS integrates physical reasoning into dynamic 3D representations, enabling accurate and consistent prediction of the future. Experiments show that ParticleGS achieves state-of-the-art performance in extrapolation while maintaining rendering quality comparable to leading dynamic 3D reconstruction methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Occlusion Boundary and Depth: Mutual Enhancement via Multi-Task Learning</title>
<link>https://arxiv.org/abs/2505.21231</link>
<guid>https://arxiv.org/abs/2505.21231</guid>
<content:encoded><![CDATA[
arXiv:2505.21231v3 Announce Type: replace 
Abstract: Occlusion Boundary Estimation (OBE) identifies boundaries arising from both inter-object occlusions and self-occlusion within individual objects. This task is closely related to Monocular Depth Estimation (MDE), which infers depth from a single image, as Occlusion Boundaries (OBs) provide critical geometric cues for resolving depth ambiguities, while depth can conversely refine occlusion reasoning. In this paper, we aim to systematically model and exploit this mutually beneficial relationship. To this end, we propose MoDOT, a novel framework for joint estimation of depth and OBs, which incorporates a new Cross-Attention Strip Module (CASM) to leverage mid-level OB features for depth prediction, and a novel OB-Depth Constraint Loss (OBDCL) to enforce geometric consistency. To facilitate this study, we contribute OB-Hypersim, a large-scale photorealistic dataset with precise depth and self-occlusion-handled OB annotations. Extensive experiments on two synthetic datasets and NYUD-v2 demonstrate that MoDOT achieves significantly better performance than single-task baselines and multi-task competitors. Furthermore, models trained solely on our synthetic data demonstrate strong generalization to real-world scenes without fine-tuning, producing depth maps with sharper boundaries and improved geometric fidelity. Collectively, these results underscore the significant benefits of jointly modeling OBs and depth. Code and resources are available at https://github.com/xul-ops/MoDOT.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S2AFormer: Strip Self-Attention for Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2505.22195</link>
<guid>https://arxiv.org/abs/2505.22195</guid>
<content:encoded><![CDATA[
arXiv:2505.22195v2 Announce Type: replace 
Abstract: Vision Transformer (ViT) has made significant advancements in computer vision, thanks to its token mixer's sophisticated ability to capture global dependencies between all tokens. However, the quadratic growth in computational demands as the number of tokens increases limits its practical efficiency. Although recent methods have combined the strengths of convolutions and self-attention to achieve better trade-offs, the expensive pairwise token affinity and complex matrix operations inherent in self-attention remain a bottleneck. To address this challenge, we propose S2AFormer, an efficient Vision Transformer architecture featuring novel Strip Self-Attention (SSA). We design simple yet effective Hybrid Perception Blocks (HPBs) to effectively integrate the local perception capabilities of CNNs with the global context modeling of Transformer's attention mechanisms. A key innovation of SSA lies in its reduction of the spatial dimensions of $K$ and $V$, while compressing the channel dimensions of $Q$ and $K$. This design significantly reduces computational overhead while preserving accuracy, striking an optimal balance between efficiency and effectiveness. We evaluate the robustness and efficiency of S2AFormer through extensive experiments on multiple vision benchmarks, including ImageNet-1k for image classification, ADE20k for semantic segmentation, and COCO for object detection and instance segmentation. Results demonstrate that S2AFormer achieves significant accuracy gains with superior efficiency in both GPU and non-GPU environments, making it a strong candidate for efficient vision Transformers.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs</title>
<link>https://arxiv.org/abs/2506.03195</link>
<guid>https://arxiv.org/abs/2506.03195</guid>
<content:encoded><![CDATA[
arXiv:2506.03195v2 Announce Type: replace 
Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on general zero-shot image classification tasks, fine-grained image classification remains challenging. It demands precise attention to subtle visual details to distinguish between visually similar subcategories--details that MLLMs may easily overlook without explicit guidance. To address this, we introduce AutoSEP, an iterative self-supervised prompt learning framework designed to enhance MLLM fine-grained classification capabilities in a fully unsupervised manner. Our core idea is to leverage unlabeled data to learn a description prompt that guides MLLMs in identifying crucial discriminative features within an image, and boosts classification accuracy. We developed an automatic self-enhancing prompt learning framework called AutoSEP to iteratively improve the description prompt using unlabeled data, based on instance-level classification scoring function. AutoSEP only requires black-box access to MLLMs, eliminating the need for any training or fine-tuning. We evaluate our approach on multiple fine-grained classification datasets. It consistently outperforms other unsupervised baselines, demonstrating the effectiveness of our self-supervised optimization framework. Notably, AutoSEP on average improves 13 percent over standard zero-shot classification and 5 percent over the best-performing baselines. Code is available at: https://github.com/yq-hong/AutoSEP
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx</title>
<link>https://arxiv.org/abs/2506.04931</link>
<guid>https://arxiv.org/abs/2506.04931</guid>
<content:encoded><![CDATA[
arXiv:2506.04931v2 Announce Type: replace 
Abstract: We introduce CzechLynx, the first large-scale, open-access dataset for individual identification, pose estimation, and instance segmentation of the Eurasian lynx (Lynx lynx). CzechLynx contains 39,760 camera trap images annotated with segmentation masks, identity labels, and 20-point skeletons and covers 319 unique individuals across 15 years of systematic monitoring in two geographically distinct regions: southwest Bohemia and the Western Carpathians. In addition to the real camera trap data, we provide a large complementary set of photorealistic synthetic images and a Unity-based generation pipeline with diffusion-based text-to-texture modeling, capable of producing arbitrarily large amounts of synthetic data spanning diverse environments, poses, and coat-pattern variations. To enable systematic testing across realistic ecological scenarios, we define three complementary evaluation protocols: (i) geo-aware, (ii) time-aware open-set, and (iii) time-aware closed-set, covering cross-regional and long-term monitoring settings. With the provided resources, CzechLynx offers a unique, flexible benchmark for robust evaluation of computer vision and machine learning models across realistic ecological scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating the Relationship between the Weighted Figure of Merit and Rosin's Measure</title>
<link>https://arxiv.org/abs/2506.05749</link>
<guid>https://arxiv.org/abs/2506.05749</guid>
<content:encoded><![CDATA[
arXiv:2506.05749v4 Announce Type: replace 
Abstract: Many studies have been conducted to solve the problem of approximating a digital boundary by piece straight-line segments for the further processing required in computer vision applications. The authors of these studies compared their schemes to determine the best one. The initial measure used to assess the goodness of fit of a polygonal approximation was the figure of merit. Later,it was noted that this measure was not an appropriate metric for a valid reason which is why Rosin-through mathematical analysis-introduced a measure called merit. However,this measure involves an optimal scheme of polygonal approximation,so it is time-consuming to compute it to assess the goodness of fit of an approximation. This led many researchers to use a weighted figure of merit as a substitute for Rosin's measure to compare sub optimal schemes. An attempt is made in this communication to investigate whether the two measures-weighted figure of merit and Rosin's measure-are related so that one can be used instead of the other, and toward this end, theoretical analysis, experimental investigation and statistical analysis are carried out. The mathematical formulas for the weighted figure of merit and Rosin's measure are analyzed, and through proof of theorems,it is found that the two measures are theoretically independent of each other. The graphical analysis of experiments carried out using a public dataset supports the results of the theoretical analysis. The statistical analysis via Pearson's correlation coefficient and non-linear correlation measure also revealed that the two measures are uncorrelated. This analysis leads one to conclude that if a suboptimal scheme is found to be better (worse) than some other suboptimal scheme,as indicated by Rosin's measure,then the same conclusion cannot be drawn using a weighted figure of merit,so one cannot use a weighted figure of merit instead of Rosin's measure.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenDance: Multimodal Controllable 3D Dance Generation with Large-scale Internet Data</title>
<link>https://arxiv.org/abs/2506.07565</link>
<guid>https://arxiv.org/abs/2506.07565</guid>
<content:encoded><![CDATA[
arXiv:2506.07565v2 Announce Type: replace 
Abstract: Music-driven 3D dance generation offers significant creative potential, yet practical applications demand versatile and multimodal control. As the highly dynamic and complex human motion covering various styles and genres, dance generation requires satisfying diverse conditions beyond just music (e.g., spatial trajectories, keyframe gestures, or style descriptions). However, the absence of a large-scale and richly annotated dataset severely hinders progress. In this paper, we build OpenDanceSet, an extensive human dance dataset comprising over 100 hours across 14 genres and 147 subjects. Each sample has rich annotations to facilitate robust cross-modal learning: 3D motion, paired music, 2D keypoints, trajectories, and expert-annotated text descriptions. Furthermore, we propose OpenDanceNet, a unified masked modeling framework for controllable dance generation, including a disentangled auto-encoder and a multimodal joint-prediction Transformer. OpenDanceNet supports generation conditioned on music and arbitrary combinations of text, keypoints, or trajectories. Comprehensive experiments demonstrate that our work achieves high-fidelity synthesis with strong diversity and realistic physical contacts, while also offering flexible control over spatial and stylistic conditions. Project Page: https://open-dance.github.io
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VITA: Zero-Shot Value Functions via Test-Time Adaptation of Vision-Language Models</title>
<link>https://arxiv.org/abs/2506.10085</link>
<guid>https://arxiv.org/abs/2506.10085</guid>
<content:encoded><![CDATA[
arXiv:2506.10085v4 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) show promise as zero-shot goal-conditioned value functions, but their frozen pre-trained representations limit generalization and temporal reasoning. We introduce VITA, a zero-shot value function learning method that enhances both capabilities via test-time adaptation. At inference, a lightweight adaptation module is updated via a gradient step on a meta-learned self-supervised loss, such that each test-time update improves value estimation. By updating sequentially over a trajectory, VITA encodes history into its parameters, addressing the temporal reasoning limitations. To mitigate shortcut learning, we propose a dissimilarity-based sampling strategy that selects semantically diverse segments of the trajectory during training. In real-world robotic manipulation tasks, VITA generalizes from a single training environment to diverse out-of-distribution tasks, environments, and embodiments, outperforming the state-of-the-art zero-shot method using autoregressive VLMs. Furthermore, we demonstrate that VITA's zero-shot value estimates can be utilized for reward shaping in offline reinforcement learning, resulting in multi-task policies on the Meta-World benchmark that exceed the performance of those trained with the simulation's fuzzy-logic dense rewards.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgriPotential: A Novel Multi-Spectral and Multi-Temporal Remote Sensing Dataset for Agricultural Potentials</title>
<link>https://arxiv.org/abs/2506.11740</link>
<guid>https://arxiv.org/abs/2506.11740</guid>
<content:encoded><![CDATA[
arXiv:2506.11740v2 Announce Type: replace 
Abstract: Remote sensing has emerged as a critical tool for large-scale Earth monitoring and land management. In this paper, we introduce AgriPotential, a novel benchmark dataset composed of Sentinel-2 satellite imagery captured over multiple months. The dataset provides pixel-level annotations of agricultural potentials for three major crop types - viticulture, market gardening, and field crops - across five ordinal classes. AgriPotential supports a broad range of machine learning tasks, including ordinal regression, multi-label classification, and spatio-temporal modeling. The data cover diverse areas in Southern France, offering rich spectral information. AgriPotential is the first public dataset designed specifically for agricultural potential prediction, aiming to improve data-driven approaches to sustainable land use planning. The dataset and the code are freely accessible at: https://zenodo.org/records/15551829
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models</title>
<link>https://arxiv.org/abs/2506.11764</link>
<guid>https://arxiv.org/abs/2506.11764</guid>
<content:encoded><![CDATA[
arXiv:2506.11764v2 Announce Type: replace 
Abstract: This paper presents DiffFuSR, a modular pipeline for super-resolving all 12 spectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling distance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a diffusion-based super-resolution (SR) model trained on high-resolution RGB imagery from the NAIP and WorldStrat datasets, harmonized to simulate Sentinel-2 characteristics; and (ii) a learned fusion network that upscales the remaining multispectral bands using the super-resolved RGB image as a spatial prior. We introduce a robust degradation model and contrastive degradation encoder to support blind SR. Extensive evaluations of the proposed SR pipeline on the OpenSR benchmark demonstrate that the proposed method outperforms current SOTA baselines in terms of reflectance fidelity, spectral consistency, spatial alignment, and hallucination suppression. Furthermore, the fusion network significantly outperforms classical and learned pansharpening approaches, enabling accurate enhancement of Sentinel-2's 20 m and 60 m bands. This work proposes a novel modular framework Sentinel-2 SR that utilizes harmonized learning with diffusion models and fusion strategies. Our code and models can be found at https://github.com/NorskRegnesentral/DiffFuSR.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-like Model as a Foundational Density Ratio Estimator</title>
<link>https://arxiv.org/abs/2506.22881</link>
<guid>https://arxiv.org/abs/2506.22881</guid>
<content:encoded><![CDATA[
arXiv:2506.22881v2 Announce Type: replace 
Abstract: Density ratio estimation is a core concept in statistical machine learning because it provides a unified mechanism for tasks such as importance weighting, divergence estimation, and likelihood-free inference, but its potential in vision and language models has not been fully explored. Modern vision-language encoders such as CLIP and SigLIP are trained with contrastive objectives that implicitly optimize log density ratios between joint and marginal image-text distributions, which implicitly learn similarity scores proportional to log density ratios. However, prior work has largely focused on their embedding utility, and the density-ratio structure induced by contrastive learning has not been systematically examined or exploited in multimodal applications. To address this gap, we reinterpret CLIP-style models as pretrained and general-purpose density ratio estimators and show that this perspective enables new algorithmic capabilities. We present a unified explanation of how contrastive objectives estimate density ratios and propose two practical applications: Importance Weight Learning and KL divergence estimation. Our Importance Weight Learning method requires only a single additional prompt and improves F1 scores by up to 7 points. We further show that CLIP-based density ratios support estimation of KL divergences that quantify how conditioning on an image or text alters the distribution of the other modality. Through qualitative examples and an N-gram analysis of captions, we find that these divergences capture semantic diversity and mode structure in multimodal data. Leveraging this property, we introduce a simple KL-guided data curation method that achieves performance competitive with LAION2B filtering.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency</title>
<link>https://arxiv.org/abs/2507.00802</link>
<guid>https://arxiv.org/abs/2507.00802</guid>
<content:encoded><![CDATA[
arXiv:2507.00802v2 Announce Type: replace 
Abstract: 3D medical image generation is essential for data augmentation and patient privacy, calling for reliable and efficient models suited for clinical practice. However, current methods suffer from limited anatomical fidelity, restricted axial length, and substantial computational cost, placing them beyond reach for regions with limited resources and infrastructure. We introduce TRACE, a framework that generates 3D medical images with spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach. TRACE models sequential 2D slices as video frame pairs, combining segmentation priors and radiology reports for anatomical alignment, incorporating optical flow to sustain temporal coherence. During inference, an overlapping-frame strategy links frame pairs into a flexible length sequence, reconstructed into a spatiotemporally and anatomically aligned 3D volume. Experimental results demonstrate that TRACE effectively balances computational efficiency with preserving anatomical fidelity and spatiotemporal consistency. Code is available at: https://github.com/VinyehShaw/TRACE.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory</title>
<link>https://arxiv.org/abs/2507.02863</link>
<guid>https://arxiv.org/abs/2507.02863</guid>
<content:encoded><![CDATA[
arXiv:2507.02863v2 Announce Type: replace 
Abstract: Dense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. Following the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. However, such implicit memory is limited in capacity and may suffer from information loss of earlier frames. We propose Point3R, an online framework targeting dense streaming 3D reconstruction. To be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. Each pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature. Information extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system. We design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient. Our method achieves competitive or state-of-the-art performance on various tasks with low training costs. Code: https://github.com/YkiWu/Point3R.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking</title>
<link>https://arxiv.org/abs/2507.06400</link>
<guid>https://arxiv.org/abs/2507.06400</guid>
<content:encoded><![CDATA[
arXiv:2507.06400v3 Announce Type: replace 
Abstract: Multiple object tracking (MOT) technology has made significant progress in terrestrial applications, but underwater tracking scenarios remain underexplored despite their importance to marine ecology and aquaculture. In this paper, we present Multiple Fish Tracking Dataset 2025 (MFT25), a comprehensive dataset specifically designed for underwater multiple fish tracking, featuring 15 diverse video sequences with 408,578 meticulously annotated bounding boxes across 48,066 frames. Our dataset captures various underwater environments, fish species, and challenging conditions including occlusions, similar appearances, and erratic motion patterns. Additionally, we introduce Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework featuring an Unscented Kalman Filter (UKF) optimized for non-linear swimming patterns of fish and a novel Fish-Intersection-over-Union (FishIoU) matching that accounts for the unique morphological characteristics of aquatic species. Extensive experiments demonstrate that our SU-T baseline achieves state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while revealing fundamental differences between fish tracking and terrestrial object tracking scenarios. The dataset and codes are released at https://vranlee.github.io/SU-T/.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taming generative video models for zero-shot optical flow extraction</title>
<link>https://arxiv.org/abs/2507.09082</link>
<guid>https://arxiv.org/abs/2507.09082</guid>
<content:encoded><![CDATA[
arXiv:2507.09082v2 Announce Type: replace 
Abstract: Extracting optical flow from videos remains a core computer vision problem. Motivated by the recent success of large general-purpose models, we ask whether frozen self-supervised video models trained only to predict future frames can be prompted, without fine-tuning, to output flow. Prior attempts to read out depth or illumination from video generators required fine-tuning; that strategy is ill-suited for flow, where labeled data is scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models for zero-shot flow extraction. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recently introduced Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time inference procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method is competitive with state-of-the-art, task-specific models on the real-world TAP-Vid DAVIS benchmark and the synthetic TAP-Vid Kubric. Our results show that counterfactual prompting of controllable generative video models is an effective alternative to supervised or photometric-loss methods for high-quality flow.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysX-3D: Physical-Grounded 3D Asset Generation</title>
<link>https://arxiv.org/abs/2507.12465</link>
<guid>https://arxiv.org/abs/2507.12465</guid>
<content:encoded><![CDATA[
arXiv:2507.12465v4 Announce Type: replace 
Abstract: 3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \textbf{PhysX-3D}, an end-to-end paradigm for physical-grounded 3D asset generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we present PhysXNet - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: absolute scale, material, affordance, kinematics, and function description. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets.2) Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for physics-grounded image-to-3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs</title>
<link>https://arxiv.org/abs/2507.13361</link>
<guid>https://arxiv.org/abs/2507.13361</guid>
<content:encoded><![CDATA[
arXiv:2507.13361v2 Announce Type: replace 
Abstract: Vision-Language Models (VLMs) excel at complex visual tasks such as VQA and chart understanding, yet recent work suggests they struggle with simple perceptual tests. We present an evaluation of vision-language models' capacity for nonlocal visual reasoning: reasoning that requires chaining evidence collected from multiple, possibly distant regions of an image. We isolate three distinct forms of nonlocal vision: comparative perception, which demands holding two images in working memory and comparing them; saccadic search, which requires making discrete, evidence-driven jumps to locate successive targets; and smooth visual search, which involves following a continuous contour. Flagship models (e.g., GPT-5, Gemini 2.5 Pro, Claude Sonnet 4), even those that perform well on prior primitive-vision benchmarks, fail these tests and barely exceed random accuracy on two variants of our tasks that are trivial for humans. Our structured evaluation suite allows us to test whether VLMs can perform visual algorithms similar to those used by humans. Our findings show that despite gains in raw visual acuity, current models lack core visual reasoning capabilities.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Parallel Diffusion Model Serving with Residual Compression</title>
<link>https://arxiv.org/abs/2507.17511</link>
<guid>https://arxiv.org/abs/2507.17511</guid>
<content:encoded><![CDATA[
arXiv:2507.17511v2 Announce Type: replace 
Abstract: Diffusion models produce realistic images and videos but require substantial computational resources, necessitating multi-accelerator parallelism for real-time deployment. However, parallel inference introduces significant communication overhead from exchanging large activations between devices, limiting efficiency and scalability. We present CompactFusion, a compression framework that significantly reduces communication while preserving generation quality. Our key observation is that diffusion activations exhibit strong temporal redundancy-adjacent steps produce highly similar activations, saturating bandwidth with near-duplicate data carrying little new information. To address this inefficiency, we seek a more compact representation that encodes only the essential information. CompactFusion achieves this via Residual Compression that transmits only compressed residuals (step-wise activation differences). Based on empirical analysis and theoretical justification, we show that it effectively removes redundant data, enabling substantial data reduction while maintaining high fidelity. We also integrate lightweight error feedback to prevent error accumulation. CompactFusion establishes a new paradigm for parallel diffusion inference, delivering lower latency and significantly higher generation quality than prior methods. On 4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also uniquely supports communication-heavy strategies like sequence parallelism on slow networks, achieving 6.7x speedup over prior overlap-based method. CompactFusion applies broadly across diffusion models and parallel settings, and integrates easily without requiring pipeline rework. Portable implementation demonstrated on xDiT is publicly available at https://github.com/Cobalt-27/CompactFusion
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReGATE: Learning Faster and Better with Fewer Tokens in MLLMs</title>
<link>https://arxiv.org/abs/2507.21420</link>
<guid>https://arxiv.org/abs/2507.21420</guid>
<content:encoded><![CDATA[
arXiv:2507.21420v2 Announce Type: replace 
Abstract: The computational cost of training multimodal large language models (MLLMs) grows rapidly with the number of processed tokens. Existing efficiency methods mainly target inference via token reduction or merging, offering limited benefits during training. We introduce ReGATE (Reference-Guided Adaptive Token Elision), an adaptive token pruning method for accelerating MLLM training. ReGATE adopts a teacher-student framework, in which a frozen teacher LLM provides per-token guidance losses that are fused with an exponential moving average of the student's difficulty estimates. This adaptive scoring mechanism dynamically selects informative tokens while skipping redundant ones in the forward pass, substantially reducing computation without altering the model architecture. Across three representative MLLMs, ReGATE matches the peak accuracy of standard training on MVBench up to 2$\times$ faster, using only 38% of the tokens. With extended training, it even surpasses the baseline across multiple multimodal benchmarks, cutting total token usage by over 41%. Code and models will be released publicly.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion Matters: Motion-guided Modulation Network for Skeleton-based Micro-Action Recognition</title>
<link>https://arxiv.org/abs/2507.21977</link>
<guid>https://arxiv.org/abs/2507.21977</guid>
<content:encoded><![CDATA[
arXiv:2507.21977v4 Announce Type: replace 
Abstract: Micro-Actions (MAs) are an important form of non-verbal communication in social interactions, with potential applications in human emotional analysis. However, existing methods in Micro-Action Recognition often overlook the inherent subtle changes in MAs, which limits the accuracy of distinguishing MAs with subtle changes. To address this issue, we present a novel Motion-guided Modulation Network (MMN) that implicitly captures and modulates subtle motion cues to enhance spatial-temporal representation learning. Specifically, we introduce a Motion-guided Skeletal Modulation module (MSM) to inject motion cues at the skeletal level, acting as a control signal to guide spatial representation modeling. In parallel, we design a Motion-guided Temporal Modulation module (MTM) to incorporate motion information at the frame level, facilitating the modeling of holistic motion patterns in micro-actions. Finally, we propose a motion consistency learning strategy to aggregate the motion cues from multi-scale features for micro-action classification. Experimental results on the Micro-Action 52 and iMiGUE datasets demonstrate that MMN achieves state-of-the-art performance in skeleton-based micro-action recognition, underscoring the importance of explicitly modeling subtle motion cues. The code will be available at https://github.com/momiji-bit/MMN.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Video Slot Attention Queries from Random Slot-Feature Pairs</title>
<link>https://arxiv.org/abs/2508.01345</link>
<guid>https://arxiv.org/abs/2508.01345</guid>
<content:encoded><![CDATA[
arXiv:2508.01345v5 Announce Type: replace 
Abstract: Unsupervised video Object-Centric Learning (OCL) is promising as it enables object-level scene representation and understanding as we humans do. Mainstream video OCL methods adopt a recurrent architecture: An aggregator aggregates current video frame into object features, termed slots, under some queries; A transitioner transits current slots to queries for the next frame. This is an effective architecture but all existing implementations both (\textit{i1}) neglect to incorporate next frame features, the most informative source for query prediction, and (\textit{i2}) fail to learn transition dynamics, the knowledge essential for query prediction. To address these issues, we propose Random Slot-Feature pair for learning Query prediction (RandSF.Q): (\textit{t1}) We design a new transitioner to incorporate both slots and features, which provides more information for query prediction; (\textit{t2}) We train the transitioner to predict queries from slot-feature pairs randomly sampled from available recurrences, which drives it to learn transition dynamics. Experiments on scene representation demonstrate that our method surpass existing video OCL methods significantly, e.g., up to 10 points on object discovery, setting new state-of-the-art. Such superiority also benefits downstream tasks like scene understanding. Source Code, Model Checkpoints, Training Logs: https://github.com/Genera1Z/RandSF.Q
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking the Unstable: Appearance-Guided Motion Modeling for Robust Multi-Object Tracking in UAV-Captured Videos</title>
<link>https://arxiv.org/abs/2508.01730</link>
<guid>https://arxiv.org/abs/2508.01730</guid>
<content:encoded><![CDATA[
arXiv:2508.01730v2 Announce Type: replace 
Abstract: Multi-object tracking (MOT) aims to track multiple objects while maintaining consistent identities across frames of a given video. In unmanned aerial vehicle (UAV) recorded videos, frequent viewpoint changes and complex UAV-ground relative motion dynamics pose significant challenges, which often lead to unstable affinity measurement and ambiguous association. Existing methods typically model motion and appearance cues separately, overlooking their spatio-temporal interplay and resulting in suboptimal tracking performance. In this work, we propose AMOT, which jointly exploits appearance and motion cues through two key components: an Appearance-Motion Consistency (AMC) matrix and a Motion-aware Track Continuation (MTC) module. Specifically, the AMC matrix computes bi-directional spatial consistency under the guidance of appearance features, enabling more reliable and context-aware identity association. The MTC module complements AMC by reactivating unmatched tracks through appearance-guided predictions that align with Kalman-based predictions, thereby reducing broken trajectories caused by missed detections. Extensive experiments on three UAV benchmarks, including VisDrone2019, UAVDT, and VT-MOT-UAV, demonstrate that our AMOT outperforms current state-of-the-art methods and generalizes well in a plug-and-play and training-free manner.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffusionFF: A Diffusion-based Framework for Joint Face Forgery Detection and Fine-Grained Artifact Localization</title>
<link>https://arxiv.org/abs/2508.01873</link>
<guid>https://arxiv.org/abs/2508.01873</guid>
<content:encoded><![CDATA[
arXiv:2508.01873v2 Announce Type: replace 
Abstract: The rapid evolution of deepfake technologies demands robust and reliable face forgery detection algorithms. While determining whether an image has been manipulated remains essential, the ability to precisely localize forgery clues is also important for enhancing model explainability and building user trust. To address this dual challenge, we introduce DiffusionFF, a diffusion-based framework that simultaneously performs face forgery detection and fine-grained artifact localization. Our key idea is to establish a novel encoder-decoder architecture: a pretrained forgery detector serves as a powerful "artifact encoder", and a denoising diffusion model is repurposed as an "artifact decoder". Conditioned on multi-scale forgery-related features extracted by the encoder, the decoder progressively synthesizes a detailed artifact localization map. We then fuse this fine-grained localization map with high-level semantic features from the forgery detector, leading to substantial improvements in detection capability. Extensive experiments show that DiffusionFF achieves state-of-the-art (SOTA) performance across multiple benchmarks, underscoring its superior effectiveness and explainability.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming</title>
<link>https://arxiv.org/abs/2508.02549</link>
<guid>https://arxiv.org/abs/2508.02549</guid>
<content:encoded><![CDATA[
arXiv:2508.02549v4 Announce Type: replace 
Abstract: Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth inputs to provide rich spatial cues for action planning, but these sensors can be costly or less accessible in real-world deployments. Recent approaches based on Vision-Language Action (VLA) models achieve strong results with monocular input, yet they still lag behind methods using panoramic RGB-D information. We present MonoDream, a lightweight VLA framework that enables monocular agents to learn a Unified Navigation Representation (UNR). This shared feature representation jointly aligns navigation-relevant visual semantics (e.g., global layout, depth, and future cues) and language-grounded action intent, enabling more reliable action prediction. MonoDream further introduces Latent Panoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to predict latent features of panoramic RGB and depth observations at both current and future steps based on only monocular input. Experiments on multiple VLN benchmarks show that MonoDream consistently improves monocular navigation performance and significantly narrows the gap with panoramic-based agents.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance</title>
<link>https://arxiv.org/abs/2508.03143</link>
<guid>https://arxiv.org/abs/2508.03143</guid>
<content:encoded><![CDATA[
arXiv:2508.03143v2 Announce Type: replace 
Abstract: Synthesizing realistic and spatially precise anomalies is essential for enhancing the robustness of industrial anomaly detection systems. While recent diffusion-based methods have demonstrated strong capabilities in modeling complex defect patterns, they often struggle with spatial controllability and fail to maintain fine-grained regional fidelity. To overcome these limitations, we propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained Diffusion with discriminative mask Guidance), a novel diffusion-based framework specifically designed for anomaly generation. Our approach introduces a Region-Constrained Diffusion (RCD) process that preserves the background by freezing it and selectively updating only the foreground anomaly regions during the reverse denoising phase, thereby effectively reducing background artifacts. Additionally, we incorporate a Discriminative Mask Guidance (DMG) module into the discriminator, enabling joint evaluation of both global realism and local anomaly fidelity, guided by pixel-level masks. Extensive experiments on the MVTec-AD and BTAD datasets show that SARD surpasses existing methods in segmentation accuracy and visual quality, setting a new state-of-the-art for pixel-level anomaly synthesis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution</title>
<link>https://arxiv.org/abs/2508.03244</link>
<guid>https://arxiv.org/abs/2508.03244</guid>
<content:encoded><![CDATA[
arXiv:2508.03244v2 Announce Type: replace 
Abstract: Event cameras offer unparalleled advantages such as high temporal resolution, low latency, and high dynamic range. However, their limited spatial resolution poses challenges for fine-grained perception tasks. In this work, we propose an ultra-lightweight, stream-based event-to-event super-resolution method based on Spiking Neural Networks (SNNs), designed for real-time deployment on resource-constrained devices. To further reduce model size, we introduce a novel Dual-Forward Polarity-Split Event Encoding strategy that decouples positive and negative events into separate forward paths through a shared SNN. Furthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss (LearnSTPLoss) that adaptively balances temporal, spatial, and polarity consistency using learnable uncertainty-based weights. Experimental results demonstrate that our method achieves competitive super-resolution performance on multiple datasets while significantly reducing model size and inference time. The lightweight design enables embedding the module into event cameras or using it as an efficient front-end preprocessing for downstream vision tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIS3R: Very Large Parallax Image Stitching via Deep 3D Reconstruction</title>
<link>https://arxiv.org/abs/2508.04236</link>
<guid>https://arxiv.org/abs/2508.04236</guid>
<content:encoded><![CDATA[
arXiv:2508.04236v2 Announce Type: replace 
Abstract: Image stitching aim to align two images taken from different viewpoints into one seamless, wider image. However, when the 3D scene contains depth variations and the camera baseline is significant, noticeable parallax occurs-meaning the relative positions of scene elements differ substantially between views. Most existing stitching methods struggle to handle such images with large parallax effectively. To address this challenge, in this paper, we propose an image stitching solution called PIS3R that is robust to very large parallax based on the novel concept of deep 3D reconstruction. First, we apply visual geometry grounded transformer to two input images with very large parallax to obtain both intrinsic and extrinsic parameters, as well as the dense 3D scene reconstruction. Subsequently, we reproject reconstructed dense point cloud onto a designated reference view using the recovered camera parameters, achieving pixel-wise alignment and generating an initial stitched image. Finally, to further address potential artifacts such as holes or noise in the initial stitching, we propose a point-conditioned image diffusion module to obtain the refined result.Compared with existing methods, our solution is very large parallax tolerant and also provides results that fully preserve the geometric integrity of all pixels in the 3D photogrammetric context, enabling direct applicability to downstream 3D vision tasks such as SfM. Experimental results demonstrate that the proposed algorithm provides accurate stitching results for images with very large parallax, and outperforms the existing methods qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images</title>
<link>https://arxiv.org/abs/2508.06224</link>
<guid>https://arxiv.org/abs/2508.06224</guid>
<content:encoded><![CDATA[
arXiv:2508.06224v2 Announce Type: replace 
Abstract: Accurate semantic segmentation of urban remote sensing images (URSIs) is essential for urban planning and environmental monitoring. However, it remains challenging due to the subtle texture differences and similar spatial structures among geospatial objects, which cause semantic ambiguity and misclassification. Additional complexities arise from irregular object shapes, blurred boundaries, and overlapping spatial distributions of objects, resulting in diverse and intricate edge morphologies. To address these issues, we propose TEFormer, a texture-aware and edge-guided Transformer. Our model features a texture-aware module (TaM) in the encoder to capture fine-grained texture distinctions between visually similar categories, thereby enhancing semantic discrimination. The decoder incorporates an edge-guided tri-branch decoder (Eg3Head) to preserve local edges and details while maintaining multiscale context-awareness. Finally, an edge-guided feature fusion module (EgFFM) effectively integrates contextual, detail, and edge information to achieve refined semantic segmentation. Extensive evaluation demonstrates that TEFormer yields mIoU scores of 88.57% on Potsdam and 81.46% on Vaihingen, exceeding the next best methods by 0.73% and 0.22%. On the LoveDA dataset, it secures the second position with an overall mIoU of 53.55%, trailing the optimal performance by a narrow margin of 0.19%.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation</title>
<link>https://arxiv.org/abs/2508.07769</link>
<guid>https://arxiv.org/abs/2508.07769</guid>
<content:encoded><![CDATA[
arXiv:2508.07769v2 Announce Type: replace 
Abstract: The synthesis of spatiotemporally coherent 4D content presents fundamental challenges in computer vision, requiring simultaneous modeling of high-fidelity spatial representations and physically plausible temporal dynamics. Current approaches often struggle to maintain view consistency while handling complex scene dynamics, particularly in large-scale environments with multiple interacting elements. This work introduces Dream4D, a novel framework that bridges this gap through a synergy of controllable video generation and neural 4D reconstruction. Our approach seamlessly combines a two-stage architecture: it first predicts optimal camera trajectories from a single image using few-shot learning, then generates geometrically consistent multi-view sequences via a specialized pose-conditioned diffusion process, which are finally converted into a persistent 4D representation. This framework is the first to leverage both rich temporal priors from video diffusion models and geometric awareness of the reconstruction models, which significantly facilitates 4D generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality</title>
<link>https://arxiv.org/abs/2508.09185</link>
<guid>https://arxiv.org/abs/2508.09185</guid>
<content:encoded><![CDATA[
arXiv:2508.09185v3 Announce Type: replace 
Abstract: Augmented Reality (AR) enriches human perception by overlaying virtual elements onto the physical world. However, this tight coupling between virtual and real content makes AR vulnerable to cognitive attacks: manipulations that distort users' semantic understanding of the environment. Existing detection methods largely focus on visual inconsistencies at the pixel or image level, offering limited semantic reasoning or interpretability. To address these limitations, we introduce CADAR, a neuro-symbolic framework for cognitive attack detection in AR that integrates neural and symbolic reasoning. CADAR fuses multimodal vision-language representations from pre-trained models into a perception graph that captures objects, relations, and temporal contextual salience. Building on this structure, a particle-filter-based statistical reasoning module infers anomalies in semantic dynamics to reveal cognitive attacks. This combination provides both the adaptability of modern vision-language models and the interpretability of probabilistic symbolic reasoning. Preliminary experiments on an AR cognitive-attack dataset demonstrate consistent advantages over existing approaches, highlighting the potential of neuro-symbolic methods for robust and interpretable AR security.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holistic Evaluation of Multimodal LLMs on Spatial Intelligence</title>
<link>https://arxiv.org/abs/2508.13142</link>
<guid>https://arxiv.org/abs/2508.13142</guid>
<content:encoded><![CDATA[
arXiv:2508.13142v4 Announce Type: replace 
Abstract: Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence (SI). We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a growing collection of newly curated ones, enabling systematic evaluation of state-of-the-art models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in SI, yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail the most advanced multimodal models. EASI is an ongoing community effort: we have open-sourced the EASI codebase that provides a one-stop and reproducible solution with standardized interfaces, integrated protocols and prompts that significantly reduce the friction of configuring and running multiple benchmarks; we have also launched an accompanying EASI leaderboard to provide a continually updated snapshot of model performance across the full SI spectrum, accelerating collective progress toward robust SI.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification</title>
<link>https://arxiv.org/abs/2508.20461</link>
<guid>https://arxiv.org/abs/2508.20461</guid>
<content:encoded><![CDATA[
arXiv:2508.20461v2 Announce Type: replace 
Abstract: We propose a novel medical image classification method that integrates dual-model weight selection with self-knowledge distillation (SKD). In real-world medical settings, deploying large-scale models is often limited by computational resource constraints, which pose significant challenges for their practical implementation. Thus, developing lightweight models that achieve comparable performance to large-scale models while maintaining computational efficiency is crucial. To address this, we employ a dual-model weight selection strategy that initializes two lightweight models with weights derived from a large pretrained model, enabling effective knowledge transfer. Next, SKD is applied to these selected models, allowing the use of a broad range of initial weight configurations without imposing additional excessive computational cost, followed by fine-tuning for the target classification tasks. By combining dual-model weight selection with self-knowledge distillation, our method overcomes the limitations of conventional approaches, which often fail to retain critical information in compact models. Extensive experiments on publicly available datasets-chest X-ray images, lung computed tomography scans, and brain magnetic resonance imaging scans-demonstrate the superior performance and robustness of our approach compared to existing methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
<link>https://arxiv.org/abs/2509.06165</link>
<guid>https://arxiv.org/abs/2509.06165</guid>
<content:encoded><![CDATA[
arXiv:2509.06165v3 Announce Type: replace 
Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual content by detecting objects and modeling their temporal interactions as structured graphs. Prior studies typically target either coarse-grained box-level or fine-grained panoptic pixel-level VidSGG, often requiring task-specific architectures and multi-stage training pipelines. In this paper, we present UNO (UNified Object-centric VidSGG), a single-stage, unified framework that jointly addresses both tasks within an end-to-end architecture. UNO is designed to minimize task-specific modifications and maximize parameter sharing, enabling generalization across different levels of visual granularity. The core of UNO is an extended slot attention mechanism that decomposes visual features into object and relation slots. To ensure robust temporal modeling, we introduce object temporal consistency learning, which enforces consistent object representations across frames without relying on explicit tracking modules. Additionally, a dynamic triplet prediction module links relation slots to corresponding object pairs, capturing evolving interactions over time. We evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results demonstrate that UNO not only achieves competitive performance across both tasks but also offers improved efficiency through a unified, object-centric design.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2509.08422</link>
<guid>https://arxiv.org/abs/2509.08422</guid>
<content:encoded><![CDATA[
arXiv:2509.08422v3 Announce Type: replace 
Abstract: Video-based AI systems are increasingly adopted in safety-critical domains such as autonomous driving and healthcare. However, interpreting their decisions remains challenging due to the inherent spatiotemporal complexity of video data and the opacity of deep learning models. Existing explanation techniques often suffer from limited temporal coherence and a lack of actionable causal insights. Current counterfactual explanation methods typically do not incorporate guidance from the target model, reducing semantic fidelity and practical utility. We introduce Latent Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework designed to explain the behavior of video-based AI models. Compared to previous approaches, LD-ViCE reduces the computational costs of generating explanations by operating in latent space using a state-of-the-art diffusion model, while producing realistic and interpretable counterfactuals through an additional refinement step. Experiments on three diverse video datasets - EchoNet-Dynamic (cardiac ultrasound), FERV39k (facial expression), and Something-Something V2 (action recognition) with multiple target models covering both classification and regression tasks, demonstrate that LD-ViCE generalizes well and achieves state-of-the-art performance. On the EchoNet-Dynamic dataset, LD-ViCE achieves significantly higher regression accuracy than prior methods and exhibits high temporal consistency, while the refinement stage further improves perceptual quality. Qualitative analyses confirm that LD-ViCE produces semantically meaningful and temporally coherent explanations, providing actionable insights into model behavior. LD-ViCE advances the trustworthiness and interpretability of video-based AI systems through visually coherent counterfactual explanations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SegDINO3D: 3D Instance Segmentation Empowered by Both Image-Level and Object-Level 2D Features</title>
<link>https://arxiv.org/abs/2509.16098</link>
<guid>https://arxiv.org/abs/2509.16098</guid>
<content:encoded><![CDATA[
arXiv:2509.16098v2 Announce Type: replace 
Abstract: In this paper, we present SegDINO3D, a novel Transformer encoder-decoder framework for 3D instance segmentation. As 3D training data is generally not as sufficient as 2D training images, SegDINO3D is designed to fully leverage 2D representation from a pre-trained 2D detection model, including both image-level and object-level features, for improving 3D representation. SegDINO3D takes both a point cloud and its associated 2D images as input. In the encoder stage, it first enriches each 3D point by retrieving 2D image features from its corresponding image views and then leverages a 3D encoder for 3D context fusion. In the decoder stage, it formulates 3D object queries as 3D anchor boxes and performs cross-attention from 3D queries to 2D object queries obtained from 2D images using the 2D detection model. These 2D object queries serve as a compact object-level representation of 2D images, effectively avoiding the challenge of keeping thousands of image feature maps in the memory while faithfully preserving the knowledge of the pre-trained 2D model. The introducing of 3D box queries also enables the model to modulate cross-attention using the predicted boxes for more precise querying. SegDINO3D achieves the state-of-the-art performance on the ScanNetV2 and ScanNet200 3D instance segmentation benchmarks. Notably, on the challenging ScanNet200 dataset, SegDINO3D significantly outperforms prior methods by +8.6 and +6.8 mAP on the validation and hidden test sets, respectively, demonstrating its superiority.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis</title>
<link>https://arxiv.org/abs/2509.20295</link>
<guid>https://arxiv.org/abs/2509.20295</guid>
<content:encoded><![CDATA[
arXiv:2509.20295v3 Announce Type: replace 
Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: https://github.com/Chhro123/fast-foreground-aware-anomaly-synthesis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAEmnesia: Erasing Concepts in Diffusion Models with Supervised Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2509.21379</link>
<guid>https://arxiv.org/abs/2509.21379</guid>
<content:encoded><![CDATA[
arXiv:2509.21379v2 Announce Type: replace 
Abstract: Concept unlearning in diffusion models is hampered by feature splitting, where concepts are distributed across many latent features, making their removal challenging and computationally expensive. We introduce SAEmnesia, a supervised sparse autoencoder framework that overcomes this by enforcing one-to-one concept-neuron mappings. By systematically labeling concepts during training, our method achieves feature centralization, binding each concept to a single, interpretable neuron. This enables highly targeted and efficient concept erasure. SAEmnesia reduces hyperparameter search by 96.7% and achieves a 9.2% improvement over the state-of-the-art on the UnlearnCanvas benchmark. Our method also demonstrates superior scalability in sequential unlearning, improving accuracy by 28.4% when removing nine objects, establishing a new standard for precise and controllable concept erasure. Moreover, SAEmnesia mitigates the possibility of generating unwanted content under adversarial attack and effectively removes nudity when evaluated with I2P.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing</title>
<link>https://arxiv.org/abs/2509.22244</link>
<guid>https://arxiv.org/abs/2509.22244</guid>
<content:encoded><![CDATA[
arXiv:2509.22244v4 Announce Type: replace 
Abstract: Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EasyOcc: 3D Pseudo-Label Supervision for Fully Self-Supervised Semantic Occupancy Prediction Models</title>
<link>https://arxiv.org/abs/2509.26087</link>
<guid>https://arxiv.org/abs/2509.26087</guid>
<content:encoded><![CDATA[
arXiv:2509.26087v3 Announce Type: replace 
Abstract: Self-supervised models have recently achieved notable advancements, particularly in the domain of semantic occupancy prediction. These models utilize sophisticated loss computation strategies to compensate for the absence of ground-truth labels. For instance, techniques such as novel view synthesis, cross-view rendering, and depth estimation have been explored to address the issue of semantic and depth ambiguity. However, such techniques typically incur high computational costs and memory usage during the training stage, especially in the case of novel view synthesis. To mitigate these issues, we propose 3D pseudo-ground-truth labels generated by the foundation models Grounded-SAM and Metric3Dv2, and harness temporal information for label densification. Our 3D pseudo-labels can be easily integrated into existing models, which yields substantial performance improvements, with mIoU increasing by 45\%, from 9.73 to 14.09, when implemented into the OccNeRF model. This stands in contrast to earlier advancements in the field, which are often not readily transferable to other architectures. Additionally, we propose a streamlined model, EasyOcc, achieving 13.86 mIoU. This model conducts learning solely from our labels, avoiding complex rendering strategies mentioned previously. Furthermore, our method enables models to attain state-of-the-art performance when evaluated on the full scene without applying the camera mask, with EasyOcc achieving 7.71 mIoU, outperforming the previous best model by 31\%. These findings highlight the critical importance of foundation models, temporal context, and the choice of loss computation space in self-supervised learning for comprehensive scene understanding.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency</title>
<link>https://arxiv.org/abs/2510.07119</link>
<guid>https://arxiv.org/abs/2510.07119</guid>
<content:encoded><![CDATA[
arXiv:2510.07119v2 Announce Type: replace 
Abstract: Monocular 3D foundation models offer an extensible solution for perception tasks, making them attractive for broader 3D vision applications. In this paper, we propose MoRe, a training-free Monocular Geometry Refinement method designed to improve cross-view consistency and achieve scale alignment. To induce inter-frame relationships, our method employs feature matching between frames to establish correspondences. Rather than applying simple least squares optimization on these matched points, we formulate a graph-based optimization framework that performs local planar approximation using the estimated 3D points and surface normals estimated by monocular foundation models. This formulation addresses the scale ambiguity inherent in monocular geometric priors while preserving the underlying 3D structure. We further demonstrate that MoRe not only enhances 3D reconstruction but also improves novel view synthesis, particularly in sparse view rendering scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation</title>
<link>https://arxiv.org/abs/2510.17529</link>
<guid>https://arxiv.org/abs/2510.17529</guid>
<content:encoded><![CDATA[
arXiv:2510.17529v2 Announce Type: replace 
Abstract: Active Surveillance (AS) is a treatment option for managing low and intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while monitoring disease progression through serial MRI and clinical follow-up. Accurate prostate segmentation is an important preliminary step for automating this process, enabling automated detection and diagnosis of PCa. However, existing deep-learning segmentation models are often trained on single-time-point and expertly annotated datasets, making them unsuitable for longitudinal AS analysis, where multiple time points and a scarcity of expert labels hinder their effective fine-tuning. To address these challenges, we propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation architecture that computes the segmentation for time point t by leveraging the MRI and the corresponding segmentation mask from the previous time point. We introduce two new components: (i) a Mamba-enhanced Cross-Attention Module, which integrates the Mamba block into cross attention to efficiently capture temporal evolution and long-range spatial dependencies, and (ii) a Shape Extractor Module that encodes the previous segmentation mask into a latent anatomical representation for refined zone delination. Moreover, we introduce a semi-supervised self-training strategy that leverages pseudo-labels generated from a pre-trained nnU-Net, enabling effective learning without expert annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results showed that it significantly outperforms state-of-the-art U-Net and Transformer-based models, achieving superior prostate zone segmentation even when trained on limited and noisy data.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM 2++: Tracking Anything at Any Granularity</title>
<link>https://arxiv.org/abs/2510.18822</link>
<guid>https://arxiv.org/abs/2510.18822</guid>
<content:encoded><![CDATA[
arXiv:2510.18822v3 Announce Type: replace 
Abstract: Video tracking aims at finding the specific target in subsequent frames given its initial state. Due to the varying granularity of target states across different tasks, most existing trackers are tailored to a single task and heavily rely on custom-designed modules within the individual task, which limits their generalization and leads to redundancy in both model design and parameters. To unify video tracking tasks, we present SAM 2++, a unified model towards tracking at any granularity, including masks, boxes, and points. First, to extend target granularity, we design task-specific prompts to encode various task inputs into general prompt embeddings, and a unified decoder to unify diverse task results into a unified form pre-output. Next, to satisfy memory matching, the core operation of tracking, we introduce a task-adaptive memory mechanism that unifies memory across different granularities. Finally, we introduce a customized data engine to support tracking training at any granularity, producing a large and diverse video tracking dataset with rich annotations at three granularities, termed Tracking-Any-Granularity, which represents a comprehensive resource for training and benchmarking on unified tracking. Comprehensive experiments on multiple benchmarks confirm that SAM 2++ sets a new state of the art across diverse tracking tasks at different granularities, establishing a unified and robust tracking framework.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autoregressive Styled Text Image Generation, but Make it Reliable</title>
<link>https://arxiv.org/abs/2510.23240</link>
<guid>https://arxiv.org/abs/2510.23240</guid>
<content:encoded><![CDATA[
arXiv:2510.23240v2 Announce Type: replace 
Abstract: Generating faithful and readable styled text images (especially for Styled Handwritten Text generation - HTG) is an open problem with several possible applications across graphic design, document understanding, and image editing. A lot of research effort in this task is dedicated to developing strategies that reproduce the stylistic characteristics of a given writer, with promising results in terms of style fidelity and generalization achieved by the recently proposed Autoregressive Transformer paradigm for HTG. However, this method requires additional inputs, lacks a proper stop mechanism, and might end up in repetition loops, generating visual artifacts. In this work, we rethink the autoregressive formulation by framing HTG as a multimodal prompt-conditioned generation task, and tackle the content controllability issues by introducing special textual input tokens for better alignment with the visual ones. Moreover, we devise a Classifier-Free-Guidance-based strategy for our autoregressive model. Through extensive experimental validation, we demonstrate that our approach, dubbed Eruku, compared to previous solutions requires fewer inputs, generalizes better to unseen styles, and follows more faithfully the textual prompt, improving content adherence.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Relative Attention Guidance for Image Editing</title>
<link>https://arxiv.org/abs/2510.24657</link>
<guid>https://arxiv.org/abs/2510.24657</guid>
<content:encoded><![CDATA[
arXiv:2510.24657v2 Announce Type: replace 
Abstract: Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00391</link>
<guid>https://arxiv.org/abs/2511.00391</guid>
<content:encoded><![CDATA[
arXiv:2511.00391v2 Announce Type: replace 
Abstract: Multimodal code generation has garnered significant interest within the research community. Despite the notable success of recent vision-language models (VLMs) on specialized tasks like chart-to-code generation, their reliance on single-task training regimens fosters a narrow paradigm that hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode \textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a unified multimodal code generation model that addresses this limitation via a two-stage training framework. We begin by constructing a large-scale Supervised Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving direct code generation and visual-based code refinement. Subsequently, we introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a coarse-to-fine reward mechanism to improve visual fidelity by calculating visual similarity across local and global image patches. Extensive experiments on diverse multimodal code generation benchmarks demonstrate that VinciCoder achieves state-of-the-art performance, surpassing recent open-source models. The ablation study further validates the effectiveness of our proposed coarse-to-fine ViRL strategy. The data, code and model is available at https://github.com/DocTron-hub/VinciCoder.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation</title>
<link>https://arxiv.org/abs/2511.04675</link>
<guid>https://arxiv.org/abs/2511.04675</guid>
<content:encoded><![CDATA[
arXiv:2511.04675v2 Announce Type: replace 
Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long interactive video synthesis via straightforward temporal autoregression. Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing some diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10x faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ambiguity-aware Truncated Flow Matching for Ambiguous Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.06857</link>
<guid>https://arxiv.org/abs/2511.06857</guid>
<content:encoded><![CDATA[
arXiv:2511.06857v2 Announce Type: replace 
Abstract: A simultaneous enhancement of accuracy and diversity of predictions remains a challenge in ambiguous medical image segmentation (AMIS) due to the inherent trade-offs. While truncated diffusion probabilistic models (TDPMs) hold strong potential with a paradigm optimization, existing TDPMs suffer from entangled accuracy and diversity of predictions with insufficient fidelity and plausibility. To address the aforementioned challenges, we propose Ambiguity-aware Truncated Flow Matching (ATFM), which introduces a novel inference paradigm and dedicated model components. Firstly, we propose Data-Hierarchical Inference, a redefinition of AMIS-specific inference paradigm, which enhances accuracy and diversity at data-distribution and data-sample level, respectively, for an effective disentanglement. Secondly, Gaussian Truncation Representation (GTR) is introduced to enhance both fidelity of predictions and reliability of truncation distribution, by explicitly modeling it as a Gaussian distribution at $T_{\text{trunc}}$ instead of using sampling-based approximations. Thirdly, Segmentation Flow Matching (SFM) is proposed to enhance the plausibility of diverse predictions by extending semantic-aware flow transformation in Flow Matching (FM). Comprehensive evaluations on LIDC and ISIC3 datasets demonstrate that ATFM outperforms SOTA methods and simultaneously achieves a more efficient inference. ATFM improves GED and HM-IoU by up to $12\%$ and $7.3\%$ compared to advanced methods.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild</title>
<link>https://arxiv.org/abs/2511.08423</link>
<guid>https://arxiv.org/abs/2511.08423</guid>
<content:encoded><![CDATA[
arXiv:2511.08423v2 Announce Type: replace 
Abstract: A truly universal AI-Generated Image (AIGI) detector must simultaneously generalize across diverse generative models and varied semantic content. Current state-of-the-art methods learn a single, entangled forgery representation, conflating content-dependent flaws with content-agnostic artifacts, and are further constrained by outdated benchmarks. To overcome these limitations, we propose OmniAID, a novel framework centered on a decoupled Mixture-of-Experts (MoE) architecture. The core of our method is a hybrid expert system designed to decouple: (1) semantic flaws across distinct content domains, and (2) content-dependent flaws from content-agnostic universal artifacts. This system employs a set of Routable Specialized Semantic Experts, each for a distinct domain (e.g., human, animal), complemented by a Fixed Universal Artifact Expert. This architecture is trained using a novel two-stage strategy: we first train the experts independently with domain-specific hard-sampling to ensure specialization, and subsequently train a lightweight gating network for effective input routing. By explicitly decoupling "what is generated" (content-specific flaws) from "how it is generated" (universal artifacts), OmniAID achieves robust generalization. To address outdated benchmarks and validate real-world applicability, we introduce Mirage, a new large-scale, contemporary dataset. Extensive experiments, using both traditional benchmarks and our Mirage dataset, demonstrate our model surpasses existing monolithic detectors, establishing a new and robust standard for AIGI authentication against modern, in-the-wild threats.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIRNet: Integrating Constrained Graph-Based Reasoning with Pre-training for Diagnostic Medical Imaging</title>
<link>https://arxiv.org/abs/2511.10013</link>
<guid>https://arxiv.org/abs/2511.10013</guid>
<content:encoded><![CDATA[
arXiv:2511.10013v2 Announce Type: replace 
Abstract: Automated interpretation of medical images demands robust modeling of complex visual-semantic relationships while addressing annotation scarcity, label imbalance, and clinical plausibility constraints. We introduce MIRNet (Medical Image Reasoner Network), a novel framework that integrates self-supervised pre-training with constrained graph-based reasoning. Tongue image diagnosis is a particularly challenging domain that requires fine-grained visual and semantic understanding. Our approach leverages self-supervised masked autoencoder (MAE) to learn transferable visual representations from unlabeled data; employs graph attention networks (GAT) to model label correlations through expert-defined structured graphs; enforces clinical priors via constraint-aware optimization using KL divergence and regularization losses; and mitigates imbalance using asymmetric loss (ASL) and boosting ensembles. To address annotation scarcity, we also introduce TongueAtlas-4K, a comprehensive expert-curated benchmark comprising 4,000 images annotated with 22 diagnostic labels--representing the largest public dataset in tongue analysis. Validation shows our method achieves state-of-the-art performance. While optimized for tongue diagnosis, the framework readily generalizes to broader diagnostic medical imaging tasks.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CephRes-MHNet: A Multi-Head Residual Network for Accurate and Robust Cephalometric Landmark Detection</title>
<link>https://arxiv.org/abs/2511.10173</link>
<guid>https://arxiv.org/abs/2511.10173</guid>
<content:encoded><![CDATA[
arXiv:2511.10173v2 Announce Type: replace 
Abstract: Accurate localization of cephalometric landmarks from 2D lateral skull X-rays is vital for orthodontic diagnosis and treatment. Manual annotation is time-consuming and error-prone, whereas automated approaches often struggle with low contrast and anatomical complexity. This paper introduces CephRes-MHNet, a multi-head residual convolutional network for robust and efficient cephalometric landmark detection. The architecture integrates residual encoding, dual-attention mechanisms, and multi-head decoders to enhance contextual reasoning and anatomical precision. Trained on the Aariz Cephalometric dataset of 1,000 radiographs, CephRes-MHNet achieved a mean radial error (MRE) of 1.23 mm and a success detection rate (SDR) @ 2.0 mm of 85.5%, outperforming all evaluated models. In particular, it exceeded the strongest baseline, the attention-driven AFPF-Net (MRE = 1.25 mm, SDR @ 2.0 mm = 84.1%), while using less than 25% of its parameters. These results demonstrate that CephRes-MHNet attains state-of-the-art accuracy through architectural efficiency, providing a practical solution for real-world orthodontic analysis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space</title>
<link>https://arxiv.org/abs/2511.10555</link>
<guid>https://arxiv.org/abs/2511.10555</guid>
<content:encoded><![CDATA[
arXiv:2511.10555v5 Announce Type: replace 
Abstract: Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reverberation: Learning the Latencies Before Forecasting Trajectories</title>
<link>https://arxiv.org/abs/2511.11164</link>
<guid>https://arxiv.org/abs/2511.11164</guid>
<content:encoded><![CDATA[
arXiv:2511.11164v2 Announce Type: replace 
Abstract: Bridging the past to the future, connecting agents both spatially and temporally, lies at the core of the trajectory prediction task. Despite great efforts, it remains challenging to explicitly learn and predict latencies, i.e., response intervals or temporal delays with which agents respond to various trajectory-changing events and adjust their future paths, whether on their own or interactively. Different agents may exhibit distinct latency preferences for noticing, processing, and reacting to a specific trajectory-changing event. The lack of consideration of such latencies may undermine the causal continuity of forecasting systems, leading to implausible or unintended trajectories. Inspired by reverberation in acoustics, we propose a new reverberation transform and the corresponding Reverberation (short for Rev) trajectory prediction model, which predicts both individual latency preferences and their stochastic variations accordingly, by using two explicit and learnable reverberation kernels, enabling latency-conditioned and controllable trajectory prediction of both non-interactive and social latencies. Experiments on multiple datasets, whether pedestrians or vehicles, demonstrate that Rev achieves competitive accuracy while revealing interpretable latency dynamics across agents and scenarios. Qualitative analyses further verify the properties of the reverberation transform, highlighting its potential as a general latency modeling approach.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CountSteer: Steering Attention for Object Counting in Diffusion Models</title>
<link>https://arxiv.org/abs/2511.11253</link>
<guid>https://arxiv.org/abs/2511.11253</guid>
<content:encoded><![CDATA[
arXiv:2511.11253v2 Announce Type: replace 
Abstract: Text-to-image diffusion models generate realistic and coherent images but often fail to follow numerical instructions in text, revealing a gap between language and visual representation. Interestingly, we found that these models are not entirely blind to numbers-they are implicitly aware of their own counting accuracy, as their internal signals shift in consistent ways depending on whether the output meets the specified count. This observation suggests that the model already encodes a latent notion of numerical correctness, which can be harnessed to guide generation more precisely. Building on this intuition, we introduce CountSteer, a training-free method that improves generation of specified object counts by steering the model's cross-attention hidden states during inference. In our experiments, CountSteer improved object-count accuracy by about 4% without compromising visual quality, demonstrating a simple yet effective step toward more controllable and semantically reliable text-to-image generation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)</title>
<link>https://arxiv.org/abs/2511.11882</link>
<guid>https://arxiv.org/abs/2511.11882</guid>
<content:encoded><![CDATA[
arXiv:2511.11882v2 Announce Type: replace 
Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes</title>
<link>https://arxiv.org/abs/2511.12932</link>
<guid>https://arxiv.org/abs/2511.12932</guid>
<content:encoded><![CDATA[
arXiv:2511.12932v3 Announce Type: replace 
Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI</title>
<link>https://arxiv.org/abs/2511.13232</link>
<guid>https://arxiv.org/abs/2511.13232</guid>
<content:encoded><![CDATA[
arXiv:2511.13232v2 Announce Type: replace 
Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Spatial Intelligence with Multimodal Foundation Models</title>
<link>https://arxiv.org/abs/2511.13719</link>
<guid>https://arxiv.org/abs/2511.13719</guid>
<content:encoded><![CDATA[
arXiv:2511.13719v2 Announce Type: replace 
Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplitFlux: Learning to Decouple Content and Style from a Single Image</title>
<link>https://arxiv.org/abs/2511.15258</link>
<guid>https://arxiv.org/abs/2511.15258</guid>
<content:encoded><![CDATA[
arXiv:2511.15258v2 Announce Type: replace 
Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Stream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single stream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases</title>
<link>https://arxiv.org/abs/2511.15656</link>
<guid>https://arxiv.org/abs/2511.15656</guid>
<content:encoded><![CDATA[
arXiv:2511.15656v2 Announce Type: replace 
Abstract: Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM</title>
<link>https://arxiv.org/abs/2511.16282</link>
<guid>https://arxiv.org/abs/2511.16282</guid>
<content:encoded><![CDATA[
arXiv:2511.16282v2 Announce Type: replace 
Abstract: We present a fast, spatio-temporal scene understanding framework based on Visual Geometry Grounded Transformer (VGGT). The proposed pipeline is designed to enable efficient, close to real-time performance, supporting applications including assistive navigation. To achieve continuous updates of the 3D scene representation, we process the image flow with a sliding window, aligning submaps, thereby overcoming VGGT's high memory demands. We exploit the VGGT tracking head to aggregate 2D semantic instance masks into 3D objects. To allow for temporal consistency and richer contextual reasoning the system stores timestamps and instance-level identities, thereby enabling the detection of changes in the environment. We evaluate the approach on well-known benchmarks and custom datasets specifically designed for assistive navigation scenarios. The results demonstrate the applicability of the framework to real-world scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios</title>
<link>https://arxiv.org/abs/2511.16901</link>
<guid>https://arxiv.org/abs/2511.16901</guid>
<content:encoded><![CDATA[
arXiv:2511.16901v2 Announce Type: replace 
Abstract: Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis</title>
<link>https://arxiv.org/abs/2511.17045</link>
<guid>https://arxiv.org/abs/2511.17045</guid>
<content:encoded><![CDATA[
arXiv:2511.17045v2 Announce Type: replace 
Abstract: We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Loomis Painter: Reconstructing the Painting Process</title>
<link>https://arxiv.org/abs/2511.17344</link>
<guid>https://arxiv.org/abs/2511.17344</guid>
<content:encoded><![CDATA[
arXiv:2511.17344v2 Announce Type: replace 
Abstract: Step-by-step painting tutorials are vital for learning artistic techniques, but existing video resources (e.g., YouTube) lack interactivity and personalization. While recent generative models have advanced artistic image synthesis, they struggle to generalize across media and often show temporal or structural inconsistencies, hindering faithful reproduction of human creative workflows. To address this, we propose a unified framework for multi-media painting process generation with a semantics-driven style control mechanism that embeds multiple media into a diffusion models conditional space and uses cross-medium style augmentation. This enables consistent texture evolution and process transfer across styles. A reverse-painting training strategy further ensures smooth, human-aligned generation. We also build a large-scale dataset of real painting processes and evaluate cross-media consistency, temporal coherence, and final-image fidelity, achieving strong results on LPIPS, DINO, and CLIP metrics. Finally, our Perceptual Distance Profile (PDP) curve quantitatively models the creative sequence, i.e., composition, color blocking, and detail refinement, mirroring human artistic progression.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Gradient Methods for Data-Consistent Local Super-Resolution of Medical Images</title>
<link>https://arxiv.org/abs/2202.10875</link>
<guid>https://arxiv.org/abs/2202.10875</guid>
<content:encoded><![CDATA[
arXiv:2202.10875v2 Announce Type: replace-cross 
Abstract: In this work, we propose a new paradigm of iterative model-based reconstruction algorithms for providing real-time solution for zooming-in and refining a region of interest in medical and clinical tomographic images. This algorithmic framework is tailored for a clinical need in medical imaging practice that after a reconstruction of the full tomographic image, the clinician may believe that some critical parts of the image are not clear enough, and may wish to see clearer these regions of interest. A naive approach (which is highly not recommended) would be to perform the global reconstruction of a higher resolution image, which has two major limitations: first, it is computationally inefficient, and second, the image regularization is still applied globally, which may over-smooth some local regions. Furthermore, if one wishes to fine-tune the regularization parameter for local parts, it would be computationally infeasible in practice for the case of using global reconstruction. Our new iterative approaches for such tasks are based on jointly utilizing the measurement information, efficient up-sampling/down-sampling across image spaces, and locally adjusted image prior for efficient and high-quality post-processing. The numerical results in low-dose X-ray CT image local zoom-in demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Sampling-Based Domain Generalization Study with Diffusion Generative Models</title>
<link>https://arxiv.org/abs/2310.09213</link>
<guid>https://arxiv.org/abs/2310.09213</guid>
<content:encoded><![CDATA[
arXiv:2310.09213v3 Announce Type: replace-cross 
Abstract: In this work, we investigate the domain generalization capabilities of diffusion models in the context of synthesizing images that are distinct from the training data. Instead of fine-tuning, we tackle this challenge from a sampling-based perspective using frozen, pre-trained diffusion models. Specifically, we demonstrate that arbitrary out-of-domain (OOD) images establish Gaussian priors in the latent spaces of a given model after inversion, and that these priors are separable from those of the original training domain. This OOD latent property allows us to synthesize new images of the target unseen domain by discovering qualified OOD latent encodings in the inverted noisy spaces, without altering the pre-trained models. Our cross-model and cross-domain experiments show that the proposed sampling-based method can expand the latent space and generate unseen images without impairing the generation quality of the original domain. We also showcase a practical application of our approach using astrophysical data, highlighting the potential of this generalization paradigm in data-sparse fields such as scientific exploration.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Solvers for Discrete Diffusion Models: Theory and Applications of High-Order Algorithms</title>
<link>https://arxiv.org/abs/2502.00234</link>
<guid>https://arxiv.org/abs/2502.00234</guid>
<content:encoded><![CDATA[
arXiv:2502.00234v2 Announce Type: replace-cross 
Abstract: Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis. However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms. Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $\tau$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $\tau$-leaping is limited by its first-order accuracy. In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error. We rigorously analyze the proposed schemes and establish the second-order accuracy of the $\theta$-Trapezoidal method in KL divergence. Empirical evaluations on GSM8K-level math-reasoning, GPT-2-level text, and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints, with consistent performance gains across models ranging from 200M to 8B. Our code is available at https://github.com/yuchen-zhu-zyc/DiscreteFastSolver.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Rendering for Multimodal Autonomous Driving: Merging Neural and Physics-Based Simulation</title>
<link>https://arxiv.org/abs/2503.09464</link>
<guid>https://arxiv.org/abs/2503.09464</guid>
<content:encoded><![CDATA[
arXiv:2503.09464v2 Announce Type: replace-cross 
Abstract: Neural reconstruction models for autonomous driving simulation have made significant strides in recent years, with dynamic models becoming increasingly prevalent. However, these models are typically limited to handling in-domain objects closely following their original trajectories. We introduce a hybrid approach that combines the strengths of neural reconstruction with physics-based rendering. This method enables the virtual placement of traditional mesh-based dynamic agents at arbitrary locations, adjustments to environmental conditions, and rendering from novel camera viewpoints. Our approach significantly enhances novel view synthesis quality -- especially for road surfaces and lane markings -- while maintaining interactive frame rates through our novel training method, NeRF2GS. This technique leverages the superior generalization capabilities of NeRF-based methods and the real-time rendering speed of 3D Gaussian Splatting (3DGS). We achieve this by training a customized NeRF model on the original images with depth regularization derived from a noisy LiDAR point cloud, then using it as a teacher model for 3DGS training. This process ensures accurate depth, surface normals, and camera appearance modeling as supervision. With our block-based training parallelization, the method can handle large-scale reconstructions (greater than or equal to 100,000 square meters) and predict segmentation masks, surface normals, and depth maps. During simulation, it supports a rasterization-based rendering backend with depth-based composition and multiple camera models for real-time camera simulation, as well as a ray-traced backend for precise LiDAR simulation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Semantic Attribute Binding for Free-Lunch Color Control in Diffusion Models</title>
<link>https://arxiv.org/abs/2503.09864</link>
<guid>https://arxiv.org/abs/2503.09864</guid>
<content:encoded><![CDATA[
arXiv:2503.09864v2 Announce Type: replace-cross 
Abstract: Recent advances in text-to-image (T2I) diffusion models have enabled remarkable control over various attributes, yet precise color specification remains a fundamental challenge. Existing approaches, such as ColorPeel, rely on model personalization, requiring additional optimization and limiting flexibility in specifying arbitrary colors. In this work, we introduce ColorWave, a novel training-free approach that achieves exact RGB-level color control in diffusion models without fine-tuning. By systematically analyzing the cross-attention mechanisms within IP-Adapter, we uncover an implicit binding between textual color descriptors and reference image features. Leveraging this insight, our method rewires these bindings to enforce precise color attribution while preserving the generative capabilities of pretrained models. Our approach maintains generation quality and diversity, outperforming prior methods in accuracy and applicability across diverse object categories. Through extensive evaluations, we demonstrate that ColorWave establishes a new paradigm for structured, color-consistent diffusion-based image synthesis.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let it Snow! Animating 3D Gaussian Scenes with Dynamic Weather Effects via Physics-Guided Score Distillation</title>
<link>https://arxiv.org/abs/2504.05296</link>
<guid>https://arxiv.org/abs/2504.05296</guid>
<content:encoded><![CDATA[
arXiv:2504.05296v2 Announce Type: replace-cross 
Abstract: 3D Gaussian Splatting has recently enabled fast and photorealistic reconstruction of static 3D scenes. However, dynamic editing of such scenes remains a significant challenge. We introduce a novel framework, Physics-Guided Score Distillation, to address a fundamental conflict: physics simulation provides a strong motion prior that is insufficient for photorealism , while video-based Score Distillation Sampling (SDS) alone cannot generate coherent motion for complex, multi-particle scenarios. We resolve this through a unified optimization framework where physics simulation guides Score Distillation to jointly refine the motion prior for photorealism while simultaneously optimizing appearance. Specifically, we learn a neural dynamics model that predicts particle motion and appearance, optimized end-to-end via a combined loss integrating Video-SDS for photorealism with our physics-guidance prior. This allows for photorealistic refinements while ensuring the dynamics remain plausible. Our framework enables scene-wide dynamic weather effects, including snowfall, rainfall, fog, and sandstorms, with physically plausible motion. Experiments demonstrate our physics-guided approach significantly outperforms baselines, with ablations confirming this joint refinement is essential for generating coherent, high-fidelity dynamics.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Axial-UNet: A Neural Weather Model for Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2504.19408</link>
<guid>https://arxiv.org/abs/2504.19408</guid>
<content:encoded><![CDATA[
arXiv:2504.19408v2 Announce Type: replace-cross 
Abstract: Accurately predicting short-term precipitation is critical for weather-sensitive applications such as disaster management, aviation, and urban planning. Traditional numerical weather prediction can be computationally intensive at high resolution and short lead times. In this work, we propose a lightweight UNet-based encoder-decoder augmented with axial-attention blocks that attend along image rows and columns to capture long-range spatial interactions, while temporal context is provided by conditioning on multiple past radar frames. Our hybrid architecture captures both local and long-range spatio-temporal dependencies from radar image sequences, enabling fixed lead-time precipitation nowcasting with modest compute. Experimental results on a preprocessed subset of the HKO-7 radar dataset demonstrate that our model outperforms ConvLSTM, pix2pix-style cGANs, and a plain UNet in pixel-fidelity metrics, reaching PSNR 47.67 and SSIM 0.9943. We report PSNR/SSIM here; extending evaluation to meteorology-oriented skill measures (e.g., CSI/FSS) is left to future work. The approach is simple, scalable, and effective for resource-constrained, real-time forecasting scenarios.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network Inversion for Uncertainty-Aware Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2505.23448</link>
<guid>https://arxiv.org/abs/2505.23448</guid>
<content:encoded><![CDATA[
arXiv:2505.23448v2 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are critical components for building safe machine learning systems, especially in real-world scenarios where unexpected inputs are inevitable. However the two problems have, until recently, separately been addressed. In this work, we propose a novel framework that combines network inversion with classifier training to simultaneously address both OOD detection and uncertainty estimation. For a standard n-class classification task, we extend the classifier to an (n+1)-class model by introducing a "garbage" class, initially populated with random gaussian noise to represent outlier inputs. After each training epoch, we use network inversion to reconstruct input images corresponding to all output classes that initially appear as noisy and incoherent and are therefore excluded to the garbage class for retraining the classifier. This cycle of training, inversion, and exclusion continues iteratively till the inverted samples begin to resemble the in-distribution data more closely, with a significant drop in the uncertainty, suggesting that the classifier has learned to carve out meaningful decision boundaries while sanitising the class manifolds by pushing OOD content into the garbage class. During inference, this training scheme enables the model to effectively detect and reject OOD samples by classifying them into the garbage class. Furthermore, the confidence scores associated with each prediction can be used to estimate uncertainty for both in-distribution and OOD inputs. Our approach is scalable, interpretable, and does not require access to external OOD datasets or post-hoc calibration techniques while providing a unified solution to the dual challenges of OOD detection and uncertainty estimation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Regularity in Deterministic Sampling of Diffusion-based Generative Models</title>
<link>https://arxiv.org/abs/2506.10177</link>
<guid>https://arxiv.org/abs/2506.10177</guid>
<content:encoded><![CDATA[
arXiv:2506.10177v2 Announce Type: replace-cross 
Abstract: Diffusion-based generative models employ stochastic differential equations (SDEs) and their equivalent probability flow ordinary differential equations (ODEs) to establish a smooth transformation between complex high-dimensional data distributions and tractable prior distributions. In this paper, we reveal a striking geometric regularity in the deterministic sampling dynamics of diffusion generative models: each simulated sampling trajectory along the gradient field lies within an extremely low-dimensional subspace, and all trajectories exhibit an almost identical boomerang shape, regardless of the model architecture, applied conditions, or generated content. We characterize several intriguing properties of these trajectories, particularly under closed-form solutions based on kernel-estimated data modeling. We also demonstrate a practical application of the discovered trajectory regularity by proposing a dynamic programming-based scheme to better align the sampling time schedule with the underlying trajectory structure. This simple strategy requires minimal modification to existing deterministic numerical solvers, incurs negligible computational overhead, and achieves superior image generation performance, especially in regions with only 5 - 10 function evaluations.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers</title>
<link>https://arxiv.org/abs/2507.06764</link>
<guid>https://arxiv.org/abs/2507.06764</guid>
<content:encoded><![CDATA[
arXiv:2507.06764v3 Announce Type: replace-cross 
Abstract: In this work, we propose Fast Equivariant Imaging (FEI), a novel unsupervised learning framework to rapidly and efficiently train deep imaging networks without ground-truth data. From the perspective of reformulating the Equivariant Imaging based optimization problem via the method of Lagrange multipliers and utilizing plug-and-play denoisers, this novel unsupervised scheme shows superior efficiency and performance compared to the vanilla Equivariant Imaging paradigm. In particular, our FEI schemes achieve an order-of-magnitude (10x) acceleration over standard EI on training U-Net for X-ray CT reconstruction and image inpainting, with improved generalization performance.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks</title>
<link>https://arxiv.org/abs/2508.01943</link>
<guid>https://arxiv.org/abs/2508.01943</guid>
<content:encoded><![CDATA[
arXiv:2508.01943v2 Announce Type: replace-cross 
Abstract: Vision-language models (VLMs) have exhibited impressive capabilities across diverse image understanding tasks, but still struggle in settings that require reasoning over extended sequences of camera frames from a video. This limits their utility in embodied settings, which require reasoning over long frame sequences from a continuous stream of visual input at each moment of a task attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo Recursively), a framework that enables the model to recursively decompose long-horizon video trajectories into segments corresponding to shorter subtasks within the trajectory. In doing so, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences without losing global context. We evaluate ROVER, implemented using an in-context learning approach, on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa that consists of 543 videos showing both expert and perturbed non-expert trajectories across 27 robotic manipulation tasks. ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. We observe that, by reducing the number of frames the model reasons over at each timestep, ROVER mitigates hallucinations, especially during unexpected or non-optimal moments of a trajectory. In addition, by enabling the implementation of a subtask-specific sliding context window, ROVER's time complexity scales linearly with video length, an asymptotic improvement over baselines. Demos, code, and data available at: https://rover-vlm.github.io
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SACA: Selective Attention-Based Clustering Algorithm</title>
<link>https://arxiv.org/abs/2508.17150</link>
<guid>https://arxiv.org/abs/2508.17150</guid>
<content:encoded><![CDATA[
arXiv:2508.17150v2 Announce Type: replace-cross 
Abstract: Clustering algorithms are fundamental tools across many fields, with density-based methods offering particular advantages in identifying arbitrarily shaped clusters and handling noise. However, their effectiveness is often limited by the requirement of critical parameter tuning by users, which typically requires significant domain expertise. This paper introduces a novel density-based clustering algorithm loosely inspired by the concept of selective attention, designed to minimize reliance on parameter tuning for most applications. The proposed method computes an adaptive threshold to exclude sparsely distributed points and outliers, constructs an initial cluster framework, and subsequently reintegrates the filtered points to refine the final results. Extensive experiments on diverse benchmark datasets demonstrate the robustness, accuracy, and ease of use of the proposed approach, establishing it as a powerful alternative to conventional density-based clustering techniques.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activation Quantization of Vision Encoders Needs Prefixing Registers</title>
<link>https://arxiv.org/abs/2510.04547</link>
<guid>https://arxiv.org/abs/2510.04547</guid>
<content:encoded><![CDATA[
arXiv:2510.04547v3 Announce Type: replace-cross 
Abstract: Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\textit{RegCache}$, a training-free algorithm that mitigates outliers in large-scale pretrained vision encoders and serves as a plug-in module that can be applied on top of other quantization methods. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watch and Learn: Learning to Use Computers from Online Videos</title>
<link>https://arxiv.org/abs/2510.04673</link>
<guid>https://arxiv.org/abs/2510.04673</guid>
<content:encoded><![CDATA[
arXiv:2510.04673v2 Announce Type: replace-cross 
Abstract: Computer-using agents (CUAs) must plan task workflows across diverse and evolving applications, yet progress is limited by the lack of large-scale, high-quality training data. Existing datasets are narrow, static, and costly to annotate, while synthetic data often yields oversimplified or misaligned behaviors. We present Watch & Learn (W&amp;L), a framework that converts readily available Internet videos of human computer use into executable UI trajectories at scale. Instead of directly generating actions or relying on handcrafted heuristics, we cast trajectory annotation as an inverse dynamics problem that predicts user actions from consecutive screen states, which simplifies learning and generalizes across domains. Through a task-aware retrieval and labeling pipeline, W&amp;L yields over 53K high-quality trajectories that enhance CUAs both as in-context exemplars and as supervised training data. On OSWorld, it consistently improves general-purpose and specialized CUAs, while on WindowsAgentArena it achieves state-of-the-art performance among 7B-scale models under the 15-step limit. These results show that web-scale human demonstration videos can serve as a practical and scalable foundation for advancing real-world CUAs.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.19732</link>
<guid>https://arxiv.org/abs/2510.19732</guid>
<content:encoded><![CDATA[
arXiv:2510.19732v2 Announce Type: replace-cross 
Abstract: To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints. Our code is available at: https://github.com/gunshi/memo.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Adaptive Quantization for Variable Rate Image Coding for Machines</title>
<link>https://arxiv.org/abs/2511.05836</link>
<guid>https://arxiv.org/abs/2511.05836</guid>
<content:encoded><![CDATA[
arXiv:2511.05836v2 Announce Type: replace-cross 
Abstract: Image Coding for Machines (ICM) has become increasingly important with the rapid integration of computer vision technology into real-world applications. However, most neural network-based ICM frameworks operate at a fixed rate, thus requiring individual training for each target bitrate. This limitation may restrict their practical usage. Existing variable rate image compression approaches mitigate this issue but often rely on additional training, which increases computational costs and complicates deployment. Moreover, variable rate control has not been thoroughly explored for ICM. To address these challenges, we propose a training-free quantization strength control scheme that enables flexible bitrate adjustment. By exploiting the scale parameter predicted by the hyperprior network, the proposed method adaptively modulates quantization step sizes across both channel and spatial dimensions. This allows the model to preserve semantically important regions while coarsely quantizing less critical areas. Our architectural design further enables continuous bitrate control through a single parameter. Experimental results demonstrate the effectiveness of our proposed method, achieving up to 11.07% BD-rate savings over the non-adaptive variable rate baseline.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation</title>
<link>https://arxiv.org/abs/2511.06754</link>
<guid>https://arxiv.org/abs/2511.06754</guid>
<content:encoded><![CDATA[
arXiv:2511.06754v2 Announce Type: replace-cross 
Abstract: Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective</title>
<link>https://arxiv.org/abs/2511.11478</link>
<guid>https://arxiv.org/abs/2511.11478</guid>
<content:encoded><![CDATA[
arXiv:2511.11478v3 Announce Type: replace-cross 
Abstract: As embodied agents operate in increasingly complex environments, the ability to perceive, track, and reason about individual object instances over time becomes essential, especially in tasks requiring sequenced interactions with visually similar objects. In these non-Markovian settings, key decision cues are often hidden in object-specific histories rather than the current scene. Without persistent memory of prior interactions (what has been interacted with, where it has been, or how it has changed) visuomotor policies may fail, repeat past actions, or overlook completed ones. To surface this challenge, we introduce LIBERO-Mem, a non-Markovian task suite for stress-testing robotic manipulation under object-level partial observability. It combines short- and long-horizon object tracking with temporally sequenced subgoals, requiring reasoning beyond the current frame. However, vision-language-action (VLA) models often struggle in such settings, with token scaling quickly becoming intractable even for tasks spanning just a few hundred frames. We propose Embodied-SlotSSM, a slot-centric VLA framework built for temporal scalability. It maintains spatio-temporally consistent slot identities and leverages them through two mechanisms: (1) slot-state-space modeling for reconstructing short-term history, and (2) a relational encoder to align the input tokens with action decoding. Together, these components enable temporally grounded, context-aware action prediction. Experiments show Embodied-SlotSSM's baseline performance on LIBERO-Mem and general tasks, offering a scalable solution for non-Markovian reasoning in object-centric robotic policies.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.12861</link>
<guid>https://arxiv.org/abs/2511.12861</guid>
<content:encoded><![CDATA[
arXiv:2511.12861v4 Announce Type: replace-cross 
Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.
]]></content:encoded>
<pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point-Supervised Facial Expression Spotting with Gaussian-Based Instance-Adaptive Intensity Modeling</title>
<link>https://arxiv.org/abs/2511.16952</link>
<guid>https://arxiv.org/abs/2511.16952</guid>
<content:encoded><![CDATA[
<div> Keywords: facial expression spotting, point-supervised learning, Gaussian-based intensity modeling, apex classification, contrastive loss

<br /><br />Summary:  
This paper addresses the challenge of automatic facial expression spotting in untrimmed videos by proposing a point-supervised facial expression spotting (P-FES) approach, which requires only a single timestamp annotation per expression instance, reducing the dependency on costly temporal boundary labels. The authors introduce a two-branch framework: the first branch incorporates a Gaussian-based instance-adaptive intensity modeling (GIM) module to generate soft pseudo-labels by modeling the expression intensity distribution for each instance, thereby overcoming issues of hard pseudo-labeling that confuse neutral and expression frames. The GIM module detects the pseudo-apex frame, estimates expression duration, and constructs an instance-level Gaussian distribution to supervise the class-agnostic expression intensity branch. The second branch is a class-aware apex classification branch designed to differentiate macro- and micro-expressions based solely on pseudo-apex frames. During inference, these branches operate independently; the intensity branch produces expression proposals, while the apex classification branch categorizes the expressions. Additionally, an intensity-aware contrastive loss is proposed to improve feature discrimination and suppress neutral noise by contrasting neutral frames against expression frames with varying intensities. Extensive experiments on SAMM-LV, CAS(ME)$^2$, and CAS(ME)$^3$ datasets confirm the effectiveness of the framework. <div>
arXiv:2511.16952v2 Announce Type: replace 
Abstract: Automatic facial expression spotting, which aims to identify facial expression instances in untrimmed videos, is crucial for facial expression analysis. Existing methods primarily focus on fully-supervised learning and rely on costly, time-consuming temporal boundary annotations. In this paper, we investigate point-supervised facial expression spotting (P-FES), where only a single timestamp annotation per instance is required for training. We propose a unique two-branch framework for P-FES. First, to mitigate the limitation of hard pseudo-labeling, which often confuses neutral and expression frames with various intensities, we propose a Gaussian-based instance-adaptive intensity modeling (GIM) module to model instance-level expression intensity distribution for soft pseudo-labeling. By detecting the pseudo-apex frame around each point label, estimating the duration, and constructing an instance-level Gaussian distribution, GIM assigns soft pseudo-labels to expression frames for more reliable intensity supervision. The GIM module is incorporated into our framework to optimize the class-agnostic expression intensity branch. Second, we design a class-aware apex classification branch that distinguishes macro- and micro-expressions solely based on their pseudo-apex frames. During inference, the two branches work independently: the class-agnostic expression intensity branch generates expression proposals, while the class-aware apex-classification branch is responsible for macro- and micro-expression classification. Furthermore, we introduce an intensity-aware contrastive loss to enhance discriminative feature learning and suppress neutral noise by contrasting neutral frames with expression frames with various intensities. Extensive experiments on the SAMM-LV, CAS(ME)$^2$, and CAS(ME)$^3$ datasets demonstrate the effectiveness of our proposed framework.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Step Diffusion Transformer for Controllable Real-World Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.17138</link>
<guid>https://arxiv.org/abs/2511.17138</guid>
<content:encoded><![CDATA[
<div> Diffusion model, image super-resolution, controllability, fidelity, transformer  

<br /><br />Summary: Recent advancements in diffusion-based real-world image super-resolution (Real-ISR) exhibit high perceptual quality but struggle to balance fidelity and controllability. Multi-step diffusion methods produce diverse but less faithful outcomes due to inherent randomness, whereas one-step methods compromise control flexibility by relying on fidelity-specific fine-tuning. To solve these challenges, the paper introduces ODTSR, a novel one-step diffusion transformer architecture based on Qwen-Image. ODTSR incorporates a Noise-hybrid Visual Stream (NVS) design with two visual streams: one ingests low-quality images (LQ) with adjustable Control Noise for user-guided flexibility, while the other uses consistent Prior Noise to preserve fidelity. Additionally, Fidelity-aware Adversarial Training (FAA) enhances the control-fidelity balance and supports efficient one-step inference. Extensive experiments demonstrate that ODTSR achieves state-of-the-art performance on generic Real-ISR tasks. Importantly, ODTSR shows strong prompt controllability and generalizability on challenging scenarios such as real-world scene text image super-resolution (STISR) for Chinese characters without specialized training data. The method’s code is publicly available, signaling practical applicability and reproducibility. Overall, ODTSR represents an innovative approach to simultaneously optimize fidelity and controllability in Real-ISR through a single-step diffusion transformer framework. <div>
arXiv:2511.17138v2 Announce Type: replace 
Abstract: Recent advances in diffusion-based real-world image super-resolution (Real-ISR) have demonstrated remarkable perceptual quality, yet the balance between fidelity and controllability remains a problem: multi-step diffusion-based methods suffer from generative diversity and randomness, resulting in low fidelity, while one-step methods lose control flexibility due to fidelity-specific finetuning. In this paper, we present ODTSR, a one-step diffusion transformer based on Qwen-Image that performs Real-ISR considering fidelity and controllability simultaneously: a newly introduced visual stream receives low-quality images (LQ) with adjustable noise (Control Noise), and the original visual stream receives LQs with consistent noise (Prior Noise), forming the Noise-hybrid Visual Stream (NVS) design. ODTSR further employs Fidelity-aware Adversarial Training (FAA) to enhance controllability and achieve one-step inference. Extensive experiments demonstrate that ODTSR not only achieves state-of-the-art (SOTA) performance on generic Real-ISR, but also enables prompt controllability on challenging scenarios such as real-world scene text image super-resolution (STISR) of Chinese characters without training on specific datasets. Codes are available at $\href{https://github.com/RedMediaTech/ODTSR}{\text{this url}}$.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination</title>
<link>https://arxiv.org/abs/2511.17490</link>
<guid>https://arxiv.org/abs/2511.17490</guid>
<content:encoded><![CDATA[
<div> Text-rich videos, video QA, visual rumination, reinforcement learning, multimodal reasoning<br /><br />Summary:<br /><br />Understanding videos rich in textual information is challenging because visible text cues tend to be small, transient, and require repeated inspection, whereas most existing video question answering (QA) models rely on single-pass analysis of fixed frames, often resulting in hallucinations and errors in identifying fine-grained evidence. To address this, the authors introduce Video-R4, a large multimodal model (LMM) designed for visual rumination, which mimics human strategies by iteratively selecting relevant frames, zooming into critical regions, re-encoding pixel data, and updating its reasoning state for improved comprehension. The paper presents two novel datasets: Video-R4-CoT-17k for supervised learning of rumination behaviors and Video-R4-RL-30k for reinforcement learning to optimize the rumination process. They develop a multi-stage training framework that fine-tunes a 7-billion parameter LMM using supervised fine-tuning (SFT) combined with a generative reinforcement policy optimization (GRPO) approach. Video-R4-7B achieves state-of-the-art performance on the M4-ViteVQA benchmark and shows strong generalization to other tasks including multi-page document QA, slides QA, and general video QA. This work demonstrates that iterative visual rumination is an effective paradigm for pixel-grounded multimodal reasoning in text-rich video understanding. <div>
arXiv:2511.17490v3 Announce Type: replace 
Abstract: Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning. Project Page: https://yunlong10.github.io/Video-R4/
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshCone: Second-Order Cone Programming for Geometrically-Constrained Mesh Enhancement</title>
<link>https://arxiv.org/abs/2412.08484</link>
<guid>https://arxiv.org/abs/2412.08484</guid>
<content:encoded><![CDATA[
<div> Mesh refinement, convex optimization, second-order cone program, geometry-aware smoothing, ShapeNet<br /><br />Summary:<br /><br />1. MeshCone is a convex optimization framework designed for guided mesh refinement that leverages reference geometry to improve deformed or degraded mesh quality.<br /><br />2. The method formulates mesh refinement as a second-order cone program optimizing vertex positions to better align with target geometry while enforcing smoothness through convex edge-length regularization.<br /><br />3. MeshCone performs geometry-aware optimization that preserves fine details within the mesh while correcting structural defects commonly found in meshes from both learning-based and classical generation pipelines.<br /><br />4. The approach has been evaluated across 56 diverse object categories from datasets like ShapeNet and ThreeDScans, outperforming traditional methods such as Laplacian smoothing and unoptimized baselines in terms of refinement quality.<br /><br />5. MeshCone operates efficiently with sub-second inference times and is particularly suitable for workflows involving reference geometries, including mesh-from-template, scan-to-CAD alignment, and quality assurance in asset production pipelines. <div>
arXiv:2412.08484v4 Announce Type: replace-cross 
Abstract: Modern mesh generation pipelines whether learning-based or classical often produce outputs requiring post-processing to achieve production-quality geometry. This work introduces MeshCone, a convex optimization framework for guided mesh refinement that leverages reference geometry to correct deformed or degraded meshes. We formulate the problem as a second-order cone program where vertex positions are optimized to align with target geometry while enforcing smoothness through convex edge-length regularization. MeshCone performs geometry-aware optimization that preserves fine details while correcting structural defects. We demonstrate robust performance across 56 diverse object categories from ShapeNet and ThreeDScans, achieving superior refinement quality compared to Laplacian smoothing and unoptimized baselines while maintaining sub-second inference times. MeshCone is particularly suited for applications where reference geometry is available, such as mesh-from-template workflows, scan-to-CAD alignment, and quality assurance in asset production pipelines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Neuro-Inspired Multi-Modal Vision-Language Models Resilient to Membership Inference Privacy Leakage?</title>
<link>https://arxiv.org/abs/2511.20710</link>
<guid>https://arxiv.org/abs/2511.20710</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal models, privacy attack, membership inference attack, vision-language models, topological regularization  

<br /><br />Summary:  
This paper addresses privacy leakage concerns in multi-modal vision-language models (VLMs) arising from membership inference attacks (MIA), which pose new threats in the era of agentic AI. Unlike prior research focusing on unimodal systems, this study explores MIA vulnerabilities in multi-modal models and investigates whether neuroscience-inspired approaches can enhance resilience. The authors introduce a topological regularization framework, termed tau, to build neuro-inspired VLMs that potentially mitigate privacy risks. Experiments are conducted on three VLMs—BLIP, PaliGemma 2, and ViT-GPT2—using three benchmark datasets: COCO, CC3M, and NoCaps. Results show that setting tau > 0 defines the neuro VLM variant, which significantly reduces MIA success rates. Notably, on the BLIP model with the COCO dataset, neuro VLMs achieve a 24% decrease in mean ROC-AUC for attack success while maintaining comparable model utility measured by text similarity metrics such as MPNet and ROUGE-2. Further testing on the other models and datasets confirms the robustness and consistency of these findings. Overall, this work demonstrates that neuro-inspired topological regularization in multi-modal VLMs can enhance privacy resilience against inference attacks without substantially sacrificing model performance. <div>
arXiv:2511.20710v1 Announce Type: new 
Abstract: In the age of agentic AI, the growing deployment of multi-modal models (MMs) has introduced new attack vectors that can leak sensitive training data in MMs, causing privacy leakage. This paper investigates a black-box privacy attack, i.e., membership inference attack (MIA) on multi-modal vision-language models (VLMs). State-of-the-art research analyzes privacy attacks primarily to unimodal AI-ML systems, while recent studies indicate MMs can also be vulnerable to privacy attacks. While researchers have demonstrated that biologically inspired neural network representations can improve unimodal model resilience against adversarial attacks, it remains unexplored whether neuro-inspired MMs are resilient against privacy attacks. In this work, we introduce a systematic neuroscience-inspired topological regularization (tau) framework to analyze MM VLMs resilience against image-text-based inference privacy attacks. We examine this phenomenon using three VLMs: BLIP, PaliGemma 2, and ViT-GPT2, across three benchmark datasets: COCO, CC3M, and NoCaps. Our experiments compare the resilience of baseline and neuro VLMs (with topological regularization), where the tau > 0 configuration defines the NEURO variant of VLM. Our results on the BLIP model using the COCO dataset illustrate that MIA attack success in NEURO VLMs drops by 24% mean ROC-AUC, while achieving similar model utility (similarities between generated and reference captions) in terms of MPNet and ROUGE-2 metrics. This shows neuro VLMs are comparatively more resilient against privacy attacks, while not significantly compromising model utility. Our extensive evaluation with PaliGemma 2 and ViT-GPT2 models, on two additional datasets: CC3M and NoCaps, further validates the consistency of the findings. This work contributes to the growing understanding of privacy risks in MMs and provides evidence on neuro VLMs privacy threat resilience.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferix: A Block-Diffusion based Next-Generation Inference Engine for World Simulation</title>
<link>https://arxiv.org/abs/2511.20714</link>
<guid>https://arxiv.org/abs/2511.20714</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, semi-autoregressive decoding, video generation, Inferix, LV-Bench<br /><br />Summary:  
World models act as foundational simulators in agentic AI, embodied AI, and gaming, enabling the creation of long, physically realistic, and interactive high-quality videos. Scaling these models holds potential for emergent abilities in visual perception, understanding, and reasoning, signaling a shift beyond current large language model (LLM)-centric vision frameworks. A critical innovation is the semi-autoregressive (block-diffusion) decoding paradigm, which combines advantages of diffusion and autoregressive models by producing video tokens block-by-block with diffusion methods, conditioned on prior blocks, leading to enhanced coherence and stability. This approach overcomes traditional video diffusion limitations by reintroducing LLM-style key-value cache management, facilitating efficient, variable-length, and high-quality video generation. Inferix is introduced as a next-generation inference engine optimized for immersive world synthesis through this semi-autoregressive decoding, distinguishing itself from other tools designed for high concurrency or classic video diffusion. It offers additional capabilities such as interactive video streaming and detailed profiling to support real-time interaction and realistic world simulation. Furthermore, Inferix integrates seamlessly with LV-Bench, a specialized benchmarking suite for evaluating minute-long video generation tasks, fostering community collaboration to push forward research and development in world model technologies. <div>
arXiv:2511.20714v1 Announce Type: new 
Abstract: World models serve as core simulators for fields such as agentic AI, embodied AI, and gaming, capable of generating long, physically realistic, and interactive high-quality videos. Moreover, scaling these models could unlock emergent capabilities in visual perception, understanding, and reasoning, paving the way for a new paradigm that moves beyond current LLM-centric vision foundation models. A key breakthrough empowering them is the semi-autoregressive (block-diffusion) decoding paradigm, which merges the strengths of diffusion and autoregressive methods by generating video tokens in block-applying diffusion within each block while conditioning on previous ones, resulting in more coherent and stable video sequences. Crucially, it overcomes limitations of standard video diffusion by reintroducing LLM-style KV Cache management, enabling efficient, variable-length, and high-quality generation.
  Therefore, Inferix is specifically designed as a next-generation inference engine to enable immersive world synthesis through optimized semi-autoregressive decoding processes. This dedicated focus on world simulation distinctly sets it apart from systems engineered for high-concurrency scenarios (like vLLM or SGLang) and from classic video diffusion models (such as xDiTs). Inferix further enhances its offering with interactive video streaming and profiling, enabling real-time interaction and realistic simulation to accurately model world dynamics. Additionally, it supports efficient benchmarking through seamless integration of LV-Bench, a new fine-grained evaluation benchmark tailored for minute-long video generation scenarios. We hope the community will work together to advance Inferix and foster world model exploration.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Object Recognition in Mobile Edge Networks: Local Tracking or Edge Detection?</title>
<link>https://arxiv.org/abs/2511.20716</link>
<guid>https://arxiv.org/abs/2511.20716</guid>
<content:encoded><![CDATA[
<div> Keywords: video object recognition, edge computing, deep reinforcement learning, federated learning, resource-constrained devices<br /><br />Summary:<br /><br />This paper addresses the challenge of fast and accurate video object recognition on resource-constrained devices like traffic cameras by leveraging mobile edge computing. The authors propose a hybrid approach where computationally intensive object detection is offloaded to powerful edge servers, while lightweight object tracking is performed locally on devices. A key problem introduced is deciding when to use edge detection versus local tracking methods. They formulate two long-term optimization problems considering temporal correlations of video frames and dynamic mobile edge network conditions in both single-device and multi-device scenarios. For the single-device scenario, the LTED-Ada algorithm is developed, which uses deep reinforcement learning to adaptively choose between local tracking and edge detection based on frame rate, recognition accuracy, and delay requirements. For the multi-device scenario, LTED-Ada is enhanced with federated learning to train policies collaboratively across multiple devices, thus improving generalization to diverse frame rates and performance criteria. Extensive hardware-in-the-loop experiments are conducted with Raspberry Pi 4B devices and a PC edge server, showing that LTED-Ada outperforms existing approaches in balancing accuracy, latency, and resource consumption effectively. The results demonstrate the practicality and efficacy of the proposed method for real-world edge video analytics applications. <div>
arXiv:2511.20716v1 Announce Type: new 
Abstract: Fast and accurate video object recognition, which relies on frame-by-frame video analytics, remains a challenge for resource-constrained devices such as traffic cameras. Recent advances in mobile edge computing have made it possible to offload computation-intensive object detection to edge servers equipped with high-accuracy neural networks, while lightweight and fast object tracking algorithms run locally on devices. This hybrid approach offers a promising solution but introduces a new challenge: deciding when to perform edge detection versus local tracking. To address this, we formulate two long-term optimization problems for both single-device and multi-device scenarios, taking into account the temporal correlation of consecutive frames and the dynamic conditions of mobile edge networks. Based on the formulation, we propose the LTED-Ada in single-device setting, a deep reinforcement learning-based algorithm that adaptively selects between local tracking and edge detection, according to the frame rate as well as recognition accuracy and delay requirement. In multi-device setting, we further enhance LTED-Ada using federated learning to enable collaborative policy training across devices, thereby improving its generalization to unseen frame rates and performance requirements. Finally, we conduct extensive hardware-in-the-loop experiments using multiple Raspberry Pi 4B devices and a personal computer as the edge server, demonstrating the superiority of LTED-Ada.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.20720</link>
<guid>https://arxiv.org/abs/2511.20720</guid>
<content:encoded><![CDATA[
<div> Vision-Language Action, DeeAD, early-exit framework, autonomous driving, latency reduction<br /><br />Summary: The paper introduces DeeAD, a novel framework designed to accelerate Vision-Language Action (VLA) models in autonomous driving by addressing their significant inference latency caused by deep transformer stacks. DeeAD is a training-free, action-guided early-exit method that evaluates the physical feasibility of intermediate trajectory predictions instead of relying on conventional confidence scores to decide when to terminate inference. This termination occurs when the predicted trajectory aligns with lightweight planning priors, such as Navigation or Low-precision Planning, within a tolerable deviation of less than 2 meters. To enhance efficiency further, the authors propose a multi-hop controller that adaptively skips redundant transformer layers based on the observed change rate in scores, thereby reducing unnecessary computation. DeeAD can be seamlessly integrated into existing VLA models like ORION without requiring any retraining, making it highly practical for deployment. Experimental validation on the Bench2Drive benchmark demonstrates that DeeAD achieves up to 28% transformer-layer sparsity and 29% reduction in latency while maintaining the quality and safety of autonomous driving trajectory planning. This approach effectively balances inference speed and planning performance, offering a promising solution for real-time autonomous driving systems. <div>
arXiv:2511.20720v1 Announce Type: new 
Abstract: Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundry: Distilling 3D Foundation Models for the Edge</title>
<link>https://arxiv.org/abs/2511.20721</link>
<guid>https://arxiv.org/abs/2511.20721</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation Model Distillation, self-supervised learning, model compression, 3D point clouds, transferability<br /><br />Summary:<br /><br />1. The paper addresses the challenge of deploying large foundation models, pre-trained with self-supervised learning (SSL), on edge devices due to their large size and high computational cost. 2. Current compression techniques like knowledge distillation produce specialist models that lose the general-purpose utility of foundation models, limiting their use in diverse downstream tasks. 3. The authors propose a new paradigm called Foundation Model Distillation (FMD), designed to compress large SSL models into smaller, efficient proxies that faithfully preserve the general representation power of the original models. 4. They introduce Foundry, the first implementation of FMD specifically applied to 3D point cloud data, which trains a student model by learning SuperTokens that reconstruct the teacher model's token-level latent representations, effectively capturing a compact basis of the teacher’s latent space. 5. The distilled Foundry model maintains strong transferability to a variety of downstream tasks—including classification, part segmentation, and few-shot learning—achieving near full performance of the foundation model but with significantly fewer tokens and reduced FLOPs, making it suitable for use on resource-limited devices such as robots and AR/VR headsets. <div>
arXiv:2511.20721v1 Announce Type: new 
Abstract: Foundation models pre-trained with self-supervised learning (SSL) on large-scale datasets have become powerful general-purpose feature extractors. However, their immense size and computational cost make them prohibitive for deployment on edge devices such as robots and AR/VR headsets. Existing compression techniques like standard knowledge distillation create efficient 'specialist' models but sacrifice the crucial, downstream-agnostic generality that makes foundation models so valuable.  In this paper, we introduce Foundation Model Distillation (FMD), a new paradigm for compressing large SSL models into compact, efficient, and faithful proxies that retain their general-purpose representational power. We present Foundry, the first implementation of FMD for 3D point clouds. Our approach, Foundry, trains a student to learn a compressed set of SuperTokens that reconstruct the teacher's token-level representations, capturing a compact basis of its latent space. A single distilled model maintains strong transferability across diverse downstream tasks-classification, part segmentation, and few-shot scenarios-approaching full foundation-model performance while using significantly fewer tokens and FLOPs, making such models more practical for deployment on resourceconstrained hardware.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DinoLizer: Learning from the Best for Generative Inpainting Localization</title>
<link>https://arxiv.org/abs/2511.20722</link>
<guid>https://arxiv.org/abs/2511.20722</guid>
<content:encoded><![CDATA[
<div> DINOv2, generative inpainting, local manipulation detection, Vision Transformer, deepfake localization<br /><br />Summary:  
This paper presents DinoLizer, a novel model based on DINOv2 for detecting and localizing manipulated regions in generative inpainting. DinoLizer leverages a DINOv2 Vision Transformer pretrained on the B-Free dataset, designed to identify synthetic images. A linear classification head is added to the patch embeddings of the ViT, enabling manipulation predictions at a $14 \times 14$ patch resolution that emphasize semantically altered areas, ignoring trivial non-semantic edits. Since ViT handles only fixed-size inputs, a sliding-window approach is used to analyze larger images, and heatmaps generated are post-processed for refined binary manipulation masks. Extensive experiments demonstrate that DinoLizer outperforms existing state-of-the-art local manipulation detectors on datasets from various generative models. It shows robustness against common post-processing techniques like resizing, noise, and JPEG compression. On average, it achieves a 12% higher Intersection-over-Union (IoU) compared to the best alternative methods, with even greater improvement after mask post-processing. Additional tests with off-the-shelf DINOv2 confirm the powerful representational ability of Vision Transformers for this detection task. Further ablation studies involving DINOv3 establish DinoLizer's superiority in deepfake localization. The authors plan to release the code publicly upon acceptance. <div>
arXiv:2511.20722v1 Announce Type: new 
Abstract: We introduce DinoLizer, a DINOv2-based model for localizing manipulated regions in generative inpainting. Our method builds on a DINOv2 model pretrained to detect synthetic images on the B-Free dataset. We add a linear classification head on top of the Vision Transformer's patch embeddings to predict manipulations at a $14\times 14$ patch resolution. The head is trained to focus on semantically altered regions, treating non-semantic edits as part of the original content. Because the ViT accepts only fixed-size inputs, we use a sliding-window strategy to aggregate predictions over larger images; the resulting heatmaps are post-processed to refine the estimated binary manipulation masks. Empirical results show that DinoLizer surpasses state-of-the-art local manipulation detectors on a range of inpainting datasets derived from different generative models. It remains robust to common post-processing operations such as resizing, noise addition, and JPEG (double) compression. On average, DinoLizer achieves a 12\% higher Intersection-over-Union (IoU) than the next best model, with even greater gains after post-processing. Our experiments with off-the-shelf DINOv2 demonstrate the strong representational power of Vision Transformers for this task. Finally, extensive ablation studies comparing DINOv2 and its successor, DINOv3, in deepfake localization confirm DinoLizer's superiority. The code will be publicly available upon acceptance of the paper.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CANVAS: A Benchmark for Vision-Language Models on Tool-Based User Interface Design</title>
<link>https://arxiv.org/abs/2511.20737</link>
<guid>https://arxiv.org/abs/2511.20737</guid>
<content:encoded><![CDATA[
<div> User interface design, vision language models, tool invocation, benchmark, design modification<br /><br />Summary:<br /><br />1. The paper addresses the iterative nature of user interface (UI) design, where designers refine designs using software such as Figma or Sketch.<br /><br />2. It explores recent advancements in vision-language models (VLMs) capable of performing tool invocations to operate design software and edit UI designs through iterative steps.<br /><br />3. Recognizing the absence of benchmarks for assessing tool-based design performance by VLMs, the authors introduce CANVAS, a novel benchmark designed to evaluate VLMs on tool-based UI design tasks.<br /><br />4. CANVAS consists of 598 tool-based design tasks drawn from 3,300 mobile UI designs covering 30 function-based categories, including onboarding and messaging.<br /><br />5. The benchmark contains two key task types: design replication, which tests a model’s ability to reproduce complete UI screens, and design modification, which assesses the ability to modify parts of existing screens.<br /><br />6. Experimental results indicate that leading VLMs demonstrate more strategic usage of tool invocations that lead to improved design quality.<br /><br />7. The study also identifies common errors made by VLMs in tool-based UI design, offering insights to guide future improvements in this domain.<br /><br />8. Overall, CANVAS serves as a foundational step toward integrating VLMs as effective collaborators in design software environments. <div>
arXiv:2511.20737v1 Announce Type: new 
Abstract: User interface (UI) design is an iterative process in which designers progressively refine their work with design software such as Figma or Sketch. Recent advances in vision language models (VLMs) with tool invocation suggest these models can operate design software to edit a UI design through iteration. Understanding and enhancing this capacity is important, as it highlights VLMs' potential to collaborate with designers within conventional software. However, as no existing benchmark evaluates tool-based design performance, the capacity remains unknown. To address this, we introduce CANVAS, a benchmark for VLMs on tool-based user interface design. Our benchmark contains 598 tool-based design tasks paired with ground-truth references sampled from 3.3K mobile UI designs across 30 function-based categories (e.g., onboarding, messaging). In each task, a VLM updates the design step-by-step through context-based tool invocations (e.g., create a rectangle as a button background), linked to design software. Specifically, CANVAS incorporates two task types: (i) design replication evaluates the ability to reproduce a whole UI screen; (ii) design modification evaluates the ability to modify a specific part of an existing screen. Results suggest that leading models exhibit more strategic tool invocations, improving design quality. Furthermore, we identify common error patterns models exhibit, guiding future work in enhancing tool-based design capabilities.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Guided Semantic Image Encoder</title>
<link>https://arxiv.org/abs/2511.20770</link>
<guid>https://arxiv.org/abs/2511.20770</guid>
<content:encoded><![CDATA[
<div> Image encoders, vision-language models, text-guided encoding, inference efficiency, query-specific grounding<br /><br />Summary:<br /><br />This paper introduces the Text-Guided Semantic Image Encoder (TIE), a novel approach for image encoders in vision-language models (VLMs) that generates image representations conditioned on input text queries, unlike traditional encoders pretrained independently and task-agnostic. By integrating text conditioning, TIE enables VLMs to achieve significantly improved performance, showing average gains of +1.5 and +1.3 points across nine image-to-text benchmarks at model scales of 1B and 3B parameters, respectively, with improvements up to 6 points on challenging tasks like DocVQA and InfoVQA. Beyond accuracy, TIE enhances inference efficiency by requiring only half as many image tokens (tiles), reducing computational cost without sacrificing quality. The method also generalizes well to generic queries, demonstrating that text-guided training effectively helps the encoder focus on the most relevant visual features based on the query context. Qualitative evaluations highlight that TIE consistently attends to query-specific image regions, improving both interpretability and the ability to ground the visual representations in the input text, making it a promising advance in aligning visual and language understanding in multimodal models. <div>
arXiv:2511.20770v1 Announce Type: new 
Abstract: Image encoders, a fundamental component of vision-language models (VLMs), are typically pretrained independently before being aligned with a language model. This standard paradigm results in encoders that process images agnostically, without regard to the specific downstream task or text query. To address this limitation, we propose the Text-Guided Semantic Image Encoder (TIE), which generates image representations conditioned on the input text query. VLMs equipped with TIE outperform their conventional counterparts by +1.5 and +1.3 points on average across nine image-to-text benchmarks at the 1B and 3B scales, respectively, with gains reaching up to 6 points on tasks such as DocVQA and InfoVQA. Moreover, TIE-based VLMs attain superior performance while utilizing only half as many image tiles (tokens), resulting in notably improved inference efficiency. TIE also generalizes well with generic queries, indicating that text-conditioned training effectively optimizes the encoder to capture key visual features. Qualitative analysis confirms that TIE consistently attends to query-relevant regions, enhancing both interpretability and query-specific grounding.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Patch is All You Need: Joint Surface Material Reconstruction and Classification from Minimal Visual Cues</title>
<link>https://arxiv.org/abs/2511.20784</link>
<guid>https://arxiv.org/abs/2511.20784</guid>
<content:encoded><![CDATA[
<div> Surface Material Reconstruction, Partial Convolution, Sparse Visual Input, Material Classification, Inpainting<br /><br />Summary:<br />1. The paper introduces SMARC, a novel model designed for surface material reconstruction and classification using minimal visual input, specifically only a single 10% contiguous patch of an image.<br />2. SMARC integrates a Partial Convolutional U-Net architecture along with a classification head to simultaneously reconstruct the full RGB surface and classify the material category, thus addressing the challenges posed by sparse and partial observations.<br />3. This approach is particularly suitable for environments where dense or full-scene visual data is unavailable, such as in robotics and material perception applications.<br />4. The model was evaluated against five state-of-the-art baselines, including convolutional autoencoders, Vision Transformer (ViT), Masked Autoencoder (MAE), Swin Transformer, and DETR, using the Touch and Go dataset consisting of real-world surface textures.<br />5. SMARC outperformed these methods, achieving a Peak Signal-to-Noise Ratio (PSNR) of 17.55 dB and a material classification accuracy of 85.10%, demonstrating the effectiveness of partial convolution in handling missing data for spatial reasoning and semantic understanding under extreme observation sparsity. <div>
arXiv:2511.20784v1 Announce Type: new 
Abstract: Understanding material surfaces from sparse visual cues is critical for applications in robotics, simulation, and material perception. However, most existing methods rely on dense or full-scene observations, limiting their effectiveness in constrained or partial view environment. To address this challenge, we introduce SMARC, a unified model for Surface MAterial Reconstruction and Classification from minimal visual input. By giving only a single 10% contiguous patch of the image, SMARC recognizes and reconstructs the full RGB surface while simultaneously classifying the material category. Our architecture combines a Partial Convolutional U-Net with a classification head, enabling both spatial inpainting and semantic understanding under extreme observation sparsity. We compared SMARC against five models including convolutional autoencoders [17], Vision Transformer (ViT) [13], Masked Autoencoder (MAE) [5], Swin Transformer [9], and DETR [2] using Touch and Go dataset [16] of real-world surface textures. SMARC achieves state-of-the-art results with a PSNR of 17.55 dB and a material classification accuracy of 85.10%. Our findings highlight the advantages of partial convolution in spatial reasoning under missing data and establish a strong foundation for minimal-vision surface understanding.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LongVT: Incentivizing "Thinking with Long Videos" via Native Tool Calling</title>
<link>https://arxiv.org/abs/2511.20785</link>
<guid>https://arxiv.org/abs/2511.20785</guid>
<content:encoded><![CDATA[
<div> Keywords: Long multimodal models, video reasoning, Chain-of-Tool-Thought, temporal grounding, VideoSIAH dataset  

<br /><br />Summary:  
This paper introduces LongVT, an innovative framework designed for improved reasoning over long videos by leveraging large multimodal models (LMMs). 1) LongVT mimics human comprehension by initially skimming videos globally and then focusing on specific clips to gather detailed information through an interleaved Multimodal Chain-of-Tool-Thought approach. 2) It utilizes the inherent temporal grounding capability of LMMs as a native video cropping tool to zoom into particular clips and resample finer video frames, enabling a global-to-local reasoning process that continues until answers are backed by visual evidence. 3) Recognizing the lack of fine-grained QA data for long video reasoning, the authors curated and will release a comprehensive dataset named VideoSIAH, comprising 247.9K training samples for cold-start fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K for reinforcement fine-tuning. 4) The evaluation benchmark includes 1,280 carefully curated QA pairs validated with human-in-the-loop methods to ensure quality. 5) Through a well-designed three-stage training strategy and extensive experiments, LongVT consistently surpasses strong existing baselines across four challenging long-video understanding and reasoning benchmarks. All codes, datasets, and model checkpoints are openly available for research and development purposes. <div>
arXiv:2511.20785v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) have shown great potential for video reasoning with textual Chain-of-Thought. However, they remain vulnerable to hallucinations, especially when processing long-form videos where evidence is sparse and temporally dispersed. Inspired by how humans comprehend long videos - by first skimming globally and then examining relevant clips for details - we introduce LongVT, an end-to-end agentic framework that enables "Thinking with Long Videos" via interleaved Multimodal Chain-of-Tool-Thought. Specifically, we exploit LMMs' inherent temporal grounding ability as a native video cropping tool to zoom in on a specific video clip and resample finer-grained video frames. This global-to-local reasoning loop continues until answers are grounded in retrieved visual evidence. Given the scarcity of fine-grained question-answering (QA) data for the long video reasoning task, we curate and will release a data suite named VideoSIAH to facilitate both training and evaluation. Specifically, our training dataset consists of 247.9K samples for tool-integrated cold-start supervised fine-tuning, 1.6K samples for agentic reinforcement learning, and 15.4K samples for agentic reinforcement fine-tuning, respectively. Our evaluation benchmark consists of 1,280 QA pairs that are carefully curated through a semi-automatic data pipeline with human-in-the-loop validation. With a meticulously designed three-stage training strategy and extensive empirical validation, LongVT consistently outperforms existing strong baselines across four challenging long-video understanding and reasoning benchmarks. Our codes, data, and model checkpoints are publicly available at https://github.com/EvolvingLMMs-Lab/LongVT .
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting KRISP: A Lightweight Reproduction and Analysis of Knowledge-Enhanced Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.20795</link>
<guid>https://arxiv.org/abs/2511.20795</guid>
<content:encoded><![CDATA[
<div> Keywords: KRISP, vision-language reasoning, lightweight model, knowledge graph, VQA

<br /><br />Summary:  
This paper revisits KRISP, a vision-language reasoning framework originally developed by Facebook AI Research that integrates structured external knowledge. The authors propose a lightweight reproduction of KRISP featuring significantly fewer parameters, making it suitable for resource-constrained environments. Although the reproduced model achieves approximately 75% of the original model's performance, the replication process reveals several design flaws and implicit issues not addressed in the original work. The study includes systematic ablation experiments that provide insights into the scalability and effectiveness of knowledge-enhanced Visual Question Answering (VQA) architectures under limited computational resources. The evaluation incorporates both a synthetic VQA dataset as a proof of concept and the DAQUAR dataset for real-world validation. A key advantage of the proposed low-parameter model is its ability to constrain outputs strictly within a specified external knowledge graph domain, effectively preventing AI hallucinations. This domain restriction enhances model reliability by generating answers grounded in valid knowledge. Moreover, the minimal parameter count allows deployment on edge devices such as smartphones and augmented/virtual reality headsets, facilitating offline visual reasoning capabilities. Overall, the work contributes to making knowledge-enhanced VQA more accessible and practical for industrial and consumer applications with limited computational budgets. <div>
arXiv:2511.20795v1 Announce Type: new 
Abstract: Facebook AI Research introduced KRISP [4], which integrates structured external knowledge into pipelines for vision-language reasoning. Despite its effectiveness, the original model has been developed for industrial-scale training, is computationally demanding, and is tightly connected to a large backbone. In this work, we reexamine KRISP from a different angle and offer a lightweight reproduction with significantly fewer parameters. Even though our replicated model performs about 75 % of the original, the replication process uncovers a number of design flaws, real-world pitfalls, and implicit problems that were not fully covered in the original paper. We offer insights into the scalability and efficacy of knowledge-enhanced VQA architectures under resource constraints through systematic ablation studies, which include a proof-of-concept on synthetic VQA data and evaluation on the DAQUAR dataset. Our model, configured with a low parameter setup and constrained by the external Knowledge graph domain, prevents AI hallucinations and generates outputs solely within that domain. Minimal parameters allow us to function on edge devices like smartphones and AR-VR, further improving offline visual reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intriguing Properties of Dynamic Sampling Networks</title>
<link>https://arxiv.org/abs/2511.20800</link>
<guid>https://arxiv.org/abs/2511.20800</guid>
<content:encoded><![CDATA[
<div> Dynamic sampling, Warping operator, Deformable convolutions, Statistical analysis, Loss landscape visualization  

<br /><br />Summary:  
This paper introduces a novel operator called "warping" that unifies and generalizes dynamic sampling mechanisms used in deep learning architectures, notably in computer vision. Warping serves as a minimal and analyzable implementation of dynamic sampling, capable of reconstructing existing methods such as deformable convolutions, active convolutional units, and spatial transformer networks. The authors provide a thorough statistical analysis of this operator by modeling inputs as independent and identically distributed (IID) variables and as homogeneous random fields. A key theoretical finding is the asymmetry that exists between the forward and backward passes during model training, highlighting the fundamentally different nature of these dynamic sampling operators compared to traditional convolutional operators which are translationally invariant. The study also investigates the discretization effects impacting training stability and outlines the necessary conditions to ensure stable training of dynamic sampling networks. Complementing the theoretical contributions, the paper presents empirical evaluations validating the framework. Additionally, a novel loss landscape visualization technique is introduced, utilizing gradient update information to gain deeper insights into model learning behavior and convergence dynamics. This comprehensive approach bridges theoretical and practical perspectives on dynamic sampling in deep learning. <div>
arXiv:2511.20800v1 Announce Type: new 
Abstract: Dynamic sampling mechanisms in deep learning architectures have demonstrated utility across many computer vision models, though the theoretical analysis of these structures has not yet been unified. In this paper we connect the various dynamic sampling methods by developing and analyzing a novel operator which generalizes existing methods, which we term "warping". Warping provides a minimal implementation of dynamic sampling which is amenable to analysis, and can be used to reconstruct existing architectures including deformable convolutions, active convolutional units, and spatial transformer networks. Using our formalism, we provide statistical analysis of the operator by modeling the inputs as both IID variables and homogeneous random fields. Extending this analysis, we discover a unique asymmetry between the forward and backward pass of the model training. We demonstrate that these mechanisms represent an entirely different class of orthogonal operators to the traditional translationally invariant operators defined by convolutions. With a combination of theoretical analysis and empirical investigation, we find the conditions necessary to ensure stable training of dynamic sampling networks. In addition, statistical analysis of discretization effects are studied. Finally, we introduce a novel loss landscape visualization which utilizes gradient update information directly, to better understand learning behavior.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\Delta$-NeRF: Incremental Refinement of Neural Radiance Fields through Residual Control and Knowledge Transfer</title>
<link>https://arxiv.org/abs/2511.20804</link>
<guid>https://arxiv.org/abs/2511.20804</guid>
<content:encoded><![CDATA[
<div> Neural Radiance Fields, Incremental Refinement, Residual Controller, Satellite Imagery, Knowledge Distillation<br /><br />Summary:<br /><br />1. This paper addresses the challenge of incrementally refining Neural Radiance Fields (NeRFs) without requiring complete retraining whenever new views are added, a critical issue in domains like satellite-based terrain analysis where data is sequentially collected.  
2. The authors introduce Δ-NeRF, a modular residual framework that refines a frozen base NeRF by injecting per-layer residual corrections through a residual controller, thereby eliminating the need for access to previously seen data.  
3. Δ-NeRF features an uncertainty-aware gating mechanism that adaptively blends the original base predictions and the residual corrections to prevent overcorrection and maintain stable refinement.  
4. A view selection strategy is proposed to reduce the amount of training data needed by up to 47% without sacrificing performance, improving efficiency.  
5. The refined model is compressed into a smaller student network using knowledge distillation, reducing the model size to 20% of the original while preserving accuracy.  
Experiments on satellite imagery demonstrate that Δ-NeRF achieves comparable or better results than joint training, improves PSNR by up to 43.5% over naive fine-tuning, and cuts training time by 30-42%, establishing it as a superior method for incremental NeRF refinement. <div>
arXiv:2511.20804v1 Announce Type: new 
Abstract: Neural Radiance Fields (NeRFs) have demonstrated remarkable capabilities in 3D reconstruction and novel view synthesis. However, most existing NeRF frameworks require complete retraining when new views are introduced incrementally, limiting their applicability in domains where data arrives sequentially. This limitation is particularly problematic in satellite-based terrain analysis, where regions are repeatedly observed over time. Incremental refinement of NeRFs remains underexplored, and naive approaches suffer from catastrophic forgetting when past data is unavailable. We propose $\Delta$-NeRF, a unique modular residual framework for incremental NeRF refinement. $\Delta$-NeRF introduces several novel techniques including: (1) a residual controller that injects per-layer corrections into a frozen base NeRF, enabling refinement without access to past data; (2) an uncertainty-aware gating mechanism that prevents overcorrection by adaptively combining base and refined predictions; and (3) a view selection strategy that reduces training data by up to 47\% while maintaining performance. Additionally, we employ knowledge distillation to compress the enhanced model into a compact student network (20\% of original size). Experiments on satellite imagery demonstrate that $\Delta$-NeRF achieves performance comparable to joint training while reducing training time by 30-42\%. $\Delta$-NeRF consistently outperforms existing baselines, achieving an improvement of up to 43.5\% in PSNR over naive fine-tuning and surpassing joint training on some metrics.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Layer-Aware Video Composition via Split-then-Merge</title>
<link>https://arxiv.org/abs/2511.20809</link>
<guid>https://arxiv.org/abs/2511.20809</guid>
<content:encoded><![CDATA[
<div> Keywords: generative video composition, data scarcity, foreground-background splitting, transformation-aware training, identity-preservation loss<br /><br />Summary:<br /><br />1. The paper introduces Split-then-Merge (StM), a novel framework designed to improve control in generative video composition while addressing the challenge of data scarcity.<br /><br />2. Unlike traditional methods that depend on annotated datasets or handcrafted rules, StM leverages a large corpus of unlabeled videos by splitting them into dynamic foreground and background layers.<br /><br />3. StM self-composes these layers to learn interactions between dynamic subjects and diverse scenes, enabling the model to capture complex compositional dynamics essential for realistic video generation.<br /><br />4. The framework incorporates a transformation-aware training pipeline featuring multi-layer fusion and augmentation, which facilitates affordance-aware composition, enhancing the realism and controllability of the generated videos.<br /><br />5. To maintain fidelity during blending, StM introduces an identity-preservation loss that preserves the integrity of the foreground elements.<br /><br />6. Experimental evaluations demonstrate that StM surpasses state-of-the-art methods in both quantitative benchmarks and qualitative assessments conducted by humans and VLLM-based evaluators.<br /><br />7. Additional information and resources about the project can be found on the dedicated project webpage. <div>
arXiv:2511.20809v1 Announce Type: new 
Abstract: We present Split-then-Merge (StM), a novel framework designed to enhance control in generative video composition and address its data scarcity problem. Unlike conventional methods relying on annotated datasets or handcrafted rules, StM splits a large corpus of unlabeled videos into dynamic foreground and background layers, then self-composes them to learn how dynamic subjects interact with diverse scenes. This process enables the model to learn the complex compositional dynamics required for realistic video generation. StM introduces a novel transformation-aware training pipeline that utilizes a multi-layer fusion and augmentation to achieve affordance-aware composition, alongside an identity-preservation loss that maintains foreground fidelity during blending. Experiments show StM outperforms SoTA methods in both quantitative benchmarks and in humans/VLLM-based qualitative evaluations. More details are available at our project page: https://split-then-merge.github.io
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPHINX: A Synthetic Environment for Visual Perception and Reasoning</title>
<link>https://arxiv.org/abs/2511.20814</link>
<guid>https://arxiv.org/abs/2511.20814</guid>
<content:encoded><![CDATA[
<div> Sphinx, visual perception, reasoning, reinforcement learning, multimodal  

<br /><br />Summary:  
The paper introduces Sphinx, a synthetic environment designed for visual perception and cognitive reasoning tasks by focusing on core cognitive primitives. Sphinx generates puzzles procedurally, combining motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions to allow precise evaluation and facilitate large-scale dataset creation. The benchmark consists of 25 diverse task types covering symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. The authors evaluate recent large vision-language models (LVLMs), including the state-of-the-art GPT-5, finding that even GPT-5 achieves only 51.1% accuracy, which is significantly below human performance levels. To address this challenge, the study presents reinforcement learning with verifiable rewards (RLVR) as a training strategy, which notably improves model accuracy on Sphinx tasks. Additionally, models trained with RLVR demonstrate performance gains on external visual reasoning benchmarks, underscoring the approach’s potential for advancing multimodal reasoning capabilities. This work highlights both the difficulty of core visual reasoning tasks for current models and the promise of RL techniques guided by verifiable rewards for enhancing multimodal AI systems. <div>
arXiv:2511.20814v1 Announce Type: new 
Abstract: We present Sphinx, a synthetic environment for visual perception and reasoning that targets core cognitive primitives. Sphinx procedurally generates puzzles using motifs, tiles, charts, icons, and geometric primitives, each paired with verifiable ground-truth solutions, enabling both precise evaluation and large-scale dataset construction. The benchmark covers 25 task types spanning symmetry detection, geometric transformations, spatial reasoning, chart interpretation, and sequence prediction. Evaluating recent large vision-language models (LVLMs) shows that even state-of-the-art GPT-5 attains only 51.1% accuracy, well below human performance. Finally, we demonstrate that reinforcement learning with verifiable rewards (RLVR) substantially improves model accuracy on these tasks and yields gains on external visual reasoning benchmarks, highlighting its promise for advancing multimodal reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion</title>
<link>https://arxiv.org/abs/2511.20821</link>
<guid>https://arxiv.org/abs/2511.20821</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion models, Optimization-based Visual Inversion, text-to-image generation, Mahalanobis loss, Nearest-Neighbor loss<br /><br />Summary:  
This paper addresses the reliance of diffusion models on diffusion prior networks for translating text embeddings into a visual latent space, which are costly in terms of computation and training data. The authors propose Optimization-based Visual Inversion (OVI), a training-free and data-free method that replaces the need for a trained prior by initializing a latent representation from random pseudo-tokens and iteratively optimizing it to maximize cosine similarity with the text prompt embedding. To ensure the optimized latent lies within the realistic image distribution, the study introduces two novel constraints: a Mahalanobis-based loss and a Nearest-Neighbor loss. Experiments on the Kandinsky 2.2 model demonstrate that OVI can effectively substitute traditional priors. Additionally, the authors reveal a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the raw text embedding as a prior yields high benchmark scores despite reduced perceptual quality. The proposed constrained OVI methods improve visual fidelity beyond this baseline, with the Nearest-Neighbor constraint achieving quantitative results comparable or superior to state-of-the-art data-efficient priors. The findings suggest that a training-free optimization approach to visual inversion is a promising direction for improving text-to-image diffusion models. The codebase will be publicly released upon acceptance. <div>
arXiv:2511.20821v1 Announce Type: new 
Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefTr: Recurrent Refinement of Confluent Trajectories for 3D Vascular Tree Centerline Graphs</title>
<link>https://arxiv.org/abs/2511.20823</link>
<guid>https://arxiv.org/abs/2511.20823</guid>
<content:encoded><![CDATA[
<div> Keywords: tubular trees, centerline detection, Transformer decoder, recurrent refinement, vascular imaging<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurately detecting centerlines of tubular trees, such as blood vessels and lung airways, which is critical for medical diagnosis and surgical planning.  
2. It proposes RefTr, a novel 3D image-to-graph model that generates vascular tree centerlines by recurrently refining confluent trajectories using a Producer-Refiner architecture based on a Transformer decoder.  
3. The Producer initially proposes a set of confluent trajectories, which the Refiner repeatedly refines to produce final centerline graphs, ensuring valid tree topology throughout the process.  
4. This trajectory-based representation and recurrent refinement scheme improve precision, reduce the number of decoder parameters by 2.4 times compared to previous state-of-the-art methods, and allow parameter reuse.  
5. In addition, the authors introduce an efficient spatial non-maximum suppression algorithm to merge duplicate branches, further enhancing precision.  
6. Experimental results on multiple public datasets demonstrate that RefTr achieves superior recall and comparable precision to prior state-of-the-art approaches, with faster inference and significantly smaller model size.  
7. Overall, RefTr presents a promising and efficient framework for vascular tree analysis, supporting safer and more accurate clinical workflows in 3D medical imaging. <div>
arXiv:2511.20823v1 Announce Type: new 
Abstract: Tubular trees, such as blood vessels and lung airways, are essential for material transport within the human body. Accurately detecting their centerlines with correct tree topology is critical for clinical tasks such as diagnosis, treatment planning, and surgical navigation. In these applications, maintaining high recall is crucial, as missing small branches can result in fatal mistakes caused by incomplete assessments or undetected abnormalities. We present RefTr, a 3D image-to-graph model for centerline generation of vascular trees via recurrent refinement of confluent trajectories. RefTr uses a Producer-Refiner architecture based on a Transformer decoder, where the Producer proposes a set of initial confluent trajectories that are recurrently refined by the Refiner to produce final trajectories, which forms the centerline graph. The confluent trajectory representation enables refinement of complete trajectories while explicitly enforcing a valid tree topology. The recurrent refinement scheme improves precision and reuses the same Refiner block across multiple steps, yielding a 2.4x reduction in decoder parameters compared to previous SOTA. We also introduce an efficient non-maximum suppression algorithm for spatial tree graphs to merge duplicate branches and boost precision. Across multiple public centerline datasets, RefTr achieves superior recall and comparable precision to previous SOTA, while offering faster inference and substantially fewer parameters, demonstrating its potential as a new state-of-the-art framework for vascular tree analysis in 3D medical imaging.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MODEST: Multi-Optics Depth-of-Field Stereo Dataset</title>
<link>https://arxiv.org/abs/2511.20853</link>
<guid>https://arxiv.org/abs/2511.20853</guid>
<content:encoded><![CDATA[
<div> Keywords: depth estimation, stereo DSLR dataset, optical realism, focal length variation, camera calibration  

<br /><br />Summary:  
1. The article addresses the challenge of reliable depth estimation in real optical conditions, which is critical for applications like autonomous robotics and augmented reality.  
2. It introduces the first high-resolution (5472×3648px) stereo DSLR dataset consisting of 18,000 images, that systematically vary focal length and aperture across complex real scenes to capture realistic optical phenomena.  
3. The dataset is collected from 9 diverse scenes under varying complexity, lighting conditions, and backgrounds, using two identical camera setups at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), resulting in 50 optical configurations and 2,000 images per scene.  
4. This extensive optics variation enables detailed analysis of geometric and optical effects useful for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D reconstruction, and novel view synthesis.  
5. Each focal setup includes dedicated calibration images supporting evaluation of classical and learning-based intrinsic and extrinsic camera calibration methods.  
6. The dataset features challenging real-world visual elements such as optical illusions, reflective and transparent surfaces, fine details, and ambient light changes, bridging the realism gap between synthetic data and real camera optics.  
7. The authors also demonstrate difficulties faced by current state-of-the-art monocular, stereo depth, and depth-of-field methods on this dataset.  
8. The dataset, calibration files, and evaluation code are publicly released to promote reproducible research on real-world optical generalization. <div>
arXiv:2511.20853v1 Announce Type: new 
Abstract: Reliable depth estimation under real optical conditions remains a core challenge for camera vision in systems such as autonomous robotics and augmented reality. Despite recent progress in depth estimation and depth-of-field rendering, research remains constrained by the lack of large-scale, high-fidelity, real stereo DSLR datasets, limiting real-world generalization and evaluation of models trained on synthetic data as shown extensively in literature. We present the first high-resolution (5472$\times$3648px) stereo DSLR dataset with 18000 images, systematically varying focal length and aperture across complex real scenes and capturing the optical realism and complexity of professional camera systems. For 9 scenes with varying scene complexity, lighting and background, images are captured with two identical camera assemblies at 10 focal lengths (28-70mm) and 5 apertures (f/2.8-f/22), spanning 50 optical configurations in 2000 images per scene. This full-range optics coverage enables controlled analysis of geometric and optical effects for monocular and stereo depth estimation, shallow depth-of-field rendering, deblurring, 3D scene reconstruction and novel view synthesis. Each focal configuration has a dedicated calibration image set, supporting evaluation of classical and learning based methods for intrinsic and extrinsic calibration. The dataset features challenging visual elements such as multi-scale optical illusions, reflective surfaces, mirrors, transparent glass walls, fine-grained details, and natural / artificial ambient light variations. This work attempts to bridge the realism gap between synthetic training data and real camera optics, and demonstrates challenges with the current state-of-the-art monocular, stereo depth and depth-of-field methods. We release the dataset, calibration files, and evaluation code to support reproducible research on real-world optical generalization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Memorability Modeling from Tip-of-the-Tongue Retrieval Queries</title>
<link>https://arxiv.org/abs/2511.20854</link>
<guid>https://arxiv.org/abs/2511.20854</guid>
<content:encoded><![CDATA[
<div> Memorability, Visual content, Tip-of-the-tongue, Unsupervised dataset, Vision-language models<br /><br />Summary:<br /><br />1. This work addresses the challenge of collecting memorability annotations for visual content, which traditionally requires expensive human input, limiting dataset diversity and scalability.  
2. The authors introduce the first large-scale unsupervised dataset explicitly designed for modeling visual memorability signals, containing over 82,000 videos paired with descriptive recall data.  
3. The dataset leverages tip-of-the-tongue (ToT) retrieval queries collected from online platforms like Reddit, capturing nuanced memorability details beyond aggregate scores.  
4. Experimental results demonstrate that large vision-language models fine-tuned on this dataset outperform state-of-the-art models, including GPT-4o, in generating open-ended memorability descriptions for visual content.  
5. Additionally, a contrastive training strategy is employed to develop the first model capable of multimodal ToT retrieval, presenting new research directions to improve understanding and prediction of visual content memorability. <div>
arXiv:2511.20854v1 Announce Type: new 
Abstract: Visual content memorability has intrigued the scientific community for decades, with applications ranging widely, from understanding nuanced aspects of human memory to enhancing content design. A significant challenge in progressing the field lies in the expensive process of collecting memorability annotations from humans. This limits the diversity and scalability of datasets for modeling visual content memorability. Most existing datasets are limited to collecting aggregate memorability scores for visual content, not capturing the nuanced memorability signals present in natural, open-ended recall descriptions. In this work, we introduce the first large-scale unsupervised dataset designed explicitly for modeling visual memorability signals, containing over 82,000 videos, accompanied by descriptive recall data. We leverage tip-of-the-tongue (ToT) retrieval queries from online platforms such as Reddit. We demonstrate that our unsupervised dataset provides rich signals for two memorability-related tasks: recall generation and ToT retrieval. Large vision-language models fine-tuned on our dataset outperform state-of-the-art models such as GPT-4o in generating open-ended memorability descriptions for visual content. We also employ a contrastive training strategy to create the first model capable of performing multimodal ToT retrieval. Our dataset and models present a novel direction, facilitating progress in visual content memorability research.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Fog Parameters from a Sequence of Stereo Images</title>
<link>https://arxiv.org/abs/2511.20865</link>
<guid>https://arxiv.org/abs/2511.20865</guid>
<content:encoded><![CDATA[
<div> fog parameter estimation, stereo foggy images, local homogeneity, SDIRF dataset, visual SLAM<br /><br />Summary:<br /><br />This paper proposes a novel method for estimating fog parameters from sequences of stereo foggy images by solving an optimization problem that estimates all parameters simultaneously, avoiding error propagation typical in sequential methods. Unlike prior approaches that assume global homogeneity, this method assumes fog is only locally homogeneous, enabling more effective handling of real-world, globally inhomogeneous fog conditions. The algorithm is designed as an add-on module compatible with existing visual Simultaneous Localization and Mapping (SLAM) or odometry systems operating in foggy environments. To facilitate evaluation, the authors introduce the Stereo Driving In Real Fog (SDIRF) dataset, featuring over 40 minutes and 34,000 high-quality consecutive stereo frames captured on real foggy roads under varied visibility conditions. Uniquely, the dataset includes camera photometric parameters calibrated in a lab setting, critical for applying atmospheric scattering models correctly, and clear-weather counterparts of the same routes to support image defogging and depth reconstruction research. Extensive experiments on synthetic and real foggy data demonstrate that the proposed approach outperforms previous methods, delivering more accurate parameter estimates and better adaptation to true fog conditions. Both the code and the SDIRF dataset are publicly released to encourage further advances in visual perception research under foggy conditions. <div>
arXiv:2511.20865v1 Announce Type: new 
Abstract: We propose a method which, given a sequence of stereo foggy images, estimates the parameters of a fog model and updates them dynamically. In contrast with previous approaches, which estimate the parameters sequentially and thus are prone to error propagation, our algorithm estimates all the parameters simultaneously by solving a novel optimisation problem. By assuming that fog is only locally homogeneous, our method effectively handles real-world fog, which is often globally inhomogeneous. The proposed algorithm can be easily used as an add-on module in existing visual Simultaneous Localisation and Mapping (SLAM) or odometry systems in the presence of fog. In order to assess our method, we also created a new dataset, the Stereo Driving In Real Fog (SDIRF), consisting of high-quality, consecutive stereo frames of real, foggy road scenes under a variety of visibility conditions, totalling over 40 minutes and 34k frames. As a first-of-its-kind, SDIRF contains the camera's photometric parameters calibrated in a lab environment, which is a prerequisite for correctly applying the atmospheric scattering model to foggy images. The dataset also includes the counterpart clear data of the same routes recorded in overcast weather, which is useful for companion work in image defogging and depth reconstruction. We conducted extensive experiments using both synthetic foggy data and real foggy sequences from SDIRF to demonstrate the superiority of the proposed algorithm over prior methods. Our method not only produces the most accurate estimates on synthetic data, but also adapts better to real fog. We make our code and SDIRF publicly available\footnote{https://github.com/SenseRoboticsLab/estimating-fog-parameters} to the community with the aim of advancing the research on visual perception in fog.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V$^{2}$-SAM: Marrying SAM2 with Multi-Prompt Experts for Cross-View Object Correspondence</title>
<link>https://arxiv.org/abs/2511.20886</link>
<guid>https://arxiv.org/abs/2511.20886</guid>
<content:encoded><![CDATA[
<div> Cross-view correspondence, SAM2 adaptation, prompt generators, cyclic consistency, multi-expert framework  

<br /><br />Summary:  
1. The paper addresses the challenging task of cross-view object correspondence, particularly ego-exo object correspondence, which involves associating identical objects across drastically different viewpoints like ego-centric and exo-centric views.  
2. Current segmentation models such as SAM2 are not straightforward to apply due to significant viewpoint and appearance variations inherent in cross-view settings.  
3. To overcome these challenges, the authors propose V²-SAM, a unified framework that adapts SAM2 from single-view segmentation to cross-view correspondence by utilizing two complementary prompt generators.  
4. The Cross-View Anchor Prompt Generator (V²-Anchor), leveraging DINOv3 features, introduces geometry-aware correspondences and enables coordinate-based prompting in cross-view scenarios for the first time.  
5. The Cross-View Visual Prompt Generator (V²-Visual) provides enhanced appearance-guided cues by aligning ego-exo representations through both feature and structural perspectives using a novel visual prompt matcher.  
6. A multi-expert design is employed to integrate the strengths of both prompt generators, supplemented with a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency metrics.  
7. Extensive experiments demonstrate that V²-SAM achieves state-of-the-art performance on several benchmarks including Ego-Exo4D for ego-exo object correspondence, DAVIS-2017 for video object tracking, and HANDAL-X for robotic-ready cross-view correspondence. <div>
arXiv:2511.20886v1 Announce Type: new 
Abstract: Cross-view object correspondence, exemplified by the representative task of ego-exo object correspondence, aims to establish consistent associations of the same object across different viewpoints (e.g., ego-centric and exo-centric). This task poses significant challenges due to drastic viewpoint and appearance variations, making existing segmentation models, such as SAM2, non-trivial to apply directly. To address this, we present V^2-SAM, a unified cross-view object correspondence framework that adapts SAM2 from single-view segmentation to cross-view correspondence through two complementary prompt generators. Specifically, the Cross-View Anchor Prompt Generator (V^2-Anchor), built upon DINOv3 features, establishes geometry-aware correspondences and, for the first time, unlocks coordinate-based prompting for SAM2 in cross-view scenarios, while the Cross-View Visual Prompt Generator (V^2-Visual) enhances appearance-guided cues via a novel visual prompt matcher that aligns ego-exo representations from both feature and structural perspectives. To effectively exploit the strengths of both prompts, we further adopt a multi-expert design and introduce a Post-hoc Cyclic Consistency Selector (PCCS) that adaptively selects the most reliable expert based on cyclic consistency. Extensive experiments validate the effectiveness of V^2-SAM, achieving new state-of-the-art performance on Ego-Exo4D (ego-exo object correspondence), DAVIS-2017 (video object tracking), and HANDAL-X (robotic-ready cross-view correspondence).
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Alignment of Text-to-Image Diffusion Models via Null-Text Embedding Optimisation</title>
<link>https://arxiv.org/abs/2511.20889</link>
<guid>https://arxiv.org/abs/2511.20889</guid>
<content:encoded><![CDATA[
<div> Keywords: Test-time alignment, Null-Text Test-Time Alignment, diffusion models, classifier-free guidance, semantic embedding<br /><br />Summary:  
1. Test-time alignment (TTA) focuses on adapting models during inference to optimize specific reward functions.  
2. Existing TTA methods often either under-optimize these reward functions or fall prey to reward hacking, where models exploit non-semantic noise to artificially boost rewards.  
3. The paper introduces Null-Text Test-Time Alignment (Null-TTA), a novel approach that optimizes the unconditional embedding within classifier-free guidance in diffusion models rather than altering latent variables or noise.  
4. Because the text embedding space is semantically structured, Null-TTA ensures that alignment happens on a coherent semantic manifold, effectively preventing reward hacking.  
5. Null-TTA steers the entire generative distribution of the model towards the target reward without changing model parameters, unlike methods that only modify individual samples.  
6. Experiments demonstrate that Null-TTA achieves state-of-the-art performance in test-time alignment across multiple reward types while maintaining strong generalization to new rewards.  
7. This approach establishes semantic-space optimization as a new and principled paradigm for improving test-time alignment in diffusion models, combining effectiveness with theoretical soundness. <div>
arXiv:2511.20889v1 Announce Type: new 
Abstract: Test-time alignment (TTA) aims to adapt models to specific rewards during inference. However, existing methods tend to either under-optimise or over-optimise (reward hack) the target reward function. We propose Null-Text Test-Time Alignment (Null-TTA), which aligns diffusion models by optimising the unconditional embedding in classifier-free guidance, rather than manipulating latent or noise variables. Due to the structured semantic nature of the text embedding space, this ensures alignment occurs on a semantically coherent manifold and prevents reward hacking (exploiting non-semantic noise patterns to improve the reward). Since the unconditional embedding in classifier-free guidance serves as the anchor for the model's generative distribution, Null-TTA directly steers model's generative distribution towards the target reward rather than just adjusting the samples, even without updating model parameters. Thanks to these desirable properties, we show that Null-TTA achieves state-of-the-art target test-time alignment while maintaining strong cross-reward generalisation. This establishes semantic-space optimisation as an effective and principled novel paradigm for TTA.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaINeR: Geometry-Aware Implicit Network Representation</title>
<link>https://arxiv.org/abs/2511.20924</link>
<guid>https://arxiv.org/abs/2511.20924</guid>
<content:encoded><![CDATA[
<div> Implicit Neural Representations, INRs, Geometry-Aware, Gaussian distributions, Image editing<br /><br />Summary:<br /><br />1. Implicit Neural Representations (INRs) have become a vital tool for modeling continuous 2D images, enabling tasks such as high-fidelity reconstruction, super-resolution, and compression. 2. Existing INR architectures like SIREN, WIRE, and FINER successfully capture fine-grained image details but often lack explicit geometric structure, limiting their capacity for local image editing and integration with physical simulations. 3. To overcome these limitations, the paper introduces GaINeR (Geometry-Aware Implicit Network Representation), a novel framework that enhances traditional INRs by integrating trainable Gaussian distributions with a neural network-based representation. 4. GaINeR retrieves the K nearest Gaussian centers for any given image coordinate, weights their embeddings based on proximity, and uses a neural network to predict the corresponding RGB values, effectively combining geometry and neural representation. 5. This method provides continuous image representations with interpretable geometric structure, enabling flexible local editing and paving the way for physically aware and interactive image manipulation. The authors have made the official implementation available publicly on GitHub. <div>
arXiv:2511.20924v1 Announce Type: new 
Abstract: Implicit Neural Representations (INRs) have become an essential tool for modeling continuous 2D images, enabling high-fidelity reconstruction, super-resolution, and compression. Popular architectures such as SIREN, WIRE, and FINER demonstrate the potential of INR for capturing fine-grained image details. However, traditional INRs often lack explicit geometric structure and have limited capabilities for local editing or integration with physical simulation, restricting their applicability in dynamic or interactive settings. To address these limitations, we propose GaINeR: Geometry-Aware Implicit Network Representation, a novel framework for 2D images that combines trainable Gaussian distributions with a neural network-based INR. For a given image coordinate, the model retrieves the K nearest Gaussians, aggregates distance-weighted embeddings, and predicts the RGB value via a neural network. This design enables continuous image representation, interpretable geometric structure, and flexible local editing, providing a foundation for physically aware and interactive image manipulation. The official implementation of our method is publicly available at https://github.com/WJakubowska/GaINeR.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A deep learning model to reduce agent dose for contrast-enhanced MRI of the cerebellopontine angle cistern</title>
<link>https://arxiv.org/abs/2511.20926</link>
<guid>https://arxiv.org/abs/2511.20926</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, low-dose MRI, contrast agent reduction, vestibular schwannoma, image quality restoration  

<br /><br />Summary:  
1. The study aimed to evaluate a deep learning (DL) model designed to reduce the contrast agent dose used in contrast-enhanced T1-weighted MRI (T1ce) of the cerebellopontine angle (CPA) cistern.  
2. A multi-center retrospective dataset consisting of T1 and T1ce MRI scans from vestibular schwannoma (VS) patients was used to simulate low-dose T1ce images by varying the contrast agent dose.  
3. DL models were trained to restore standard-dose quality images from these low-dose inputs.  
4. Image quality and tumor segmentation performance were assessed for the DL-restored images, with additional qualitative evaluation by a head and neck radiologist.  
5. The dataset included 203 MRI studies from 72 patients (mean age 58.51 years), revealing that image restoration quality improved significantly as the input contrast dose increased, measured by SSIM and PSNR.  
6. At a 10% input contrast dose, DL-restored images enhanced segmentation accuracy metrics, including Dice coefficient, Hausdorff distance, and average surface distance.  
7. Radiologist ratings indicated that DL-restored images from both 10% and 30% dose inputs were of excellent diagnostic quality, with 30% being more informative.  
8. Overall, the DL approach enables reliable lesion detection and diagnostic characterization in CPA cistern MRI with only 10%–30% of the standard contrast agent dose, potentially reducing patient exposure and risk. <div>
arXiv:2511.20926v1 Announce Type: new 
Abstract: Objectives: To evaluate a deep learning (DL) model for reducing the agent dose of contrast-enhanced T1-weighted MRI (T1ce) of the cerebellopontine angle (CPA) cistern. Materials and methods: In this multi-center retrospective study, T1 and T1ce of vestibular schwannoma (VS) patients were used to simulate low-dose T1ce with varying reductions of contrast agent dose. DL models were trained to restore standard-dose T1ce from the low-dose simulation. The image quality and segmentation performance of the DL-restored T1ce were evaluated. A head and neck radiologist was asked to rate DL-restored images in multiple aspects, including image quality and diagnostic characterization. Results: 203 MRI studies from 72 VS patients (mean age, 58.51 \pm 14.73, 39 men) were evaluated. As the input dose increased, the structural similarity index measure of the restored T1ce increased from 0.639 \pm 0.113 to 0.993 \pm 0.009, and the peak signal-to-noise ratio increased from 21.6 \pm 3.73 dB to 41.4 \pm 4.84 dB. At 10% input dose, using DL-restored T1ce for segmentation improved the Dice from 0.673 to 0.734, the 95% Hausdorff distance from 2.38 mm to 2.07 mm, and the average surface distance from 1.00 mm to 0.59 mm. Both DL-restored T1ce from 10% and 30% input doses showed excellent images, with the latter being considered more informative. Conclusion: The DL model improved the image quality of low-dose MRI of the CPA cistern, which makes lesion detection and diagnostic characterization possible with 10% - 30% of the standard dose.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smooth regularization for efficient video recognition</title>
<link>https://arxiv.org/abs/2511.20928</link>
<guid>https://arxiv.org/abs/2511.20928</guid>
<content:encoded><![CDATA[
<div> Keywords: temporal inductive bias, video recognition, Gaussian Random Walk, smooth regularization, lightweight models<br /><br />Summary:<br />1. The paper introduces a smooth regularization technique designed to embed a strong temporal inductive bias in video recognition models, with emphasis on enhancing lightweight architectures.<br />2. The method works by encouraging smooth transitions in intermediate-layer embeddings of consecutive video frames, modeling their changes as a Gaussian Random Walk (GRW).<br />3. This approach penalizes abrupt shifts in representation, promoting low-acceleration changes that mimic natural temporal coherence observed in videos.<br />4. By enforcing this smoothness, the method helps lightweight video models better capture complex temporal dynamics, improving their overall performance.<br />5. Applied to lightweight architectures, this technique achieves significant accuracy gains, including 3.8% to 6.4% improvements on the Kinetics-600 dataset.<br />6. Specifically, the MoViNets model family trained with this smooth regularization outperforms prior state-of-the-art models by 3.8% to 6.1% within equivalent FLOP constraints.<br />7. MobileNetV3 and MoViNets-Stream models also benefit, showing 4.9% to 6.4% improvements compared to previous best models with similar memory requirements.<br />8. The authors provide the code and trained models publicly at their GitHub repository: https://github.com/gilgoldm/grw-smoothing, facilitating reproduction and further research. <div>
arXiv:2511.20928v1 Announce Type: new 
Abstract: We propose a smooth regularization technique that instills a strong temporal inductive bias in video recognition models, particularly benefiting lightweight architectures. Our method encourages smoothness in the intermediate-layer embeddings of consecutive frames by modeling their changes as a Gaussian Random Walk (GRW). This penalizes abrupt representational shifts, thereby promoting low-acceleration solutions that better align with the natural temporal coherence inherent in videos. By leveraging this enforced smoothness, lightweight models can more effectively capture complex temporal dynamics. Applied to such models, our technique yields a 3.8% to 6.4% accuracy improvement on Kinetics-600. Notably, the MoViNets model family trained with our smooth regularization improves the current state of the art by 3.8% to 6.1% within their respective FLOP constraints, while MobileNetV3 and the MoViNets-Stream family achieve gains of 4.9% to 6.4% over prior state-of-the-art models with comparable memory footprints. Our code and models are available at https://github.com/gilgoldm/grw-smoothing.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Vocabulary Compositional Explanations for Neuron Alignment</title>
<link>https://arxiv.org/abs/2511.20931</link>
<guid>https://arxiv.org/abs/2511.20931</guid>
<content:encoded><![CDATA[
<div> Keywords: neurons, compositional explanations, open vocabulary, semantic segmentation, deep neural networks<br /><br />Summary:<br /><br />This paper focuses on understanding how neurons in deep neural networks encode information by using compositional explanations that relate neuron activations to human-understood concepts. Traditional compositional explanations depend on human-annotated datasets, limiting their use to specific domains with predefined concepts. To overcome this, the authors propose a new framework for the vision domain that enables users to probe neurons with arbitrary concepts across various datasets. The framework operates in three steps: first, specifying arbitrary concepts of interest; second, generating semantic segmentation masks through open vocabulary models, which do not rely on fixed annotated labels; third, computing compositional explanations based on these model-generated masks. The paper evaluates this framework in comparison with prior methods, showing both quantitative improvements and enhanced human interpretability. Additionally, the authors analyze how explanations change when shifting from human-annotated data to model-annotated data. Finally, the proposed approach demonstrates greater flexibility, allowing explanations to adapt to different tasks and attributes, broadening its applicability and usability in diverse settings within vision-based deep learning. <div>
arXiv:2511.20931v1 Announce Type: new 
Abstract: Neurons are the fundamental building blocks of deep neural networks, and their interconnections allow AI to achieve unprecedented results. Motivated by the goal of understanding how neurons encode information, compositional explanations leverage logical relationships between concepts to express the spatial alignment between neuron activations and human knowledge. However, these explanations rely on human-annotated datasets, restricting their applicability to specific domains and predefined concepts. This paper addresses this limitation by introducing a framework for the vision domain that allows users to probe neurons for arbitrary concepts and datasets. Specifically, the framework leverages masks generated by open vocabulary semantic segmentation to compute open vocabulary compositional explanations. The proposed framework consists of three steps: specifying arbitrary concepts, generating semantic segmentation masks using open vocabulary models, and deriving compositional explanations from these masks. The paper compares the proposed framework with previous methods for computing compositional explanations both in terms of quantitative metrics and human interpretability, analyzes the differences in explanations when shifting from human-annotated data to model-annotated data, and showcases the additional capabilities provided by the framework in terms of flexibility of the explanations with respect to the tasks and properties of interest.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UruDendro4: A Benchmark Dataset for Automatic Tree-Ring Detection in Cross-Section Images of Pinus taeda L</title>
<link>https://arxiv.org/abs/2511.20935</link>
<guid>https://arxiv.org/abs/2511.20935</guid>
<content:encoded><![CDATA[
<div> Tree rings, Pinus taeda, automated detection, UruDendro4 dataset, volumetric modeling  

<br /><br />Summary:  
This article addresses the challenge of accurately quantifying annual tree-ring growth, which is essential for assessing silvicultural practices but traditionally measured manually along limited radial directions, making it time-consuming and prone to imprecision. To overcome the scarcity of quality wood cross-section data, the authors introduce UruDendro4, a novel dataset consisting of 102 images of Pinus taeda L. samples manually annotated with annual rings. Unlike earlier datasets, UruDendro4 contains samples taken from multiple heights along the tree stem, enabling volumetric modeling of annual growth from cross-sectional images. The paper also presents a performance baseline of automatic ring detection on this dataset using state-of-the-art deep learning methods, with the DeepCS-TRD method achieving the best results—mean Average Precision of 0.838, mean Average Recall of 0.782, and an Adapted Rand Error of 0.084. Ablation experiments validate the chosen parameter configurations. Moreover, the study demonstrates that incorporating UruDendro4 data in model training enhances generalization ability for tree-ring detection tasks, underscoring the dataset’s value for improving automated dendrochronological analysis. <div>
arXiv:2511.20935v1 Announce Type: new 
Abstract: Tree-ring growth represents the annual wood increment for a tree, and quantifying it allows researchers to assess which silvicultural practices are best suited for each species. Manual measurement of this growth is time-consuming and often imprecise, as it is typically performed along 4 to 8 radial directions on a cross-sectional disc. In recent years, automated algorithms and datasets have emerged to enhance accuracy and automate the delineation of annual rings in cross-sectional images.
  To address the scarcity of wood cross-section data, we introduce the UruDendro4 dataset, a collection of 102 image samples of Pinus taeda L., each manually annotated with annual growth rings. Unlike existing public datasets, UruDendro4 includes samples extracted at multiple heights along the stem, allowing for the volumetric modeling of annual growth using manually delineated rings. This dataset (images and annotations) allows the development of volumetric models for annual wood estimation based on cross-sectional imagery.
  Additionally, we provide a performance baseline for automatic ring detection on this dataset using state-of-the-art methods. The highest performance was achieved by the DeepCS-TRD method, with a mean Average Precision of 0.838, a mean Average Recall of 0.782, and an Adapted Rand Error score of 0.084. A series of ablation experiments were conducted to empirically validate the final parameter configuration. Furthermore, we empirically demonstrate that training a learning model including this dataset improves the model's generalization in the tree-ring detection task.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BUSTR: Breast Ultrasound Text Reporting with a Descriptor-Aware Vision-Language Model</title>
<link>https://arxiv.org/abs/2511.20956</link>
<guid>https://arxiv.org/abs/2511.20956</guid>
<content:encoded><![CDATA[
<div> Breast ultrasound, radiology report generation, vision-language model, multitask learning, descriptor-aware representation<br /><br />Summary: The paper addresses the challenge of automated radiology report generation (RRG) for breast ultrasound (BUS), focusing on overcoming the lack of paired image-report datasets and minimizing hallucination risks associated with large language models. It introduces BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR operates by constructing reports based on structured descriptors such as BI-RADS, pathology, and histology, alongside radiomics features. The framework employs a multi-head Swin encoder to learn descriptor-aware visual representations through a multitask loss applied across dataset-specific descriptor sets. It further aligns visual and textual token representations using a dual-level objective combining token-level cross-entropy loss and cosine-similarity alignment loss between input and output features. The model was evaluated on two public BUS datasets, BrEaST and BUS-BRA, which vary in size and descriptor availability. BUSTR consistently improved both standard natural language generation metrics and clinical efficacy metrics across these datasets, with significant gains in key diagnostic indicators like BI-RADS category and pathology. The results demonstrate that a descriptor-aware vision model trained with combined token-level and alignment losses can enhance automatic report quality and clinical relevance without dependence on paired image-report datasets. Source code is publicly available for reproducibility and further study. <div>
arXiv:2511.20956v1 Announce Type: new 
Abstract: Automated radiology report generation (RRG) for breast ultrasound (BUS) is limited by the lack of paired image-report datasets and the risk of hallucinations from large language models. We propose BUSTR, a multitask vision-language framework that generates BUS reports without requiring paired image-report supervision. BUSTR constructs reports from structured descriptors (e.g., BI-RADS, pathology, histology) and radiomics features, learns descriptor-aware visual representations with a multi-head Swin encoder trained using a multitask loss over dataset-specific descriptor sets, and aligns visual and textual tokens via a dual-level objective that combines token-level cross-entropy with a cosine-similarity alignment loss between input and output representations. We evaluate BUSTR on two public BUS datasets, BrEaST and BUS-BRA, which differ in size and available descriptors. Across both datasets, BUSTR consistently improves standard natural language generation metrics and clinical efficacy metrics, particularly for key targets such as BI-RADS category and pathology. Our results show that this descriptor-aware vision model, trained with a combined token-level and alignment loss, improves both automatic report metrics and clinical efficacy without requiring paired image-report data. The source code can be found at https://github.com/AAR-UNLV/BUSTR
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Realism: Learning the Art of Expressive Composition with StickerNet</title>
<link>https://arxiv.org/abs/2511.20957</link>
<guid>https://arxiv.org/abs/2511.20957</guid>
<content:encoded><![CDATA[
<div> Keywords: expressive composition, image editing, StickerNet, user behavior, stylistic diversity<br /><br />Summary:<br />1. The paper introduces a new task called expressive composition, which redefines image composition to prioritize stylistic diversity and flexible placement rather than strict visual realism and semantic plausibility. <br />2. Expressive composition reflects real-world editing behavior, particularly on online platforms where users create playful, artistic, or socially engaging content instead of purely realistic images. <br />3. To tackle this task, the authors propose StickerNet, a two-stage framework that first classifies the type of composition and then predicts placement parameters including opacity, mask, location, and scale.<br />4. The authors construct a novel dataset using 1.8 million genuine editing actions collected from an anonymous online visual creation and editing platform, capturing authentic user-validated placement decisions. This contrasts with prior work that relied on simulated object placements.<br />5. User studies and quantitative evaluations demonstrate that StickerNet outperforms standard baselines and closely models human placement behaviors, highlighting the benefits of training on real-world data despite the ambiguous nature of the task. This work opens a new avenue in visual understanding focused on user intent and expressiveness rather than just realism. <div>
arXiv:2511.20957v1 Announce Type: new 
Abstract: As a widely used operation in image editing workflows, image composition has traditionally been studied with a focus on achieving visual realism and semantic plausibility. However, in practical editing scenarios of the modern content creation landscape, many compositions are not intended to preserve realism. Instead, users of online platforms motivated by gaining community recognition often aim to create content that is more artistic, playful, or socially engaging. Taking inspiration from this observation, we define the expressive composition task, a new formulation of image composition that embraces stylistic diversity and looser placement logic, reflecting how users edit images on real-world creative platforms. To address this underexplored problem, we present StickerNet, a two-stage framework that first determines the composition type, then predicts placement parameters such as opacity, mask, location, and scale accordingly. Unlike prior work that constructs datasets by simulating object placements on real images, we directly build our dataset from 1.8 million editing actions collected on an anonymous online visual creation and editing platform, each reflecting user-community validated placement decisions. This grounding in authentic editing behavior ensures strong alignment between task definition and training supervision. User studies and quantitative evaluations show that StickerNet outperforms common baselines and closely matches human placement behavior, demonstrating the effectiveness of learning from real-world editing patterns despite the inherent ambiguity of the task. This work introduces a new direction in visual understanding that emphasizes expressiveness and user intent over realism.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs</title>
<link>https://arxiv.org/abs/2511.20965</link>
<guid>https://arxiv.org/abs/2511.20965</guid>
<content:encoded><![CDATA[
<div> Traffic cameras; multi-camera analysis; Vision-Language Models; text generation; traffic management  

<br /><br />Summary: TrafficLens is a novel algorithm designed to improve multi-camera traffic video analysis at intersections, addressing the challenge of processing large volumes of video data efficiently. It leverages overlapping camera coverage areas to process information sequentially, applying Vision-Language Models (VLMs) with different token limits iteratively and using outputs from previous cameras as prompts for the next, which accelerates the generation of detailed textual descriptions. To reduce redundancy and save computational resources, TrafficLens incorporates an object-level similarity detector that intelligently bypasses unnecessary VLM invocations. By integrating these strategies, TrafficLens significantly cuts down the video-to-text conversion time by up to four times, while still preserving the accuracy of the information extracted. This advancement supports faster and more effective traffic monitoring, law enforcement, pedestrian safety measures, and incident investigation through timely insights derived from multi-camera feeds. Real-world experiments validate the approach, demonstrating its potential for practical deployment in intelligent transportation systems where rapid and precise video analysis is crucial. <div>
arXiv:2511.20965v1 Announce Type: new 
Abstract: Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\times$ while maintaining information accuracy.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Vision Transformer Learning Leveraging Lightweight Homomorphic Encryption in Medical AI</title>
<link>https://arxiv.org/abs/2511.20983</link>
<guid>https://arxiv.org/abs/2511.20983</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, homomorphic encryption, Vision Transformer, histopathology classification, privacy preservation<br /><br />Summary: This paper addresses privacy concerns in collaborative machine learning across healthcare institutions, where direct sharing of patient data is restricted by privacy regulations such as HIPAA. It proposes a novel federated learning (FL) framework combining Vision Transformers (ViT) and homomorphic encryption (HE) to securely classify histopathology images across multiple clients. The method uses the ViT CLS token, a compact 768-dimensional feature vector, as a privacy-preserving representation that is encrypted using CKKS homomorphic encryption before transmission. Encrypting CLS tokens significantly reduces communication overhead by 30 times compared to conventional gradient encryption. Experimental evaluation on a three-client lung cancer histopathology classification task demonstrates that while gradients are vulnerable to inversion attacks capable of reconstructing near-perfect images (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), the proposed CLS token-based encryption prevents such attacks effectively. Encrypted inference is achieved directly on ciphertexts with only 326 KB per aggregation round, maintaining strong privacy. The framework attains high classification accuracy, achieving 96.12% in the unencrypted domain and 90.02% in the encrypted domain, demonstrating a practical balance between privacy, efficiency, and model performance for secure multi-institutional medical AI applications. <div>
arXiv:2511.20983v1 Announce Type: new 
Abstract: Collaborative machine learning across healthcare institutions promises improved diagnostic accuracy by leveraging diverse datasets, yet privacy regulations such as HIPAA prohibit direct patient data sharing. While federated learning (FL) enables decentralized training without raw data exchange, recent studies show that model gradients in conventional FL remain vulnerable to reconstruction attacks, potentially exposing sensitive medical information. This paper presents a privacy-preserving federated learning framework combining Vision Transformers (ViT) with homomorphic encryption (HE) for secure multi-institutional histopathology classification. The approach leverages the ViT CLS token as a compact 768-dimensional feature representation for secure aggregation, encrypting these tokens using CKKS homomorphic encryption before transmission to the server. We demonstrate that encrypting CLS tokens achieves a 30-fold communication reduction compared to gradient encryption while maintaining strong privacy guarantees. Through evaluation on a three-client federated setup for lung cancer histopathology classification, we show that gradients are highly susceptible to model inversion attacks (PSNR: 52.26 dB, SSIM: 0.999, NMI: 0.741), enabling near-perfect image reconstruction. In contrast, the proposed CLS-protected HE approach prevents such attacks while enabling encrypted inference directly on ciphertexts, requiring only 326 KB of encrypted data transmission per aggregation round. The framework achieves 96.12 percent global classification accuracy in the unencrypted domain and 90.02 percent in the encrypted domain.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inversion-Free Style Transfer with Dual Rectified Flows</title>
<link>https://arxiv.org/abs/2511.20986</link>
<guid>https://arxiv.org/abs/2511.20986</guid>
<content:encoded><![CDATA[
<div> Keywords: style transfer, inversion-free, dual rectified flows, velocity field, attention injection<br /><br />Summary: This paper addresses the challenge of style transfer in image processing by proposing a novel inversion-free framework that eliminates the need for computationally expensive and error-prone inversion processes common in diffusion-based methods. The method uses dual rectified flows to predict content and style trajectories in parallel from the input content and style images, enabling the stylized image distribution to be found through a forward pass only. By employing a dynamic midpoint interpolation, it fuses the velocity fields from both trajectories while adapting to changes in the evolving stylized image to ensure robust fusion without simple overlay artifacts. The velocity field jointly models content, style, and stylized distributions, enhancing the fidelity of the final image. Additionally, attention injection is integrated to guide style incorporation more effectively, improving visual quality, preserving content details, and increasing computational efficiency. Extensive experiments demonstrate the approach's generalization ability across diverse content and style combinations, making it a practical and efficient pipeline for high-quality style transfer that outperforms traditional inversion-based diffusion models in both speed and output quality. <div>
arXiv:2511.20986v1 Announce Type: new 
Abstract: Style transfer, a pivotal task in image processing, synthesizes visually compelling images by seamlessly blending realistic content with artistic styles, enabling applications in photo editing and creative design. While mainstream training-free diffusion-based methods have greatly advanced style transfer in recent years, their reliance on computationally inversion processes compromises efficiency and introduces visual distortions when inversion is inaccurate. To address these limitations, we propose a novel \textit{inversion-free} style transfer framework based on dual rectified flows, which tackles the challenge of finding an unknown stylized distribution from two distinct inputs (content and style images), \textit{only with forward pass}. Our approach predicts content and style trajectories in parallel, then fuses them through a dynamic midpoint interpolation that integrates velocities from both paths while adapting to the evolving stylized image. By jointly modeling the content, style, and stylized distributions, our velocity field design achieves robust fusion and avoids the shortcomings of naive overlays. Attention injection further guides style integration, enhancing visual fidelity, content preservation, and computational efficiency. Extensive experiments demonstrate generalization across diverse styles and content, providing an effective and efficient pipeline for style transfer.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefOnce: Distilling References into a Prototype Memory for Referring Camouflaged Object Detection</title>
<link>https://arxiv.org/abs/2511.20989</link>
<guid>https://arxiv.org/abs/2511.20989</guid>
<content:encoded><![CDATA[
<div> Referring Camouflaged Object Detection, class-prototype memory, query-conditioned mixture, bidirectional attention alignment, R2C7K benchmark

<br /><br />Summary:  
This paper addresses the task of Referring Camouflaged Object Detection (Ref-COD), which focuses on segmenting specific camouflaged objects in an image by utilizing referring images. Existing methods rely on a dual-branch architecture that requires reference images during inference, which limits practical deployment due to increased latency and the need to collect additional data at test time. The authors propose a novel Ref-COD framework that eliminates the need for test-time references by distilling reference information into a class-prototype memory during training. Specifically, the framework maintains an Exponential Moving Average (EMA) updated prototype vector for each category and synthesizes a reference vector at inference using a query-conditioned mixture of these prototypes. To effectively bridge the representation gap between reference statistics and camouflaged query features, a bidirectional attention alignment module is introduced, which jointly adapts both query and class prototype features. The approach simplifies the Ref-COD pipeline while maintaining or improving performance. The proposed method is evaluated on the large-scale R2C7K benchmark, demonstrating competitive or superior results compared to state-of-the-art models. Code and models are made publicly available, enhancing reproducibility and potential real-world application. <div>
arXiv:2511.20989v1 Announce Type: new 
Abstract: Referring Camouflaged Object Detection (Ref-COD) segments specified camouflaged objects in a scene by leveraging a small set of referring images. Though effective, current systems adopt a dual-branch design that requires reference images at test time, which limits deployability and adds latency and data-collection burden. We introduce a Ref-COD framework that distills references into a class-prototype memory during training and synthesizes a reference vector at inference via a query-conditioned mixture of prototypes. Concretely, we maintain an EMA-updated prototype per category and predict mixture weights from the query to produce a guidance vector without any test-time references. To bridge the representation gap between reference statistics and camouflaged query features, we propose a bidirectional attention alignment module that adapts both the query features and the class representation. Thus, our approach yields a simple, efficient path to Ref-COD without mandatory references. We evaluate the proposed method on the large-scale R2C7K benchmark. Extensive experiments demonstrate competitive or superior performance of the proposed method compared with recent state-of-the-arts. Code is available at https://github.com/yuhuan-wu/RefOnce.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wavefront-Constrained Passive Obscured Object Detection</title>
<link>https://arxiv.org/abs/2511.20991</link>
<guid>https://arxiv.org/abs/2511.20991</guid>
<content:encoded><![CDATA[
<div> Wavefront Propagation, Complex Amplitude, Low Signal-to-Noise, Multi-scale Receptive Fields, Robustness<br /><br />Summary:<br /><br />1. The paper addresses the challenge of accurately localizing and segmenting obscured objects using faint light patterns beyond the field of view, where multiple scattering and medium-induced perturbations complicate coherent light propagation.<br /><br />2. Existing methods largely rely on real-valued models or local convolutional operations, which fail to fully capture the physics of coherent light behavior and often yield non-physical solutions, especially under low signal-to-noise scenarios.<br /><br />3. To overcome these limitations, the authors propose the Wavefront Propagating Compensation Network (WavePCNet), a physics-driven approach designed to simulate wavefront propagation and improve perception of obscured objects.<br /><br />4. WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP), which uses complex amplitude transfer operators to accurately constrain coherent wave propagation, alongside a momentum memory mechanism that suppresses cumulative perturbations.<br /><br />5. The model also introduces a High-frequency Cross-layer Compensation Enhancement strategy, constructing frequency-selective pathways with multi-scale receptive fields to dynamically maintain structural consistency across layers, resulting in enhanced robustness and interpretability.<br /><br />6. Extensive experimental validation on four physically collected datasets shows that WavePCNet consistently outperforms state-of-the-art methods in terms of accuracy and robustness under complex environmental conditions. <div>
arXiv:2511.20991v1 Announce Type: new 
Abstract: Accurately localizing and segmenting obscured objects from faint light patterns beyond the field of view is highly challenging due to multiple scattering and medium-induced perturbations. Most existing methods, based on real-valued modeling or local convolutional operations, are inadequate for capturing the underlying physics of coherent light propagation. Moreover, under low signal-to-noise conditions, these methods often converge to non-physical solutions, severely compromising the stability and reliability of the observation. To address these challenges, we propose a novel physics-driven Wavefront Propagating Compensation Network (WavePCNet) to simulate wavefront propagation and enhance the perception of obscured objects. This WavePCNet integrates the Tri-Phase Wavefront Complex-Propagation Reprojection (TriWCP) to incorporate complex amplitude transfer operators to precisely constrain coherent propagation behavior, along with a momentum memory mechanism to effectively suppress the accumulation of perturbations. Additionally, a High-frequency Cross-layer Compensation Enhancement is introduced to construct frequency-selective pathways with multi-scale receptive fields and dynamically model structural consistency across layers, further boosting the model's robustness and interpretability under complex environmental conditions. Extensive experiments conducted on four physically collected datasets demonstrate that WavePCNet consistently outperforms state-of-the-art methods across both accuracy and robustness.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuardTrace-VL: Detecting Unsafe Multimodel Reasoning via Iterative Safety Supervision</title>
<link>https://arxiv.org/abs/2511.20994</link>
<guid>https://arxiv.org/abs/2511.20994</guid>
<content:encoded><![CDATA[
<div> Multimodal reasoning, safety auditing, vision-language models, reasoning trace, GuardTrace dataset<br /><br />Summary:<br /><br />1. The paper addresses safety concerns in multimodal large reasoning models (MLRMs) used for vision-language tasks, specifically focusing on the unsafe content that can arise during intermediate reasoning steps even if the final answer is safe.<br />2. Current safety mechanisms mostly analyze only the input question and final answer, overlooking harmful content that may surface within the reasoning trace, such as biased deductions or misuse of visual information.<br />3. To mitigate this, the authors propose GuardTrace-VL, a vision-aware safety auditor that monitors the entire Question-Thinking-Answer (QTA) process using joint image-text analysis to detect unsafe content as it emerges during reasoning.<br />4. The study also introduces the GuardTrace dataset, created through diverse prompting and vetted with a combination of MLRM-based and human verification to support training and evaluation.<br />5. A three-stage progressive training scheme combined with data refinement enables the model to learn nuanced, context-dependent safety preferences across varying risk levels.<br />6. Experimental results show that GuardTrace-VL achieves an F1 score of 93.1% on unsafe reasoning detection, outperforming prior state-of-the-art multimodal safety defenses by 13.5% in F1 score.<br />7. The authors plan to publicly release the code to facilitate further research and deployment. <div>
arXiv:2511.20994v1 Announce Type: new 
Abstract: Multimodal large reasoning models (MLRMs) are increasingly deployed for vision-language tasks that produce explicit intermediate rationales. However, reasoning traces can contain unsafe content even when the final answer is non-harmful, creating deployment risks. Existing multimodal safety guards primarily evaluate only the input question and the final answer, neglecting the intermediate reasoning process. This oversight allows undetected harm, such as biased inferences or policy-violating use of visual context, to emerge during reasoning. We introduce GuardTrace-VL, a vision-aware safety auditor that monitors the full Question-Thinking-Answer (QTA) pipeline via joint image-text analysis, enabling detection of unsafe content as it emerges in the reasoning stage. To support training and evaluation, we construct the GuardTrace dataset, which is generated through diverse prompting strategies and refined via a MLRM- and human-based voting and verification pipeline. Furthermore, we propose a three-stage progressive training scheme combined with the data refinement process, enabling the model to learn nuanced and context-dependent safety preferences according to different risk levels. On our proposed test set covering both in-domain and out-of-domain scenarios, GuardTrace-VL model achieves an F1 score of 93.1% on unsafe reasoning detection tasks, representing a 13.5% improvement in F1 score compared to the previous strongest multimodal safety defense methods. The codes will be made publicly available.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Inpainting to Layer Decomposition: Repurposing Generative Inpainting Models for Image Layer Decomposition</title>
<link>https://arxiv.org/abs/2511.20996</link>
<guid>https://arxiv.org/abs/2511.20996</guid>
<content:encoded><![CDATA[
<div> Keywords: layered image decomposition, diffusion-based inpainting, multi-modal context fusion, synthetic dataset, object removal

<br /><br />Summary:  
This paper addresses the challenge of decomposing a single image into multiple layers, specifically separating foreground objects from the background, which has important applications in image editing and content creation. The authors highlight the difficulty in layer decomposition due to the scarcity of existing methods and annotated data. They identify a conceptual link between layer decomposition and image inpainting/outpainting tasks. Leveraging this insight, they adapt a diffusion-based inpainting model for the purpose of layer decomposition, employing lightweight finetuning to specialize the model without extensive retraining. To enhance the preservation of image details in the latent space, the paper introduces a novel multi-modal context fusion module characterized by linear attention complexity, which efficiently integrates diverse contextual information. The proposed model is trained exclusively on a synthetic dataset compiled from open-source assets, circumventing the need for expensive real-world annotations. Experimental results demonstrate that the approach performs strongly in tasks such as object removal and occlusion recovery. Overall, this work provides a robust framework for layer decomposition that facilitates independent element editing and promises novel possibilities for downstream image editing and creative applications. <div>
arXiv:2511.20996v1 Announce Type: new 
Abstract: Images can be viewed as layered compositions, foreground objects over background, with potential occlusions. This layered representation enables independent editing of elements, offering greater flexibility for content creation. Despite the progress in large generative models, decomposing a single image into layers remains challenging due to limited methods and data. We observe a strong connection between layer decomposition and in/outpainting tasks, and propose adapting a diffusion-based inpainting model for layer decomposition using lightweight finetuning. To further preserve detail in the latent space, we introduce a novel multi-modal context fusion module with linear attention complexity. Our model is trained purely on a synthetic dataset constructed from open-source assets and achieves superior performance in object removal and occlusion recovery, unlocking new possibilities in downstream editing and creative applications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Completes the Vision: A Multimodal Entity-aware Retrieval-Augmented Generation Framework for News Image Captioning</title>
<link>https://arxiv.org/abs/2511.21002</link>
<guid>https://arxiv.org/abs/2511.21002</guid>
<content:encoded><![CDATA[
<div> News image captioning, multimodal knowledge base, entity-aware retrieval, cross-modal alignment, visual-entity grounding<br /><br />Summary:<br /><br />News image captioning requires generating informative descriptions by combining image content with contextual information from related news articles. Existing methods face challenges such as incomplete information coverage, weak alignment between modalities (text and image), and poor grounding of visual entities within captions. To address these limitations, the paper proposes MERGE, a novel Multimodal Entity-aware Retrieval-augmented GEneration framework tailored for news image captioning. MERGE builds an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge sources, enriching background retrieval capabilities. It improves cross-modal alignment by using a multistage hypothesis-caption strategy, which allows better integration of visual and textual data. Additionally, MERGE enhances visual-entity grounding through dynamic retrieval guided by the actual image content to better associate visual elements with named entities. Extensive experiments conducted on benchmark datasets GoodNews and NYTimes800k demonstrate that MERGE significantly outperforms state-of-the-art models, with notable CIDEr metric improvements (+6.84 and +1.16) and gains in named entity recognition F1-scores (+4.14 and +2.64). Importantly, MERGE generalizes well to unseen data, exemplified by strong performance on the Visual News dataset, where it achieved +20.17 in CIDEr and +6.22 in F1-score, highlighting its robustness and adaptability across domains. <div>
arXiv:2511.21002v1 Announce Type: new 
Abstract: News image captioning aims to produce journalistically informative descriptions by combining visual content with contextual cues from associated articles. Despite recent advances, existing methods struggle with three key challenges: (1) incomplete information coverage, (2) weak cross-modal alignment, and (3) suboptimal visual-entity grounding. To address these issues, we introduce MERGE, the first Multimodal Entity-aware Retrieval-augmented GEneration framework for news image captioning. MERGE constructs an entity-centric multimodal knowledge base (EMKB) that integrates textual, visual, and structured knowledge, enabling enriched background retrieval. It improves cross-modal alignment through a multistage hypothesis-caption strategy and enhances visual-entity matching via dynamic retrieval guided by image content. Extensive experiments on GoodNews and NYTimes800k show that MERGE significantly outperforms state-of-the-art baselines, with CIDEr gains of +6.84 and +1.16 in caption quality, and F1-score improvements of +4.14 and +2.64 in named entity recognition. Notably, MERGE also generalizes well to the unseen Visual News dataset, achieving +20.17 in CIDEr and +6.22 in F1-score, demonstrating strong robustness and domain adaptability.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaRank: Task-Aware Metric Selection for Model Transferability Estimation</title>
<link>https://arxiv.org/abs/2511.21007</link>
<guid>https://arxiv.org/abs/2511.21007</guid>
<content:encoded><![CDATA[
<div> Model Transferability Estimation, meta-learning, metric selection, pretrained language model, transfer learning<br /><br />Summary:<br /><br />1. Selecting an appropriate pretrained source model for transfer learning is crucial but computationally expensive, motivating the need for efficient Model Transferability Estimation (MTE) methods to rank models without full fine-tuning.<br /><br />2. Existing MTE metrics are not universally optimal and their effectiveness varies significantly depending on the target task or dataset.<br /><br />3. To overcome this, the paper introduces MetaRank, a meta-learning framework designed to automatically select the most appropriate MTE metric tailored to the specific target dataset.<br /><br />4. MetaRank formulates metric selection as a learning-to-rank problem, leveraging textual descriptions of both datasets and MTE metrics encoded by a pretrained language model into a shared semantic embedding space.<br /><br />5. The meta-predictor is trained offline on diverse meta-tasks with a listwise ranking objective, enabling it to capture complex relationships between dataset characteristics and metric performance.<br /><br />6. In practice, MetaRank can efficiently rank candidate MTE metrics for new, unseen target datasets based solely on their textual descriptions, thus aiding practitioners in metric selection before model training.<br /><br />7. Extensive experiments involving 11 pretrained models and 11 datasets validate the effectiveness and robustness of the approach, showing improved metric selection tailored to specific tasks. <div>
arXiv:2511.21007v1 Announce Type: new 
Abstract: Selecting an appropriate pre-trained source model is a critical, yet computationally expensive, task in transfer learning. Model Transferability Estimation (MTE) methods address this by providing efficient proxy metrics to rank models without full fine-tuning. In practice, the choice of which MTE metric to use is often ad hoc or guided simply by a metric's average historical performance. However, we observe that the effectiveness of MTE metrics is highly task-dependent and no single metric is universally optimal across all target datasets. To address this gap, we introduce MetaRank, a meta-learning framework for automatic, task-aware MTE metric selection. We formulate metric selection as a learning-to-rank problem. Rather than relying on conventional meta-features, MetaRank encodes textual descriptions of both datasets and MTE metrics using a pretrained language model, embedding them into a shared semantic space. A meta-predictor is then trained offline on diverse meta-tasks to learn the intricate relationship between dataset characteristics and metric mechanisms, optimized with a listwise objective that prioritizes correctly ranking the top-performing metrics. During the subsequent online phase, MetaRank efficiently ranks the candidate MTE metrics for a new, unseen target dataset based on its textual description, enabling practitioners to select the most appropriate metric a priori. Extensive experiments across 11 pretrained models and 11 target datasets demonstrate the strong effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure-Aware Prototype Guided Trusted Multi-View Classification</title>
<link>https://arxiv.org/abs/2511.21021</link>
<guid>https://arxiv.org/abs/2511.21021</guid>
<content:encoded><![CDATA[
<div> Trustworthy multi-view classification, intra-view dependencies, inter-view consistency, prototypes, cross-view consensus<br /><br />Summary: Trustworthy multi-view classification (TMVC) aims to provide reliable decision-making when dealing with heterogeneous, inconsistent, or conflicting multi-source information. Current TMVC methods mainly use globally dense neighbor relationships to model intra-view dependencies, which results in high computational costs and difficulty in ensuring consistency across different views. Additionally, these existing approaches often combine evidence from multiple views using manually assigned weights, without guaranteeing that the multi-view neighbor structures align well within the class space, leading to less trustworthy classification results. To address these challenges, the paper introduces a novel TMVC framework that uses prototypes to represent neighbor structures within each view. This prototype-based representation simplifies the process of learning intra-view neighbor relations and enables dynamic alignment between intra-view and inter-view structures. Consequently, the proposed method facilitates a more efficient and consistent identification of consensus across different views. Extensive experiments conducted on multiple public multi-view datasets demonstrate that this approach achieves competitive performance and robustness when compared to state-of-the-art TMVC methods. <div>
arXiv:2511.21021v1 Announce Type: new 
Abstract: Trustworthy multi-view classification (TMVC) addresses the challenge of achieving reliable decision-making in complex scenarios where multi-source information is heterogeneous, inconsistent, or even conflicting. Existing TMVC approaches predominantly rely on globally dense neighbor relationships to model intra-view dependencies, leading to high computational costs and an inability to directly ensure consistency across inter-view relationships. Furthermore, these methods typically aggregate evidence from different views through manually assigned weights, lacking guarantees that the learned multi-view neighbor structures are consistent within the class space, thus undermining the trustworthiness of classification outcomes. To overcome these limitations, we propose a novel TMVC framework that introduces prototypes to represent the neighbor structures of each view. By simplifying the learning of intra-view neighbor relations and enabling dynamic alignment of intra- and inter-view structure, our approach facilitates more efficient and consistent discovery of cross-view consensus. Extensive experiments on multiple public multi-view datasets demonstrate that our method achieves competitive downstream performance and robustness compared to prevalent TMVC methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CameraMaster: Unified Camera Semantic-Parameter Control for Photography Retouching</title>
<link>https://arxiv.org/abs/2511.21024</link>
<guid>https://arxiv.org/abs/2511.21024</guid>
<content:encoded><![CDATA[
<div> Camera retouching, diffusion models, parameter control, image editing, cross-attention<br /><br />Summary:<br /><br />The paper introduces CameraMaster, a unified framework designed for physically consistent image retouching with precise control over camera parameters such as exposure, white balance, and zoom. Unlike existing methods that depend on ambiguous text prompts or separate parameter adjustment modules, CameraMaster explicitly decouples photographer intent from specific camera settings by integrating two information streams: a directive representation capturing user intent and a parameter embedding encoding exact camera configurations. The model modulates both the directive and content semantics using the camera parameter embedding and injects this modulated directive into content features through cross-attention. This results in a camera-sensitive semantic context that allows for finely tuned edits. Additionally, CameraMaster inputs the directive and camera embeddings as conditioning and gating signals into the time embedding, facilitating unified, layer-wise modulation throughout the diffusion denoising process, and ensuring tight alignment of semantics and parameters. To support training and evaluation, the authors created a large dataset with 78,000 image-prompt pairs annotated with camera parameters. Experimental results demonstrate that CameraMaster achieves monotonic and near-linear responses to camera parameter adjustments, supports seamless multi-parameter editing compositions, and significantly outperforms current state-of-the-art methods in controllable image retouching. <div>
arXiv:2511.21024v1 Announce Type: new 
Abstract: Text-guided diffusion models have greatly advanced image editing and generation. However, achieving physically consistent image retouching with precise parameter control (e.g., exposure, white balance, zoom) remains challenging. Existing methods either rely solely on ambiguous and entangled text prompts, which hinders precise camera control, or train separate heads/weights for parameter adjustment, which compromises scalability, multi-parameter composition, and sensitivity to subtle variations. To address these limitations, we propose CameraMaster, a unified camera-aware framework for image retouching. The key idea is to explicitly decouple the camera directive and then coherently integrate two critical information streams: a directive representation that captures the photographer's intent, and a parameter embedding that encodes precise camera settings. CameraMaster first uses the camera parameter embedding to modulate both the camera directive and the content semantics. The modulated directive is then injected into the content features via cross-attention, yielding a strongly camera-sensitive semantic context. In addition, the directive and camera embeddings are injected as conditioning and gating signals into the time embedding, enabling unified, layer-wise modulation throughout the denoising process and enforcing tight semantic-parameter alignment. To train and evaluate CameraMaster, we construct a large-scale dataset of 78K image-prompt pairs annotated with camera parameters. Extensive experiments show that CameraMaster produces monotonic and near-linear responses to parameter variations, supports seamless multi-parameter composition, and significantly outperforms existing methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaptionQA: Is Your Caption as Useful as the Image Itself?</title>
<link>https://arxiv.org/abs/2511.21025</link>
<guid>https://arxiv.org/abs/2511.21025</guid>
<content:encoded><![CDATA[
<div> Keywords: Image captions, CaptionQA, multimodal evaluation, downstream utility, domain-specific tasks<br /><br />Summary: This paper addresses the limitation in current image caption evaluation methods by introducing CaptionQA, a utility-based benchmark designed to assess how well model-generated captions can replace images in downstream tasks. CaptionQA spans four diverse domains—Natural, Document, E-commerce, and Embodied AI—with detailed taxonomies encompassing 25 top-level and 69 subcategories to identify the most relevant information for domain-specific applications. The benchmark consists of 33,027 multiple-choice questions, averaging over 50 questions per image, which require explicit visual reasoning to answer, thus thoroughly probing the utility of captions. The evaluation approach involves using a large language model (LLM) to answer these questions relying solely on captions, enabling a direct measurement of the information preserved from the original images. Results indicate significant discrepancies in performance between using images and captions, with state-of-the-art multimodal language models showing up to a 32% reduction in task utility when relying on captions. This highlights gaps in current captioning techniques and their downstream applicability. The authors provide CaptionQA as an extensible resource along with an open-source framework that facilitates expansion into new domains, encouraging further research and development in improving image-to-caption effectiveness for practical multimodal uses. <div>
arXiv:2511.21025v1 Announce Type: new 
Abstract: Image captions serve as efficient surrogates for visual content in multimodal systems such as retrieval, recommendation, and multi-step agentic inference pipelines. Yet current evaluation practices miss a fundamental question: Can captions stand-in for images in real downstream tasks? We propose a utility-based benchmark, CaptionQA, to evaluate model-generated captions, where caption quality is measured by how well it supports downstream tasks. CaptionQA is an extensible domain-dependent benchmark covering 4 domains--Natural, Document, E-commerce, and Embodied AI--each with fine-grained taxonomies (25 top-level and 69 subcategories) that identify useful information for domain-specific tasks. CaptionQA builds 33,027 densely annotated multiple-choice questions (50.3 per image on average) that explicitly require visual information to answer, providing a comprehensive probe of caption utility. In our evaluation protocol, an LLM answers these questions using captions alone, directly measuring whether captions preserve image-level utility and are utilizable by a downstream LLM. Evaluating state-of-the-art MLLMs reveals substantial gaps between the image and its caption utility. Notably, models nearly identical on traditional image-QA benchmarks lower by up to 32% in caption utility. We release CaptionQA along with an open-source pipeline for extension to new domains. The code is available at https://github.com/bronyayang/CaptionQA.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowerDance: MeanFlow for Efficient and Refined 3D Dance Generation</title>
<link>https://arxiv.org/abs/2511.21029</link>
<guid>https://arxiv.org/abs/2511.21029</guid>
<content:encoded><![CDATA[
<div> Keywords: Music-to-dance generation, FlowerDance, physical plausibility, generation efficiency, motion editing<br /><br />Summary:<br /><br />1. The paper introduces FlowerDance, a novel music-to-dance generation framework designed to translate auditory signals into expressive and physically plausible human motion, targeting applications like virtual reality, choreography, and digital entertainment.  
2. FlowerDance addresses limitations in existing methods related to generation efficiency, enabling high-fidelity 3D rendering and enhancing the expressiveness of 3D characters in real-world use cases.  
3. The approach combines MeanFlow with Physical Consistency Constraints, allowing it to generate high-quality motion with only a few sampling steps, improving both speed and resource usage.  
4. Its architecture includes a BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which facilitates non-autoregressive, efficient dance generation.  
5. Additionally, FlowerDance supports interactive motion editing, letting users refine dance sequences in real time. Extensive experiments on the AIST++ and FineDance datasets demonstrate that FlowerDance achieves state-of-the-art performance in terms of both motion quality and generation efficiency. Code release is planned upon acceptance. <div>
arXiv:2511.21029v1 Announce Type: new 
Abstract: Music-to-dance generation aims to translate auditory signals into expressive human motion, with broad applications in virtual reality, choreography, and digital entertainment. Despite promising progress, the limited generation efficiency of existing methods leaves insufficient computational headroom for high-fidelity 3D rendering, thereby constraining the expressiveness of 3D characters during real-world applications. Thus, we propose FlowerDance, which not only generates refined motion with physical plausibility and artistic expressiveness, but also achieves significant generation efficiency on inference speed and memory utilization . Specifically, FlowerDance combines MeanFlow with Physical Consistency Constraints, which enables high-quality motion generation with only a few sampling steps. Moreover, FlowerDance leverages a simple but efficient model architecture with BiMamba-based backbone and Channel-Level Cross-Modal Fusion, which generates dance with efficient non-autoregressive manner. Meanwhile, FlowerDance supports motion editing, enabling users to interactively refine dance sequences. Extensive experiments on AIST++ and FineDance show that FlowerDance achieves state-of-the-art results in both motion quality and generation efficiency. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules</title>
<link>https://arxiv.org/abs/2511.21042</link>
<guid>https://arxiv.org/abs/2511.21042</guid>
<content:encoded><![CDATA[
<div> Keywords: LungNoduleAgent, multi-agent system, lung CT scans, lung nodules, malignancy grading  

<br /><br />Summary: Diagnosing lung cancer relies heavily on identifying lung nodules in CT scans and generating accurate diagnostic reports. Existing multimodal large language models face challenges in precisely describing nodule morphology and incorporating expert medical knowledge, limiting their clinical reliability. To address these issues, the authors propose LungNoduleAgent, a collaborative multi-agent system tailored for lung CT scan analysis. The system divides the diagnostic process into three sequential modules: the Nodule Spotter for accurate nodule detection, the Radiologist module for detailed localized image description and CT report generation, and the Doctor Agent System which performs malignancy reasoning by leveraging both images and reports, supported by a pathology knowledge base. LungNoduleAgent was extensively tested on two private datasets as well as the public LIDC-IDRI dataset, outperforming mainstream vision-language models, other agent systems, and specialized expert models. The research highlights the importance of region-level semantic alignment and multi-agent collaboration in improving diagnostic accuracy. Overall, LungNoduleAgent demonstrates significant potential as a foundational clinical tool for enhancing lung nodule analysis and malignancy assessment in medical practice. <div>
arXiv:2511.21042v1 Announce Type: new 
Abstract: Diagnosing lung cancer typically involves physicians identifying lung nodules in Computed tomography (CT) scans and generating diagnostic reports based on their morphological features and medical expertise. Although advancements have been made in using multimodal large language models for analyzing lung CT scans, challenges remain in accurately describing nodule morphology and incorporating medical expertise. These limitations affect the reliability and effectiveness of these models in clinical settings. Collaborative multi-agent systems offer a promising strategy for achieving a balance between generality and precision in medical applications, yet their potential in pathology has not been thoroughly explored. To bridge these gaps, we introduce LungNoduleAgent, an innovative collaborative multi-agent system specifically designed for analyzing lung CT scans. LungNoduleAgent streamlines the diagnostic process into sequential components, improving precision in describing nodules and grading malignancy through three primary modules. The first module, the Nodule Spotter, coordinates clinical detection models to accurately identify nodules. The second module, the Radiologist, integrates localized image description techniques to produce comprehensive CT reports. Finally, the Doctor Agent System performs malignancy reasoning by using images and CT reports, supported by a pathology knowledge base and a multi-agent system framework. Extensive testing on two private datasets and the public LIDC-IDRI dataset indicates that LungNoduleAgent surpasses mainstream vision-language models, agent systems, and advanced expert models. These results highlight the importance of region-level semantic alignment and multi-agent collaboration in diagnosing nodules. LungNoduleAgent stands out as a promising foundational tool for supporting clinical analyses of lung nodules.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PG-ControlNet: A Physics-Guided ControlNet for Generative Spatially Varying Image Deblurring</title>
<link>https://arxiv.org/abs/2511.21043</link>
<guid>https://arxiv.org/abs/2511.21043</guid>
<content:encoded><![CDATA[
<div> Keywords: spatially varying deblurring, model-based deep unrolling, generative models, physical constraints, ControlNet<br /><br />Summary:<br /><br />1. The paper addresses the challenging problem of spatially varying image deblurring, which is particularly difficult due to complex mixtures of motion blur and other degradation under significant noise.<br />2. Existing state-of-the-art methods generally follow two paradigms: model-based deep unrolling approaches incorporating physical constraints but often yielding over-smoothed or artifact-laden results, and generative models that provide better perceptual quality but tend to hallucinate details due to weak physical guidance.<br />3. The authors propose a novel framework that combines the strengths of both paradigms by enforcing explicit, dense physical constraints on a powerful generative prior.<br />4. Instead of simplifying the degradation modeling, the method represents the degradation as a dense continuum of high-dimensional compressed kernels, allowing detailed capture of fine-grained variations in motion and blur patterns.<br />5. This rich representation is used to condition a ControlNet architecture, which robustly guides the diffusion sampling process during image restoration.<br />6. Extensive experiments show that the proposed approach successfully bridges the gap between physical accuracy and perceptual realism, outperforming both prior model-based and generative methods in heavily blurred and noisy image scenarios. <div>
arXiv:2511.21043v1 Announce Type: new 
Abstract: Spatially varying image deblurring remains a fundamentally ill-posed problem, especially when degradations arise from complex mixtures of motion and other forms of blur under significant noise. State-of-the-art learning-based approaches generally fall into two paradigms: model-based deep unrolling methods that enforce physical constraints by modeling the degradations, but often produce over-smoothed, artifact-laden textures, and generative models that achieve superior perceptual quality yet hallucinate details due to weak physical constraints. In this paper, we propose a novel framework that uniquely reconciles these paradigms by taming a powerful generative prior with explicit, dense physical constraints. Rather than oversimplifying the degradation field, we model it as a dense continuum of high-dimensional compressed kernels, ensuring that minute variations in motion and other degradation patterns are captured. We leverage this rich descriptor field to condition a ControlNet architecture, strongly guiding the diffusion sampling process. Extensive experiments demonstrate that our method effectively bridges the gap between physical accuracy and perceptual realism, outperforming state-of-the-art model-based methods as well as generative baselines in challenging, severely blurred scenarios.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUSE: Manipulating Unified Framework for Synthesizing Emotions in Images via Test-Time Optimization</title>
<link>https://arxiv.org/abs/2511.21051</link>
<guid>https://arxiv.org/abs/2511.21051</guid>
<content:encoded><![CDATA[
<div> Image Emotional Synthesis, MUSE framework, emotion-guided generation, Test-Time Scaling, multi-emotion loss<br /><br />Summary: This paper introduces MUSE, a novel unified framework designed to perform both emotional image generation and editing within a single model, addressing inefficiencies in current Image Emotional Synthesis (IES) methods that treat these tasks separately. MUSE leverages a strategy inspired by Test-Time Scaling (TTS), commonly used in language and diffusion models, allowing it to operate without retraining or relying on specialized emotional datasets. The framework tackles three main challenges: (1) How to guide the synthesis process effectively by utilizing an off-the-shelf emotion classifier combined with gradient-based optimization on emotional tokens; (2) When to apply emotional guidance during synthesis by determining the optimal timing based on semantic similarity as a supervisory signal; and (3) Which emotion to choose for guidance by implementing a multi-emotion loss function that reduces conflicts from similar or inherent emotions in images. Experimental evaluations demonstrate that MUSE outperforms existing methods in both generation and editing tasks, enhancing emotional accuracy and promoting semantic diversity. Furthermore, it effectively balances the fidelity of the desired content, adherence to the original text prompts, and realistic emotional expression, thereby establishing a new paradigm for emotional synthesis in images. <div>
arXiv:2511.21051v1 Announce Type: new 
Abstract: Images evoke emotions that profoundly influence perception, often prioritized over content. Current Image Emotional Synthesis (IES) approaches artificially separate generation and editing tasks, creating inefficiencies and limiting applications where these tasks naturally intertwine, such as therapeutic interventions or storytelling. In this work, we introduce MUSE, the first unified framework capable of both emotional generation and editing. By adopting a strategy conceptually aligned with Test-Time Scaling (TTS) that widely used in both LLM and diffusion model communities, it avoids the requirement for additional updating diffusion model and specialized emotional synthesis datasets. More specifically, MUSE addresses three key questions in emotional synthesis: (1) HOW to stably guide synthesis by leveraging an off-the-shelf emotion classifier with gradient-based optimization of emotional tokens; (2) WHEN to introduce emotional guidance by identifying the optimal timing using semantic similarity as a supervisory signal; and (3) WHICH emotion to guide synthesis through a multi-emotion loss that reduces interference from inherent and similar emotions. Experimental results show that MUSE performs favorably against all methods for both generation and editing, improving emotional accuracy and semantic diversity while maintaining an optimal balance between desired content, adherence to text prompts, and realistic emotional expression. It establishes a new paradigm for emotion synthesis.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Term Alzheimers Disease Prediction: A Novel Image Generation Method Using Temporal Parameter Estimation with Normal Inverse Gamma Distribution on Uneven Time Series</title>
<link>https://arxiv.org/abs/2511.21057</link>
<guid>https://arxiv.org/abs/2511.21057</guid>
<content:encoded><![CDATA[
<div> Keywords: Alzheimer's Disease, image generation, temporal parameter, Normal Inverse Gamma Distribution, uncertainty estimation<br /><br />Summary:<br /><br />This article addresses the challenge of predicting Alzheimer's Disease (AD) progression using image generation, particularly when sequential brain imaging data have irregular time intervals. The authors propose a novel model named Temporal Normal Inverse Gamma Distribution (T-NIG) that integrates a temporal parameter into the Normal Inverse Gamma distribution. This allows the model to capture how disease-related imaging features evolve over uneven time intervals. T-NIG uses brain images from two different time points to generate intermediate images and forecast future brain states, thereby aiding long-term AD prediction. The model employs coordinate neighborhoods for feature identification within the brain images, enhancing its understanding of spatial relationships. Moreover, T-NIG incorporates uncertainty estimation techniques to mitigate both epistemic (model-related) and aleatoric (data-related) uncertainties, which are common challenges due to insufficient and irregular temporal data. Experimental results demonstrate that the T-NIG model achieves state-of-the-art performance in both short-term and long-term prediction tasks on the evaluated dataset. Importantly, T-NIG maintains disease-related characteristics in generated images even when faced with irregular temporal distributions, highlighting its potential as a reliable tool for forecasting AD progression through imaging data. <div>
arXiv:2511.21057v1 Announce Type: new 
Abstract: Image generation can provide physicians with an imaging diagnosis basis in the prediction of Alzheimer's Disease (AD). Recent research has shown that long-term AD predictions by image generation often face difficulties maintaining disease-related characteristics when dealing with irregular time intervals in sequential data. Considering that the time-related aspects of the distribution can reflect changes in disease-related characteristics when images are distributed unevenly, this research proposes a model to estimate the temporal parameter within the Normal Inverse Gamma Distribution (T-NIG) to assist in generating images over the long term. The T-NIG model employs brain images from two different time points to create intermediate brain images, forecast future images, and predict the disease. T-NIG is designed by identifying features using coordinate neighborhoods. It incorporates a time parameter into the normal inverse gamma distribution to understand how features change in brain imaging sequences that have varying time intervals. Additionally, T-NIG utilizes uncertainty estimation to reduce both epistemic and aleatoric uncertainties in the model, which arise from insufficient temporal data. In particular, the T-NIG model demonstrates state-of-the-art performance in both short-term and long-term prediction tasks within the dataset. Experimental results indicate that T-NIG is proficient in forecasting disease progression while maintaining disease-related characteristics, even when faced with an irregular temporal data distribution.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIRA: Multimodal Iterative Reasoning Agent for Image Editing</title>
<link>https://arxiv.org/abs/2511.21087</link>
<guid>https://arxiv.org/abs/2511.21087</guid>
<content:encoded><![CDATA[
<div> Keywords: instruction-guided image editing, diffusion models, multimodal reasoning, iterative perception-reasoning-action, atomic edit instructions  

<br /><br />Summary:  
1. The paper addresses challenges in instruction-guided image editing where diffusion-based models struggle with complex instructions involving compositional relationships and contextual understanding.  
2. To overcome this, the authors propose MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play reasoning agent that performs editing through an iterative loop mimicking multi-turn human-model interactions.  
3. Unlike traditional single-prompt methods, MIRA breaks down editing tasks into atomic steps, using visual feedback at each iteration to guide decision-making and improve instruction interpretation.  
4. The authors introduce a new dataset, MIRA-Editing, containing 150K multimodal tool-use examples, and employ a two-stage training pipeline combining supervised fine-tuning (SFT) and gated reinforcement policy optimization (GRPO).  
5. When integrated with existing open-source editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA demonstrates significant improvements in both semantic accuracy and visual quality, achieving results on par with or surpassing proprietary editing systems like GPT-Image and Nano-Banana. <div>
arXiv:2511.21087v1 Announce Type: new 
Abstract: Instruction-guided image editing offers an intuitive way for users to edit images with natural language. However, diffusion-based editing models often struggle to accurately interpret complex user instructions, especially those involving compositional relationships, contextual cues, or referring expressions, leading to edits that drift semantically or fail to reflect the intended changes. We tackle this problem by proposing MIRA (Multimodal Iterative Reasoning Agent), a lightweight, plug-and-play multimodal reasoning agent that performs editing through an iterative perception-reasoning-action loop, effectively simulating multi-turn human-model interaction processes. Instead of issuing a single prompt or static plan, MIRA predicts atomic edit instructions step by step, using visual feedback to make its decisions. Our 150K multimodal tool-use dataset, MIRA-Editing, combined with a two-stage SFT + GRPO training pipeline, enables MIRA to perform reasoning and editing over complex editing instructions. When paired with open-source image editing models such as Flux.1-Kontext, Step1X-Edit, and Qwen-Image-Edit, MIRA significantly improves both semantic consistency and perceptual quality, achieving performance comparable to or exceeding proprietary systems such as GPT-Image and Nano-Banana.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLRecogEye : Curriculum Learning towards exploiting convolution features for Dynamic Iris Recognition</title>
<link>https://arxiv.org/abs/2511.21097</link>
<guid>https://arxiv.org/abs/2511.21097</guid>
<content:encoded><![CDATA[
<div> Keywords: Iris Authentication, 3D-CNN, Spatio-Spatial-Temporal Features, Curriculum Learning, Deep Metric Learning<br /><br />Summary:<br />1. This paper addresses the challenges faced by iris authentication systems, including variations in rotation, scale, specular reflections, and defocus blur, which affect their robustness in real-world applications such as border control and citizen identification.  
2. Unlike traditional methods that use simple point-to-point comparisons like cosine or L2 distances, this work proposes a novel matching pipeline that learns rich spatio-spatial-temporal representations from iris features.  
3. The method involves splitting each iris image along one dimension to create a sequence of sub-images, which are input to a 3D Convolutional Neural Network (3D-CNN) to capture complex spatial and temporal relationships within the iris patterns.  
4. To better model these spatio-spatial-temporal dynamics, the network is trained using curriculum learning, allowing it to progressively embed temporal dependencies into the feature space for improved discriminability.  
5. The framework is trained end-to-end using combined triplet loss and ArcFace loss, producing highly discriminative embeddings that maintain robustness against common image distortions, making the system generalizable for practical iris authentication scenarios. Additionally, the authors provide open-source code for reproducibility and further research. <div>
arXiv:2511.21097v1 Announce Type: new 
Abstract: Iris authentication algorithms have achieved impressive recognition performance, making them highly promising for real-world applications such as border control, citizen identification, and both criminal investigations and commercial systems. However, their robustness is still challenged by variations in rotation, scale, specular reflections, and defocus blur. In addition, most existing approaches rely on straightforward point-to-point comparisons, typically using cosine or L2 distance, without effectively leveraging the spatio-spatial-temporal structure of iris patterns. To address these limitations, we propose a novel and generalized matching pipeline that learns rich spatio-spatial-temporal representations of iris features. Our approach first splits each iris image along one dimension, generating a sequence of sub-images that serve as input to a 3D-CNN, enabling the network to capture both spatial and spatio-spatial-temporal cues. To further enhance the modeling of spatio-spatial-temporal feature dynamics, we train the model in curriculum manner. This design allows the network to embed temporal dependencies directly into the feature space, improving discriminability in the deep metric domain. The framework is trained end-to-end with triplet and ArcFace loss in a curriculum manner, enforcing highly discriminative embeddings despite challenges like rotation, scale, reflections, and blur. This design yields a robust and generalizable solution for iris authentication.Github code: https://github.com/GeetanjaliGTZ/CLRecogEye
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pygmalion Effect in Vision: Image-to-Clay Translation for Reflective Geometry Reconstruction</title>
<link>https://arxiv.org/abs/2511.21098</link>
<guid>https://arxiv.org/abs/2511.21098</guid>
<content:encoded><![CDATA[
<div> Reflection, 3D reconstruction, image-to-clay translation, BRDF, normal accuracy  

<br /><br />Summary:  
This paper addresses the challenge of accurately reconstructing 3D geometry of reflective objects, which is complicated by the mixture of appearance and geometry due to view-dependent reflections. The authors propose a novel framework called the Pygmalion Effect in Vision, inspired by the myth of Pygmalion, which performs image-to-clay translation to "sculpt" reflective objects into neutral, clay-like forms. This approach suppresses specular reflections while maintaining intrinsic geometric details, enabling more robust reconstructions from multi-view images with complex reflections. The core technical contribution is a dual-branch neural network: one branch models reflections using a BRDF-based approach, while the other branch is guided by the clay-like images to stabilize geometry and refine surface normals. Both branches are trained jointly with synthesized reflection-free clay images, which act as a neutral supervision signal complementary to reflective observations. Experimental results on synthetic and real datasets show significant improvements in surface normal accuracy and mesh completeness compared to current state-of-the-art methods that handle reflections. Beyond performance gains, the work demonstrates that converting radiance into a neutral, reflection-free domain can provide a powerful inductive bias for learning object geometry under reflective effects. <div>
arXiv:2511.21098v1 Announce Type: new 
Abstract: Understanding reflection remains a long-standing challenge in 3D reconstruction due to the entanglement of appearance and geometry under view-dependent reflections. In this work, we present the Pygmalion Effect in Vision, a novel framework that metaphorically "sculpts" reflective objects into clay-like forms through image-to-clay translation. Inspired by the myth of Pygmalion, our method learns to suppress specular cues while preserving intrinsic geometric consistency, enabling robust reconstruction from multi-view images containing complex reflections. Specifically, we introduce a dual-branch network in which a BRDF-based reflective branch is complemented by a clay-guided branch that stabilizes geometry and refines surface normals. The two branches are trained jointly using the synthesized clay-like images, which provide a neutral, reflection-free supervision signal that complements the reflective views. Experiments on both synthetic and real datasets demonstrate substantial improvement in normal accuracy and mesh completeness over existing reflection-handling methods. Beyond technical gains, our framework reveals that seeing by unshining, translating radiance into neutrality, can serve as a powerful inductive bias for reflective object geometry learning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Foundation Models for Radar Scene Understanding</title>
<link>https://arxiv.org/abs/2511.21105</link>
<guid>https://arxiv.org/abs/2511.21105</guid>
<content:encoded><![CDATA[
<div> Radar, Foundation Model, Spatial Language, Contrastive Learning, Localization Metrics<br /><br />Summary: This paper presents RadarFM, a novel radar foundation model designed to unify scene-level radar perception across various downstream tasks. The authors identify a key limitation in existing radar approaches, which are task-specific with separate architectures and training objectives, hindering knowledge transfer between tasks. To overcome this, RadarFM learns unified representations supervised by structured spatial language that encodes vehicle distributions directly in native radar coordinates. To enhance spatial reasoning, the paper introduces a hash-aware contrastive learning objective that models continuous scene similarity rather than binary matching, allowing for more nuanced understanding of spatial relationships. Utilizing the CARLA simulator, the authors generate extensive, annotated radar datasets representing diverse driving scenarios to train and evaluate the model. Additionally, new localization-aware evaluation metrics are proposed, offering a more precise assessment of spatial accuracy that goes beyond conventional detection performance measures. Collectively, these contributions enable RadarFM to provide more reliable and transferable radar perception capabilities in adverse weather, lighting, and long-range conditions, addressing significant challenges in multimodal sensor integration for autonomous driving and robotics applications. <div>
arXiv:2511.21105v1 Announce Type: new 
Abstract: Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EM-KD: Distilling Efficient Multimodal Large Language Model with Unbalanced Vision Tokens</title>
<link>https://arxiv.org/abs/2511.21106</link>
<guid>https://arxiv.org/abs/2511.21106</guid>
<content:encoded><![CDATA[
arXiv:2511.21106v1 Announce Type: new 
Abstract: Efficient Multimodal Large Language Models (MLLMs) compress vision tokens to reduce resource consumption, but the loss of visual information can degrade comprehension capabilities. Although some priors introduce Knowledge Distillation to enhance student models, they overlook the fundamental differences in fine-grained vision comprehension caused by unbalanced vision tokens between the efficient student and vanilla teacher. In this paper, we propose EM-KD, a novel paradigm that enhances the Efficient MLLMs with Knowledge Distillation. To overcome the challenge of unbalanced vision tokens, we first calculate the Manhattan distance between the vision logits of teacher and student, and then align them in the spatial dimension with the Hungarian matching algorithm. After alignment, EM-KD introduces two distillation strategies: 1) Vision-Language Affinity Distillation (VLAD) and 2) Vision Semantic Distillation (VSD). Specifically, VLAD calculates the affinity matrix between text tokens and aligned vision tokens, and minimizes the smooth L1 distance of the student and the teacher affinity matrices. Considering the semantic richness of vision logits in the final layer, VSD employs the reverse KL divergence to measure the discrete probability distributions of the aligned vision logits over the vocabulary space. Comprehensive evaluation on diverse benchmarks demonstrates that EM-KD trained model outperforms prior Efficient MLLMs on both accuracy and efficiency with a large margin, validating its effectiveness. Compared with previous distillation methods, which are equipped with our proposed vision token matching strategy for fair comparison, EM-KD also achieves better performance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaithFusion: Harmonizing Reconstruction and Generation via Pixel-wise Information Gain</title>
<link>https://arxiv.org/abs/2511.21113</link>
<guid>https://arxiv.org/abs/2511.21113</guid>
<content:encoded><![CDATA[
arXiv:2511.21113v1 Announce Type: new 
Abstract: In controllable driving-scene reconstruction and 3D scene generation, maintaining geometric fidelity while synthesizing visually plausible appearance under large viewpoint shifts is crucial. However, effective fusion of geometry-based 3DGS and appearance-driven diffusion models faces inherent challenges, as the absence of pixel-wise, 3D-consistent editing criteria often leads to over-restoration and geometric drift. To address these issues, we introduce \textbf{FaithFusion}, a 3DGS-diffusion fusion framework driven by pixel-wise Expected Information Gain (EIG). EIG acts as a unified policy for coherent spatio-temporal synthesis: it guides diffusion as a spatial prior to refine high-uncertainty regions, while its pixel-level weighting distills the edits back into 3DGS. The resulting plug-and-play system is free from extra prior conditions and structural modifications.Extensive experiments on the Waymo dataset demonstrate that our approach attains SOTA performance across NTA-IoU, NTL-IoU, and FID, maintaining an FID of 107.47 even at 6 meters lane shift. Our code is available at https://github.com/wangyuanbiubiubiu/FaithFusion.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deformation-aware Temporal Generation for Early Prediction of Alzheimers Disease</title>
<link>https://arxiv.org/abs/2511.21114</link>
<guid>https://arxiv.org/abs/2511.21114</guid>
<content:encoded><![CDATA[
arXiv:2511.21114v1 Announce Type: new 
Abstract: Alzheimer's disease (AD), a degenerative brain condition, can benefit from early prediction to slow its progression. As the disease progresses, patients typically undergo brain atrophy. Current prediction methods for Alzheimers disease largely involve analyzing morphological changes in brain images through manual feature extraction. This paper proposes a novel method, the Deformation-Aware Temporal Generative Network (DATGN), to automate the learning of morphological changes in brain images about disease progression for early prediction. Given the common occurrence of missing data in the temporal sequences of MRI images, DATGN initially interpolates incomplete sequences. Subsequently, a bidirectional temporal deformation-aware module guides the network in generating future MRI images that adhere to the disease's progression, facilitating early prediction of Alzheimer's disease. DATGN was tested for the generation of temporal sequences of future MRI images using the ADNI dataset, and the experimental results are competitive in terms of PSNR and MMSE image quality metrics. Furthermore, when DATGN-generated synthetic data was integrated into the SVM vs. CNN vs. 3DCNN-based classification methods, significant improvements were achieved from 6. 21\% to 16\% in AD vs. NC classification accuracy and from 7. 34\% to 21. 25\% in AD vs. MCI vs. NC classification accuracy. The qualitative visualization results indicate that DATGN produces MRI images consistent with the brain atrophy trend in Alzheimer's disease, enabling early disease prediction.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Which Layer Causes Distribution Deviation? Entropy-Guided Adaptive Pruning for Diffusion and Flow Models</title>
<link>https://arxiv.org/abs/2511.21122</link>
<guid>https://arxiv.org/abs/2511.21122</guid>
<content:encoded><![CDATA[
arXiv:2511.21122v1 Announce Type: new 
Abstract: Large-scale vision generative models, including diffusion and flow models, have demonstrated remarkable performance in visual generation tasks. However, transferring these pre-trained models to downstream tasks often results in significant parameter redundancy. In this paper, we propose EntPruner, an entropy-guided automatic progressive pruning framework for diffusion and flow models. First, we introduce entropy-guided pruning, a block-level importance assessment strategy specifically designed for generative models. Unlike discriminative models, generative models require preserving the diversity and condition-fidelity of the output distribution. As the importance of each module can vary significantly across downstream tasks, EntPruner prioritizes pruning of less important blocks using data-dependent Conditional Entropy Deviation (CED) as a guiding metric. CED quantifies how much the distribution diverges from the learned conditional data distribution after removing a block. Second, we propose a zero-shot adaptive pruning framework to automatically determine when and how much to prune during training. This dynamic strategy avoids the pitfalls of one-shot pruning, mitigating mode collapse, and preserving model performance. Extensive experiments on DiT and SiT models demonstrate the effectiveness of EntPruner, achieving up to 2.22$\times$ inference speedup while maintaining competitive generation quality on ImageNet and three downstream datasets.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CtrlVDiff: Controllable Video Generation via Unified Multimodal Video Diffusion</title>
<link>https://arxiv.org/abs/2511.21129</link>
<guid>https://arxiv.org/abs/2511.21129</guid>
<content:encoded><![CDATA[
arXiv:2511.21129v1 Announce Type: new 
Abstract: We tackle the dual challenges of video understanding and controllable video generation within a unified diffusion framework. Our key insights are two-fold: geometry-only cues (e.g., depth, edges) are insufficient: they specify layout but under-constrain appearance, materials, and illumination, limiting physically meaningful edits such as relighting or material swaps and often causing temporal drift. Enriching the model with additional graphics-based modalities (intrinsics and semantics) provides complementary constraints that both disambiguate understanding and enable precise, predictable control during generation.
  However, building a single model that uses many heterogeneous cues introduces two core difficulties. Architecturally, the model must accept any subset of modalities, remain robust to missing inputs, and inject control signals without sacrificing temporal consistency. Data-wise, training demands large-scale, temporally aligned supervision that ties real videos to per-pixel multimodal annotations.
  We then propose CtrlVDiff, a unified diffusion model trained with a Hybrid Modality Control Strategy (HMCS) that routes and fuses features from depth, normals, segmentation, edges, and graphics-based intrinsics (albedo, roughness, metallic), and re-renders videos from any chosen subset with strong temporal coherence. To enable this, we build MMVideo, a hybrid real-and-synthetic dataset aligned across modalities and captions. Across understanding and generation benchmarks, CtrlVDiff delivers superior controllability and fidelity, enabling layer-wise edits (relighting, material adjustment, object insertion) and surpassing state-of-the-art baselines while remaining robust when some modalities are unavailable.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepRFTv2: Kernel-level Learning for Image Deblurring</title>
<link>https://arxiv.org/abs/2511.21132</link>
<guid>https://arxiv.org/abs/2511.21132</guid>
<content:encoded><![CDATA[
arXiv:2511.21132v1 Announce Type: new 
Abstract: It is well-known that if a network aims to learn how to deblur, it should understand the blur process. Blurring is naturally caused by the convolution of the sharp image with the blur kernel. Thus, allowing the network to learn the blur process in the kernel-level can significantly improve the image deblurring performance. But, current deep networks are still at the pixel-level learning stage, either performing end-to-end pixel-level restoration or stage-wise pseudo kernel-level restoration, failing to enable the deblur model to understand the essence of the blur. To this end, we propose Fourier Kernel Estimator (FKE), which considers the activation operation in Fourier space and converts the convolution problem in the spatial domain to a multiplication problem in Fourier space. Our FKE, jointly optimized with the deblur model, enables the network to learn the kernel-level blur process with low complexity and without any additional supervision. Furthermore, we change the convolution object of the kernel from ``image" to network extracted ``feature", whose rich semantic and structural information is more suitable to blur process learning. With the convolution of the feature and the estimated kernel, our model can learn the essence of blur in kernel-level. To further improve the efficiency of feature extraction, we design a decoupled multi-scale architecture with multiple hierarchical sub-unets with a reversible strategy, which allows better multi-scale encoding and decoding in low training memory. Extensive experiments indicate that our method achieves state-of-the-art motion deblurring results and show potential for handling other kernel-related problems. Analysis also shows our kernel estimator is able to learn physically meaningful kernels. The code will be available at https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Training for Human Video Generation with Entropy-Guided Prioritized Progressive Learning</title>
<link>https://arxiv.org/abs/2511.21136</link>
<guid>https://arxiv.org/abs/2511.21136</guid>
<content:encoded><![CDATA[
arXiv:2511.21136v1 Announce Type: new 
Abstract: Human video generation has advanced rapidly with the development of diffusion models, but the high computational cost and substantial memory consumption associated with training these models on high-resolution, multi-frame data pose significant challenges. In this paper, we propose Entropy-Guided Prioritized Progressive Learning (Ent-Prog), an efficient training framework tailored for diffusion models on human video generation. First, we introduce Conditional Entropy Inflation (CEI) to assess the importance of different model components on the target conditional generation task, enabling prioritized training of the most critical components. Second, we introduce an adaptive progressive schedule that adaptively increases computational complexity during training by measuring the convergence efficiency. Ent-Prog reduces both training time and GPU memory consumption while maintaining model performance. Extensive experiments across three datasets, demonstrate the effectiveness of Ent-Prog, achieving up to 2.2$\times$ training speedup and 2.4$\times$ GPU memory reduction without compromising generative performance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Referring Video Object Segmentation with Cross-Modality Proxy Queries</title>
<link>https://arxiv.org/abs/2511.21139</link>
<guid>https://arxiv.org/abs/2511.21139</guid>
<content:encoded><![CDATA[
arXiv:2511.21139v1 Announce Type: new 
Abstract: Referring video object segmentation (RVOS) is an emerging cross-modality task that aims to generate pixel-level maps of the target objects referred by given textual expressions. The main concept involves learning an accurate alignment of visual elements and language expressions within a semantic space. Recent approaches address cross-modality alignment through conditional queries, tracking the target object using a query-response based mechanism built upon transformer structure. However, they exhibit two limitations: (1) these conditional queries lack inter-frame dependency and variation modeling, making accurate target tracking challenging amid significant frame-to-frame variations; and (2) they integrate textual constraints belatedly, which may cause the video features potentially focus on the non-referred objects. Therefore, we propose a novel RVOS architecture called ProxyFormer, which introduces a set of proxy queries to integrate visual and text semantics and facilitate the flow of semantics between them. By progressively updating and propagating proxy queries across multiple stages of video feature encoder, ProxyFormer ensures that the video features are focused on the object of interest. This dynamic evolution also enables the establishment of inter-frame dependencies, enhancing the accuracy and coherence of object tracking. To mitigate high computational costs, we decouple cross-modality interactions into temporal and spatial dimensions. Additionally, we design a Joint Semantic Consistency (JSC) training strategy to align semantic consensus between the proxy queries and the combined video-text pairs. Comprehensive experiments on four widely used RVOS benchmarks demonstrate the superiority of our ProxyFormer to the state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TEAR: Temporal-aware Automated Red-teaming for Text-to-Video Models</title>
<link>https://arxiv.org/abs/2511.21145</link>
<guid>https://arxiv.org/abs/2511.21145</guid>
<content:encoded><![CDATA[
arXiv:2511.21145v1 Announce Type: new 
Abstract: Text-to-Video (T2V) models are capable of synthesizing high-quality, temporally coherent dynamic video content, but the diverse generation also inherently introduces critical safety challenges. Existing safety evaluation methods,which focus on static image and text generation, are insufficient to capture the complex temporal dynamics in video generation. To address this, we propose a TEmporal-aware Automated Red-teaming framework, named TEAR, an automated framework designed to uncover safety risks specifically linked to the dynamic temporal sequencing of T2V models. TEAR employs a temporal-aware test generator optimized via a two-stage approach: initial generator training and temporal-aware online preference learning, to craft textually innocuous prompts that exploit temporal dynamics to elicit policy-violating video output. And a refine model is adopted to improve the prompt stealthiness and adversarial effectiveness cyclically. Extensive experimental evaluation demonstrates the effectiveness of TEAR across open-source and commercial T2V systems with over 80% attack success rate, a significant boost from prior best result of 57%.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLaVA-UHD v3: Progressive Visual Compression for Efficient Native-Resolution Encoding in MLLMs</title>
<link>https://arxiv.org/abs/2511.21150</link>
<guid>https://arxiv.org/abs/2511.21150</guid>
<content:encoded><![CDATA[
arXiv:2511.21150v1 Announce Type: new 
Abstract: Visual encoding followed by token condensing has become the standard architectural paradigm in multi-modal large language models (MLLMs). Many recent MLLMs increasingly favor global native- resolution visual encoding over slice-based methods. To investigate this trend, we systematically compare their behavior on vision-language understanding and attention patterns, revealing that global encoding enhances overall capability but at the expense of greater computational overhead. To address this issue, we present LLaVA-UHD v3, an MLLM centered upon our proposed Progressive Visual Compression (PVC) method, which can be seamlessly integrated into standard Vision Transformer (ViT) to enable efficient native-resolution encoding. The PVC approach consists of two key modules: (i) refined patch embedding, which supports flexible patch-size scaling for fine-grained visual model- ing, (ii) windowed token compression, hierarchically deployed across ViT layers to progressively aggregate local token representations. Jointly modulated by these two modules, a widely pretrained ViT can be reconfigured into an efficient architecture while largely preserving generality. Evaluated across extensive benchmarks, the transformed ViT, termed ViT-UHD, demonstrates competitive performance with MoonViT while reducing TTFT (time-to-first-token) by 2.4x, when developed within an identical MLLM architecture. Building upon ViT-UHD, LLaVA-UHD v3 also achieves competitive performance to Qwen2-VL, while further reducing TTFT by 1.9x. We will release all code and checkpoints to support future research on efficient MLLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Progress by Pieces: Test-Time Scaling for Autoregressive Image Generation</title>
<link>https://arxiv.org/abs/2511.21185</link>
<guid>https://arxiv.org/abs/2511.21185</guid>
<content:encoded><![CDATA[
arXiv:2511.21185v1 Announce Type: new 
Abstract: Recent visual autoregressive (AR) models have shown promising capabilities in text-to-image generation, operating in a manner similar to large language models. While test-time computation scaling has brought remarkable success in enabling reasoning-enhanced outputs for challenging natural language tasks, its adaptation to visual AR models remains unexplored and poses unique challenges. Naively applying test-time scaling strategies such as Best-of-N can be suboptimal: they consume full-length computation on erroneous generation trajectories, while the raster-scan decoding scheme lacks a blueprint of the entire canvas, limiting scaling benefits as only a few prompt-aligned candidates are generated. To address these, we introduce GridAR, a test-time scaling framework designed to elicit the best possible results from visual AR models. GridAR employs a grid-partitioned progressive generation scheme in which multiple partial candidates for the same position are generated within a canvas, infeasible ones are pruned early, and viable ones are fixed as anchors to guide subsequent decoding. Coupled with this, we present a layout-specified prompt reformulation strategy that inspects partial views to infer a feasible layout for satisfying the prompt. The reformulated prompt then guides subsequent image generation to mitigate the blueprint deficiency. Together, GridAR achieves higher-quality results under limited test-time scaling: with N=4, it even outperforms Best-of-N (N=8) by 14.4% on T2I-CompBench++ while reducing cost by 25.6%. It also generalizes to autoregressive image editing, showing comparable edit quality and a 13.9% gain in semantic preservation on PIE-Bench over larger-N baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AnchorOPT: Towards Optimizing Dynamic Anchors for Adaptive Prompt Learning</title>
<link>https://arxiv.org/abs/2511.21188</link>
<guid>https://arxiv.org/abs/2511.21188</guid>
<content:encoded><![CDATA[
arXiv:2511.21188v1 Announce Type: new 
Abstract: Existing prompt learning methods, which are built upon CLIP models, leverage textual tokens as anchors to guide the learnable soft tokens. This guidance improves CLIP generalizations. However, these anchors-static in both value and position-lack cross-task and stage-adaptive flexibility. To address this limitation, we propose AnchorOPT, a dynamic anchor-based prompt learning framework. Specifically, AnchorOPT introduces dynamism in two key dimensions: (i) anchor values eschew handcrafted explicit textual tokens (e.g., "shape", "color"), instead learning dynamically from task-specific data; and (ii) the positional relationship between anchor and soft tokens is no longer fixed but adaptively optimized via a learnable position matrix conditioned on the training stage and task context. Training occurs in two stages: we first learn the anchor tokens, then freeze and transfer them to the second stage for optimization of soft tokens and the position matrix. Extensive experiments demonstrate that using only a simple learnable anchor and position matrix achieves performance comparable to or exceeding some methods incorporating additional learnable modules or regularization techniques. As a plug-and-play module, AnchorOPT integrates seamlessly into existing frameworks, yielding consistent performance gains across diverse datasets. Code is publicly available at https://github.com/zhengli97/ATPrompt.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scenes as Tokens: Multi-Scale Normal Distributions Transform Tokenizer for General 3D Vision-Language Understanding</title>
<link>https://arxiv.org/abs/2511.21191</link>
<guid>https://arxiv.org/abs/2511.21191</guid>
<content:encoded><![CDATA[
arXiv:2511.21191v1 Announce Type: new 
Abstract: Recent advances in 3D vision-language models (VLMs) highlight a strong potential for 3D scene understanding and reasoning. However, effectively tokenizing 3D scenes into holistic scene tokens, and leveraging these tokens across diverse 3D understanding tasks, remain highly challenging. We present NDTokenizer3D, a generalist 3D VLM that performs a wide range of 3D scene understanding tasks while naturally supporting human interactions, thereby bridging language-level reasoning with 3D spatial understanding. The core of our approach is a novel three-stage scene tokenization pipeline built upon a Multi-Scale Normal Distributions Transform (NDT) representation, paired with a Multi-Scale NDT Decoder (MSDec). Specifically, NDTokenizer3D first constructs a multi-scale NDT representation from raw high-resolution point clouds, preserving both global context and fine-grained geometric details. Next, the MSDec progressively fuses cross-scale NDT features, producing holistic scene tokens consumable by LLM endpoints. Beyond tokenization, MSDec is repurposed as a general interface for human-interactive prompting (points, boxes, masks) and segmentation-mask decoding, unifying diverse 3D scene understanding tasks within a single architecture. With this compact and unified design, NDTokenizer3D offers a fine-grained, general-purpose 3D VLM, achieving remarkable improvements in 3D Referring Segmentation, 3D Visual Question Answering, and 3D Dense Captioning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.21192</link>
<guid>https://arxiv.org/abs/2511.21192</guid>
<content:encoded><![CDATA[
arXiv:2511.21192v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models are vulnerable to adversarial attacks, yet universal and transferable attacks remain underexplored, as most existing patches overfit to a single model and fail in black-box settings. To address this gap, we present a systematic study of universal, transferable adversarial patches against VLA-driven robots under unknown architectures, finetuned variants, and sim-to-real shifts. We introduce UPA-RFAS (Universal Patch Attack via Robust Feature, Attention, and Semantics), a unified framework that learns a single physical patch in a shared feature space while promoting cross-model transfer. UPA-RFAS combines (i) a feature-space objective with an $\ell_1$ deviation prior and repulsive InfoNCE loss to induce transferable representation shifts, (ii) a robustness-augmented two-phase min-max procedure where an inner loop learns invisible sample-wise perturbations and an outer loop optimizes the universal patch against this hardened neighborhood, and (iii) two VLA-specific losses: Patch Attention Dominance to hijack text$\to$vision attention and Patch Semantic Misalignment to induce image-text mismatch without labels. Experiments across diverse VLA models, manipulation suites, and physical executions show that UPA-RFAS consistently transfers across models, tasks, and viewpoints, exposing a practical patch-based attack surface and establishing a strong baseline for future defenses.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>You Can Trust Your Clustering Model: A Parameter-free Self-Boosting Plug-in for Deep Clustering</title>
<link>https://arxiv.org/abs/2511.21193</link>
<guid>https://arxiv.org/abs/2511.21193</guid>
<content:encoded><![CDATA[
arXiv:2511.21193v1 Announce Type: new 
Abstract: Recent deep clustering models have produced impressive clustering performance. However, a common issue with existing methods is the disparity between global and local feature structures. While local structures typically show strong consistency and compactness within class samples, global features often present intertwined boundaries and poorly separated clusters. Motivated by this observation, we propose DCBoost, a parameter-free plug-in designed to enhance the global feature structures of current deep clustering models. By harnessing reliable local structural cues, our method aims to elevate clustering performance effectively. Specifically, we first identify high-confidence samples through adaptive $k$-nearest neighbors-based consistency filtering, aiming to select a sufficient number of samples with high label reliability to serve as trustworthy anchors for self-supervision. Subsequently, these samples are utilized to compute a discriminative loss, which promotes both intra-class compactness and inter-class separability, to guide network optimization. Extensive experiments across various benchmark datasets showcase that our DCBoost significantly improves the clustering performance of diverse existing deep clustering models. Notably, our method improves the performance of current state-of-the-art baselines (e.g., ProPos) by more than 3% and amplifies the silhouette coefficient by over $7\times$. Code is available at .
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BotaCLIP: Contrastive Learning for Botany-Aware Representation of Earth Observation Data</title>
<link>https://arxiv.org/abs/2511.21194</link>
<guid>https://arxiv.org/abs/2511.21194</guid>
<content:encoded><![CDATA[
arXiv:2511.21194v1 Announce Type: new 
Abstract: Foundation models have demonstrated a remarkable ability to learn rich, transferable representations across diverse modalities such as images, text, and audio. In modern machine learning pipelines, these representations often replace raw data as the primary input for downstream tasks. In this paper, we address the challenge of adapting a pre-trained foundation model to inject domain-specific knowledge, without retraining from scratch or incurring significant computational costs. To this end, we introduce BotaCLIP, a lightweight multimodal contrastive framework that adapts a pre-trained Earth Observation foundation model (DOFA) by aligning high-resolution aerial imagery with botanical relev\'es. Unlike generic embeddings, BotaCLIP internalizes ecological structure through contrastive learning with a regularization strategy that mitigates catastrophic forgetting. Once trained, the resulting embeddings serve as transferable representations for downstream predictors. Motivated by real-world applications in biodiversity modeling, we evaluated BotaCLIP representations in three ecological tasks: plant presence prediction, butterfly occurrence modeling, and soil trophic group abundance estimation. The results showed consistent improvements over those derived from DOFA and supervised baselines. More broadly, this work illustrates how domain-aware adaptation of foundation models can inject expert knowledge into data-scarce settings, enabling frugal representation learning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition</title>
<link>https://arxiv.org/abs/2511.21202</link>
<guid>https://arxiv.org/abs/2511.21202</guid>
<content:encoded><![CDATA[
arXiv:2511.21202v1 Announce Type: new 
Abstract: Fine-grained action recognition (FGAR) aims to identify subtle and distinctive differences among fine-grained action categories. However, current recognition methods often capture coarse-grained motion patterns but struggle to identify subtle details in local regions evolving over time. In this work, we introduce the Action-Region Tracking (ART) framework, a novel solution leveraging a query-response mechanism to discover and track the dynamics of distinctive local details, enabling effective distinction of similar actions. Specifically, we propose a region-specific semantic activation module that employs discriminative and text-constrained semantics as queries to capture the most action-related region responses in each video frame, facilitating interaction among spatial and temporal dimensions with corresponding video features. The captured region responses are organized into action tracklets, which characterize region-based action dynamics by linking related responses across video frames in a coherent sequence. The text-constrained queries encode nuanced semantic representations derived from textual descriptions of action labels extracted by language branches within Visual Language Models (VLMs). To optimize the action tracklets, we design a multi-level tracklet contrastive constraint among region responses at spatial and temporal levels, enabling effective discrimination within each frame and correlation between adjacent frames. Additionally, a task-specific fine-tuning mechanism refines textual semantics such that semantic representations encoded by VLMs are preserved while optimized for task preferences. Comprehensive experiments on widely used action recognition benchmarks demonstrate the superiority to previous state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Diffusion to One-Step Generation: A Comparative Study of Flow-Based Models with Application to Image Inpainting</title>
<link>https://arxiv.org/abs/2511.21215</link>
<guid>https://arxiv.org/abs/2511.21215</guid>
<content:encoded><![CDATA[
arXiv:2511.21215v1 Announce Type: new 
Abstract: We present a comprehensive comparative study of three generative modeling paradigms: Denoising Diffusion Probabilistic Models (DDPM), Conditional Flow Matching (CFM), and MeanFlow. While DDPM and CFM require iterative sampling, MeanFlow enables direct one-step generation by modeling the average velocity over time intervals. We implement all three methods using a unified TinyUNet architecture (<1.5M parameters) on CIFAR-10, demonstrating that CFM achieves an FID of 24.15 with 50 steps, significantly outperforming DDPM (FID 402.98). MeanFlow achieves FID 29.15 with single-step sampling -- a 50X reduction in inference time. We further extend CFM to image inpainting, implementing mask-guided sampling with four mask types (center, random bbox, irregular, half). Our fine-tuned inpainting model achieves substantial improvements: PSNR increases from 4.95 to 8.57 dB on center masks (+73%), and SSIM improves from 0.289 to 0.418 (+45%), demonstrating the effectiveness of inpainting-aware training.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3-Tracer: A Tri-level Temporal-Aware Framework for Audio Forgery Detection and Localization</title>
<link>https://arxiv.org/abs/2511.21237</link>
<guid>https://arxiv.org/abs/2511.21237</guid>
<content:encoded><![CDATA[
arXiv:2511.21237v1 Announce Type: new 
Abstract: Recently, partial audio forgery has emerged as a new form of audio manipulation. Attackers selectively modify partial but semantically critical frames while preserving the overall perceptual authenticity, making such forgeries particularly difficult to detect. Existing methods focus on independently detecting whether a single frame is forged, lacking the hierarchical structure to capture both transient and sustained anomalies across different temporal levels. To address these limitations, We identify three key levels relevant to partial audio forgery detection and present T3-Tracer, the first framework that jointly analyzes audio at the frame, segment, and audio levels to comprehensively detect forgery traces. T3-Tracer consists of two complementary core modules: the Frame-Audio Feature Aggregation Module (FA-FAM) and the Segment-level Multi-Scale Discrepancy-Aware Module (SMDAM). FA-FAM is designed to detect the authenticity of each audio frame. It combines both frame-level and audio-level temporal information to detect intra-frame forgery cues and global semantic inconsistencies. To further refine and correct frame detection, we introduce SMDAM to detect forgery boundaries at the segment level. It adopts a dual-branch architecture that jointly models frame features and inter-frame differences across multi-scale temporal windows, effectively identifying abrupt anomalies that appeared on the forged boundaries. Extensive experiments conducted on three challenging datasets demonstrate that our approach achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIELDS: Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision</title>
<link>https://arxiv.org/abs/2511.21245</link>
<guid>https://arxiv.org/abs/2511.21245</guid>
<content:encoded><![CDATA[
arXiv:2511.21245v1 Announce Type: new 
Abstract: Facial expressions convey the bulk of emotional information in human communication, yet existing 3D face reconstruction methods often miss subtle affective details due to reliance on 2D supervision and lack of 3D ground truth. We propose FIELDS (Face reconstruction with accurate Inference of Expression using Learning with Direct Supervision) to address these limitations by extending self-supervised 2D image consistency cues with direct 3D expression parameter supervision and an auxiliary emotion recognition branch. Our encoder is guided by authentic expression parameters from spontaneous 4D facial scans, while an intensity-aware emotion loss encourages the 3D expression parameters to capture genuine emotion content without exaggeration. This dual-supervision strategy bridges the 2D/3D domain gap and mitigates expression-intensity bias, yielding high-fidelity 3D reconstructions that preserve subtle emotional cues. From a single image, FIELDS produces emotion-rich face models with highly realistic expressions, significantly improving in-the-wild facial expression recognition performance without sacrificing naturalness.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shift-Equivariant Complex-Valued Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2511.21250</link>
<guid>https://arxiv.org/abs/2511.21250</guid>
<content:encoded><![CDATA[
arXiv:2511.21250v1 Announce Type: new 
Abstract: Convolutional neural networks have shown remarkable performance in recent years on various computer vision problems. However, the traditional convolutional neural network architecture lacks a critical property: shift equivariance and invariance, broken by downsampling and upsampling operations. Although data augmentation techniques can help the model learn the latter property empirically, a consistent and systematic way to achieve this goal is by designing downsampling and upsampling layers that theoretically guarantee these properties by construction. Adaptive Polyphase Sampling (APS) introduced the cornerstone for shift invariance, later extended to shift equivariance with Learnable Polyphase up/downsampling (LPS) applied to real-valued neural networks. In this paper, we extend the work on LPS to complex-valued neural networks both from a theoretical perspective and with a novel building block of a projection layer from $\mathbb{C}$ to $\mathbb{R}$ before the Gumbel Softmax. We finally evaluate this extension on several computer vision problems, specifically for either the invariance property in classification tasks or the equivariance property in both reconstruction and semantic segmentation problems, using polarimetric Synthetic Aperture Radar images.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs</title>
<link>https://arxiv.org/abs/2511.21251</link>
<guid>https://arxiv.org/abs/2511.21251</guid>
<content:encoded><![CDATA[
arXiv:2511.21251v1 Announce Type: new 
Abstract: The threat of Audio-Video (AV) forgery is rapidly evolving beyond human-centric deepfakes to include more diverse manipulations across complex natural scenes. However, existing benchmarks are still confined to DeepFake-based forgeries and single-granularity annotations, thus failing to capture the diversity and complexity of real-world forgery scenarios. To address this, we introduce AVFakeBench, the first comprehensive audio-video forgery detection benchmark that spans rich forgery semantics across both human subject and general subject. AVFakeBench comprises 12K carefully curated audio-video questions, covering seven forgery types and four levels of annotations. To ensure high-quality and diverse forgeries, we propose a multi-stage hybrid forgery framework that integrates proprietary models for task planning with expert generative models for precise manipulation. The benchmark establishes a multi-task evaluation framework covering binary judgment, forgery types classification, forgery detail selection, and explanatory reasoning. We evaluate 11 Audio-Video Large Language Models (AV-LMMs) and 2 prevalent detection methods on AVFakeBench, demonstrating the potential of AV-LMMs as emerging forgery detectors while revealing their notable weaknesses in fine-grained perception and reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LaGen: Towards Autoregressive LiDAR Scene Generation</title>
<link>https://arxiv.org/abs/2511.21256</link>
<guid>https://arxiv.org/abs/2511.21256</guid>
<content:encoded><![CDATA[
arXiv:2511.21256v1 Announce Type: new 
Abstract: Generative world models for autonomous driving (AD) have become a trending topic. Unlike the widely studied image modality, in this work we explore generative world models for LiDAR data. Existing generation methods for LiDAR data only support single frame generation, while existing prediction approaches require multiple frames of historical input and can only deterministically predict multiple frames at once, lacking interactivity. Both paradigms fail to support long-horizon interactive generation. To this end, we introduce LaGen, which to the best of our knowledge is the first framework capable of frame-by-frame autoregressive generation of long-horizon LiDAR scenes. LaGen is able to take a single-frame LiDAR input as a starting point and effectively utilize bounding box information as conditions to generate high-fidelity 4D scene point clouds. In addition, we introduce a scene decoupling estimation module to enhance the model's interactive generation capability for object-level content, as well as a noise modulation module to mitigate error accumulation during long-horizon generation. We construct a protocol based on nuScenes for evaluating long-horizon LiDAR scene generation. Experimental results comprehensively demonstrate LaGen outperforms state-of-the-art LiDAR generation and prediction models, especially on the later frames.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Zero-shot Potential of Semi-dense Image Matching via Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.21265</link>
<guid>https://arxiv.org/abs/2511.21265</guid>
<content:encoded><![CDATA[
arXiv:2511.21265v1 Announce Type: new 
Abstract: Learning-based image matching critically depends on large-scale, diverse, and geometrically accurate training data. 3D Gaussian Splatting (3DGS) enables photorealistic novel-view synthesis and thus is attractive for data generation. However, its geometric inaccuracies and biased depth rendering currently prevent robust correspondence labeling. To address this, we introduce MatchGS, the first framework designed to systematically correct and leverage 3DGS for robust, zero-shot image matching. Our approach is twofold: (1) a geometrically-faithful data generation pipeline that refines 3DGS geometry to produce highly precise correspondence labels, enabling the synthesis of a vast and diverse range of viewpoints without compromising rendering fidelity; and (2) a 2D-3D representation alignment strategy that infuses 3DGS' explicit 3D knowledge into the 2D matcher, guiding 2D semi-dense matchers to learn viewpoint-invariant 3D representations. Our generated ground-truth correspondences reduce the epipolar error by up to 40 times compared to existing datasets, enable supervision under extreme viewpoint changes, and provide self-supervisory signals through Gaussian attributes. Consequently, state-of-the-art matchers trained solely on our data achieve significant zero-shot performance gains on public benchmarks, with improvements of up to 17.7%. Our work demonstrates that with proper geometric refinement, 3DGS can serve as a scalable, high-fidelity, and structurally-rich data source, paving the way for a new generation of robust zero-shot image matchers.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Co-Training Vision Language Models for Remote Sensing Multi-task Learning</title>
<link>https://arxiv.org/abs/2511.21272</link>
<guid>https://arxiv.org/abs/2511.21272</guid>
<content:encoded><![CDATA[
arXiv:2511.21272v1 Announce Type: new 
Abstract: With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model's object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathMamba: A Hybrid Mamba-Transformer for Topologically Coherent Road Segmentation in Satellite Imagery</title>
<link>https://arxiv.org/abs/2511.21298</link>
<guid>https://arxiv.org/abs/2511.21298</guid>
<content:encoded><![CDATA[
arXiv:2511.21298v1 Announce Type: new 
Abstract: Achieving both high accuracy and topological continuity in road segmentation from satellite imagery is a critical goal for applications ranging from urban planning to disaster response. State-of-the-art methods often rely on Vision Transformers, which excel at capturing global context, yet their quadratic complexity is a significant barrier to efficient deployment, particularly for on-board processing in resource-constrained platforms. In contrast, emerging State Space Models like Mamba offer linear-time efficiency and are inherently suited to modeling long, continuous structures. We posit that these architectures have complementary strengths. To this end, we introduce PathMamba, a novel hybrid architecture that integrates Mamba's sequential modeling with the Transformer's global reasoning. Our design strategically uses Mamba blocks to trace the continuous nature of road networks, preserving topological structure, while integrating Transformer blocks to refine features with global context. This approach yields topologically superior segmentation maps without the prohibitive scaling costs of pure attention-based models. Our experiments on the DeepGlobe Road Extraction and Massachusetts Roads datasets demonstrate that PathMamba sets a new state-of-the-art. Notably, it significantly improves topological continuity, as measured by the APLS metric, setting a new benchmark while remaining computationally competitive.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaliTex: Geometry-Calibrated Attention for View-Coherent 3D Texture Generation</title>
<link>https://arxiv.org/abs/2511.21309</link>
<guid>https://arxiv.org/abs/2511.21309</guid>
<content:encoded><![CDATA[
arXiv:2511.21309v1 Announce Type: new 
Abstract: Despite major advances brought by diffusion-based models, current 3D texture generation systems remain hindered by cross-view inconsistency -- textures that appear convincing from one viewpoint often fail to align across others. We find that this issue arises from attention ambiguity, where unstructured full attention is applied indiscriminately across tokens and modalities, causing geometric confusion and unstable appearance-structure coupling. To address this, we introduce CaliTex, a framework of geometry-calibrated attention that explicitly aligns attention with 3D structure. It introduces two modules: Part-Aligned Attention that enforces spatial alignment across semantically matched parts, and Condition-Routed Attention which routes appearance information through geometry-conditioned pathways to maintain spatial fidelity. Coupled with a two-stage diffusion transformer, CaliTex makes geometric coherence an inherent behavior of the network rather than a byproduct of optimization. Empirically, CaliTex produces seamless and view-consistent textures and outperforms both open-source and commercial baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTTM: Head-wise Temporal Token Merging for Faster VGGT</title>
<link>https://arxiv.org/abs/2511.21317</link>
<guid>https://arxiv.org/abs/2511.21317</guid>
<content:encoded><![CDATA[
arXiv:2511.21317v1 Announce Type: new 
Abstract: The Visual Geometry Grounded Transformer (VGGT) marks a significant leap forward in 3D scene reconstruction, as it is the first model that directly infers all key 3D attributes (camera poses, depths, and dense geometry) jointly in one pass. However, this joint inference mechanism requires global attention layers that perform all-to-all attention computation on tokens from all views. For reconstruction of large scenes with long-sequence inputs, this causes a significant latency bottleneck. In this paper, we propose head-wise temporal merging (HTTM), a training-free 3D token merging method for accelerating VGGT. Existing merging techniques merge tokens uniformly across different attention heads, resulting in identical tokens in the layers' output, which hinders the model's representational ability. HTTM tackles this problem by merging tokens in multi-head granularity, which preserves the uniqueness of feature tokens after head concatenation. Additionally, this enables HTTM to leverage the spatial locality and temporal correspondence observed at the head level to achieve higher merging ratios with lower merging costs compared to existing methods. Thus, HTTM achieves up to 7x acceleration with negligible performance drops in a GPU-based inference.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The More, the Merrier: Contrastive Fusion for Higher-Order Multimodal Alignment</title>
<link>https://arxiv.org/abs/2511.21331</link>
<guid>https://arxiv.org/abs/2511.21331</guid>
<content:encoded><![CDATA[
arXiv:2511.21331v1 Announce Type: new 
Abstract: Learning joint representations across multiple modalities remains a central challenge in multimodal machine learning. Prevailing approaches predominantly operate in pairwise settings, aligning two modalities at a time. While some recent methods aim to capture higher-order interactions among multiple modalities, they often overlook or insufficiently preserve pairwise relationships, limiting their effectiveness on single-modality tasks. In this work, we introduce Contrastive Fusion (ConFu), a framework that jointly embeds both individual modalities and their fused combinations into a unified representation space, where modalities and their fused counterparts are aligned. ConFu extends traditional pairwise contrastive objectives with an additional fused-modality contrastive term, encouraging the joint embedding of modality pairs with a third modality. This formulation enables ConFu to capture higher-order dependencies, such as XOR-like relationships, that cannot be recovered through pairwise alignment alone, while still maintaining strong pairwise correspondence. We evaluate ConFu on synthetic and real-world multimodal benchmarks, assessing its ability to exploit cross-modal complementarity, capture higher-order dependencies, and scale with increasing multimodal complexity. Across these settings, ConFu demonstrates competitive performance on retrieval and classification tasks, while supporting unified one-to-one and two-to-one retrieval within a single contrastive framework.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid SIFT-SNN for Efficient Anomaly Detection of Traffic Flow-Control Infrastructure</title>
<link>https://arxiv.org/abs/2511.21337</link>
<guid>https://arxiv.org/abs/2511.21337</guid>
<content:encoded><![CDATA[
arXiv:2511.21337v1 Announce Type: new 
Abstract: This paper presents the SIFT-SNN framework, a low-latency neuromorphic signal-processing pipeline for real-time detection of structural anomalies in transport infrastructure. The proposed approach integrates Scale-Invariant Feature Transform (SIFT) for spatial feature encoding with a latency-driven spike conversion layer and a Leaky Integrate-and-Fire (LIF) Spiking Neural Network (SNN) for classification. The Auckland Harbour Bridge dataset is recorded under various weather and lighting conditions, comprising 6,000 labelled frames that include both real and synthetically augmented unsafe cases. The presented system achieves a classification accuracy of 92.3% (+- 0.8%) with a per-frame inference time of 9.5 ms. Achieved sub-10 millisecond latency, combined with sparse spike activity (8.1%), enables real-time, low-power edge deployment. Unlike conventional CNN-based approaches, the hybrid SIFT-SNN pipeline explicitly preserves spatial feature grounding, enhances interpretability, supports transparent decision-making, and operates efficiently on embedded hardware. Although synthetic augmentation improved robustness, generalisation to unseen field conditions remains to be validated. The SIFT-SNN framework is validated through a working prototype deployed on a consumer-grade system and framed as a generalisable case study in structural safety monitoring for movable concrete barriers, which, as a traffic flow-control infrastructure, is deployed in over 20 cities worldwide.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding</title>
<link>https://arxiv.org/abs/2511.21339</link>
<guid>https://arxiv.org/abs/2511.21339</guid>
<content:encoded><![CDATA[
arXiv:2511.21339v1 Announce Type: new 
Abstract: Recent advances in multimodal large language models (LLMs) have highlighted their potential for medical and surgical applications. However, existing surgical datasets predominantly adopt a Visual Question Answering (VQA) format with heterogeneous taxonomies and lack support for pixel-level segmentation, limiting consistent evaluation and applicability. We present SurgMLLMBench, a unified multimodal benchmark explicitly designed for developing and evaluating interactive multimodal LLMs for surgical scene understanding, including the newly collected Micro-surgical Artificial Vascular anastomosIS (MAVIS) dataset. It integrates pixel-level instrument segmentation masks and structured VQA annotations across laparoscopic, robot-assisted, and micro-surgical domains under a unified taxonomy, enabling comprehensive evaluation beyond traditional VQA tasks and richer visual-conversational interactions. Extensive baseline experiments show that a single model trained on SurgMLLMBench achieves consistent performance across domains and generalizes effectively to unseen datasets. SurgMLLMBench will be publicly released as a robust resource to advance multimodal surgical AI research, supporting reproducible evaluation and development of interactive surgical reasoning models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PFF-Net: Patch Feature Fitting for Point Cloud Normal Estimation</title>
<link>https://arxiv.org/abs/2511.21365</link>
<guid>https://arxiv.org/abs/2511.21365</guid>
<content:encoded><![CDATA[
arXiv:2511.21365v1 Announce Type: new 
Abstract: Estimating the normal of a point requires constructing a local patch to provide center-surrounding context, but determining the appropriate neighborhood size is difficult when dealing with different data or geometries. Existing methods commonly employ various parameter-heavy strategies to extract a full feature description from the input patch. However, they still have difficulties in accurately and efficiently predicting normals for various point clouds. In this work, we present a new idea of feature extraction for robust normal estimation of point clouds. We use the fusion of multi-scale features from different neighborhood sizes to address the issue of selecting reasonable patch sizes for various data or geometries. We seek to model a patch feature fitting (PFF) based on multi-scale features to approximate the optimal geometric description for normal estimation and implement the approximation process via multi-scale feature aggregation and cross-scale feature compensation. The feature aggregation module progressively aggregates the patch features of different scales to the center of the patch and shrinks the patch size by removing points far from the center. It not only enables the network to precisely capture the structure characteristic in a wide range, but also describes highly detailed geometries. The feature compensation module ensures the reusability of features from earlier layers of large scales and reveals associated information in different patch sizes. Our approximation strategy based on aggregating the features of multiple scales enables the model to achieve scale adaptation of varying local patches and deliver the optimal feature description. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both synthetic and real-world datasets with fewer network parameters and running time.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endo-G$^{2}$T: Geometry-Guided &amp; Temporally Aware Time-Embedded 4DGS For Endoscopic Scenes</title>
<link>https://arxiv.org/abs/2511.21367</link>
<guid>https://arxiv.org/abs/2511.21367</guid>
<content:encoded><![CDATA[
arXiv:2511.21367v1 Announce Type: new 
Abstract: Endoscopic (endo) video exhibits strong view-dependent effects such as specularities, wet reflections, and occlusions. Pure photometric supervision misaligns with geometry and triggers early geometric drift, where erroneous shapes are reinforced during densification and become hard to correct. We ask how to anchor geometry early for 4D Gaussian splatting (4DGS) while maintaining temporal consistency and efficiency in dynamic endoscopic scenes. Thus, we present Endo-G$^{2}$T, a geometry-guided and temporally aware training scheme for time-embedded 4DGS. First, geo-guided prior distillation converts confidence-gated monocular depth into supervision with scale-invariant depth and depth-gradient losses, using a warm-up-to-cap schedule to inject priors softly and avoid early overfitting. Second, a time-embedded Gaussian field represents dynamics in XYZT with a rotor-like rotation parameterization, yielding temporally coherent geometry with lightweight regularization that favors smooth motion and crisp opacity boundaries. Third, keyframe-constrained streaming improves efficiency and long-horizon stability through keyframe-focused optimization under a max-points budget, while non-keyframes advance with lightweight updates. Across EndoNeRF and StereoMIS-P1 datasets, Endo-G$^{2}$T achieves state-of-the-art results among monocular reconstruction baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.21375</link>
<guid>https://arxiv.org/abs/2511.21375</guid>
<content:encoded><![CDATA[
arXiv:2511.21375v1 Announce Type: new 
Abstract: Spatio-temporal video grounding (STVG) requires localizing a target object in untrimmed videos both temporally and spatially from natural language descriptions. Despite their strong language understanding, multimodal large language models (MLLMs) underperform on STVG due to misaligned training objectives and weak fine-grained region-word alignment in standard visual encoders. To address this, we propose STVG-o1, the first framework that enables off-the-shelf MLLMs to achieve state-of-the-art STVG performance without any architectural modifications. Our method introduces a bounding-box chain-of-thought mechanism that explicitly reasons about spatio-temporal locations in an intermediate step before producing the final prediction. We further design a multi-dimensional reinforcement reward function consisting of format, consistency, temporal, spatial, and think rewards, which provides geometry-aware supervision through reinforcement fine-tuning. Evaluated on HCSTVG-v1/v2 and VidSTG, STVG-o1 sets new state-of-the-art results on HCSTVG, outperforming the best task-specific method by 7.3\% m\_tIoU on HCSTVG-v1, matching specialized models on VidSTG, and surpassing all existing MLLM-based approaches by large margins. It also demonstrates strong open-vocabulary generalization across datasets, establishing MLLMs as viable and powerful backbones for precise spatio-temporal grounding. Our code and models will be released.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monet: Reasoning in Latent Visual Space Beyond Images and Language</title>
<link>https://arxiv.org/abs/2511.21395</link>
<guid>https://arxiv.org/abs/2511.21395</guid>
<content:encoded><![CDATA[
arXiv:2511.21395v1 Announce Type: new 
Abstract: "Thinking with images" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis</title>
<link>https://arxiv.org/abs/2511.21397</link>
<guid>https://arxiv.org/abs/2511.21397</guid>
<content:encoded><![CDATA[
arXiv:2511.21397v1 Announce Type: new 
Abstract: How does irrelevant information (i.e., distractors) affect test-time scaling in vision-language models (VLMs)? Prior studies on language models have reported an inverse scaling effect, where textual distractors lead to longer but less effective reasoning. To investigate whether similar phenomena occur in multimodal settings, we introduce Idis (Images with distractors), a visual question-answering dataset that systematically varies distractors along semantic, numerical, and spatial dimensions. Our analyses reveal that visual distractors differ fundamentally from textual ones: although inverse scaling persists, adding visual distractors reduces accuracy without increasing reasoning length. We further show that tracking attribute counts within reasoning traces provides key insights into how distractors, reasoning length, and accuracy interact. Finally, we demonstrate that these trends extend to established visual bias benchmarks such as Waterbirds, and we propose a simple prompting strategy to mitigate bias-driven predictions in reasoning models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiverseVAR: Balancing Diversity and Quality of Next-Scale Visual Autoregressive Models</title>
<link>https://arxiv.org/abs/2511.21415</link>
<guid>https://arxiv.org/abs/2511.21415</guid>
<content:encoded><![CDATA[
arXiv:2511.21415v1 Announce Type: new 
Abstract: We introduce DiverseVAR, a framework that enhances the diversity of text-conditioned visual autoregressive models (VAR) at test time without requiring retraining, fine-tuning, or substantial computational overhead. While VAR models have recently emerged as strong competitors to diffusion and flow models for image generation, they suffer from a critical limitation in diversity, often producing nearly identical images even for simple prompts. This issue has largely gone unnoticed amid the predominant focus on image quality. We address this limitation at test time in two stages. First, inspired by diversity enhancement techniques in diffusion models, we propose injecting noise into the text embedding. This introduces a trade-off between diversity and image quality: as diversity increases, the image quality sharply declines. To preserve quality, we propose scale-travel: a novel latent refinement technique inspired by time-travel strategies in diffusion models. Specifically, we use a multi-scale autoencoder to extract coarse-scale tokens that enable us to resume generation at intermediate stages. Extensive experiments show that combining text-embedding noise injection with our scale-travel refinement significantly enhances diversity while minimizing image-quality degradation, achieving a new Pareto frontier in the diversity-quality trade-off.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning</title>
<link>https://arxiv.org/abs/2511.21420</link>
<guid>https://arxiv.org/abs/2511.21420</guid>
<content:encoded><![CDATA[
arXiv:2511.21420v1 Announce Type: new 
Abstract: Remote sensing change captioning is an emerging and popular research task that aims to describe, in natural language, the content of interest that has changed between two remote sensing images captured at different times. Existing methods typically employ CNNs/Transformers to extract visual representations from the given images or incorporate auxiliary tasks to enhance the final results, with weak region awareness and limited temporal alignment. To address these issues, this paper explores the use of the SAM (Segment Anything Model) foundation model to extract region-level representations and inject region-of-interest knowledge into the captioning framework. Specifically, we employ a CNN/Transformer model to extract global-level vision features, leverage the SAM foundation model to delineate semantic- and motion-level change regions, and utilize a specially constructed knowledge graph to provide information about objects of interest. These heterogeneous sources of information are then fused via cross-attention, and a Transformer decoder is used to generate the final natural language description of the observed changes. Extensive experimental results demonstrate that our method achieves state-of-the-art performance across multiple widely used benchmark datasets. The source code of this paper will be released on https://github.com/Event-AHU/SAM_ChangeCaptioning
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-M3RF: An Equivariant Multimodal 3D Re-assembly Framework</title>
<link>https://arxiv.org/abs/2511.21422</link>
<guid>https://arxiv.org/abs/2511.21422</guid>
<content:encoded><![CDATA[
arXiv:2511.21422v1 Announce Type: new 
Abstract: 3D reassembly is a fundamental geometric problem, and in recent years it has increasingly been challenged by deep learning methods rather than classical optimization. While learning approaches have shown promising results, most still rely primarily on geometric features to assemble a whole from its parts. As a result, methods struggle when geometry alone is insufficient or ambiguous, for example, for small, eroded, or symmetric fragments. Additionally, solutions do not impose physical constraints that explicitly prevent overlapping assemblies. To address these limitations, we introduce E-M3RF, an equivariant multimodal 3D reassembly framework that takes as input the point clouds, containing both point positions and colors of fractured fragments, and predicts the transformations required to reassemble them using SE(3) flow matching. Each fragment is represented by both geometric and color features: i) 3D point positions are encoded as rotationconsistent geometric features using a rotation-equivariant encoder, ii) the colors at each 3D point are encoded with a transformer. The two feature sets are then combined to form a multimodal representation. We experimented on four datasets: two synthetic datasets, Breaking Bad and Fantastic Breaks, and two real-world cultural heritage datasets, RePAIR and Presious, demonstrating that E-M3RF on the RePAIR dataset reduces rotation error by 23.1% and translation error by 13.2%, while Chamfer Distance decreases by 18.4% compared to competing methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings</title>
<link>https://arxiv.org/abs/2511.21428</link>
<guid>https://arxiv.org/abs/2511.21428</guid>
<content:encoded><![CDATA[
arXiv:2511.21428v1 Announce Type: new 
Abstract: We present a novel unsupervised framework to unlock vast unlabeled human demonstration data from continuous industrial video streams for Vision-Language-Action (VLA) model pre-training. Our method first trains a lightweight motion tokenizer to encode motion dynamics, then employs an unsupervised action segmenter leveraging a novel "Latent Action Energy" metric to discover and segment semantically coherent action primitives. The pipeline outputs both segmented video clips and their corresponding latent action sequences, providing structured data directly suitable for VLA pre-training. Evaluations on public benchmarks and a proprietary electric motor assembly dataset demonstrate effective segmentation of key tasks performed by humans at workstations. Further clustering and quantitative assessment via a Vision-Language Model confirm the semantic coherence of the discovered action primitives. To our knowledge, this is the first fully automated end-to-end system for extracting and organizing VLA pre-training data from unstructured industrial videos, offering a scalable solution for embodied AI integration in manufacturing.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvRainDrop: HyperGraph-guided Completion for Effective Frame and Event Stream Aggregation</title>
<link>https://arxiv.org/abs/2511.21439</link>
<guid>https://arxiv.org/abs/2511.21439</guid>
<content:encoded><![CDATA[
arXiv:2511.21439v1 Announce Type: new 
Abstract: Event cameras produce asynchronous event streams that are spatially sparse yet temporally dense. Mainstream event representation learning algorithms typically use event frames, voxels, or tensors as input. Although these approaches have achieved notable progress, they struggle to address the undersampling problem caused by spatial sparsity. In this paper, we propose a novel hypergraph-guided spatio-temporal event stream completion mechanism, which connects event tokens across different times and spatial locations via hypergraphs and leverages contextual information message passing to complete these sparse events. The proposed method can flexibly incorporate RGB tokens as nodes in the hypergraph within this completion framework, enabling multi-modal hypergraph-based information completion. Subsequently, we aggregate hypergraph node information across different time steps through self-attention, enabling effective learning and fusion of multi-modal features. Extensive experiments on both single- and multi-label event classification tasks fully validated the effectiveness of our proposed framework. The source code of this paper will be released on https://github.com/Event-AHU/EvRainDrop.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileI2V: Fast and High-Resolution Image-to-Video on Mobile Devices</title>
<link>https://arxiv.org/abs/2511.21475</link>
<guid>https://arxiv.org/abs/2511.21475</guid>
<content:encoded><![CDATA[
arXiv:2511.21475v1 Announce Type: new 
Abstract: Recently, video generation has witnessed rapid advancements, drawing increasing attention to image-to-video (I2V) synthesis on mobile devices. However, the substantial computational complexity and slow generation speed of diffusion models pose significant challenges for real-time, high-resolution video generation on resource-constrained mobile devices. In this work, we propose MobileI2V, a 270M lightweight diffusion model for real-time image-to-video generation on mobile devices. The core lies in: (1) We analyzed the performance of linear attention modules and softmax attention modules on mobile devices, and proposed a linear hybrid architecture denoiser that balances generation efficiency and quality. (2) We design a time-step distillation strategy that compresses the I2V sampling steps from more than 20 to only two without significant quality loss, resulting in a 10-fold increase in generation speed. (3) We apply mobile-specific attention optimizations that yield a 2-fold speed-up for attention operations during on-device inference. MobileI2V enables, for the first time, fast 720p image-to-video generation on mobile devices, with quality comparable to existing models. Under one-step conditions, the generation speed of each frame of 720p video is less than 100 ms. Our code is available at: https://github.com/hustvl/MobileI2V.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frequency-Aware Token Reduction for Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2511.21477</link>
<guid>https://arxiv.org/abs/2511.21477</guid>
<content:encoded><![CDATA[
arXiv:2511.21477v1 Announce Type: new 
Abstract: Vision Transformers have demonstrated exceptional performance across various computer vision tasks, yet their quadratic computational complexity concerning token length remains a significant challenge. To address this, token reduction methods have been widely explored. However, existing approaches often overlook the frequency characteristics of self-attention, such as rank collapsing and over-smoothing phenomenon. In this paper, we propose a frequency-aware token reduction strategy that improves computational efficiency while preserving performance by mitigating rank collapsing. Our method partitions tokens into high-frequency tokens and low-frequency tokens. high-frequency tokens are selectively preserved, while low-frequency tokens are aggregated into a compact direct current token to retain essential low-frequency components. Through extensive experiments and analysis, we demonstrate that our approach significantly improves accuracy while reducing computational overhead and mitigating rank collapsing and over smoothing. Furthermore, we analyze the previous methods, shedding light on their implicit frequency characteristics and limitations.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning</title>
<link>https://arxiv.org/abs/2511.21490</link>
<guid>https://arxiv.org/abs/2511.21490</guid>
<content:encoded><![CDATA[
arXiv:2511.21490v1 Announce Type: new 
Abstract: We present a novel training approach, named Merge-and-Bound (M&amp;B) for Class Incremental Learning (CIL), which directly manipulates model weights in the parameter space for optimization. Our algorithm involves two types of weight merging: inter-task weight merging and intra-task weight merging. Inter-task weight merging unifies previous models by averaging the weights of models from all previous stages. On the other hand, intra-task weight merging facilitates the learning of current task by combining the model parameters within current stage. For reliable weight merging, we also propose a bounded update technique that aims to optimize the target model with minimal cumulative updates and preserve knowledge from previous tasks; this strategy reveals that it is possible to effectively obtain new models near old ones, reducing catastrophic forgetting. M&amp;B is seamlessly integrated into existing CIL methods without modifying architecture components or revising learning objectives. We extensively evaluate our algorithm on standard CIL benchmarks and demonstrate superior performance compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CanKD: Cross-Attention-based Non-local operation for Feature-based Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.21503</link>
<guid>https://arxiv.org/abs/2511.21503</guid>
<content:encoded><![CDATA[
arXiv:2511.21503v1 Announce Type: new 
Abstract: We propose Cross-Attention-based Non-local Knowledge Distillation (CanKD), a novel feature-based knowledge distillation framework that leverages cross-attention mechanisms to enhance the knowledge transfer process. Unlike traditional self-attention-based distillation methods that align teacher and student feature maps independently, CanKD enables each pixel in the student feature map to dynamically consider all pixels in the teacher feature map. This non-local knowledge transfer more thoroughly captures pixel-wise relationships, improving feature representation learning. Our method introduces only an additional loss function to achieve superior performance compared with existing attention-guided distillation methods. Extensive experiments on object detection and image segmentation tasks demonstrate that CanKD outperforms state-of-the-art feature and hybrid distillation methods. These experimental results highlight CanKD's potential as a new paradigm for attention-guided distillation in computer vision tasks. Code is available at https://github.com/tori-hotaru/CanKD
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Design Choices for Deepfake Detectors</title>
<link>https://arxiv.org/abs/2511.21507</link>
<guid>https://arxiv.org/abs/2511.21507</guid>
<content:encoded><![CDATA[
arXiv:2511.21507v1 Announce Type: new 
Abstract: The effectiveness of deepfake detection methods often depends less on their core design and more on implementation details such as data preprocessing, augmentation strategies, and optimization techniques. These factors make it difficult to fairly compare detectors and to understand which factors truly contribute to their performance. To address this, we systematically investigate how different design choices influence the accuracy and generalization capabilities of deepfake detection models, focusing on aspects related to training, inference, and incremental updates. By isolating the impact of individual factors, we aim to establish robust, architecture-agnostic best practices for the design and development of future deepfake detection systems. Our experiments identify a set of design choices that consistently improve deepfake detection and enable state-of-the-art performance on the AI-GenBench benchmark.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Paced Learning for Images of Antinuclear Antibodies</title>
<link>https://arxiv.org/abs/2511.21519</link>
<guid>https://arxiv.org/abs/2511.21519</guid>
<content:encoded><![CDATA[
arXiv:2511.21519v1 Announce Type: new 
Abstract: Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sj\"ogren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?</title>
<link>https://arxiv.org/abs/2511.21523</link>
<guid>https://arxiv.org/abs/2511.21523</guid>
<content:encoded><![CDATA[
arXiv:2511.21523v1 Announce Type: new 
Abstract: Recent advances in foundation models have shown great promise in domains such as natural language processing and computer vision, and similar efforts are now emerging in the Earth Observation community. These models aim to generalize across tasks with limited supervision, reducing the need for training separate models for each task. However, current strategies, which largely focus on scaling model size and dataset volume, require prohibitive computational and data resources, limiting accessibility to only a few large institutions. Moreover, this paradigm of ever-larger models stands in stark contrast with the principles of sustainable and environmentally responsible AI, as it leads to immense carbon footprints and resource inefficiency. In this work, we present a novel and efficient alternative: an Ensemble-of-Specialists framework for building Remote Sensing Foundation Models (RSFMs). Our method decomposes the training process into lightweight, task-specific ConvNeXtV2 specialists that can be frozen and reused. This modular approach offers strong advantages in efficiency, interpretability, and extensibility. Moreover, it naturally supports federated training, pruning, and continuous specialist integration, making it particularly well-suited for collaborative and resource-constrained settings. Our framework sets a new direction for building scalable and efficient RSFMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Age-specific Alzheimer 's Disease Prediction with Characteristic Constraints in Nonuniform Time Span</title>
<link>https://arxiv.org/abs/2511.21530</link>
<guid>https://arxiv.org/abs/2511.21530</guid>
<content:encoded><![CDATA[
arXiv:2511.21530v1 Announce Type: new 
Abstract: Alzheimer's disease is a debilitating disorder marked by a decline in cognitive function. Timely identification of the disease is essential for the development of personalized treatment strategies that aim to mitigate its progression. The application of generated images for the prediction of Alzheimer's disease poses challenges, particularly in accurately representing the disease's characteristics when input sequences are captured at irregular time intervals. This study presents an innovative methodology for sequential image generation, guided by quantitative metrics, to maintain the essential features indicative of disease progression. Furthermore, an age-scaling factor is integrated into the process to produce age-specific MRI images, facilitating the prediction of advanced stages of the disease. The results obtained from the ablation study suggest that the inclusion of quantitative metrics significantly improves the accuracy of MRI image synthesis. Furthermore, the application of age-scaled pixel loss contributed to the enhanced iterative generation of MRI images. In terms of long-term disease prognosis, the Structural Similarity Index reached a peak value of 0.882, indicating a substantial degree of similarity in the synthesized images.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Video Generation Models Are Good Latent Reward Models</title>
<link>https://arxiv.org/abs/2511.21541</link>
<guid>https://arxiv.org/abs/2511.21541</guid>
<content:encoded><![CDATA[
arXiv:2511.21541v1 Announce Type: new 
Abstract: Reward feedback learning (ReFL) has proven effective for aligning image generation with human preferences. However, its extension to video generation faces significant challenges. Existing video reward models rely on vision-language models designed for pixel-space inputs, confining ReFL optimization to near-complete denoising steps after computationally expensive VAE decoding. This pixel-space approach incurs substantial memory overhead and increased training time, and its late-stage optimization lacks early-stage supervision, refining only visual quality rather than fundamental motion dynamics and structural coherence. In this work, we show that pre-trained video generation models are naturally suited for reward modeling in the noisy latent space, as they are explicitly designed to process noisy latent representations at arbitrary timesteps and inherently preserve temporal information through their sequential modeling capabilities. Accordingly, we propose Process Reward Feedback Learning~(PRFL), a framework that conducts preference optimization entirely in latent space, enabling efficient gradient backpropagation throughout the full denoising chain without VAE decoding. Extensive experiments demonstrate that PRFL significantly improves alignment with human preferences, while achieving substantial reductions in memory consumption and training time compared to RGB ReFL.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UAVLight: A Benchmark for Illumination-Robust 3D Reconstruction in Unmanned Aerial Vehicle (UAV) Scenes</title>
<link>https://arxiv.org/abs/2511.21565</link>
<guid>https://arxiv.org/abs/2511.21565</guid>
<content:encoded><![CDATA[
arXiv:2511.21565v1 Announce Type: new 
Abstract: Illumination inconsistency is a fundamental challenge in multi-view 3D reconstruction. Variations in sunlight direction, cloud cover, and shadows break the constant-lighting assumption underlying both classical multi-view stereo (MVS) and structure from motion (SfM) pipelines and recent neural rendering methods, leading to geometry drift, color inconsistency, and shadow imprinting. This issue is especially critical in UAV-based reconstruction, where long flight durations and outdoor environments make lighting changes unavoidable. However, existing datasets either restrict capture to short time windows, thus lacking meaningful illumination diversity, or span months and seasons, where geometric and semantic changes confound the isolated study of lighting robustness. We introduce UAVLight, a controlled-yet-real benchmark for illumination-robust 3D reconstruction. Each scene is captured along repeatable, geo-referenced flight paths at multiple fixed times of day, producing natural lighting variation under consistent geometry, calibration, and viewpoints. With standardized evaluation protocols across lighting conditions, UAVLight provides a reliable foundation for developing and benchmarking reconstruction methods that are consistent, faithful, and relightable in real outdoor environments.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Robust Prompt Distillation for 3D Point Cloud Models</title>
<link>https://arxiv.org/abs/2511.21574</link>
<guid>https://arxiv.org/abs/2511.21574</guid>
<content:encoded><![CDATA[
arXiv:2511.21574v1 Announce Type: new 
Abstract: Adversarial attacks pose a significant threat to learning-based 3D point cloud models, critically undermining their reliability in security-sensitive applications. Existing defense methods often suffer from (1) high computational overhead and (2) poor generalization ability across diverse attack types. To bridge these gaps, we propose a novel yet efficient teacher-student framework, namely Multimodal Robust Prompt Distillation (MRPD) for distilling robust 3D point cloud model. It learns lightweight prompts by aligning student point cloud model's features with robust embeddings from three distinct teachers: a vision model processing depth projections, a high-performance 3D model, and a text encoder. To ensure a reliable knowledge transfer, this distillation is guided by a confidence-gated mechanism which dynamically balances the contribution of all input modalities. Notably, since the distillation is all during the training stage, there is no additional computational cost at inference. Extensive experiments demonstrate that MRPD substantially outperforms state-of-the-art defense methods against a wide range of white-box and black-box attacks, while even achieving better performance on clean data. Our work presents a new, practical paradigm for building robust 3D vision systems by efficiently harnessing multimodal knowledge.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Landmark Detection Model in Pelvic Fluoroscopy using 2D/3D Registration Loss</title>
<link>https://arxiv.org/abs/2511.21575</link>
<guid>https://arxiv.org/abs/2511.21575</guid>
<content:encoded><![CDATA[
arXiv:2511.21575v1 Announce Type: new 
Abstract: Automated landmark detection offers an efficient approach for medical professionals to understand patient anatomic structure and positioning using intra-operative imaging. While current detection methods for pelvic fluoroscopy demonstrate promising accuracy, most assume a fixed Antero-Posterior view of the pelvis. However, orientation often deviates from this standard view, either due to repositioning of the imaging unit or of the target structure itself. To address this limitation, we propose a novel framework that incorporates 2D/3D landmark registration into the training of a U-Net landmark prediction model. We analyze the performance difference by comparing landmark detection accuracy between the baseline U-Net, U-Net trained with Pose Estimation Loss, and U-Net fine-tuned with Pose Estimation Loss under realistic intra-operative conditions where patient pose is variable.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmony: Harmonizing Audio and Video Generation through Cross-Task Synergy</title>
<link>https://arxiv.org/abs/2511.21579</link>
<guid>https://arxiv.org/abs/2511.21579</guid>
<content:encoded><![CDATA[
arXiv:2511.21579v1 Announce Type: new 
Abstract: The synthesis of synchronized audio-visual content is a key challenge in generative AI, with open-source models facing challenges in robust audio-video alignment. Our analysis reveals that this issue is rooted in three fundamental challenges of the joint diffusion process: (1) Correspondence Drift, where concurrently evolving noisy latents impede stable learning of alignment; (2) inefficient global attention mechanisms that fail to capture fine-grained temporal cues; and (3) the intra-modal bias of conventional Classifier-Free Guidance (CFG), which enhances conditionality but not cross-modal synchronization. To overcome these challenges, we introduce Harmony, a novel framework that mechanistically enforces audio-visual synchronization. We first propose a Cross-Task Synergy training paradigm to mitigate drift by leveraging strong supervisory signals from audio-driven video and video-driven audio generation tasks. Then, we design a Global-Local Decoupled Interaction Module for efficient and precise temporal-style alignment. Finally, we present a novel Synchronization-Enhanced CFG (SyncCFG) that explicitly isolates and amplifies the alignment signal during inference. Extensive experiments demonstrate that Harmony establishes a new state-of-the-art, significantly outperforming existing methods in both generation fidelity and, critically, in achieving fine-grained audio-visual synchronization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning-Based Multiclass Classification of Oral Lesions with Stratified Augmentation</title>
<link>https://arxiv.org/abs/2511.21582</link>
<guid>https://arxiv.org/abs/2511.21582</guid>
<content:encoded><![CDATA[
arXiv:2511.21582v1 Announce Type: new 
Abstract: Oral cancer is highly common across the globe and is mostly diagnosed during the later stages due to the close visual similarity to benign, precancerous, and malignant lesions in the oral cavity. Implementing computer aided diagnosis systems early on has the potential to greatly improve clinical outcomes. This research intends to use deep learning to build a multiclass classifier for sixteen different oral lesions. To overcome the challenges of limited and imbalanced datasets, the proposed technique combines stratified data splitting and advanced data augmentation and oversampling to perform the classification. The experimental results, which achieved 83.33 percent accuracy, 89.12 percent precision, and 77.31 percent recall, demonstrate the superiority of the suggested model over state of the art methods now in use. The suggested model effectively conveys the effectiveness of oversampling and augmentation strategies in situations where the minority class classification performance is noteworthy. As a first step toward trustworthy computer aided diagnostic systems for the early detection of oral cancer in clinical settings, the suggested framework shows promise.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoGAN: Improving Motion Quality in Video Diffusion via Few-Step Motion Adversarial Post-Training</title>
<link>https://arxiv.org/abs/2511.21592</link>
<guid>https://arxiv.org/abs/2511.21592</guid>
<content:encoded><![CDATA[
arXiv:2511.21592v1 Announce Type: new 
Abstract: Video diffusion models achieve strong frame-level fidelity but still struggle with motion coherence, dynamics and realism, often producing jitter, ghosting, or implausible dynamics. A key limitation is that the standard denoising MSE objective provides no direct supervision on temporal consistency, allowing models to achieve low loss while still generating poor motion. We propose MoGAN, a motion-centric post-training framework that improves motion realism without reward models or human preference data. Built atop a 3-step distilled video diffusion model, we train a DiT-based optical-flow discriminator to differentiate real from generated motion, combined with a distribution-matching regularizer to preserve visual fidelity. With experiments on Wan2.1-T2V-1.3B, MoGAN substantially improves motion quality across benchmarks. On VBench, MoGAN boosts motion score by +7.3% over the 50-step teacher and +13.3% over the 3-step DMD model. On VideoJAM-Bench, MoGAN improves motion score by +7.4% over the teacher and +8.8% over DMD, while maintaining comparable or even better aesthetic and image-quality scores. A human study further confirms that MoGAN is preferred for motion quality (52% vs. 38% for the teacher; 56% vs. 29% for DMD). Overall, MoGAN delivers significantly more realistic motion without sacrificing visual fidelity or efficiency, offering a practical path toward fast, high-quality video generation. Project webpage is: https://xavihart.github.io/mogan.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSAM: Refine, Requery, and Reinforce: Self-Prompting Point-Supervised Segmentation for Remote Sensing Images</title>
<link>https://arxiv.org/abs/2511.21606</link>
<guid>https://arxiv.org/abs/2511.21606</guid>
<content:encoded><![CDATA[
arXiv:2511.21606v1 Announce Type: new 
Abstract: Interactive segmentation models such as the Segment Anything Model (SAM) have demonstrated remarkable generalization on natural images, but perform suboptimally on remote sensing imagery (RSI) due to severe domain shift and the scarcity of dense annotations. To address this, we propose a self-prompting, point-supervised framework that adapts SAM to RSIs using only sparse point annotations. Our method employs a Refine-Requery-Reinforce loop, where coarse pseudo-masks are generated from initial points (Refine), improved with self-constructed box prompts (Requery), and embeddings are aligned across iterations to reduce confirmation bias (Reinforce). Without relying on full-mask supervision, our approach progressively enhances SAM's segmentation quality and domain robustness through self-guided prompt adaptation . We evaluate our proposed method on three RSI benchmark datasets, including WHU, HRSID, and NWPU VHR-10, showing that our method consistently surpasses pretrained SAM and recent point-supervised segmentation methods. Our results demonstrate that self-prompting and semantic alignment provide an efficient path towards scalable, point-level adaptation of foundation segmentation models for remote sensing applications.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Learning for GCN-based Action Recognition</title>
<link>https://arxiv.org/abs/2511.21625</link>
<guid>https://arxiv.org/abs/2511.21625</guid>
<content:encoded><![CDATA[
arXiv:2511.21625v1 Announce Type: new 
Abstract: Despite the notable success of graph convolutional networks (GCNs) in skeleton-based action recognition, their performance often depends on large volumes of labeled data, which are frequently scarce in practical settings. To address this limitation, we propose a novel label-efficient GCN model. Our work makes two primary contributions. First, we develop a novel acquisition function that employs an adversarial strategy to identify a compact set of informative exemplars for labeling. This selection process balances representativeness, diversity, and uncertainty. Second, we introduce bidirectional and stable GCN architectures. These enhanced networks facilitate a more effective mapping between the ambient and latent data spaces, enabling a better understanding of the learned exemplar distribution. Extensive evaluations on two challenging skeleton-based action recognition benchmarks reveal significant improvements achieved by our label-efficient GCNs compared to prior work.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Qwen3-VL Technical Report</title>
<link>https://arxiv.org/abs/2511.21631</link>
<guid>https://arxiv.org/abs/2511.21631</guid>
<content:encoded><![CDATA[
arXiv:2511.21631v1 Announce Type: new 
Abstract: We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Error Correction on Low-Resource Devices</title>
<link>https://arxiv.org/abs/2511.21652</link>
<guid>https://arxiv.org/abs/2511.21652</guid>
<content:encoded><![CDATA[
arXiv:2511.21652v1 Announce Type: new 
Abstract: The proliferation of AI models in everyday devices has highlighted a critical challenge: prediction errors that degrade user experience. While existing solutions focus on error detection, they rarely provide efficient correction mechanisms, especially for resource-constrained devices. We present a novel system enabling users to correct AI misclassifications through few-shot learning, requiring minimal computational resources and storage. Our approach combines server-side foundation model training with on-device prototype-based classification, enabling efficient error correction through prototype updates rather than model retraining. The system consists of two key components: (1) a server-side pipeline that leverages knowledge distillation to transfer robust feature representations from foundation models to device-compatible architectures, and (2) a device-side mechanism that enables ultra-efficient error correction through prototype adaptation. We demonstrate our system's effectiveness on both image classification and object detection tasks, achieving over 50% error correction in one-shot scenarios on Food-101 and Flowers-102 datasets while maintaining minimal forgetting (less than 0.02%) and negligible computational overhead. Our implementation, validated through an Android demonstration app, proves the system's practicality in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaFlow: Enhancing Long-Term Action Quality Assessment with Causal Counterfactual Flow</title>
<link>https://arxiv.org/abs/2511.21653</link>
<guid>https://arxiv.org/abs/2511.21653</guid>
<content:encoded><![CDATA[
arXiv:2511.21653v1 Announce Type: new 
Abstract: Action Quality Assessment (AQA) predicts fine-grained execution scores from action videos and is widely applied in sports, rehabilitation, and skill evaluation. Long-term AQA, as in figure skating or rhythmic gymnastics, is especially challenging since it requires modeling extended temporal dynamics while remaining robust to contextual confounders. Existing approaches either depend on costly annotations or rely on unidirectional temporal modeling, making them vulnerable to spurious correlations and unstable long-term representations. To this end, we propose CaFlow, a unified framework that integrates counterfactual de-confounding with bidirectional time-conditioned flow. The Causal Counterfactual Regularization (CCR) module disentangles causal and confounding features in a self-supervised manner and enforces causal robustness through counterfactual interventions, while the BiT-Flow module models forward and backward dynamics with a cycle-consistency constraint to produce smoother and more coherent representations. Extensive experiments on multiple long-term AQA benchmarks demonstrate that CaFlow achieves state-of-the-art performance. Code is available at https://github.com/Harrison21/CaFlow
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Crit: Benchmarking Multimodal Judges on Pluralistic Criteria-Following</title>
<link>https://arxiv.org/abs/2511.21662</link>
<guid>https://arxiv.org/abs/2511.21662</guid>
<content:encoded><![CDATA[
arXiv:2511.21662v1 Announce Type: new 
Abstract: Large multimodal models (LMMs) are increasingly adopted as judges in multimodal evaluation systems due to their strong instruction following and consistency with human preferences. However, their ability to follow diverse, fine-grained evaluation criteria remains underexplored. We develop Multi-Crit, a benchmark for evaluating multimodal judges on their capacity to follow pluralistic criteria and produce reliable criterion-level judgments. Covering both open-ended generation and verifiable reasoning tasks, Multi-Crit is built through a rigorous data curation pipeline that gathers challenging response pairs with multi-criterion human annotations. It further introduces three novel metrics for systematically assessing pluralistic adherence, criterion-switching flexibility, and the ability to recognize criterion-level preference conflicts. Comprehensive analysis of 25 LMMs reveals that 1) proprietary models still struggle to maintain consistent adherence to pluralistic criteria--especially in open-ended evaluation; 2) open-source models lag further behind in flexibly following diverse criteria; and 3) critic fine-tuning with holistic judgment signals enhances visual grounding but fails to generalize to pluralistic criterion-level judgment. Additional analyses on reasoning fine-tuning, test-time scaling, and boundary consistency between open-source and proprietary models further probe the limits of current multimodal judges. As a pioneering study, Multi-Crit lays the foundation for building reliable and steerable multimodal AI evaluation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.21663</link>
<guid>https://arxiv.org/abs/2511.21663</guid>
<content:encoded><![CDATA[
arXiv:2511.21663v1 Announce Type: new 
Abstract: In recent years, Vision-Language-Action (VLA) models in embodied intelligence have developed rapidly. However, existing adversarial attack methods require costly end-to-end training and often generate noticeable perturbation patches. To address these limitations, we propose ADVLA, a framework that directly applies adversarial perturbations on features projected from the visual encoder into the textual feature space. ADVLA efficiently disrupts downstream action predictions under low-amplitude constraints, and attention guidance allows the perturbations to be both focused and sparse. We introduce three strategies that enhance sensitivity, enforce sparsity, and concentrate perturbations. Experiments demonstrate that under an $L_{\infty}=4/255$ constraint, ADVLA combined with Top-K masking modifies less than 10% of the patches while achieving an attack success rate of nearly 100%. The perturbations are concentrated on critical regions, remain almost imperceptible in the overall image, and a single-step iteration takes only about 0.06 seconds, significantly outperforming conventional patch-based attacks. In summary, ADVLA effectively weakens downstream action predictions of VLA models under low-amplitude and locally sparse conditions, avoiding the high training costs and conspicuous perturbations of traditional patch attacks, and demonstrates unique effectiveness and practical value for attacking VLA feature spaces.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revolutionizing Glioma Segmentation &amp; Grading Using 3D MRI - Guided Hybrid Deep Learning Models</title>
<link>https://arxiv.org/abs/2511.21673</link>
<guid>https://arxiv.org/abs/2511.21673</guid>
<content:encoded><![CDATA[
arXiv:2511.21673v1 Announce Type: new 
Abstract: Gliomas are brain tumor types that have a high mortality rate which means early and accurate diagnosis is important for therapeutic intervention for the tumors. To address this difficulty, the proposed research will develop a hybrid deep learning model which integrates U-Net based segmentation and a hybrid DenseNet-VGG classification network with multihead attention and spatial-channel attention capabilities. The segmentation model will precisely demarcate the tumors in a 3D volume of MRI data guided by spatial and contextual information. The classification network which combines a branch of both DenseNet and VGG, will incorporate the demarcated tumor on which features with attention mechanisms would be focused on clinically relevant features. High-dimensional 3D MRI data could successfully be utilized in the model through preprocessing steps which are normalization, resampling, and data augmentation. Through a variety of measures the framework is evaluated: measures of performance in segmentation are Dice coefficient and Mean Intersection over Union (IoU) and measures of performance in classification are accuracy precision, recall, and F1-score. The hybrid framework that has been proposed has demonstrated through physical testing that it has the capability of obtaining a Dice coefficient of 98% in tumor segmentation, and 99% on classification accuracy, outperforming traditional CNN models and attention-free methods. Utilizing multi-head attention mechanisms enhances notions of priority in aspects of the tumor that are clinically significant, and enhances interpretability and accuracy. The results suggest a great potential of the framework in facilitating the timely and reliable diagnosis and grading of glioma by clinicians is promising, allowing for better planning of patient treatment.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seeing without Pixels: Perception from Camera Trajectories</title>
<link>https://arxiv.org/abs/2511.21681</link>
<guid>https://arxiv.org/abs/2511.21681</guid>
<content:encoded><![CDATA[
arXiv:2511.21681v1 Announce Type: new 
Abstract: Can one perceive a video's content without seeing its pixels, just from the camera trajectory-the path it carves through space? This paper is the first to systematically investigate this seemingly implausible question. Towards this end, we propose a contrastive learning framework to train CamFormer, a dedicated encoder that projects camera pose trajectories into a joint embedding space, aligning them with natural language. We find that, contrary to its apparent simplicity, the camera trajectory is a remarkably informative signal to uncover video content. In other words, "how you move" can indeed reveal "what you are doing" (egocentric) or "observing" (exocentric). We demonstrate the versatility of our learned CamFormer embeddings on a diverse suite of downstream tasks, ranging from cross-modal alignment to classification and temporal analysis. Importantly, our representations are robust across diverse camera pose estimation methods, including both high-fidelity multi-sensored and standard RGB-only estimators. Our findings establish camera trajectory as a lightweight, robust, and versatile modality for perceiving video content.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>G$^2$VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning</title>
<link>https://arxiv.org/abs/2511.21688</link>
<guid>https://arxiv.org/abs/2511.21688</guid>
<content:encoded><![CDATA[
arXiv:2511.21688v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) still lack robustness in spatial intelligence, demonstrating poor performance on spatial understanding and reasoning tasks. We attribute this gap to the absence of a visual geometry learning process capable of reconstructing 3D space from 2D images. We present G$^2$VLM, a geometry grounded vision-language model that bridges two fundamental aspects of spatial intelligence: spatial 3D reconstruction and spatial understanding. G$^2$VLM natively leverages learned 3D visual geometry features to directly predict 3D attributes and enhance spatial reasoning tasks via in-context learning and interleaved reasoning. Our unified design is highly scalable for spatial understanding: it trains on abundant multi-view image and video data, while simultaneously leveraging the benefits of 3D visual priors that are typically only derived from hard-to-collect annotations. Experimental results demonstrate G$^2$VLM is proficient in both tasks, achieving comparable results to state-of-the-art feed-forward 3D reconstruction models and achieving better or competitive results across spatial understanding and reasoning tasks. By unifying a semantically strong VLM with low-level 3D vision tasks, we hope G$^2$VLM can serve as a strong baseline for the community and unlock more future applications, such as 3D scene editing.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Canvas-to-Image: Compositional Image Generation with Multimodal Controls</title>
<link>https://arxiv.org/abs/2511.21691</link>
<guid>https://arxiv.org/abs/2511.21691</guid>
<content:encoded><![CDATA[
arXiv:2511.21691v1 Announce Type: new 
Abstract: While modern diffusion models excel at generating high-quality and diverse images, they still struggle with high-fidelity compositional and multimodal control, particularly when users simultaneously specify text prompts, subject references, spatial arrangements, pose constraints, and layout annotations. We introduce Canvas-to-Image, a unified framework that consolidates these heterogeneous controls into a single canvas interface, enabling users to generate images that faithfully reflect their intent. Our key idea is to encode diverse control signals into a single composite canvas image that the model can directly interpret for integrated visual-spatial reasoning. We further curate a suite of multi-task datasets and propose a Multi-Task Canvas Training strategy that optimizes the diffusion model to jointly understand and integrate heterogeneous controls into text-to-image generation within a unified learning paradigm. This joint training enables Canvas-to-Image to reason across multiple control modalities rather than relying on task-specific heuristics, and it generalizes well to multi-control scenarios during inference. Extensive experiments show that Canvas-to-Image significantly outperforms state-of-the-art methods in identity preservation and control adherence across challenging benchmarks, including multi-person composition, pose-controlled composition, layout-constrained generation, and multi-control generation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fractional Variational Approach to Spectral Filtering Using the Fourier Transform</title>
<link>https://arxiv.org/abs/2511.20675</link>
<guid>https://arxiv.org/abs/2511.20675</guid>
<content:encoded><![CDATA[
arXiv:2511.20675v1 Announce Type: cross 
Abstract: The interference of fluorescence signals and noise remains a significant challenge in Raman spectrum analysis, often obscuring subtle spectral features that are critical for accurate analysis. Inspired by variational methods similar to those used in image denoising, our approach minimizes a functional involving fractional derivatives to balance noise suppression with the preservation of essential chemical features of the signal, such as peak position, intensity, and area. The original problem is reformulated in the frequency domain through the Fourier transform, making the implementation simple and fast. In this work, we discuss the theoretical framework, practical implementation, and the advantages and limitations of this method in the context of {simulated} Raman data, as well as in image processing. The main contribution of this article is the combination of a variational approach in the frequency domain, the use of fractional derivatives, and the optimization of the {regularization parameter and} derivative order through the concept of Shannon entropy. This work explores how the fractional order, combined with the regularization parameter, affects noise removal and preserves the essential features of the spectrum {and image}. Finally, the study shows that the combination of the proposed strategies produces an efficient, robust, and easily implementable filter.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Aware Adaptive Elastic Weight Consolidation for Continual Learning in Medical Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.20732</link>
<guid>https://arxiv.org/abs/2511.20732</guid>
<content:encoded><![CDATA[
arXiv:2511.20732v1 Announce Type: cross 
Abstract: Medical AI systems face catastrophic forgetting when deployed in clinical settings, where models must learn new imaging protocols while retaining prior diagnostic capabilities. This challenge is particularly acute for medical vision-language models that must preserve complex cross-modal alignments between medical images and clinical terminology across diverse imaging modalities. We introduce Prompt- Aware Adaptive Elastic Weight Consolidation (PA-EWC), a novel continual learning approach that addresses catastrophic forgetting through prompt-guided parameter specialization. Our method systematically categorizes model parameters based on their functional roles in processing visual-descriptive, spatial-guided, and medical-semantic information, enabling targeted protection of critical knowledge while allowing adaptation to new clinical requirements. PA-EWC incorporates adaptive Fisher Information computation with gradient stability analysis and develops weighted complexity metrics based on medical terminology density. We evaluate our approach across five medical imaging datasets (Kvasir-SEG, ISIC 2018, CheXlocalize, BUSI, CAMUS) representing diverse modalities including endoscopy, dermoscopy, radiography, and ultrasound. Experimental results demonstrate that PA-EWC reduces catastrophic forgetting by up to 17.58% compared to baseline methods, with performance improvements of 4.30% on chest X-ray pathology localization and 6.06% on polyp segmentation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Histopathologic Assessment of Hirschsprung Disease Using a Multi-Stage Vision Transformer Framework</title>
<link>https://arxiv.org/abs/2511.20734</link>
<guid>https://arxiv.org/abs/2511.20734</guid>
<content:encoded><![CDATA[
arXiv:2511.20734v1 Announce Type: cross 
Abstract: Hirschsprung Disease is characterized by the absence of ganglion cells in the myenteric plexus. Therefore, their correct identification is crucial for diagnosing Hirschsprung disease. We introduce a three-stage segmentation framework based on a Vision Transformer (ViT-B/16) that mimics the pathologist's diagnostic approach. The framework sequentially segments the muscularis propria, delineates the myenteric plexus, and identifies ganglion cells within anatomically valid regions. 30 whole-slide images of colon tissue were used, each containing expert manual annotations of muscularis, plexus, and ganglion cells at varying levels of certainty. A 5-fold cross-validation scheme was applied to each stage, along with resolution-specific tiling strategies and tailored postprocessing to ensure anatomical consistency. The proposed method achieved a Dice coefficient of 89.9% and a Plexus Inclusion Rate of 100% for muscularis segmentation. Plexus segmentation reached a recall of 94.8%, a precision of 84.2% and a Ganglia Inclusion Rate of 99.7%. For high-certainty ganglion cells, the model achieved 62.1% precision and 89.1% recall, while joint certainty scores yielded 67.0% precision. These results indicate that ViT-based models are effective at leveraging global tissue context and capturing cellular morphology at small scales, even within complex histological tissue structures. This multi-stage methodology has great potential to support digital pathology workflows by reducing inter-observer variability and assisting in the evaluation of Hirschsprung disease. The clinical impact will be evaluated in future work with larger multi-center datasets and additional expert annotations.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHiQPM: Calibrated Hierarchical Interpretable Image Classification</title>
<link>https://arxiv.org/abs/2511.20779</link>
<guid>https://arxiv.org/abs/2511.20779</guid>
<content:encoded><![CDATA[
arXiv:2511.20779v1 Announce Type: cross 
Abstract: Globally interpretable models are a promising approach for trustworthy AI in safety-critical domains. Alongside global explanations, detailed local explanations are a crucial complement to effectively support human experts during inference. This work proposes the Calibrated Hierarchical QPM (CHiQPM) which offers uniquely comprehensive global and local interpretability, paving the way for human-AI complementarity. CHiQPM achieves superior global interpretability by contrastively explaining the majority of classes and offers novel hierarchical explanations that are more similar to how humans reason and can be traversed to offer a built-in interpretable Conformal prediction (CP) method. Our comprehensive evaluation shows that CHiQPM achieves state-of-the-art accuracy as a point predictor, maintaining 99% accuracy of non-interpretable models. This demonstrates a substantial improvement, where interpretability is incorporated without sacrificing overall accuracy. Furthermore, its calibrated set prediction is competitively efficient to other CP methods, while providing interpretable predictions of coherent sets along its hierarchical explanation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Multi-Task Learning for Liver Tumor Segmentation, Dynamic Enhancement Regression, and Classification</title>
<link>https://arxiv.org/abs/2511.20793</link>
<guid>https://arxiv.org/abs/2511.20793</guid>
<content:encoded><![CDATA[
arXiv:2511.20793v1 Announce Type: cross 
Abstract: Liver tumor segmentation, dynamic enhancement regression, and classification are critical for clinical assessment and diagnosis. However, no prior work has attempted to achieve these tasks simultaneously in an end-to-end framework, primarily due to the lack of an effective framework that captures inter-task relevance for mutual improvement and the absence of a mechanism to extract dynamic MRI information effectively. To address these challenges, we propose the Multi-Task Interaction adversarial learning Network (MTI-Net), a novel integrated framework designed to tackle these tasks simultaneously. MTI-Net incorporates Multi-domain Information Entropy Fusion (MdIEF), which utilizes entropy-aware, high-frequency spectral information to effectively integrate features from both frequency and spectral domains, enhancing the extraction and utilization of dynamic MRI data. The network also introduces a task interaction module that establishes higher-order consistency between segmentation and regression, thus fostering inter-task synergy and improving overall performance. Additionally, we designed a novel task-driven discriminator (TDD) to capture internal high-order relationships between tasks. For dynamic MRI information extraction, we employ a shallow Transformer network to perform positional encoding, which captures the relationships within dynamic MRI sequences. In experiments on a dataset of 238 subjects, MTI-Net demonstrates high performance across multiple tasks, indicating its strong potential for assisting in the clinical assessment of liver tumors. The code is available at: https://github.com/xiaojiao929/MTI-Net.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guaranteed Optimal Compositional Explanations for Neurons</title>
<link>https://arxiv.org/abs/2511.20934</link>
<guid>https://arxiv.org/abs/2511.20934</guid>
<content:encoded><![CDATA[
arXiv:2511.20934v1 Announce Type: cross 
Abstract: While neurons are the basic units of deep neural networks, it is still unclear what they learn and if their knowledge is aligned with that of humans. Compositional explanations aim to answer this question by describing the spatial alignment between neuron activations and concepts through logical rules. These logical descriptions are typically computed via a search over all possible concept combinations. Since computing the spatial alignment over the entire state space is computationally infeasible, the literature commonly adopts beam search to restrict the space. However, beam search cannot provide any theoretical guarantees of optimality, and it remains unclear how close current explanations are to the true optimum. In this theoretical paper, we address this gap by introducing the first framework for computing guaranteed optimal compositional explanations. Specifically, we propose: (i) a decomposition that identifies the factors influencing the spatial alignment, (ii) a heuristic to estimate the alignment at any stage of the search, and (iii) the first algorithm that can compute optimal compositional explanations within a feasible time. Using this framework, we analyze the differences between optimal and non-optimal explanations in the most popular settings for compositional explanations, the computer vision domain and Convolutional Neural Networks. In these settings, we demonstrate that 10-40 percent of explanations obtained with beam search are suboptimal when overlapping concepts are involved. Finally, we evaluate a beam-search variant guided by our proposed decomposition and heuristic, showing that it matches or improves runtime over prior methods while offering greater flexibility in hyperparameters and computational resources.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction</title>
<link>https://arxiv.org/abs/2511.20937</link>
<guid>https://arxiv.org/abs/2511.20937</guid>
<content:encoded><![CDATA[
arXiv:2511.20937v1 Announce Type: cross 
Abstract: Embodied cognition argues that intelligence arises from sensorimotor interaction rather than passive observation. It raises an intriguing question: do modern vision-language models (VLMs), trained largely in a disembodied manner, exhibit signs of embodied cognition? We introduce ENACT, a benchmark that casts evaluation of embodied cognition as world modeling from egocentric interaction in a visual question answering (VQA) format. Framed as a partially observable Markov decision process (POMDP) whose actions are scene graph changes, ENACT comprises two complementary sequence reordering tasks: forward world modeling (reorder shuffled observations given actions) and inverse world modeling (reorder shuffled actions given observations). While conceptually simple, solving these tasks implicitly demands capabilities central to embodied cognition-affordance recognition, action-effect reasoning, embodied awareness, and interactive, long-horizon memory from partially observable egocentric input, while avoiding low-level image synthesis that could confound the evaluation. We provide a scalable pipeline that synthesizes QA pairs from robotics simulation (BEHAVIOR) and evaluates models on 8,972 QA pairs spanning long-horizon home-scale activities. Experiments reveal a performance gap between frontier VLMs and humans that widens with interaction horizon. Models consistently perform better on the inverse task than the forward one and exhibit anthropocentric biases, including a preference for right-handed actions and degradation when camera intrinsics or viewpoints deviate from human vision. Website at https://enact-embodied-cognition.github.io/.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Wildfire Spread Prediction Using an Autoregressive Conditional Generative Adversarial Network</title>
<link>https://arxiv.org/abs/2511.21019</link>
<guid>https://arxiv.org/abs/2511.21019</guid>
<content:encoded><![CDATA[
arXiv:2511.21019v1 Announce Type: cross 
Abstract: Climate change has intensified the frequency and severity of wildfires, making rapid and accurate prediction of fire spread essential for effective mitigation and response. Physics-based simulators such as FARSITE offer high-fidelity predictions but are computationally intensive, limiting their applicability in real-time decision-making, while existing deep learning models often yield overly smooth predictions that fail to capture the complex, nonlinear dynamics of wildfire propagation. This study proposes an autoregressive conditional generative adversarial network (CGAN) for probabilistic wildfire spread prediction. By formulating the prediction task as an autoregressive problem, the model learns sequential state transitions, ensuring long-term prediction stability. Experimental results demonstrate that the proposed CGAN-based model outperforms conventional deep learning models in both overall predictive accuracy and boundary delineation of fire perimeters. These results demonstrate that adversarial learning allows the model to capture the strong nonlinearity and uncertainty of wildfire spread, instead of simply fitting the pixel average. Furthermore, the autoregressive framework facilitates systematic temporal forecasting of wildfire evolution. The proposed CGAN-based autoregressive framework enhances both the accuracy and physical interpretability of wildfire spread prediction, offering a promising foundation for time-sensitive response and evacuation planning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Parameter Interpolation for Scalar Conditioning</title>
<link>https://arxiv.org/abs/2511.21028</link>
<guid>https://arxiv.org/abs/2511.21028</guid>
<content:encoded><![CDATA[
arXiv:2511.21028v1 Announce Type: cross 
Abstract: We propose deep parameter interpolation (DPI), a general-purpose method for transforming an existing deep neural network architecture into one that accepts an additional scalar input. Recent deep generative models, including diffusion models and flow matching, employ a single neural network to learn a time- or noise level-dependent vector field. Designing a network architecture to accurately represent this vector field is challenging because the network must integrate information from two different sources: a high-dimensional vector (usually an image) and a scalar. Common approaches either encode the scalar as an additional image input or combine scalar and vector information in specific network components, which restricts architecture choices. Instead, we propose to maintain two learnable parameter sets within a single network and to introduce the scalar dependency by dynamically interpolating between the parameter sets based on the scalar value during training and sampling. DPI is a simple, architecture-agnostic method for adding scalar dependence to a neural network. We demonstrate that our method improves denoising performance and enhances sample quality for both diffusion and flow matching models, while achieving computational efficiency comparable to standard scalar conditioning techniques. Code is available at https://github.com/wustl-cig/parameter_interpolation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-LSTM Hybrid Architecture for Over-the-Air Automatic Modulation Classification Using SDR</title>
<link>https://arxiv.org/abs/2511.21040</link>
<guid>https://arxiv.org/abs/2511.21040</guid>
<content:encoded><![CDATA[
arXiv:2511.21040v1 Announce Type: cross 
Abstract: Automatic Modulation Classification (AMC) is a core technology for future wireless communication systems, enabling the identification of modulation schemes without prior knowledge. This capability is essential for applications in cognitive radio, spectrum monitoring, and intelligent communication networks. We propose an AMC system based on a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) architecture, integrated with a Software Defined Radio (SDR) platform. The proposed architecture leverages CNNs for spatial feature extraction and LSTMs for capturing temporal dependencies, enabling efficient handling of complex, time-varying communication signals. The system's practical ability was demonstrated by identifying over-the-air (OTA) signals from a custom-built FM transmitter alongside other modulation schemes. The system was trained on a hybrid dataset combining the RadioML2018 dataset with a custom-generated dataset, featuring samples at Signal-to-Noise Ratios (SNRs) from 0 to 30dB. System performance was evaluated using accuracy, precision, recall, F1 score, and the Area Under the Receiver Operating Characteristic Curve (AUC-ROC). The optimized model achieved 93.48% accuracy, 93.53% precision, 93.48% recall, and an F1 score of 93.45%. The AUC-ROC analysis confirmed the model's discriminative power, even in noisy conditions. This paper's experimental results validate the effectiveness of the hybrid CNN-LSTM architecture for AMC, suggesting its potential application in adaptive spectrum management and advanced cognitive radio systems.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios</title>
<link>https://arxiv.org/abs/2511.21053</link>
<guid>https://arxiv.org/abs/2511.21053</guid>
<content:encoded><![CDATA[
arXiv:2511.21053v1 Announce Type: cross 
Abstract: Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection</title>
<link>https://arxiv.org/abs/2511.21064</link>
<guid>https://arxiv.org/abs/2511.21064</guid>
<content:encoded><![CDATA[
arXiv:2511.21064v1 Announce Type: cross 
Abstract: Open-Vocabulary Object Detection (OVOD) aims to enable detectors to generalize across categories by leveraging semantic information. Although existing methods are pretrained on large vision-language datasets, their inference is still limited to fixed category names, creating a gap between multimodal training and unimodal inference. Previous work has shown that improving textual representation can significantly enhance OVOD performance, indicating that the textual space is still underexplored. To this end, we propose OVOD-Agent, which transforms passive category matching into proactive visual reasoning and self-evolving detection. Inspired by the Chain-of-Thought (CoT) paradigm, OVOD-Agent extends the textual optimization process into an interpretable Visual-CoT with explicit actions. OVOD's lightweight nature makes LLM-based management unsuitable; instead, we model visual context transitions as a Weakly Markovian Decision Process (w-MDP) over eight state spaces, which naturally represents the agent's state, memory, and interaction dynamics. A Bandit module generates exploration signals under limited supervision, helping the agent focus on uncertain regions and adapt its detection policy. We further integrate Markov transition matrices with Bandit trajectories for self-supervised Reward Model (RM) optimization, forming a closed loop from Bandit exploration to RM learning. Experiments on COCO and LVIS show that OVOD-Agent provides consistent improvements across OVOD backbones, particularly on rare categories, confirming the effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation</title>
<link>https://arxiv.org/abs/2511.21135</link>
<guid>https://arxiv.org/abs/2511.21135</guid>
<content:encoded><![CDATA[
arXiv:2511.21135v1 Announce Type: cross 
Abstract: Embodied navigation that adheres to social norms remains an open research challenge. Our \textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical "brain-action" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR: Smartphone-analogous Typing in Augmented Reality</title>
<link>https://arxiv.org/abs/2511.21143</link>
<guid>https://arxiv.org/abs/2511.21143</guid>
<content:encoded><![CDATA[
arXiv:2511.21143v1 Announce Type: cross 
Abstract: While text entry is an essential and frequent task in Augmented Reality (AR) applications, devising an efficient and easy-to-use text entry method for AR remains an open challenge. This research presents STAR, a smartphone-analogous AR text entry technique that leverages a user's familiarity with smartphone two-thumb typing. With STAR, a user performs thumb typing on a virtual QWERTY keyboard that is overlain on the skin of their hands. During an evaluation study of STAR, participants achieved a mean typing speed of 21.9 WPM (i.e., 56% of their smartphone typing speed), and a mean error rate of 0.3% after 30 minutes of practice. We further analyze the major factors implicated in the performance gap between STAR and smartphone typing, and discuss ways this gap could be narrowed.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AV-Edit: Multimodal Generative Sound Effect Editing via Audio-Visual Semantic Joint Control</title>
<link>https://arxiv.org/abs/2511.21146</link>
<guid>https://arxiv.org/abs/2511.21146</guid>
<content:encoded><![CDATA[
arXiv:2511.21146v1 Announce Type: cross 
Abstract: Sound effect editing-modifying audio by adding, removing, or replacing elements-remains constrained by existing approaches that rely solely on low-level signal processing or coarse text prompts, often resulting in limited flexibility and suboptimal audio quality. To address this, we propose AV-Edit, a generative sound effect editing framework that enables fine-grained editing of existing audio tracks in videos by jointly leveraging visual, audio, and text semantics. Specifically, the proposed method employs a specially designed contrastive audio-visual masking autoencoder (CAV-MAE-Edit) for multimodal pre-training, learning aligned cross-modal representations. These representations are then used to train an editorial Multimodal Diffusion Transformer (MM-DiT) capable of removing visually irrelevant sounds and generating missing audio elements consistent with video content through a correlation-based feature gating training strategy. Furthermore, we construct a dedicated video-based sound editing dataset as an evaluation benchmark. Experiments demonstrate that the proposed AV-Edit generates high-quality audio with precise modifications based on visual content, achieving state-of-the-art performance in the field of sound effect editing and exhibiting strong competitiveness in the domain of audio generation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale</title>
<link>https://arxiv.org/abs/2511.21270</link>
<guid>https://arxiv.org/abs/2511.21270</guid>
<content:encoded><![CDATA[
arXiv:2511.21270v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have transformed text-to-speech (TTS) synthesis, inspiring autoregressive frameworks that represent speech as sequences of discrete codec tokens. Among them, single-codebook TTS LLMs have emerged as compact and streamable architectures that jointly model semantic and acoustic integration. However, despite their efficiency, these models often exhibit unstable prosody, speaker drift, and degraded naturalness. To address these issues, we propose a multi-reward Group Relative Policy Optimization (GRPO) framework that directly optimizes the token generation policy of single-codebook TTS LLMs. Beyond standard intelligibility and speaker similarity objectives, our design integrates three rule-based rewards: a length penalty for duration consistency, an entropy regularization reward for decoding stability, and an LLM-annotated prosody alignment reward that explicitly supervises rhythm. In this prosody reward, an external reasoning LLM predicts multiple plausible pause structures via in-context learning, providing a human-preference-aligned supervisory signal for GRPO training. To assess universality, we further attach a flow-matching (FM) decoder on top of the GRPO-optimized AR backbone and observe consistent additional gains, indicating that our reinforcement optimization enhances the intrinsic AR policy. We further conduct a scalability analysis across data sizes and model scales, revealing that the proposed method consistently enhances prosodic stability, speaker similarity, and overall speech naturalness in single-codebook TTS LLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaMM-Disaster: A Multimodal Transformer-Based Deep Learning Framework for Multiclass Disaster Classification in Bangla</title>
<link>https://arxiv.org/abs/2511.21364</link>
<guid>https://arxiv.org/abs/2511.21364</guid>
<content:encoded><![CDATA[
arXiv:2511.21364v1 Announce Type: cross 
Abstract: Natural disasters remain a major challenge for Bangladesh, so real-time monitoring and quick response systems are essential. In this study, we present BanglaMM-Disaster, an end-to-end deep learning-based multimodal framework for disaster classification in Bangla, using both textual and visual data from social media. We constructed a new dataset of 5,037 Bangla social media posts, each consisting of a caption and a corresponding image, annotated into one of nine disaster-related categories. The proposed model integrates transformer-based text encoders, including BanglaBERT, mBERT, and XLM-RoBERTa, with CNN backbones such as ResNet50, DenseNet169, and MobileNetV2, to process the two modalities. Using early fusion, the best model achieves 83.76% accuracy. This surpasses the best text-only baseline by 3.84% and the image-only baseline by 16.91%. Our analysis also shows reduced misclassification across all classes, with noticeable improvements for ambiguous examples. This work fills a key gap in Bangla multimodal disaster analysis and demonstrates the benefits of combining multiple data types for real-time disaster response in low-resource settings.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bangla Sign Language Translation: Dataset Creation Challenges, Benchmarking and Prospects</title>
<link>https://arxiv.org/abs/2511.21533</link>
<guid>https://arxiv.org/abs/2511.21533</guid>
<content:encoded><![CDATA[
arXiv:2511.21533v1 Announce Type: cross 
Abstract: Bangla Sign Language Translation (BdSLT) has been severely constrained so far as the language itself is very low resource. Standard sentence level dataset creation for BdSLT is of immense importance for developing AI based assistive tools for deaf and hard of hearing people of Bangla speaking community. In this paper, we present a dataset, IsharaKhobor , and two subset of it for enabling research. We also present the challenges towards developing the dataset and present some way forward by benchmarking with landmark based raw and RQE embedding. We do some ablation on vocabulary restriction and canonicalization of the same within the dataset, which resulted in two more datasets, IsharaKhobor_small and IsharaKhobor_canonical_small. The dataset is publicly available at: www.kaggle.com/datasets/hasanssl/isharakhobor [1].
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanisms of Non-Monotonic Scaling in Vision Transformers</title>
<link>https://arxiv.org/abs/2511.21635</link>
<guid>https://arxiv.org/abs/2511.21635</guid>
<content:encoded><![CDATA[
arXiv:2511.21635v1 Announce Type: cross 
Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Visual Object Pose Estimation</title>
<link>https://arxiv.org/abs/2511.21666</link>
<guid>https://arxiv.org/abs/2511.21666</guid>
<content:encoded><![CDATA[
arXiv:2511.21666v1 Announce Type: cross 
Abstract: Quantifying the uncertainty of an object's pose estimate is essential for robust control and planning. Although pose estimation is a well-studied robotics problem, attaching statistically rigorous uncertainty is not well understood without strict distributional assumptions. We develop distribution-free pose uncertainty bounds about a given pose estimate in the monocular setting. Our pose uncertainty only requires high probability noise bounds on pixel detections of 2D semantic keypoints on a known object. This noise model induces an implicit, non-convex set of pose uncertainty constraints. Our key contribution is SLUE (S-Lemma Uncertainty Estimation), a convex program to reduce this set to a single ellipsoidal uncertainty bound that is guaranteed to contain the true object pose with high probability. SLUE solves a relaxation of the minimum volume bounding ellipsoid problem inspired by the celebrated S-lemma. It requires no initial guess of the bound's shape or size and is guaranteed to contain the true object pose with high probability. For tighter uncertainty bounds at the same confidence, we extend SLUE to a sum-of-squares relaxation hierarchy which is guaranteed to converge to the minimum volume ellipsoidal uncertainty bound for a given set of keypoint constraints. We show this pose uncertainty bound can easily be projected to independent translation and axis-angle orientation bounds. We evaluate SLUE on two pose estimation datasets and a real-world drone tracking scenario. Compared to prior work, SLUE generates substantially smaller translation bounds and competitive orientation bounds. We release code at https://github.com/MIT-SPARK/PoseUncertaintySets.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos</title>
<link>https://arxiv.org/abs/2511.21690</link>
<guid>https://arxiv.org/abs/2511.21690</guid>
<content:encoded><![CDATA[
arXiv:2511.21690v1 Announce Type: cross 
Abstract: Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D "trace-space" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LTD: Low Temperature Distillation for Gradient Masking-free Adversarial Training</title>
<link>https://arxiv.org/abs/2111.02331</link>
<guid>https://arxiv.org/abs/2111.02331</guid>
<content:encoded><![CDATA[
arXiv:2111.02331v4 Announce Type: replace 
Abstract: Adversarial training is a widely adopted strategy to bolster the robustness of neural network models against adversarial attacks. This paper revisits the fundamental assumptions underlying image classification and suggests that representing data as one-hot labels is a key factor that leads to vulnerabilities. However, in real-world datasets, data ambiguity often arises, with samples exhibiting characteristics of multiple classes, rendering one-hot label representations imprecise. To address this, we introduce a novel approach, Low-Temperature Distillation (LTD), designed to refine label representations. Unlike previous approaches, LTD incorporates a relatively low temperature in the teacher model, while maintaining a fixed temperature for the student model during both training and inference. This strategy not only refines assumptions about data distribution but also strengthens model robustness and avoids the gradient masking problem commonly encountered in defensive distillation. Experimental results demonstrate the efficacy of the proposed method when combined with existing frameworks, achieving robust accuracy rates of 58.19%, 31.13%, and 42.08% on the CIFAR-10, CIFAR-100, and ImageNet datasets, respectively, without the need for additional data.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoPFormer: Concentrating on Prototypical Parts in Vision Transformers for Interpretable Image Recognition</title>
<link>https://arxiv.org/abs/2208.10431</link>
<guid>https://arxiv.org/abs/2208.10431</guid>
<content:encoded><![CDATA[
arXiv:2208.10431v3 Announce Type: replace 
Abstract: Prototypical part network (ProtoPNet) has drawn wide attention and boosted many follow-up studies due to its self-explanatory property for explainable artificial intelligence (XAI). However, when directly applying ProtoPNet on vision transformer (ViT) backbones, learned prototypes have a "distraction" problem: they have a relatively high probability of being activated by the background and pay less attention to the foreground. The powerful capability of modeling long-term dependency makes the transformer-based ProtoPNet hard to focus on prototypical parts, thus severely impairing its inherent interpretability. This paper proposes prototypical part transformer (ProtoPFormer) for appropriately and effectively applying the prototype-based method with ViTs for interpretable image recognition. The proposed method introduces global and local prototypes for capturing and highlighting the representative holistic and partial features of targets according to the architectural characteristics of ViTs. The global prototypes are adopted to provide the global view of objects to guide local prototypes to concentrate on the foreground while eliminating the influence of the background. Afterwards, local prototypes are explicitly supervised to concentrate on their respective prototypical visual parts, increasing the overall interpretability. Extensive experiments demonstrate that our proposed global and local prototypes can mutually correct each other and jointly make final decisions, which faithfully and transparently reason the decision-making processes associatively from the whole and local perspectives, respectively. Moreover, ProtoPFormer consistently achieves superior performance and visualization results over the state-of-the-art (SOTA) prototype-based baselines. Our code has been released at https://github.com/zju-vipa/ProtoPFormer.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMLP: Adjustable Masking Lesion Patches for Self-Supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2309.04312</link>
<guid>https://arxiv.org/abs/2309.04312</guid>
<content:encoded><![CDATA[
arXiv:2309.04312v2 Announce Type: replace 
Abstract: Self-supervised masked image modeling (MIM) methods have shown promising performances on analyzing natural images. However, directly applying such methods to medical image segmentation tasks still cannot achieve satisfactory results. The challenges arise from the facts that (i) medical images are inherently more complex compared to natural images, and the subjects in medical images often exhibit more distinct contour features; (ii) moreover, the conventional high and fixed masking ratio in MIM is likely to mask the background, limiting the scope of learnable information. To address these problems, we propose a new self-supervised medical image segmentation framework, called Adjustable Masking Lesion Patches (AMLP), which employs Masked Patch Selection~(MPS) strategy to identify patches with high probabilities of containing lesions to help model achieve precise lesion reconstruction. To improve the categorization of patches in MPS, we further introduce Relative Reconstruction Loss (RRL) to better learn hard-to-reconstruct lesion patches. Then, Category Consistency Loss (CCL) is proposed to refine patch categorization based on reconstruction difficulty, enhancing difference between lesions and backgrounds. Moreover, an Adjustable Masking Ratio (AMR) strategy is proposed to gradually increase the masking ratio over training to expand~the scope of learnable mutual information. Extensive~experiments on two medical segmentation datasets demonstrate the superior performances of the proposed AMLP w.r.t. the SOTA self-supervised methods; the results prove that AMLP effectively addresses the challenges of applying masked modeling to medical images and capturing accurate lesion details that are crucial for segmentation tasks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restoration-Oriented Video Frame Interpolation with Region-Distinguishable Priors from SAM</title>
<link>https://arxiv.org/abs/2312.15868</link>
<guid>https://arxiv.org/abs/2312.15868</guid>
<content:encoded><![CDATA[
arXiv:2312.15868v2 Announce Type: replace 
Abstract: In existing restoration-oriented Video Frame Interpolation (VFI) approaches, the motion estimation between neighboring frames plays a crucial role. However, the estimation accuracy in existing methods remains a challenge, primarily due to the inherent ambiguity in identifying corresponding areas in adjacent frames for interpolation. Therefore, enhancing accuracy by distinguishing different regions before motion estimation is of utmost importance. In this paper, we introduce a novel solution involving the utilization of open-world segmentation models, e.g., SAM2 (Segment Anything Model2) for frames, to derive Region-Distinguishable Priors (RDPs) in different frames. These RDPs are represented as spatial-varying Gaussian mixtures, distinguishing an arbitrary number of areas with a unified modality. RDPs can be integrated into existing motion-based VFI methods to enhance features for motion estimation, facilitated by our designed play-and-plug Hierarchical Region-aware Feature Fusion Module (HRFFM). HRFFM incorporates RDP into various hierarchical stages of VFI's encoder, using RDP-guided Feature Normalization (RDPFN) in a residual learning manner. With HRFFM and RDP, the features within VFI's encoder exhibit similar representations for matched regions in neighboring frames, thus improving the synthesis of intermediate frames. Extensive experiments demonstrate that HRFFM consistently enhances VFI performance across various scenes.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Framework Towards Vision-based Traffic Signal Control with Microscopic Simulation</title>
<link>https://arxiv.org/abs/2403.06884</link>
<guid>https://arxiv.org/abs/2403.06884</guid>
<content:encoded><![CDATA[
arXiv:2403.06884v2 Announce Type: replace 
Abstract: Traffic signal control (TSC) is crucial for reducing traffic congestion leading to smoother traffic flow, reduced idle time, and mitigated CO2 emissions. In this paper, we explore the computer vision approach for TSC that modulates on-road traffic flows through visual observation. Unlike traditional feature-based approaches, vision-based methods depend much less on heuristics and predefined features, bringing promising potentials for end-to-end learning and optimization of traffic signals. Thus, we introduce a simple traffic simulation framework called TrafficDojo towards vision-based TSC and its benchmark by integrating the microscopic traffic flow provided in SUMO into the 3D driving simulator MetaDrive. This proposed framework offers a versatile traffic environment for in-depth analysis and comprehensive evaluation of traffic signal controllers across diverse traffic conditions and scenarios. We establish and compare baseline algorithms including both traditional and Reinforcement Learning (RL) approaches. This work sheds light on the design and development of vision-based TSC approaches and opens up new research opportunities
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Activator: GLU Activation Function as the Core Component of a Vision Transformer</title>
<link>https://arxiv.org/abs/2405.15953</link>
<guid>https://arxiv.org/abs/2405.15953</guid>
<content:encoded><![CDATA[
arXiv:2405.15953v4 Announce Type: replace 
Abstract: The transformer architecture has driven many successes in a variety of tasks within the field of deep learning, in particular the recent advances in natural language processing (NLP) culminating with large language models (LLM). Adding to that success, transformer architecture has found widespread interest from computer vision (CV) researchers and practitioners, allowing for many advancements in vision-related tasks and opening the door for multitask and multi-modal deep learning architectures that share the same principle of operation. One drawback to these architectures is their reliance on the scaled dot product attention mechanism with the softmax activation function, which is computationally expensive and requires large compute capabilities for both training and inference. This paper investigates substituting the MLP and attention mechanism usually adopted for transformer architecture with an architecture based on incorporating a gated linear unit (GLU) activation function structure with the aim of reducing the computational cost. The equalized experimental assessments conducted in this work show that the proposed modification with the targeted reductions in computational complexity offers competitive performance compared to the selected baseline architectures. The results are significantly in support of the aims of this work, in which the focus was to extensively utilize GLU-based MLPs, establishing a more efficient but capable alternative to the traditional MLP and the attention mechanism as the core component in the design of transformer architectures.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOAP: Enhancing Spatio-Temporal Relation and Motion Information Capturing for Few-Shot Action Recognition</title>
<link>https://arxiv.org/abs/2407.16344</link>
<guid>https://arxiv.org/abs/2407.16344</guid>
<content:encoded><![CDATA[
arXiv:2407.16344v4 Announce Type: replace 
Abstract: High frame-rate (HFR) videos of action recognition improve fine-grained expression while reducing the spatio-temporal relation and motion information density. Thus, large amounts of video samples are continuously required for traditional data-driven training. However, samples are not always sufficient in real-world scenarios, promoting few-shot action recognition (FSAR) research. We observe that most recent FSAR works build spatio-temporal relation of video samples via temporal alignment after spatial feature extraction, cutting apart spatial and temporal features within samples. They also capture motion information via narrow perspectives between adjacent frames without considering density, leading to insufficient motion information capturing. Therefore, we propose a novel plug-and-play architecture for FSAR called Spatio-tempOral frAme tuPle enhancer (SOAP) in this paper. The model we designed with such architecture refers to SOAP-Net. Temporal connections between different feature channels and spatio-temporal relation of features are considered instead of simple feature extraction. Comprehensive motion information is also captured, using frame tuples with multiple frames containing more motion information than adjacent frames. Combining frame tuples of diverse frame counts further provides a broader perspective. SOAP-Net achieves new state-of-the-art performance across well-known benchmarks such as SthSthV2, Kinetics, UCF101, and HMDB51. Extensive empirical evaluations underscore the competitiveness, pluggability, generalization, and robustness of SOAP. The code is released at https://github.com/wenbohuang1002/SOAP.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Gray-box Attack against Latent Diffusion Model-based Image Editing by Posterior Collapse</title>
<link>https://arxiv.org/abs/2408.10901</link>
<guid>https://arxiv.org/abs/2408.10901</guid>
<content:encoded><![CDATA[
arXiv:2408.10901v4 Announce Type: replace 
Abstract: Recent advancements in Latent Diffusion Models (LDMs) have revolutionized image synthesis and manipulation, raising significant concerns about data misappropriation and intellectual property infringement. While adversarial attacks have been extensively explored as a protective measure against such misuse of generative AI, current approaches are severely limited by their heavy reliance on model-specific knowledge and substantial computational costs. Drawing inspiration from the posterior collapse phenomenon observed in VAE training, we propose the Posterior Collapse Attack (PCA), a novel framework for protecting images from unauthorized manipulation. Through comprehensive theoretical analysis and empirical validation, we identify two distinct collapse phenomena during VAE inference: diffusion collapse and concentration collapse. Based on this discovery, we design a unified loss function that can flexibly achieve both types of collapse through parameter adjustment, each corresponding to different protection objectives in preventing image manipulation. Our method significantly reduces dependence on model-specific knowledge by requiring access to only the VAE encoder, which constitutes less than 4\% of LDM parameters. Notably, PCA achieves prompt-invariant protection by operating on the VAE encoder before text conditioning occurs, eliminating the need for empty prompt optimization required by existing methods. This minimal requirement enables PCA to maintain adequate transferability across various VAE-based LDM architectures while effectively preventing unauthorized image editing. Extensive experiments show PCA outperforms existing techniques in protection effectiveness, computational efficiency (runtime and VRAM), and generalization across VAE-based LDM variants. Our code is available at https://github.com/ZhongliangGuo/PosteriorCollapseAttack.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Occlusion Boundary Estimation through Exploitation of Synthetic Data</title>
<link>https://arxiv.org/abs/2408.15038</link>
<guid>https://arxiv.org/abs/2408.15038</guid>
<content:encoded><![CDATA[
arXiv:2408.15038v3 Announce Type: replace 
Abstract: Occlusion boundaries (OBs) geometrically localize occlusion events in 2D images and provide critical cues for scene understanding. In this paper, we present the first systematic study of Interactive Occlusion Boundary Estimation (IOBE), introducing MS\textsuperscript{3}PE, a novel multi-scribble-guided deep-learning framework that advances IOBE through two key innovations: (1) an intuitive multi-scribble interaction mechanism, and (2) a 3-encoding-path network enhanced with multi-scale strip convolutions. Our MS\textsuperscript{3}PE surpasses adapted baselines from seven state-of-the-art interactive segmentation methods, and demonstrates strong potential for OB benchmark construction through our real-user experiment. Besides, to address the scarcity of well-annotated real-world data, we propose using synthetic data for training IOBE models, and developed Mesh2OB, the first automated tool for generating precise ground-truth OBs from 3D scenes with self-occlusions explicitly handled, enabling creation of the OB-FUTURE synthetic benchmark that facilitates generalizable training without domain adaptation. Finally, we introduce OB-LIGM, a high-quality real-world benchmark comprising 120 meticulously annotated high-resolution images advancing evaluation standards in OB research. Source code and resources are available at https://github.com/xul-ops/IOBE.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Vocabulary Monocular 3D Object Detection</title>
<link>https://arxiv.org/abs/2411.16833</link>
<guid>https://arxiv.org/abs/2411.16833</guid>
<content:encoded><![CDATA[
arXiv:2411.16833v2 Announce Type: replace 
Abstract: We propose and study open-vocabulary monocular 3D detection, a novel task that aims to detect objects of any categores in metric 3D space from a single RGB image. Existing 3D object detectors either rely on costly sensors such as LiDAR or multi-view setups, or remain confined to closed vocabularies settings with limited categories, restricting their applicability. We identify two key challenges in this new setting. First, the scarcity of 3D bounding box annotations limits the ability to train generalizable models. To reduce dependence on 3D supervision, we propose a framework that effectively integrates pretrained 2D and 3D vision foundation models. Second, missing labels and semantic ambiguities (\eg, table vs. desk) in existing datasets hinder reliable evaluation. To address this, we design a novel metric that captures model performance while mitigating annotation issues. Our approach achieves state-of-the-art results in zero-shot 3D detection of novel categories as well as in-domain detection on seen classes. We hope our method provides a strong baseline and our evaluation protocol establishes a reliable benchmark for future research.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Negative Loss: A Robust Framework for Learning with Noisy Labels</title>
<link>https://arxiv.org/abs/2412.02373</link>
<guid>https://arxiv.org/abs/2412.02373</guid>
<content:encoded><![CDATA[
arXiv:2412.02373v2 Announce Type: replace 
Abstract: Deep supervised learning has achieved remarkable success across a wide range of tasks, yet it remains susceptible to overfitting when confronted with noisy labels. To address this issue, noise-robust loss functions offer an effective solution for enhancing learning in the presence of label noise. In this work, we systematically investigate the limitation of the recently proposed Active Passive Loss (APL), which employs Mean Absolute Error (MAE) as its passive loss function. Despite the robustness brought by MAE, one of its key drawbacks is that it pays equal attention to clean and noisy samples; this feature slows down convergence and potentially makes training difficult, particularly in large-scale datasets. To overcome these challenges, we introduce a novel loss function class, termed Normalized Negative Loss Functions (NNLFs), which serve as passive loss functions within the APL framework. NNLFs effectively address the limitations of MAE by concentrating more on memorized clean samples. By replacing MAE in APL with our proposed NNLFs, we enhance APL and present a new framework called Active Negative Loss (ANL). Moreover, in non-symmetric noise scenarios, we propose an entropy-based regularization technique to mitigate the vulnerability to the label imbalance. Extensive experiments demonstrate that the new loss functions adopted by our ANL framework can achieve better or comparable performance to state-of-the-art methods across various label noise types and in image segmentation tasks. The source code is available at: https://github.com/Virusdoll/Active-Negative-Loss.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Segmentation by Diffusing, Walking and Cutting</title>
<link>https://arxiv.org/abs/2412.04678</link>
<guid>https://arxiv.org/abs/2412.04678</guid>
<content:encoded><![CDATA[
arXiv:2412.04678v2 Announce Type: replace 
Abstract: We propose an unsupervised image segmentation method using features from pre-trained text-to-image diffusion models. Inspired by classic spectral clustering approaches, we construct adjacency matrices from self-attention layers between image patches and recursively partition using Normalised Cuts. A key insight is that self-attention probability distributions, which capture semantic relations between patches, can be interpreted as a transition matrix for random walks across the image. We leverage this by first using Random Walk Normalized Cuts directly on these self-attention activations to partition the image, minimizing transition probabilities between clusters while maximizing coherence within clusters. Applied recursively, this yields a hierarchical segmentation that reflects the rich semantics in the pre-trained attention layers, without any additional training. Next, we explore other ways to build the NCuts adjacency matrix from features, and how we can use the random walk interpretation of self-attention to capture long-range relationships. Finally, we propose an approach to automatically determine the NCut cost criterion, avoiding the need to tune this manually. We quantitatively analyse the effect incorporating different features, a constant versus dynamic NCut threshold, and incorporating multi-node paths when constructing the NCuts adjacency matrix. We show that our approach surpasses all existing methods for zero-shot unsupervised segmentation, achieving state-of-the-art results on COCO-Stuff-27 and Cityscapes.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gen-3Diffusion: Realistic Image-to-3D Generation via 2D &amp; 3D Diffusion Synergy</title>
<link>https://arxiv.org/abs/2412.06698</link>
<guid>https://arxiv.org/abs/2412.06698</guid>
<content:encoded><![CDATA[
arXiv:2412.06698v2 Announce Type: replace 
Abstract: Creating realistic 3D objects and clothed avatars from a single RGB image is an attractive yet challenging problem. Due to its ill-posed nature, recent works leverage powerful prior from 2D diffusion models pretrained on large datasets. Although 2D diffusion models demonstrate strong generalization capability, they cannot guarantee the generated multi-view images are 3D consistent. In this paper, we propose Gen-3Diffusion: Realistic Image-to-3D Generation via 2D & 3D Diffusion Synergy. We leverage a pre-trained 2D diffusion model and a 3D diffusion model via our elegantly designed process that synchronizes two diffusion models at both training and sampling time. The synergy between the 2D and 3D diffusion models brings two major advantages: 1) 2D helps 3D in generalization: the pretrained 2D model has strong generalization ability to unseen images, providing strong shape priors for the 3D diffusion model; 2) 3D helps 2D in multi-view consistency: the 3D diffusion model enhances the 3D consistency of 2D multi-view sampling process, resulting in more accurate multi-view generation. We validate our idea through extensive experiments in image-based objects and clothed avatar generation tasks. Results show that our method generates realistic 3D objects and avatars with high-fidelity geometry and texture. Extensive ablations also validate our design choices and demonstrate the strong generalization ability to diverse clothing and compositional shapes. Our code and pretrained models will be publicly released on https://yuxuan-xue.com/gen-3diffusion.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Systematic Evaluation and Guidelines for Segment Anything Model in Surgical Video Analysis</title>
<link>https://arxiv.org/abs/2501.00525</link>
<guid>https://arxiv.org/abs/2501.00525</guid>
<content:encoded><![CDATA[
arXiv:2501.00525v3 Announce Type: replace 
Abstract: Surgical video segmentation is critical for AI to interpret spatial-temporal dynamics in surgery, yet model performance is constrained by limited annotated data. The SAM2 model, pretrained on natural videos, offers potential for zero-shot surgical segmentation, but its applicability in complex surgical environments, with challenges like tissue deformation and instrument variability, remains unexplored. We present the first comprehensive evaluation of the zero-shot capability of SAM2 in 9 surgical datasets (17 surgery types), covering laparoscopic, endoscopic, and robotic procedures. We analyze various prompting (points, boxes, mask) and {finetuning (dense, sparse) strategies}, robustness to surgical challenges, and generalization across procedures and anatomies. Key findings reveal that while SAM2 demonstrates notable zero-shot adaptability in structured scenarios (e.g., instrument segmentation, {multi-organ segmentation}, and scene segmentation), its performance varies under dynamic surgical conditions, highlighting gaps in handling temporal coherence and domain-specific artifacts. These results highlight future pathways to adaptive data-efficient solutions for the surgical data science field.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LASER: Lip Landmark Assisted Speaker Detection for Robustness</title>
<link>https://arxiv.org/abs/2501.11899</link>
<guid>https://arxiv.org/abs/2501.11899</guid>
<content:encoded><![CDATA[
arXiv:2501.11899v2 Announce Type: replace 
Abstract: Active Speaker Detection (ASD) aims to identify who is speaking in complex visual scenes. While humans naturally rely on lip-audio synchronization, existing ASD models often misclassify non-speaking instances when lip movements and audio are unsynchronized. To address this, we propose Lip landmark Assisted Speaker dEtection for Robustness (LASER), which explicitly incorporates lip landmarks during training to guide the model's attention to speech-relevant regions. Given a face track, LASER extracts visual features and encodes 2D lip landmarks into dense maps. To handle failure cases such as low resolution or occlusion, we introduce an auxiliary consistency loss that aligns lip-aware and face-only predictions, removing the need for landmark detectors at test time. LASER outperforms state-of-the-art models across both in-domain and out-of-domain benchmarks. To further evaluate robustness in realistic conditions, we introduce LASER-bench, a curated dataset of modern video clips with varying levels of background noise. On the high-noise subset, LASER improves mAP by 3.3 and 4.3 points over LoCoNet and TalkNet, respectively, demonstrating strong resilience to real-world acoustic challenges.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement</title>
<link>https://arxiv.org/abs/2501.14265</link>
<guid>https://arxiv.org/abs/2501.14265</guid>
<content:encoded><![CDATA[
arXiv:2501.14265v3 Announce Type: replace 
Abstract: In image enhancement tasks, such as low-light and underwater image enhancement, a degraded image can correspond to multiple plausible target images due to dynamic photography conditions. This naturally results in a one-to-many mapping problem. To address this, we propose a Bayesian Enhancement Model (BEM) that incorporates Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse outputs. To enable fast inference, we introduce a BNN-DNN framework: a BNN is first employed to model the one-to-many mapping in a low-dimensional space, followed by a Deterministic Neural Network (DNN) that refines fine-grained image details. Extensive experiments on multiple low-light and underwater image enhancement benchmarks demonstrate the effectiveness of our method.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Object Detection for Indoor Navigation Assistance: A Performance Evaluation of Real-Time Algorithms</title>
<link>https://arxiv.org/abs/2501.18444</link>
<guid>https://arxiv.org/abs/2501.18444</guid>
<content:encoded><![CDATA[
arXiv:2501.18444v2 Announce Type: replace 
Abstract: This study addresses the need for accurate and efficient object detection in assistive technologies for visually impaired individuals. We evaluate four real-time object detection algorithms YOLO, SSD, Faster R-CNN, and Mask R-CNN within the context of indoor navigation assistance. Using the Indoor Objects Detection dataset, we analyze detection accuracy, processing speed, and adaptability to indoor environments. Our findings highlight the trade-offs between precision and efficiency, offering insights into selecting optimal algorithms for realtime assistive navigation. This research advances adaptive machine learning applications, enhancing indoor navigation solutions for the visually impaired and promoting accessibility.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Consistent and Controllable Image Synthesis for Face Editing</title>
<link>https://arxiv.org/abs/2502.02465</link>
<guid>https://arxiv.org/abs/2502.02465</guid>
<content:encoded><![CDATA[
arXiv:2502.02465v3 Announce Type: replace 
Abstract: Face editing methods, essential for tasks like virtual avatars, digital human synthesis and identity preservation, have traditionally been built upon GAN-based techniques, while recent focus has shifted to diffusion-based models due to their success in image reconstruction. However, diffusion models still face challenges in controlling specific attributes and preserving the consistency of other unchanged attributes especially the identity characteristics. To address these issues and facilitate more convenient editing of face images, we propose a novel approach that leverages the power of Stable-Diffusion (SD) models and crude 3D face models to control the lighting, facial expression and head pose of a portrait photo. We observe that this task essentially involves the combinations of target background, identity and face attributes aimed to edit. We strive to sufficiently disentangle the control of these factors to enable consistency of face editing. Specifically, our method, coined as RigFace, contains: 1) A Spatial Attribute Encoder that provides presise and decoupled conditions of background, pose, expression and lighting; 2) A high-consistency FaceFusion method that transfers identity features from the Identity Encoder to the denoising UNet of a pre-trained SD model; 3) An Attribute Rigger that injects those conditions into the denoising UNet. Our model achieves comparable or even superior performance in both identity preservation and photorealism compared to existing face editing models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Without Paired Labeled Data: End-to-End Self-Supervised Learning for Drone-view Geo-Localization</title>
<link>https://arxiv.org/abs/2502.11381</link>
<guid>https://arxiv.org/abs/2502.11381</guid>
<content:encoded><![CDATA[
arXiv:2502.11381v5 Announce Type: replace 
Abstract: Drone-view Geo-Localization (DVGL) aims to achieve accurate localization of drones by retrieving the most relevant GPS-tagged satellite images. However, most existing methods heavily rely on strictly pre-paired drone-satellite images for supervised learning. When the target region shifts, new paired samples are typically required to adapt to the distribution changes. The high cost of annotation and the limited transferability of these methods significantly hinder the practical deployment of DVGL in open-world scenarios. To address these limitations, we propose a novel end-to-end self-supervised learning method with a shallow backbone network, called the dynamic memory-driven and neighborhood information learning (DMNIL) method. It employs a clustering algorithm to generate pseudo-labels and adopts a dual-path contrastive learning framework to learn discriminative intra-view representations. Furthermore, DMNIL incorporates two core modules, including the dynamic hierarchical memory learning (DHML) module and the information consistency evolution learning (ICEL) module. The DHML module combines short-term and long-term memory to enhance intra-view feature consistency and discriminability. Meanwhile, the ICEL module utilizes a neighborhood-driven dynamic constraint mechanism to systematically capture implicit cross-view semantic correlations, consequently improving cross-view feature alignment. To further stabilize and strengthen the self-supervised training process, a pseudo-label enhancement strategy is introduced to enhance the quality of pseudo supervision. Extensive experiments on three public benchmark datasets demonstrate that the proposed method consistently outperforms existing self-supervised methods and even surpasses several state-of-the-art supervised methods. Our code is available at https://github.com/ISChenawei/DMNIL.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness</title>
<link>https://arxiv.org/abs/2502.14914</link>
<guid>https://arxiv.org/abs/2502.14914</guid>
<content:encoded><![CDATA[
arXiv:2502.14914v4 Announce Type: replace 
Abstract: Visual captioning benchmarks have become outdated with the emergence of modern multimodal large language models (MLLMs), as the brief ground-truth sentences and traditional metrics fail to assess detailed captions effectively. While recent benchmarks attempt to address this by focusing on keyword extraction or object-centric evaluation, they remain limited to vague-view or object-view analyses and incomplete visual element coverage. In this paper, we introduce CAPability, a comprehensive multi-view benchmark for evaluating visual captioning across 12 dimensions spanning six critical views. We curate nearly 11K human-annotated images and videos with visual element annotations to evaluate the generated captions. CAPability stably assesses both the correctness and thoroughness of captions with \textit{precision} and \textit{hit} metrics. By converting annotations to QA pairs, we further introduce a heuristic metric, \textit{know but cannot tell} ($K\bar{T}$), indicating a significant performance gap between QA and caption capabilities. Our work provides a holistic analysis of MLLMs' captioning abilities, as we identify their strengths and weaknesses across various dimensions, guiding future research to enhance specific aspects of their capabilities.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-Independent Increment: An Efficient Approach for Multi-label Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2503.00515</link>
<guid>https://arxiv.org/abs/2503.00515</guid>
<content:encoded><![CDATA[
arXiv:2503.00515v2 Announce Type: replace 
Abstract: Current research on class-incremental learning primarily focuses on single-label classification tasks. However, real-world applications often involve multi-label scenarios, such as image retrieval and medical imaging. Therefore, this paper focuses on the challenging yet practical multi-label class-incremental learning (MLCIL) problem. In addition to the challenge of catastrophic forgetting, MLCIL encounters issues related to feature confusion, encompassing inter-session and intra-feature confusion. To address these problems, we propose a novel MLCIL approach called class-independent increment (CLIN). Specifically, in contrast to existing methods that extract image-level features, we propose a class-independent incremental network (CINet) to extract multiple class-level embeddings for multi-label samples. It learns and preserves the knowledge of different classes by constructing class-specific tokens. On this basis, we develop two novel loss functions, optimizing the learning of class-specific tokens and class-level embeddings, respectively. These losses aim to distinguish between new and old classes, further alleviating the problem of feature confusion. Extensive experiments on MS-COCO and PASCAL VOC datasets demonstrate the effectiveness of our method for improving recognition performance and mitigating forgetting on various MLCIL tasks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Limited Labels to Open Domains:An Efficient Learning Method for Drone-view Geo-Localization</title>
<link>https://arxiv.org/abs/2503.07520</link>
<guid>https://arxiv.org/abs/2503.07520</guid>
<content:encoded><![CDATA[
arXiv:2503.07520v3 Announce Type: replace 
Abstract: Traditional supervised drone-view geo-localization (DVGL) methods heavily depend on paired training data and encounter difficulties in learning cross-view correlations from unpaired data. Moreover, when deployed in a new domain, these methods require obtaining the new paired data and subsequent retraining for model adaptation, which significantly increases computational overhead. Existing unsupervised methods have enabled to generate pseudo-labels based on cross-view similarity to infer the pairing relationships. However, geographical similarity and spatial continuity often cause visually analogous features at different geographical locations. The feature confusion compromises the reliability of pseudo-label generation, where incorrect pseudo-labels drive negative optimization. Given these challenges inherent in both supervised and unsupervised DVGL methods, we propose a novel cross-domain invariant knowledge transfer network (CDIKTNet) with limited supervision, whose architecture consists of a cross-domain invariance sub-network (CDIS) and a cross-domain transfer sub-network (CDTS). This architecture facilitates a closed-loop framework for invariance feature learning and knowledge transfer. The CDIS is designed to learn cross-view structural and spatial invariance from a small amount of paired data that serves as prior knowledge. It endows the shared feature space of unpaired data with similar implicit cross-view correlations at initialization, which alleviates feature confusion. Based on this, the CDTS employs dual-path contrastive learning to further optimize each subspace while preserving consistency in a shared feature space. Extensive experiments demonstrate that CDIKTNet achieves state-of-the-art performance under full supervision compared with those supervised methods, and further surpasses existing unsupervised methods in both few-shot and cross-domain initialization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</title>
<link>https://arxiv.org/abs/2503.08594</link>
<guid>https://arxiv.org/abs/2503.08594</guid>
<content:encoded><![CDATA[
arXiv:2503.08594v2 Announce Type: replace 
Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Filter Like You Test: Data-Driven Data Filtering for CLIP Pretraining</title>
<link>https://arxiv.org/abs/2503.08805</link>
<guid>https://arxiv.org/abs/2503.08805</guid>
<content:encoded><![CDATA[
arXiv:2503.08805v3 Announce Type: replace 
Abstract: We introduce Filter Like You Test (FLYT), an algorithm for curating large-scale vision-language datasets that learns the usefulness of each data point as a pretraining example. FLYT trains a scoring model that learns to weigh each example's features using gradient signals from downstream tasks training sets. Based on FLYT, we implement Mixing-FLYT (M-FLYT), which takes the per-example scores generated by different scoring methods as features, and learns to unify them into a single score. FLYT naturally produces a distribution over the training examples, which we leverage through Soft Cap Sampling (SCS), a strategy for obtaining a filtered pretraining dataset from per-example probabilities that samples examples while preventing over-representation through a repetition penalty. Using these methods, we achieve 40.1% ImageNet zero-shot accuracy on the DataComp medium scale filtering benchmark, a 2% absolute accuracy increase over all previous results and a 5.5% increase over results that - like us - use only public resources. Our approach also yields 37.7\% on the average of 38 DataComp evaluation tasks, outperforming previous public-resource approaches by 0.4\%.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowTok: Flowing Seamlessly Across Text and Image Tokens</title>
<link>https://arxiv.org/abs/2503.10772</link>
<guid>https://arxiv.org/abs/2503.10772</guid>
<content:encoded><![CDATA[
arXiv:2503.10772v3 Announce Type: replace 
Abstract: Bridging different modalities lies at the heart of cross-modality generation. While conventional approaches treat the text modality as a conditioning signal that gradually guides the denoising process from Gaussian noise to the target image modality, we explore a much simpler paradigm-directly evolving between text and image modalities through flow matching. This requires projecting both modalities into a shared latent space, which poses a significant challenge due to their inherently different representations: text is highly semantic and encoded as 1D tokens, whereas images are spatially redundant and represented as 2D latent embeddings. To address this, we introduce FlowTok, a minimal framework that seamlessly flows across text and images by encoding images into a compact 1D token representation. Compared to prior methods, this design reduces the latent space size by 3.3x at an image resolution of 256, eliminating the need for complex conditioning mechanisms or noise scheduling. Moreover, FlowTok naturally extends to image-to-text generation under the same formulation. With its streamlined architecture centered around compact 1D tokens, FlowTok is highly memory-efficient, requires significantly fewer training resources, and achieves much faster sampling speeds-all while delivering performance comparable to state-of-the-art models. Code is available at https://github.com/TACJu/FlowTok.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OuroMamba: A Data-Free Quantization Framework for Vision Mamba</title>
<link>https://arxiv.org/abs/2503.10959</link>
<guid>https://arxiv.org/abs/2503.10959</guid>
<content:encoded><![CDATA[
arXiv:2503.10959v2 Announce Type: replace 
Abstract: We present OuroMamba, the first data-free post-training quantization (DFQ) method for vision Mamba-based models (VMMs). We identify two key challenges in enabling DFQ for VMMs, (1) VMM's recurrent state transitions restricts capturing of long-range interactions and leads to semantically weak synthetic data, (2) VMM activations exhibit dynamic outlier variations across time-steps, rendering existing static PTQ techniques ineffective. To address these challenges, OuroMamba presents a two-stage framework: (1) OuroMamba-Gen to generate semantically rich and meaningful synthetic data. It applies contrastive learning on patch level VMM features generated through neighborhood interactions in the latent state space, (2) OuroMamba-Quant to employ mixed-precision quantization with lightweight dynamic outlier detection during inference. In specific, we present a thresholding based outlier channel selection strategy for activations that gets updated every time-step. Extensive experiments across vision and generative tasks show that our data-free OuroMamba surpasses existing data-driven PTQ techniques, achieving state-of-the-art performance across diverse quantization settings. Additionally, we implement efficient GPU kernels to achieve practical latency speedup of up to 2.36x. Code and synthetic dataset are available here: https://github.com/georgia-tech-synergy-lab/ICCV-OuroMamba
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image</title>
<link>https://arxiv.org/abs/2503.17358</link>
<guid>https://arxiv.org/abs/2503.17358</guid>
<content:encoded><![CDATA[
arXiv:2503.17358v4 Announce Type: replace 
Abstract: In many robotics and VR/AR applications, fast camera motions lead to a high level of motion blur, causing existing camera pose estimation methods to fail. In this work, we propose a novel framework that leverages motion blur as a rich cue for motion estimation rather than treating it as an unwanted artifact. Our approach works by predicting a dense motion flow field and a monocular depth map directly from a single motion-blurred image. We then recover the instantaneous camera velocity by solving a linear least squares problem under the small motion assumption. In essence, our method produces an IMU-like measurement that robustly captures fast and aggressive camera movements. To train our model, we construct a large-scale dataset with realistic synthetic motion blur derived from ScanNet++v2 and further refine our model by training end-to-end on real data using our fully differentiable pipeline. Extensive evaluations on real-world benchmarks demonstrate that our method achieves state-of-the-art angular and translational velocity estimates, outperforming current methods like MASt3R and COLMAP.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stream and Query-guided Feature Aggregation for Efficient and Effective 3D Occupancy Prediction</title>
<link>https://arxiv.org/abs/2503.22087</link>
<guid>https://arxiv.org/abs/2503.22087</guid>
<content:encoded><![CDATA[
arXiv:2503.22087v2 Announce Type: replace 
Abstract: 3D occupancy prediction has become a key perception task in autonomous driving, as it enables comprehensive scene understanding. Recent methods enhance this understanding by incorporating spatiotemporal information through multi-frame fusion, but they suffer from a trade-off: dense voxel-based representations provide high accuracy at significant computational cost, whereas sparse representations improve efficiency but lose spatial detail. To mitigate this trade-off, we introduce DuOcc, which employs a dual aggregation strategy that retains dense voxel representations to preserve spatial fidelity while maintaining high efficiency. DuOcc consists of two key components: (i) Stream-based Voxel Aggregation, which recurrently accumulates voxel features over time and refines them to suppress warping-induced distortions, preserving a clear separation between occupied and free space. (ii) Query-guided Aggregation, which complements the limitations of voxel accumulation by selectively injecting instance-level query features into the voxel regions occupied by dynamic objects. Experiments on the widely used Occ3D-nuScenes and SurroundOcc datasets demonstrate that DuOcc achieves state-of-the-art performance in real-time settings, while reducing memory usage by over 40% compared to prior methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Contrast Information for Efficient Document Shadow Removal</title>
<link>https://arxiv.org/abs/2504.00385</link>
<guid>https://arxiv.org/abs/2504.00385</guid>
<content:encoded><![CDATA[
arXiv:2504.00385v2 Announce Type: replace 
Abstract: Document shadows are a major obstacle in the digitization process. Due to the dense information in text and patterns covered by shadows, document shadow removal requires specialized methods. Existing document shadow removal methods, although showing some progress, still rely on additional information such as shadow masks or lack generalization and effectiveness across different shadow scenarios. This often results in incomplete shadow removal or loss of original document content and tones. Moreover, these methods tend to underutilize the information present in the original shadowed document image. In this paper, we refocus our approach on the document images themselves, which inherently contain rich information.We propose an end-to-end document shadow removal method guided by contrast representation, following a coarse-to-fine refinement approach. By extracting document contrast information, we can effectively and quickly locate shadow shapes and positions without the need for additional masks. This information is then integrated into the refined shadow removal process, providing better guidance for network-based removal and feature fusion. Extensive qualitative and quantitative experiments show that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation</title>
<link>https://arxiv.org/abs/2504.06220</link>
<guid>https://arxiv.org/abs/2504.06220</guid>
<content:encoded><![CDATA[
arXiv:2504.06220v4 Announce Type: replace 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt powerful Foundation Models (FMs) to diverse downstream tasks while preserving and unleashing their inherent capabilities. However, we have observed that existing PEFT methods, which are often designed with natural imagery in mind, struggle when applied to Remote Sensing (RS) scenarios. This is primarily due to their inability to handle artifact influences, a problem particularly severe in RS image features. To tackle this challenge, we introduce Earth-Adapter, the first PEFT method specifically designed for RS artifacts conquering. Earth-Adapter introduces a novel Mixture of Frequency Adaptation process that combines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT). By utilizing DFT, Earth-Adapter can decompose features into different frequency components, precisely separating artifacts from original features. The MoA then dynamically assigns weights to each adapter expert, allowing for the combination of features across various frequency domains. These simple-yet-effective approaches enable Earth-Adapter to more efficiently overcome the disturbances caused by artifacts than previous PEFT methods, significantly enhancing the FMs' performance on RS scenarios. Experiments on Domain Adaptation (DA), and Domain Generalization (DG) semantic segmentation benchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline Rein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG benchmarks. Our code will be released at https://github.com/VisionXLab/Earth-Adapter.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?</title>
<link>https://arxiv.org/abs/2505.12307</link>
<guid>https://arxiv.org/abs/2505.12307</guid>
<content:encoded><![CDATA[
arXiv:2505.12307v2 Announce Type: replace 
Abstract: Recent advances in Large Multimodal Models (LMMs) have revolutionized their reasoning and Optical Character Recognition (OCR) capabilities. However, their complex logical reasoning performance on text-rich images remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark comprising 2780 questions with two subsets, i.e., LogicOCR-Gen with 1100 multi-choice questions on generated images, and LogicOCR-Real with 1680 meticulously designed free-form questions on real-world images. For constructing LogicOCR-Gen, we first curate a text corpus from the Chinese National Civil Servant Examination, and customize an automatic pipeline to steer GPT-Image-1 to generate images with varied layouts and fonts, ensuring contextual relevance and visual realism. Then, the generated images are manually verified. We evaluate a range of representative LMMs under Chain-of-Thought (CoT) and direct-answer settings. Our multi-dimensional analysis reveals key insights, such as the impact of test-time scaling, input modality differences, and sensitivity to visual-text orientation. Notably, LMMs still lag in multimodal reasoning compared to text-only inputs, indicating that they have not fully bridged visual reading with reasoning. Moreover, we propose TextCue, a training-free method that enhances LMMs' perception of image regions containing important text cues for solving questions. We leverage LMMs' attention maps and an off-the-shelf text segmentation specialist to determine the region, which is then cropped and enlarged to augment the original image. Experiments show its effectiveness, e.g., a 1.8% accuracy gain over LLaVA-OV-1.5-8B under the CoT setting. Our benchmark is available at https://github.com/MiliLab/LogicOCR.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometrically Regularized Transfer Learning with On-Manifold and Off-Manifold Perturbation</title>
<link>https://arxiv.org/abs/2505.15191</link>
<guid>https://arxiv.org/abs/2505.15191</guid>
<content:encoded><![CDATA[
arXiv:2505.15191v2 Announce Type: replace 
Abstract: Transfer learning under domain shift remains a fundamental challenge due to the divergence between source and target data manifolds. In this paper, we propose MAADA (Manifold-Aware Adversarial Data Augmentation), a novel framework that decomposes adversarial perturbations into on-manifold and off-manifold components to simultaneously capture semantic variation and model brittleness. We theoretically demonstrate that enforcing on-manifold consistency reduces hypothesis complexity and improves generalization, while off-manifold regularization smooths decision boundaries in low-density regions. Moreover, we introduce a geometry-aware alignment loss that minimizes geodesic discrepancy between source and target manifolds. Experiments on DomainNet, VisDA, and Office-Home show that MAADA consistently outperforms existing adversarial and adaptation methods in both unsupervised and few-shot settings, demonstrating superior structural robustness and cross-domain generalization.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangled Geometric Alignment with Adaptive Contrastive Perturbation for Reliable Domain Transfer</title>
<link>https://arxiv.org/abs/2505.15241</link>
<guid>https://arxiv.org/abs/2505.15241</guid>
<content:encoded><![CDATA[
arXiv:2505.15241v2 Announce Type: replace 
Abstract: Despite progress in geometry-aware domain adaptation, current methods such as GAMA still suffer from two unresolved issues: (1) insufficient disentanglement of task-relevant and task-irrelevant manifold dimensions, and (2) rigid perturbation schemes that ignore per-class alignment asymmetries. To address this, we propose GAMA++, a novel framework that introduces (i) latent space disentanglement to isolate label-consistent manifold directions from nuisance factors, and (ii) an adaptive contrastive perturbation strategy that tailors both on- and off-manifold exploration to class-specific manifold curvature and alignment discrepancy. We further propose a cross-domain contrastive consistency loss that encourages local semantic clusters to align while preserving intra-domain diversity. Our method achieves state-of-the-art results on DomainNet, Office-Home, and VisDA benchmarks under both standard and few-shot settings, with notable improvements in class-level alignment fidelity and boundary robustness. GAMA++ sets a new standard for semantic geometry alignment in transfer learning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Step Diffusion-Based Image Compression with Semantic Distillation</title>
<link>https://arxiv.org/abs/2505.16687</link>
<guid>https://arxiv.org/abs/2505.16687</guid>
<content:encoded><![CDATA[
arXiv:2505.16687v2 Announce Type: replace 
Abstract: While recent diffusion-based generative image codecs have shown impressive performance, their iterative sampling process introduces unpleasing latency. In this work, we revisit the design of a diffusion-based codec and argue that multi-step sampling is not necessary for generative compression. Based on this insight, we propose OneDC, a One-step Diffusion-based generative image Codec -- that integrates a latent compression module with a one-step diffusion generator. Recognizing the critical role of semantic guidance in one-step diffusion, we propose using the hyperprior as a semantic signal, overcoming the limitations of text prompts in representing complex visual content. To further enhance the semantic capability of the hyperprior, we introduce a semantic distillation mechanism that transfers knowledge from a pretrained generative tokenizer to the hyperprior codec. Additionally, we adopt a hybrid pixel- and latent-domain optimization to jointly enhance both reconstruction fidelity and perceptual realism. Extensive experiments demonstrate that OneDC achieves SOTA perceptual quality even with one-step generation, offering over 39% bitrate reduction and 20x faster decoding compared to prior multi-step diffusion-based codecs. Project: https://onedc-codec.github.io/
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</title>
<link>https://arxiv.org/abs/2505.19386</link>
<guid>https://arxiv.org/abs/2505.19386</guid>
<content:encoded><![CDATA[
arXiv:2505.19386v2 Announce Type: replace 
Abstract: Recent advances in video generation models have sparked interest in world models capable of simulating realistic environments. While navigation has been well-explored, physically meaningful interactions that mimic real-world forces remain largely understudied. In this work, we investigate using physical forces as a control signal for video generation and propose force prompts which enable users to interact with images through both localized point forces, such as poking a plant, and global wind force fields, such as wind blowing on fabric. We demonstrate that these force prompts can enable videos to respond realistically to physical control signals by leveraging the visual and motion prior in the original pretrained model, without using any 3D asset or physics simulator at inference. The primary challenge of force prompting is the difficulty in obtaining high quality paired force-video training data, both in the real world due to the difficulty of obtaining force signals, and in synthetic data due to limitations in the visual quality and domain diversity of physics simulators. Our key finding is that video generation models can generalize remarkably well when adapted to follow physical force conditioning from videos synthesized by Blender, even with limited demonstrations of few objects. Our method can generate videos which simulate forces across diverse geometries, settings, and materials. We also try to understand the source of this generalization and perform ablations that reveal two key elements: visual diversity and the use of specific text keywords during training. Our approach is trained on only around 15k training examples for a single day on four A100 GPUs, and outperforms existing methods on force adherence and physics realism, bringing world models closer to real-world physics interactions. We release all datasets, code, weights, and interactive video demos at our project page.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation</title>
<link>https://arxiv.org/abs/2505.20935</link>
<guid>https://arxiv.org/abs/2505.20935</guid>
<content:encoded><![CDATA[
arXiv:2505.20935v2 Announce Type: replace 
Abstract: Text-to-image diffusion models have recently become highly capable, yet their behavior in multi-object scenes remains unreliable: models often produce an incorrect number of instances and exhibit semantics leaking across objects. We trace these failures to vague instance boundaries; self-attention already reveals instance layouts early in the denoising process, but existing approaches act only on semantic signals. We introduce $\textbf{ISAC}$ ($\textbf{I}$nstance-to-$\textbf{S}$emantic $\textbf{A}$ttention $\textbf{C}$ontrol), a training-free, model-agnostic objective that performs hierarchical attention control by first carving out instance layouts from self-attention and then binding semantics to these instances. In Phase 1, ISAC clusters self-attention into the number of instances and repels overlaps, establishing an instance-level structural hierarchy; in Phase 2, it injects these instance cues into cross-attention to obtain instance-aware semantic masks and decomposes mixing semantics by tying attributes within each instance. ISAC yields consistent gains on T2I-CompBench, HRS-Bench, and IntraCompBench, our new benchmark for intra-class compositions where failures are most frequent, with improvements of at least 50% in multi-class accuracy and 7% in multi-instance accuracy on IntraCompBench, without any fine-tuning or external models. Beyond text-to-image setups, ISAC also strengthens layout-to-image controllers under overlapping boxes by refining coarse box layouts into dense instance masks, indicating that hierarchical decoupling of instance formation and semantic assignment is a key principle for robust, controllable multi-object generation. Code will be released upon publication.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-Denoised Hyperspectral Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.21890</link>
<guid>https://arxiv.org/abs/2505.21890</guid>
<content:encoded><![CDATA[
arXiv:2505.21890v3 Announce Type: replace 
Abstract: Hyperspectral imaging (HSI) has been widely used in agricultural applications for non-destructive estimation of plant nutrient composition and precise quantification of sample nutritional elements. Recently, 3D reconstruction methods, such as Neural Radiance Field (NeRF), have been used to create implicit neural representations of HSI scenes. This capability enables the rendering of hyperspectral channel compositions at every spatial location, thereby helping localize the target object's nutrient composition both spatially and spectrally. However, it faces limitations in training time and rendering speed. In this paper, we propose Diffusion-Denoised Hyperspectral Gaussian Splatting (DD-HGS), which enhances the state-of-the-art 3D Gaussian Splatting (3DGS) method with wavelength-aware spherical harmonics, a Kullback-Leibler divergence-based spectral loss, and a diffusion-based denoiser to enable 3D explicit reconstruction of the hyperspectral scenes for the entire spectral range. We present extensive evaluations on diverse real-world hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of our DD-HGS. The results demonstrate that DD-HGS achieves the new state-of-the-art performance compared to all the previously published methods. Project page: https://dragonpg2000.github.io/DDHGS-website/
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation</title>
<link>https://arxiv.org/abs/2505.22105</link>
<guid>https://arxiv.org/abs/2505.22105</guid>
<content:encoded><![CDATA[
arXiv:2505.22105v2 Announce Type: replace 
Abstract: Power transmission corridor hazard segmentation (PTCHS) aims to separate transmission equipment and surrounding hazards from complex background, conveying great significance to maintaining electric power transmission safety. Recently, the Segment Anything Model (SAM) has emerged as a foundational vision model and pushed the boundaries of segmentation tasks. However, SAM struggles to deal with the target objects in complex transmission corridor scenario, especially those with fine structure. In this paper, we propose ELE-SAM, adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt Adapter to achieve better prompt tokens via incorporating global-local features and focusing more on key regions. Subsequently, to tackle the hazard objects with fine structure in complex background, we design a High-Fidelity Mask Decoder by leveraging multi-granularity mask features and then scaling them to a higher resolution. Moreover, to train ELE-SAM and advance this field, we construct the ELE-40K benchmark, the first large-scale and real-world dataset for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K demonstrate the superior performance that ELE-SAM outperforms the baseline model with the average 16.8% mIoU and 20.6% mBIoU performance improvement. Moreover, compared with the state-of-the-art method on HQSeg-44K, the average 2.9% mIoU and 3.8% mBIoU absolute improvements further validate the effectiveness of our method on high-quality generic object segmentation. The source code and dataset are available at https://github.com/Hhaizee/ELE-SAM.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision Remember: Recovering Visual Information in Efficient LVLM with Vision Feature Resampling</title>
<link>https://arxiv.org/abs/2506.03928</link>
<guid>https://arxiv.org/abs/2506.03928</guid>
<content:encoded><![CDATA[
arXiv:2506.03928v2 Announce Type: replace 
Abstract: The computational expense of redundant vision tokens in Large Vision-Language Models (LVLMs) has led many existing methods to compress them via a vision projector. However, this compression may lose visual information that is crucial for tasks relying on fine-grained spatial relationships, such as OCR and Chart&amp;Table Understanding. In this paper, we propose to resample original vision features across the LLM decoder layers to recover visual information and attain efficiency. Following this principle, we introduce Vision Remember, which includes two key modules: (1) Token-Feature Cross-Attention Layer and (2) Token Bidirectional Self-Attention Layer. In the Token bidirectional attention, we employ self-attention mechanism to maintain the bidirectional interaction between vision tokens and the text-guided token. In the Token-Feature interaction attention, we introduce local cross-attention to resample the visual feature and utilize the multi-level fusion to enrich the visual representation. We conduct comprehensive experiments on multiple visual understanding benchmarks and the results with the LLaVA-NeXT baseline show that Vision Remember outperforms TokenPacker by +2.7 and FastV by +5.7 across nearly all the settings. Compared with previous vision feature re-fusion methods, our approach also surpasses DeepStack by +3.9 and SVA Aggregator by +3.4 on the same baseline. The experimental results validate the generalization capability of the proposed method when combined with various efficient vision projectors and LVLMs.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training</title>
<link>https://arxiv.org/abs/2506.04263</link>
<guid>https://arxiv.org/abs/2506.04263</guid>
<content:encoded><![CDATA[
arXiv:2506.04263v2 Announce Type: replace 
Abstract: Adversarial training is among the most effective strategies for defending deep neural networks against adversarial examples. A key limitation of existing adversarial training approaches lies in their reliance on a fixed perturbation budget, which fails to account for instance-specific robustness characteristics. While prior works such as IAAT and MMA introduce instance-level adaptations, they often rely on heuristic or static approximations of data robustness. In this paper, we propose Dynamic Epsilon Scheduling (DES), a novel framework that adaptively adjusts the adversarial perturbation budget per instance and per training iteration. DES integrates three key factors: (1) the distance to the decision boundary approximated via gradient-based proxies, (2) prediction confidence derived from softmax entropy, and (3) model uncertainty estimated via Monte Carlo dropout. By combining these cues into a unified scheduling strategy, DES tailors the perturbation budget dynamically to guide more effective adversarial learning. Experimental results on CIFAR-10 and CIFAR-100 show that our method consistently improves both adversarial robustness and standard accuracy compared to fixed-epsilon baselines and prior adaptive methods. Moreover, we provide theoretical insights into the stability and convergence of our scheduling policy. This work opens a new avenue for instance-aware, data-driven adversarial training methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetricHMSR:Metric Human Mesh and Scene Recovery from Monocular Images</title>
<link>https://arxiv.org/abs/2506.09919</link>
<guid>https://arxiv.org/abs/2506.09919</guid>
<content:encoded><![CDATA[
arXiv:2506.09919v2 Announce Type: replace 
Abstract: We introduce MetricHMSR (Metric Human Mesh and Scene Recovery), a novel approach for metric human mesh and scene recovery from monocular images. Due to unrealistic assumptions in the camera model and inherent challenges in metric perception, existing approaches struggle to achieve human pose and metric 3D position estimation through a unified module. To address this limitation, MetricHMSR incorporates camera rays to comprehensively encode both the bounding box information and the intrinsic parameters of perspective projection. Then we proposed Human Mixture-of-Experts (MoE), the model dynamically routes image features and ray features to task-specific experts for specialized understanding of different data aspects, enabling a unified framework that simultaneously perceives the local pose and the global 3D position. Based on the results above, we further refine the existing monocular metric depth estimation method to achieve more accurate results, ultimately enabling the seamless overlay of humans and scenes in 3D space. Comprehensive experimental results demonstrate that the proposed method achieves state-of-the-art performance on both human mesh and scene recovery.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking the Trustworthiness in Multimodal LLMs for Video Understanding</title>
<link>https://arxiv.org/abs/2506.12336</link>
<guid>https://arxiv.org/abs/2506.12336</guid>
<content:encoded><![CDATA[
arXiv:2506.12336v3 Announce Type: replace 
Abstract: Recent advancements in multimodal large language models for video understanding (videoLLMs) have enhanced their capacity to process complex spatiotemporal data. However, challenges such as factual inaccuracies, harmful content, biases, hallucinations, and privacy risks compromise their reliability. This study introduces Trust-videoLLMs, a first comprehensive benchmark evaluating 23 state-of-the-art videoLLMs (5 commercial, 18 open-source) across five critical dimensions: truthfulness, robustness, safety, fairness, and privacy. Comprising 30 tasks with adapted, synthetic, and annotated videos, the framework assesses spatiotemporal risks, temporal consistency and cross-modal impact. Results reveal significant limitations in dynamic scene comprehension, cross-modal perturbation resilience and real-world risk mitigation. While open-source models occasionally outperform, proprietary models generally exhibit superior credibility, though scaling does not consistently improve performance. These findings underscore the need for enhanced training datat diversity and robust multimodal alignment. Trust-videoLLMs provides a publicly available, extensible toolkit for standardized trustworthiness assessments, addressing the critical gap between accuracy-focused benchmarks and demands for robustness, safety, fairness, and privacy.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Normals of Noisy Points by Local Gradient-Aware Surface Filtering</title>
<link>https://arxiv.org/abs/2507.03394</link>
<guid>https://arxiv.org/abs/2507.03394</guid>
<content:encoded><![CDATA[
arXiv:2507.03394v2 Announce Type: replace 
Abstract: Estimating normals for noisy point clouds is a persistent challenge in 3D geometry processing, particularly for end-to-end oriented normal estimation. Existing methods generally address relatively clean data and rely on supervised priors to fit local surfaces within specific neighborhoods. In this paper, we propose a novel approach for learning normals from noisy point clouds through local gradient-aware surface filtering. Our method projects noisy points onto the underlying surface by utilizing normals and distances derived from an implicit function constrained by local gradients. We start by introducing a distance measurement operator for global surface fitting on noisy data, which integrates projected distances along normals. Following this, we develop an implicit field-based filtering approach for surface point construction, adding projection constraints on these points during filtering. To address issues of over-smoothing and gradient degradation, we further incorporate local gradient consistency constraints, as well as local gradient orientation and aggregation. Comprehensive experiments on normal estimation, surface reconstruction, and point cloud denoising demonstrate the state-of-the-art performance of our method. The source code and trained models are available at https://github.com/LeoQLi/LGSF.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction</title>
<link>https://arxiv.org/abs/2507.06806</link>
<guid>https://arxiv.org/abs/2507.06806</guid>
<content:encoded><![CDATA[
arXiv:2507.06806v3 Announce Type: replace 
Abstract: Plant traits such as leaf carbon content and leaf mass are essential variables in the study of biodiversity and climate change. However, conventional field sampling cannot feasibly cover trait variation at ecologically meaningful spatial scales. Machine learning represents a valuable solution for plant trait prediction across ecosystems, leveraging hyperspectral data from remote sensing. Nevertheless, trait prediction from hyperspectral data is challenged by label scarcity and substantial domain shifts (\eg across sensors, ecological distributions), requiring robust cross-domain methods. Here, we present GreenHyperSpectra, a pretraining dataset encompassing real-world cross-sensor and cross-ecosystem samples designed to benchmark trait prediction with semi- and self-supervised methods. We adopt an evaluation framework encompassing in-distribution and out-of-distribution scenarios. We successfully leverage GreenHyperSpectra to pretrain label-efficient multi-output regression models that outperform the state-of-the-art supervised baseline. Our empirical analyses demonstrate substantial improvements in learning spectral representations for trait prediction, establishing a comprehensive methodological framework to catalyze research at the intersection of representation learning and plant functional traits assessment. All code and data are available at: https://github.com/echerif18/HyspectraSSL.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrast-Prior Enhanced Duality for Mask-Free Shadow Removal</title>
<link>https://arxiv.org/abs/2507.21949</link>
<guid>https://arxiv.org/abs/2507.21949</guid>
<content:encoded><![CDATA[
arXiv:2507.21949v2 Announce Type: replace 
Abstract: Existing shadow removal methods often rely on shadow masks, which are challenging to acquire in real-world scenarios. Exploring intrinsic image cues, such as local contrast information, presents a potential alternative for guiding shadow removal in the absence of explicit masks. However, the cue's inherent ambiguity becomes a critical limitation in complex scenes, where it can fail to distinguish true shadows from low-reflectance objects and intricate background textures. To address this motivation, we propose the Adaptive Gated Dual-Branch Attention (AGBA) mechanism. AGBA dynamically filters and re-weighs the contrast prior to effectively disentangle shadow features from confounding visual elements. Furthermore, to tackle the persistent challenge of restoring soft shadow boundaries and fine-grained details, we introduce a diffusion-based Frequency-Contrast Fusion Network (FCFN) that leverages high-frequency and contrast cues to guide the generative process. Extensive experiments demonstrate that our method achieves state-of-the-art results among mask-free approaches while maintaining competitive performance relative to mask-based methods.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonAct: Progressive Training for Fine-Grained Video Reasoning in Small Models</title>
<link>https://arxiv.org/abs/2508.01533</link>
<guid>https://arxiv.org/abs/2508.01533</guid>
<content:encoded><![CDATA[
arXiv:2508.01533v2 Announce Type: replace 
Abstract: While recent multimodal models have shown progress in vision-language tasks, small-scale variants still struggle with the fine-grained temporal reasoning required for video understanding. We introduce ReasonAct, a method that enhances video reasoning in smaller models through a three-stage training process: first building a foundation with text-only reasoning, then fine-tuning on video, and finally refining with temporal-aware reinforcement learning. We build upon Temporal Group Relative Policy Optimization (T-GRPO) by incorporating temporal consistency modeling into policy optimization. We also propose a biomechanically-motivated sub-action decomposition mechanism that provides graduated rewards for constituent action phases. Through experiments on HMDB51, UCF-101, and Kinetics-400, our 3B-parameter model achieves 67.2%, 94.1%, and 78.9% accuracy respectively, demonstrating improvements of 17.9, 15.8, and 12.3 points over baselines. Ablation studies validate that our progressive training enables smaller models to achieve competitive video reasoning performance while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning</title>
<link>https://arxiv.org/abs/2508.10133</link>
<guid>https://arxiv.org/abs/2508.10133</guid>
<content:encoded><![CDATA[
arXiv:2508.10133v2 Announce Type: replace 
Abstract: Multimodal learning has gained much success in recent years. However, current multimodal fusion methods adopt the attention mechanism of Transformers to implicitly learn the underlying correlation of multimodal features. As a result, the multimodal model cannot capture the essential features of each modality, making it difficult to comprehend complex structures and correlations of multimodal inputs. This paper introduces a novel Multimodal Attention-based Normalizing Flow (MANGO) approach to developing explicit, interpretable, and tractable multimodal fusion learning. In particular, we propose a new Invertible Cross-Attention (ICA) layer to develop the Normalizing Flow-based Model for multimodal data. To efficiently capture the complex, underlying correlations in multimodal data in our proposed invertible cross-attention layer, we propose three new cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based Normalizing Flow to enable the scalability of our proposed method to high-dimensional multimodal data. Our experimental results on three different multimodal learning tasks, i.e., semantic segmentation, image-to-image translation, and movie genre classification, have illustrated the state-of-the-art (SoTA) performance of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Spatially Consistent Image Generation: On Incorporating Intrinsic Scene Properties into Diffusion Models</title>
<link>https://arxiv.org/abs/2508.10382</link>
<guid>https://arxiv.org/abs/2508.10382</guid>
<content:encoded><![CDATA[
arXiv:2508.10382v2 Announce Type: replace 
Abstract: Image generation models trained on large datasets can synthesize high-quality images but often produce spatially inconsistent and distorted images due to limited information about the underlying structures and spatial layouts. In this work, we leverage intrinsic scene properties (e.g., depth, segmentation maps) that provide rich information about the underlying scene, unlike prior approaches that solely rely on image-text pairs or use intrinsics as conditional inputs. Our approach aims to co-generate both images and their corresponding intrinsics, enabling the model to implicitly capture the underlying scene structure and generate more spatially consistent and realistic images. Specifically, we first extract rich intrinsic scene properties from a large image dataset with pre-trained estimators, eliminating the need for additional scene information or explicit 3D representations. We then aggregate various intrinsic scene properties into a single latent variable using an autoencoder. Building upon pre-trained large-scale Latent Diffusion Models (LDMs), our method simultaneously denoises the image and intrinsic domains by carefully sharing mutual information so that the image and intrinsic reflect each other without degrading image quality. Experimental results demonstrate that our method corrects spatial inconsistencies and produces a more natural layout of scenes while maintaining the fidelity and textual alignment of the base model (e.g., Stable Diffusion).
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models</title>
<link>https://arxiv.org/abs/2508.14264</link>
<guid>https://arxiv.org/abs/2508.14264</guid>
<content:encoded><![CDATA[
arXiv:2508.14264v2 Announce Type: replace 
Abstract: Large multimodal models (LMMs) have gained impressive performance due to their outstanding capability in various understanding tasks. However, these models still suffer from some fundamental limitations related to robustness and generalization due to the alignment and correlation between visual and textual features. In this paper, we introduce a simple but efficient learning mechanism for improving the robust alignment between visual and textual modalities by solving shuffling problems. In particular, the proposed approach can improve reasoning capability, visual understanding, and cross-modality alignment by introducing two new tasks: reconstructing the image order and the text order into the LMM's pre-training and fine-tuning phases. In addition, we propose a new directed-token approach to capture visual and textual knowledge, enabling the capability to reconstruct the correct order of visual inputs. Then, we introduce a new Image-to-Response Guided loss to further improve the visual understanding of the LMM in its responses. The proposed approach consistently achieves state-of-the-art (SoTA) performance compared with prior LMMs on academic task-oriented and instruction-following LMM benchmarks.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics</title>
<link>https://arxiv.org/abs/2508.17247</link>
<guid>https://arxiv.org/abs/2508.17247</guid>
<content:encoded><![CDATA[
arXiv:2508.17247v2 Announce Type: replace 
Abstract: With the rapid evolution of deepfake technologies and the wide dissemination of digital media, personal privacy is facing increasingly serious security threats. Deepfake proactive forensics, which involves embedding imperceptible watermarks to enable reliable source tracking, serves as a crucial defense against these threats. Although existing methods show strong forensic ability, they rely on an idealized assumption of single watermark embedding, which proves impractical in real-world scenarios. In this paper, we formally define and demonstrate the existence of Multi-Embedding Attacks (MEA) for the first time. When a previously protected image undergoes additional rounds of watermark embedding, the original forensic watermark can be destroyed or removed, rendering the entire proactive forensic mechanism ineffective. To address this vulnerability, we propose a general training paradigm named Adversarial Interference Simulation (AIS). Rather than modifying the network architecture, AIS explicitly simulates MEA scenarios during fine-tuning and introduces a resilience-driven loss function to enforce the learning of sparse and stable watermark representations. Our method enables the model to maintain the ability to extract the original watermark correctly even after a second embedding. Extensive experiments demonstrate that our plug-and-play AIS training paradigm significantly enhances the robustness of various existing methods against MEA.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastAvatar: Instant 3D Gaussian Splatting for Faces from Single Unconstrained Poses</title>
<link>https://arxiv.org/abs/2508.18389</link>
<guid>https://arxiv.org/abs/2508.18389</guid>
<content:encoded><![CDATA[
arXiv:2508.18389v2 Announce Type: replace 
Abstract: We present FastAvatar, a fast and robust algorithm for single-image 3D face reconstruction using 3D Gaussian Splatting (3DGS). Given a single input image from an arbitrary pose, FastAvatar recovers a high-quality, full-head 3DGS avatar in approximately 3 seconds on a single NVIDIA A100 GPU. We use a two-stage design: a feed-forward encoder-decoder predicts coarse face geometry by regressing Gaussian structure from a pose-invariant identity embedding, and a lightweight test-time refinement stage then optimizes the appearance parameters for photorealistic rendering. This hybrid strategy combines the speed and stability of direct prediction with the accuracy of optimization, enabling strong identity preservation even under extreme input poses. FastAvatar achieves state-of-the-art reconstruction quality (24.01 dB PSNR, 0.91 SSIM) while running over 600x faster than existing per-subject optimization methods (e.g., FlashAvatar, GaussianAvatars, GASP). Once reconstructed, our avatars support photorealistic novel-view synthesis and FLAME-guided expression animation, enabling controllable reenactment from a single image. By jointly offering high fidelity, robustness to pose, and rapid reconstruction, FastAvatar significantly broadens the applicability of 3DGS-based facial avatars.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Splits Are Equal: Rethinking Attribute Generalization Across Unrelated Categories</title>
<link>https://arxiv.org/abs/2509.06998</link>
<guid>https://arxiv.org/abs/2509.06998</guid>
<content:encoded><![CDATA[
arXiv:2509.06998v2 Announce Type: replace 
Abstract: Can models generalize attribute knowledge across semantically and perceptually dissimilar categories? While prior work has addressed attribute prediction within narrow taxonomic or visually similar domains, it remains unclear whether current models can abstract attributes and apply them to conceptually distant categories. This work presents the first explicit evaluation for the robustness of the attribute prediction task under such conditions, testing whether models can correctly infer shared attributes between unrelated object types: e.g., identifying that the attribute "has four legs" is common to both "dogs" and "chairs". To enable this evaluation, we introduce train-test split strategies that progressively reduce correlation between training and test sets, based on: LLM-driven semantic grouping, embedding similarity thresholding, embedding-based clustering, and supercategory-based partitioning using ground-truth labels. Results show a sharp drop in performance as the correlation between training and test categories decreases, indicating strong sensitivity to split design. Among the evaluated methods, clustering yields the most effective trade-off, reducing hidden correlations while preserving learnability. These findings offer new insights into the limitations of current representations and inform future benchmark construction for attribute reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture</title>
<link>https://arxiv.org/abs/2509.12247</link>
<guid>https://arxiv.org/abs/2509.12247</guid>
<content:encoded><![CDATA[
arXiv:2509.12247v2 Announce Type: replace 
Abstract: Efficient nutrient management is critical for crop growth and sustainable resource consumption (e.g., nitrogen, energy). Current approaches require lengthy analyses, preventing real-time optimization; similarly, imaging facilitates rapid phenotyping but can be computationally intensive, preventing deployment under resource constraints. This study proposes a flexible, tiered pipeline for anomaly detection and status estimation (fresh weight, dry mass, and tissue nutrients), including a comprehensive energy analysis of approaches that span the efficiency-accuracy spectrum. Using a nutrient depletion experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer strength) and multispectral imaging (MSI), we developed a hierarchical pipeline using an autoencoder (AE) for early warning. Further, we compared two status estimation modules of different complexity for more detailed analysis: vegetation index (VI) features with machine learning (Random Forest, RF) and raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated high-efficiency anomaly detection (73% net detection of T3 samples 9 days after transplanting) at substantially lower energy than embodied energy in wasted nitrogen. The state estimation modules show trade-offs, with ViT outperforming RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at higher energy cost. With our modular pipeline, this work opens opportunities for edge diagnostics and practical opportunities for agricultural sustainability.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</title>
<link>https://arxiv.org/abs/2509.12718</link>
<guid>https://arxiv.org/abs/2509.12718</guid>
<content:encoded><![CDATA[
arXiv:2509.12718v2 Announce Type: replace 
Abstract: Most existing spatial reasoning benchmarks focus on static or globally observable environments, failing to capture the challenges of long-horizon reasoning and memory utilization under partial observability and dynamic changes. We introduce two dynamic spatial benchmarks, locally observable maze navigation and match-2 elimination that systematically evaluate models' abilities in spatial understanding and adaptive planning when local perception, environment feedback, and global objectives are tightly coupled. Each action triggers structural changes in the environment, requiring continuous update of cognition and strategy. We further propose a subjective experience-based memory mechanism for cross-task experience transfer and validation. Experiments show that our benchmarks reveal key limitations of mainstream models in dynamic spatial reasoning and long-term memory, providing a comprehensive platform for future methodological advances. Our code and data are available at https://anonymous.4open.science/r/EvoEmpirBench-143C/.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration</title>
<link>https://arxiv.org/abs/2509.17429</link>
<guid>https://arxiv.org/abs/2509.17429</guid>
<content:encoded><![CDATA[
arXiv:2509.17429v3 Announce Type: replace 
Abstract: Accurate temporal prediction is the bridge between comprehensive scene understanding and embodied artificial intelligence. However, predicting multiple fine-grained states of a scene at multiple temporal scales is difficult for vision-language models. We formalize the Multi-Scale Temporal Prediction (MSTP) task in general and surgical scenes by decomposing multi-scale into two orthogonal dimensions: the temporal scale, forecasting states of humans and surgery at varying look-ahead intervals, and the state scale, modeling a hierarchy of states in general and surgical scenes. For example, in general scenes, states of contact relationships are finer-grained than states of spatial relationships. In surgical scenes, medium-level steps are finer-grained than high-level phases yet remain constrained by their encompassing phase. To support this unified task, we introduce the first MSTP Benchmark, featuring synchronized annotations across multiple state scales and temporal scales. We further propose a method, Incremental Generation and Multi-agent Collaboration (IG-MC), which integrates two key innovations. First, we present a plug-and-play incremental generation module that continuously synthesizes up-to-date visual previews at expanding temporal scales to inform multiple decision-making agents, keeping decisions and generated visuals synchronized and preventing performance degradation as look-ahead intervals lengthen. Second, we present a decision-driven multi-agent collaboration framework for multi-state prediction, comprising generation, initiation, and multi-state assessment agents that dynamically trigger and evaluate prediction cycles to balance global coherence and local fidelity.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ControlEvents: Controllable Synthesis of Event Camera Datawith Foundational Prior from Image Diffusion Models</title>
<link>https://arxiv.org/abs/2509.22864</link>
<guid>https://arxiv.org/abs/2509.22864</guid>
<content:encoded><![CDATA[
arXiv:2509.22864v2 Announce Type: replace 
Abstract: In recent years, event cameras have gained significant attention due to their bio-inspired properties, such as high temporal resolution and high dynamic range. However, obtaining large-scale labeled ground-truth data for event-based vision tasks remains challenging and costly. In this paper, we present ControlEvents, a diffusion-based generative model designed to synthesize high-quality event data guided by diverse control signals such as class text labels, 2D skeletons, and 3D body poses. Our key insight is to leverage the diffusion prior from foundation models, such as Stable Diffusion, enabling high-quality event data generation with minimal fine-tuning and limited labeled data. Our method streamlines the data generation process and significantly reduces the cost of producing labeled event datasets. We demonstrate the effectiveness of our approach by synthesizing event data for visual recognition, 2D skeleton estimation, and 3D body pose estimation. Our experiments show that the synthesized labeled event data enhances model performance in all tasks. Additionally, our approach can generate events based on unseen text labels during training, illustrating the powerful text-based generation capabilities inherited from foundation models.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</title>
<link>https://arxiv.org/abs/2510.05613</link>
<guid>https://arxiv.org/abs/2510.05613</guid>
<content:encoded><![CDATA[
arXiv:2510.05613v2 Announce Type: replace 
Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Neural Architecture Design for Industrial Defect Detection</title>
<link>https://arxiv.org/abs/2510.06669</link>
<guid>https://arxiv.org/abs/2510.06669</guid>
<content:encoded><![CDATA[
arXiv:2510.06669v2 Announce Type: replace 
Abstract: Industrial surface defect detection (SDD) is critical for ensuring product quality and manufacturing reliability. Due to the diverse shapes and sizes of surface defects, SDD faces two main challenges: intraclass difference and interclass similarity. Existing methods primarily utilize manually designed models, which require extensive trial and error and often struggle to address both challenges effectively. To overcome this, we propose AutoNAD, an automated neural architecture design framework for SDD that jointly searches over convolutions, transformers, and multi-layer perceptrons. This hybrid design enables the model to capture both fine-grained local variations and long-range semantic context, addressing the two key challenges while reducing the cost of manual network design. To support efficient training of such a diverse search space, AutoNAD introduces a cross weight sharing strategy, which accelerates supernet convergence and improves subnet performance. Additionally, a searchable multi-level feature aggregation module (MFAM) is integrated to enhance multi-scale feature learning. Beyond detection accuracy, runtime efficiency is essential for industrial deployment. To this end, AutoNAD incorporates a latency-aware prior to guide the selection of efficient architectures. The effectiveness of AutoNAD is validated on three industrial defect datasets and further applied within a defect imaging and detection platform. Code is available at https://github.com/Yuxi104/AutoNAD.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XYZCylinder: Towards Compatible Feed-Forward 3D Gaussian Splatting for Driving Scenes via Unified Cylinder Lifting Method</title>
<link>https://arxiv.org/abs/2510.07856</link>
<guid>https://arxiv.org/abs/2510.07856</guid>
<content:encoded><![CDATA[
arXiv:2510.07856v2 Announce Type: replace 
Abstract: Feed-forward paradigms for 3D reconstruction have become a focus of recent research, which learn implicit, fixed view transformations to generate a single scene representation. However, their application to complex driving scenes reveals significant limitations. Two core challenges are responsible for this performance gap. First, the reliance on a fixed view transformation hinders compatibility to varying camera configurations. Second, the inherent difficulty of learning complex driving scenes from sparse 360{\deg} views with minimal overlap compromises the final reconstruction fidelity. To handle these difficulties, we introduce XYZCylinder, a novel method built upon a unified cylinder lifting method that integrates camera modeling and feature lifting. To tackle the compatibility problem, we design a Unified Cylinder Camera Modeling (UCCM) strategy. This strategy explicitly models projection parameters to unify diverse camera setups, thus bypassing the need for learning viewpoint-dependent correspondences. To improve the reconstruction accuracy, we propose a hybrid representation with several dedicated modules based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image features to 3D space. Extensive evaluations confirm that XYZCylinder not only achieves state-of-the-art performance under different evaluation settings but also demonstrates remarkable compatibility in entirely new scenes with different camera settings in a zero-shot manner. Project page: \href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation</title>
<link>https://arxiv.org/abs/2510.10160</link>
<guid>https://arxiv.org/abs/2510.10160</guid>
<content:encoded><![CDATA[
arXiv:2510.10160v2 Announce Type: replace 
Abstract: Referring Image Segmentation (RIS) aims to segment the target object in an image given a natural language expression. While recent methods leverage pre-trained vision backbones and more training corpus to achieve impressive results, they predominantly focus on simple expressions--short, clear noun phrases like "red car" or "left girl". This simplification often reduces RIS to a key word/concept matching problem, limiting the model's ability to handle referential ambiguity in expressions. In this work, we identify two challenging real-world scenarios: object-distracting expressions, which involve multiple entities with contextual cues, and category-implicit expressions, where the object class is not explicitly stated. To address the challenges, we propose a novel framework, SaFiRe, which mimics the human two-phase cognitive process--first forming a global understanding, then refining it through detail-oriented inspection. This is naturally supported by Mamba's scan-then-update property, which aligns with our phased design and enables efficient multi-cycle refinement with linear complexity. We further introduce aRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous referring expressions. Extensive experiments on both standard and proposed datasets demonstrate the superiority of SaFiRe over state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment</title>
<link>https://arxiv.org/abs/2510.11473</link>
<guid>https://arxiv.org/abs/2510.11473</guid>
<content:encoded><![CDATA[
arXiv:2510.11473v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting has recently emerged as an efficient solution for high-quality and real-time novel view synthesis. However, its capability for accurate surface reconstruction remains underexplored. Due to the discrete and unstructured nature of Gaussians, supervision based solely on image rendering loss often leads to inaccurate geometry and inconsistent multi-view alignment. In this work, we propose a novel method that enhances the geometric representation of 3D Gaussians through view alignment (VA). Specifically, we incorporate edge-aware image cues into the rendering loss to improve surface boundary delineation. To enforce geometric consistency across views, we introduce a visibility-aware photometric alignment loss that models occlusions and encourages accurate spatial relationships among Gaussians. To further mitigate ambiguities caused by lighting variations, we incorporate normal-based constraints to refine the spatial orientation of Gaussians and improve local surface estimation. Additionally, we leverage deep image feature embeddings to enforce cross-view consistency, enhancing the robustness of the learned geometry under varying viewpoints and illumination. Extensive experiments on standard benchmarks demonstrate that our method achieves state-of-the-art performance in both surface reconstruction and novel view synthesis. The source code is available at https://github.com/LeoQLi/VA-GS.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decorrelation Speeds Up Vision Transformers</title>
<link>https://arxiv.org/abs/2510.14657</link>
<guid>https://arxiv.org/abs/2510.14657</guid>
<content:encoded><![CDATA[
arXiv:2510.14657v2 Announce Type: replace 
Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label data regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by nitegrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. To mimic constrained-data scenarios, we evaluate our approach on ImageNet-1K pre-training and ADE20K fine-tuning using randomly sampled subsets of each dataset. Under this setting, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4%, and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.
  Keywords: Deep learning, Vision transformers, Efficient AI, Decorrelation
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SARVLM: A Vision Language Foundation Model for Semantic Understanding and Target Recognition in SAR Imagery</title>
<link>https://arxiv.org/abs/2510.22665</link>
<guid>https://arxiv.org/abs/2510.22665</guid>
<content:encoded><![CDATA[
arXiv:2510.22665v2 Announce Type: replace 
Abstract: Synthetic Aperture Radar (SAR) is a crucial imaging modality thanks to its all-weather capability. Although recent advances in self-supervised learning and masked image modeling (MIM) have enabled SAR foundation models, these methods largely emphasize low-level visual features and often overlook multimodal alignment and zero-shot target recognition in SAR imagery. To address this, we construct SARVLM-1M, a large-scale vision-language dataset with over one million image-text pairs aggregated from existing datasets. We further propose a domain transfer training strategy to mitigate the large gap between natural and SAR imagery. Building on this, we develop SARVLM, the first vision language foundation model (VLM) tailored to SAR, comprising SARCLIP and SARCap. SARVLM is trained with a vision-language contrastive objective under the proposed domain transfer strategy, bridging SAR imagery and textual descriptions. Extensive experiments on image text retrieval, zero-shot classification, semantic localization, and imagery captioning demonstrate that SARVLM delivers superior feature extraction and interpretation, outperforming state-of-the-art VLMs and advancing SAR semantic understanding. Code and datasets will be released soon.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Saliency-R1: Incentivizing Unified Saliency Reasoning Capability in MLLM with Confidence-Guided Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.00396</link>
<guid>https://arxiv.org/abs/2511.00396</guid>
<content:encoded><![CDATA[
arXiv:2511.00396v3 Announce Type: replace 
Abstract: Although multimodal large language models (MLLMs) excel in high-level vision-language reasoning, they lack inherent awareness of visual saliency, making it difficult to identify key visual elements. To bridge this gap, we propose Saliency-R1, the first unified MLLM framework that jointly tackles three representative and heterogeneous saliency tasks: Salient Object Detection (SOD), Salient Instance Segmentation (SIS), and Co-salient Object Detection (CoSOD), enhancing the model's capacity for saliency reasoning. We introduce a textual interface with structured tags (, <ins>) to encode region- and instance-level referring expressions, enabling a single referring segmenter to produce task-appropriate masks. To train the MLLM efficiently, we propose Confidence-Guided Policy Optimization (CGPO), a novel single-sample reinforcement learning algorithm. CGPO improves on GRPO by replacing group-normalized advantages with a per-sample signal based on reward-confidence discrepancy, thereby reducing computational waste, mitigating signal dilution, and lowering training overhead. Our model exceeds or matches the performance of robust open/closed-source MLLMs and specialized state-of-the-art methods across all three tasks, demonstrating the efficacy of our framework in saliency reasoning.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Robustness for Free? Revisiting Training via a Benchmark</title>
<link>https://arxiv.org/abs/2511.01724</link>
<guid>https://arxiv.org/abs/2511.01724</guid>
<content:encoded><![CDATA[
arXiv:2511.01724v2 Announce Type: replace 
Abstract: Deep learning models are notoriously vulnerable to imperceptible perturbations. Most existing research centers on adversarial robustness (AR), which evaluates models under worst-case scenarios by examining the existence of deterministic adversarial examples (AEs). In contrast, probabilistic robustness (PR) adopts a statistical perspective, measuring the probability that predictions remain correct under stochastic perturbations. While PR is widely regarded as a practical complement to AR, dedicated training methods for improving PR are still relatively underexplored, albeit with emerging progress. Among the few PR-targeted training methods, we identify three limitations: i non-comparable evaluation protocols; ii limited comparisons to strong AT baselines despite anecdotal PR gains from AT; and iii no unified framework to compare the generalization of these methods. Thus, we introduce PRBench, the first benchmark dedicated to evaluating improvements in PR achieved by different robustness training methods. PRBench empirically compares most common AT and PR-targeted training methods using a comprehensive set of metrics, including clean accuracy, PR and AR performance, training efficiency, and generalization error (GE). We also provide theoretical analysis on the GE of PR performance across different training methods. Main findings revealed by PRBench include: AT methods are more versatile than PR-targeted training methods in terms of improving both AR and PR performance across diverse hyperparameter settings, while PR-targeted training methods consistently yield lower GE and higher clean accuracy. A leaderboard comprising 222 trained models across 7 datasets and 10 model architectures is publicly available at https://tmpspace.github.io/PRBenchLeaderboard/.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniChange: Unifying Change Detection with Multimodal Large Language Model</title>
<link>https://arxiv.org/abs/2511.02607</link>
<guid>https://arxiv.org/abs/2511.02607</guid>
<content:encoded><![CDATA[
arXiv:2511.02607v2 Announce Type: replace 
Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land cover dynamics. While recent high performance models and high quality datasets have significantly advanced the field, a critical limitation persists. Current models typically acquire limited knowledge from single-type annotated data and cannot concurrently leverage diverse binary change detection (BCD) and semantic change detection (SCD) datasets. This constraint leads to poor generalization and limited versatility. The recent advancements in Multimodal Large Language Models (MLLMs) introduce new possibilities for a unified CD framework. We leverage the language priors and unification capabilities of MLLMs to develop UniChange, the first MLLM-based unified change detection model. UniChange integrates generative language abilities with specialized CD functionalities. Our model successfully unifies both BCD and SCD tasks through the introduction of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange utilizes text prompts to guide the identification of change categories, eliminating the reliance on predefined classification heads. This design allows UniChange to effectively acquire knowledge from multi-source datasets, even when their class definitions conflict. Experiments on four public benchmarks (WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance, achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively, surpassing all previous methods. The code is available at https://github.com/Erxucomeon/UniChange.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2511.05876</link>
<guid>https://arxiv.org/abs/2511.05876</guid>
<content:encoded><![CDATA[
arXiv:2511.05876v2 Announce Type: replace 
Abstract: In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks</title>
<link>https://arxiv.org/abs/2511.06283</link>
<guid>https://arxiv.org/abs/2511.06283</guid>
<content:encoded><![CDATA[
arXiv:2511.06283v2 Announce Type: replace 
Abstract: While Vision Language Models (VLMs) have demonstrated remarkable capabilities in general visual understanding, their application in the chemical domain has been limited, with previous works predominantly focusing on text and thus overlooking critical visual information, such as molecular structures. Current approaches that directly adopt standard VLMs for chemical tasks suffer from two primary issues: (i) computational inefficiency of processing entire chemical images with non-informative backgrounds. (ii) a narrow scope on molecular-level tasks that restricts progress in chemical reasoning. In this work, we propose \textbf{TinyChemVL}, an efficient and powerful chemical VLM that leverages visual token reduction and reaction-level tasks to improve model efficiency and reasoning capacity. Also, we propose \textbf{ChemRxn-V}, a reaction-level benchmark for assessing vision-based reaction recognition and prediction tasks. Directly predicting reaction products from molecular images poses a non-trivial challenge, as it requires models to integrate both recognition and reasoning capacities. Our results demonstrate that with only 4B parameters, TinyChemVL achieves superior performance on both molecular and reaction tasks while demonstrating faster inference and training speeds compared to existing models. Notably, TinyChemVL outperforms ChemVLM while utilizing only 1/16th of the visual tokens. This work builds efficient yet powerful VLMs for chemical domains by co-designing model architecture and task complexity.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures</title>
<link>https://arxiv.org/abs/2511.09298</link>
<guid>https://arxiv.org/abs/2511.09298</guid>
<content:encoded><![CDATA[
arXiv:2511.09298v2 Announce Type: replace 
Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DWFF-Net : A Multi-Scale Farmland System Habitat Identification Method with Adaptive Dynamic Weight</title>
<link>https://arxiv.org/abs/2511.11659</link>
<guid>https://arxiv.org/abs/2511.11659</guid>
<content:encoded><![CDATA[
arXiv:2511.11659v2 Announce Type: replace 
Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of the habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 69.79% and an F1-score of 80.49%, outperforming the baseline network by 2.1% and 1.61%, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes. (The complete code repository can be accessed via GitHub at the following URL: https://github.com/sysau/DWFF-Net)
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video</title>
<link>https://arxiv.org/abs/2511.13802</link>
<guid>https://arxiv.org/abs/2511.13802</guid>
<content:encoded><![CDATA[
arXiv:2511.13802v2 Announce Type: replace 
Abstract: We target passive dementia screening from short camera-facing talking head video, developing a facial temporal micro dynamics analysis for language free detection of early neuro cognitive change. This enables unscripted, in the wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per channel effects transparent. We also introduce YT DemTalk, a new dataset curated from publicly available, in the wild camera facing videos. It contains 300 clips (150 with self reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.14386</link>
<guid>https://arxiv.org/abs/2511.14386</guid>
<content:encoded><![CDATA[
arXiv:2511.14386v3 Announce Type: replace 
Abstract: Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception. Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored. To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving. Our method employs a 3D PAE with global camouflage texture rather than a local 2D patch-based one, ensuring both visual consistency and attack effectiveness across different viewpoints of stereo cameras. To cope with the disparity effect of these cameras, we also propose a new 3D stereo matching rendering module that allows the PAE to be aligned with real-world positions and headings in binocular vision. We further propose a novel merging attack that seamlessly blends the target into the environment through fine-grained PAE optimization. It has significantly enhanced stealth and lethality upon existing hiding attacks that fail to get seamlessly merged into the background. Extensive evaluations show that our PAEs can successfully fool the stereo models into producing erroneous depth information.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Visually, Reason Textually: Vision-Language Synergy in ARC</title>
<link>https://arxiv.org/abs/2511.15703</link>
<guid>https://arxiv.org/abs/2511.15703</guid>
<content:encoded><![CDATA[
arXiv:2511.15703v2 Announce Type: replace 
Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33\% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code is released at https://github.com/InternLM/ARC-VL.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding</title>
<link>https://arxiv.org/abs/2511.16595</link>
<guid>https://arxiv.org/abs/2511.16595</guid>
<content:encoded><![CDATA[
arXiv:2511.16595v2 Announce Type: replace 
Abstract: We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Visually Prompted Keyword Localisation in Real Low-Resource Settings</title>
<link>https://arxiv.org/abs/2409.06013</link>
<guid>https://arxiv.org/abs/2409.06013</guid>
<content:encoded><![CDATA[
arXiv:2409.06013v2 Announce Type: replace-cross 
Abstract: Given an image query, visually prompted keyword localisation (VPKL) aims to find occurrences of the depicted word in a speech collection. This can be useful when transcriptions are not available for a low-resource language (e.g. if it is unwritten). Previous work showed that VPKL can be performed with a visually grounded speech model trained on paired images and unlabelled speech. But all experiments were done on English. Moreover, transcriptions were used to get positive and negative pairs for the contrastive loss. This paper introduces a few-shot learning scheme to mine pairs automatically without transcriptions. On English, this results in only a small drop in performance. We also - for the first time - consider VPKL on a real low-resource language, Yoruba. While scores are reasonable, here we see a bigger drop in performance compared to using ground truth pairs because the mining is less accurate in Yoruba.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CroMe: Multimodal Fake News Detection using Cross-Modal Tri-Transformer and Metric Learning</title>
<link>https://arxiv.org/abs/2501.12422</link>
<guid>https://arxiv.org/abs/2501.12422</guid>
<content:encoded><![CDATA[
arXiv:2501.12422v2 Announce Type: replace-cross 
Abstract: Multimodal Fake News Detection has received increasing attention recently. Existing methods rely on independently encoded unimodal data and overlook the advantages of capturing intra-modality relationships and integrating inter-modal similarities using advanced techniques. To address these issues, Cross-Modal Tri-Transformer and Metric Learning for Multimodal Fake News Detection (CroMe) is proposed. CroMe utilizes Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (BLIP2) as encoders to capture detailed text, image and combined image-text representations. The metric learning module employs a proxy anchor method to capture intra-modality relationships while the feature fusion module uses a Cross-Modal and Tri-Transformer for effective integration. The final fake news detector processes the fused features through a classifier to predict the authenticity of the content. Experiments on datasets show that CroMe excels in multimodal fake news detection.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering</title>
<link>https://arxiv.org/abs/2505.06370</link>
<guid>https://arxiv.org/abs/2505.06370</guid>
<content:encoded><![CDATA[
arXiv:2505.06370v2 Announce Type: replace-cross 
Abstract: Lung cancer is the leading cause of patient mortality in the world. Early diagnosis of malignant pulmonary nodules in CT images can have a significant impact on reducing disease mortality and morbidity. In this work, we propose LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity filtering. Benign and malignant nodules have significant differences in their intensity profile of HU, which was not exploited in the literature. Our method considers the intensity pattern as well as the texture for the prediction of malignancies. LMLCC-Net extracts features from multiple branches that each use a separate learnable HU-based intensity filtering stage. Various combinations of branches and learnable ranges of filters were explored to finally produce the best-performing model. In addition, we propose a semi-supervised learning scheme for labeling ambiguous cases and also developed a lightweight model to classify the nodules. The experimental evaluations are carried out on the LUNA16 dataset. The proposed LMLCC-Net was evaluated using the LUNA16 dataset. Our proposed method achieves a classification accuracy of 91.96%, a sensitivity of 92.94%, and an area under the curve of 94.07%, showing improved performance compared to existing methods The proposed method can have a significant impact in helping radiologists in the classification of pulmonary nodules and improving patient care.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable cardiac substructures segmentation from contrast and non-contrast CTs using pretrained transformers</title>
<link>https://arxiv.org/abs/2505.10855</link>
<guid>https://arxiv.org/abs/2505.10855</guid>
<content:encoded><![CDATA[
arXiv:2505.10855v2 Announce Type: replace-cross 
Abstract: Automated AI segmentations for radiation treatment planning deteriorate when applied to cases with different characteristics than the training dataset. We developed a hybrid transformer convolutional network to segment cardiac substructures in lung and breast cancer patients with varying imaging contrasts and scan positions. Cohort I (56 contrast-enhanced CT [CECT], 124 non-contrast CT [NCCT] scans from lung cancer patients, supine position) was used to train an oracle model (180 cases), contrast-only model (56 CECTs), and balanced model (32 CECT, 32 NCCT). All models were evaluated on 60 held-out cohort I patients and 66 cohort II breast cancer patients (45 supine, 21 prone). Accuracy was measured using Dice similarity coefficient (DSC), 95th percentile Hausdorff distance (HD95), and dosimetric metrics, with TotalSegmentator as benchmark. Oracle and balanced models achieved similar accuracy (DSC: Oracle vs Balanced: Cohort I: 0.84 $\pm$ 0.10 vs 0.82 $\pm$ 0.10; Cohort II: 0.81 $\pm$ 0.12 vs 0.80 $\pm$ 0.13), both outperforming TotalSegmentator and the contrast-only models. The balanced model, using 64% fewer training cases, produced dosimetrically equivalent contours to manual delineations. It was robust to contrast variations (6 out of 8 substructures) and positioning variations (5 out of 8 substructures), with low correlation to patient age or body mass index. Our balanced model demonstrated robust geometric and dosimetric accuracy across varying imaging protocols and patient characteristics, which is essential for clinical deployment. Combining pretraining with balanced NCCT/CECT distribution enabled reliable segmentation with substantially fewer labeled cases than conventional approaches.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DEMIST: Decoupled Multi-stream latent diffusion for Quantitative Myelin Map Synthesis</title>
<link>https://arxiv.org/abs/2511.12396</link>
<guid>https://arxiv.org/abs/2511.12396</guid>
<content:encoded><![CDATA[
arXiv:2511.12396v2 Announce Type: replace-cross 
Abstract: Quantitative magnetization transfer (qMT) imaging provides myelin-sensitive biomarkers, such as the pool size ratio (PSR), which is valuable for multiple sclerosis (MS) assessment. However, qMT requires specialized 20-30 minute scans. We propose DEMIST to synthesize PSR maps from standard T1w and FLAIR images using a 3D latent diffusion model with three complementary conditioning mechanisms. Our approach has two stages: first, we train separate autoencoders for PSR and anatomical images to learn aligned latent representations. Second, we train a conditional diffusion model in this latent space on top of a frozen diffusion foundation backbone. Conditioning is decoupled into: (i) \textbf{semantic} tokens via cross-attention, (ii) \textbf{spatial} per-scale residual hints via a 3D ControlNet branch, and (iii) \textbf{adaptive} LoRA-modulated attention. We include edge-aware loss terms to preserve lesion boundaries and alignment losses to maintain quantitative consistency, while keeping the number of trainable parameters low and retaining the inductive bias of the pretrained model. We evaluate on 163 scans from 99 subjects using 5-fold cross-validation. Our method outperforms VAE, GAN and diffusion baselines on multiple metrics, producing sharper boundaries and better quantitative agreement with ground truth. Our code is publicly available at https://github.com/MedICL-VU/MS-Synthesis-3DcLDM.
]]></content:encoded>
<pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Video Translation via Token Warping</title>
<link>https://arxiv.org/abs/2402.12099</link>
<guid>https://arxiv.org/abs/2402.12099</guid>
<content:encoded><![CDATA[
<div> Keywords: TokenWarping, video translation, temporal coherence, diffusion model, self-attention<br /><br />Summary:<br />1. The paper addresses the challenge of achieving temporally coherent video translation, noting that current video generation models lag behind image models in visual quality and user control. <br />2. It introduces TokenWarping, a novel framework that enhances temporal consistency in diffusion-based video editing by warping query, key, and value patches in the self-attention mechanism across video frames. <br />3. Unlike existing methods that only warp key and value patches, TokenWarping also warps query patches, which improves feature aggregation and contributes significantly to temporal coherence. <br />4. The framework first extracts optical flow from source videos, then uses these flows during the diffusion model’s denoising steps to align token patches of previous frames to the current frame. <br />5. This approach imposes explicit constraints on the self-attention layer outputs, ensuring consistent frame-to-frame translation without needing extra training or fine-tuning. <br />6. TokenWarping can be easily integrated with existing text-to-image editing techniques and shows superior performance over state-of-the-art methods in both qualitative and quantitative evaluations across various video translation tasks. <br />7. The authors provide video demonstrations and publicly release the code to facilitate further research and practical applications. <div>
arXiv:2402.12099v4 Announce Type: replace 
Abstract: With the revolution of generative AI, video-related tasks have been widely studied. However, current state-of-the-art video models still lag behind image models in visual quality and user control over generated content. In this paper, we introduce TokenWarping, a novel framework for temporally coherent video translation. Existing diffusion-based video editing approaches rely solely on key and value patches in self-attention to ensure temporal consistency, often sacrificing the preservation of local and structural regions. Critically, these methods overlook the significance of the query patches in achieving accurate feature aggregation and temporal coherence. In contrast, TokenWarping leverages complementary token priors by constructing temporal correlations across different frames. Our method begins by extracting optical flows from source videos. During the denoising process of the diffusion model, these optical flows are used to warp the previous frame's query, key, and value patches, aligning them with the current frame's patches. By directly warping the query patches, we enhance feature aggregation in self-attention, while warping the key and value patches ensures temporal consistency across frames. This token warping imposes explicit constraints on the self-attention layer outputs, effectively ensuring temporally coherent translation. Our framework does not require any additional training or fine-tuning and can be seamlessly integrated with existing text-to-image editing methods. We conduct extensive experiments on various video translation tasks, demonstrating that TokenWarping surpasses state-of-the-art methods both qualitatively and quantitatively. Video demonstrations can be found on our project webpage: https://alex-zhu1.github.io/TokenWarping/. Code is available at: https://github.com/Alex-Zhu1/TokenWarping.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model</title>
<link>https://arxiv.org/abs/2510.14528</link>
<guid>https://arxiv.org/abs/2510.14528</guid>
<content:encoded><![CDATA[
<div> Keywords: PaddleOCR-VL, vision-language model, document parsing, multilingual, SOTA performance<br /><br />Summary:<br /><br />1. This report introduces PaddleOCR-VL, a state-of-the-art (SOTA) and resource-efficient model specifically designed for document parsing tasks.<br />2. The core of the system is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model that combines a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model, enabling accurate recognition of document elements.<br />3. The model efficiently supports 109 languages and is capable of recognizing complex document components, including text, tables, formulas, and charts, while maintaining minimal resource usage.<br />4. Extensive evaluations on both widely used public benchmarks and proprietary in-house benchmarks demonstrate that PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition.<br />5. PaddleOCR-VL significantly outperforms existing solutions, competes strongly with other top-tier vision-language models, and offers fast inference speeds, making it highly suitable for deployment in real-world applications. The source code is publicly available at https://github.com/PaddlePaddle/PaddleOCR. <div>
arXiv:2510.14528v4 Announce Type: replace 
Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios. Code is available at https://github.com/PaddlePaddle/PaddleOCR .
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Think and When to Look: Uncertainty-Guided Lookback</title>
<link>https://arxiv.org/abs/2511.15613</link>
<guid>https://arxiv.org/abs/2511.15613</guid>
<content:encoded><![CDATA[
<div> Keywords: test-time thinking, large vision language models, visual reasoning, uncertainty guided lookback, decoding strategy  

<br /><br />Summary:  
1. The paper investigates the impact of test-time thinking—generating explicit intermediate reasoning chains—on the visual reasoning performance of large vision language models (LVLMs).  
2. The authors conduct a large-scale, controlled evaluation of ten model variants from the InternVL3.5 and Qwen3-VL families on the MMMU-val dataset using generous token budgets and multi-pass decoding.  
3. Results reveal that longer reasoning chains are not always beneficial; often, lengthy chains lead to incorrect reasoning paths that neglect the image context and perform worse than standard instruct mode.  
4. Analysis uncovers that successful reasoning chains frequently include short "lookback" phrases explicitly referencing back to the image, improving visual grounding and overall performance.  
5. Leveraging this insight, the authors propose a novel, training-free decoding method called uncertainty guided lookback, which integrates uncertainty signals with adaptive lookback prompts and breadth-first search to improve reasoning quality.  
6. This approach leads to enhanced performance on MMMU, especially in categories where traditional thinking performs poorly, surpassing strong decoding baselines and setting a new state of the art under fixed model and token constraints.  
7. The method’s generalizability is confirmed across five additional benchmarks, encompassing broad multimodal datasets and math-focused visual reasoning tasks, consistently delivering performance gains. <div>
arXiv:2511.15613v2 Announce Type: replace 
Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click</title>
<link>https://arxiv.org/abs/2511.15948</link>
<guid>https://arxiv.org/abs/2511.15948</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Scene Graph Generation, Interactive Framework, Human Guidance, Panoptic Segmentation, Semantic Reasoning<br /><br />Summary: This article introduces Click2Graph, a novel interactive framework designed for Panoptic Video Scene Graph Generation (PVSG) that integrates visual prompting with spatial, temporal, and semantic understanding. Unlike traditional VSGG systems that function as closed, feed-forward pipelines without human interaction, Click2Graph allows user input such as clicks or bounding boxes to initiate the process. From this single cue, the system segments and tracks the subject over time, autonomously identifies interacting objects, and predicts relational triplets to build a temporally consistent scene graph. The framework features two main components: a Dynamic Interaction Discovery Module that creates subject-conditioned object prompts and a Semantic Classification Head responsible for joint entity and predicate reasoning. Experimental results on the OpenPVSG benchmark validate Click2Graph’s capability as a foundation for user-guided PVSG, demonstrating that human prompting can be effectively combined with panoptic grounding and relational inference. The system achieves controllable and interpretable video scene understanding, bridging the gap between interactive segmentation models like SAM2 and complex semantic and relational video analysis tasks. This approach paves the way for enhanced user involvement in video understanding applications. <div>
arXiv:2511.15948v2 Announce Type: replace 
Abstract: State-of-the-art Video Scene Graph Generation (VSGG) systems provide structured visual understanding but operate as closed, feed-forward pipelines with no ability to incorporate human guidance. In contrast, promptable segmentation models such as SAM2 enable precise user interaction but lack semantic or relational reasoning. We introduce Click2Graph, the first interactive framework for Panoptic Video Scene Graph Generation (PVSG) that unifies visual prompting with spatial, temporal, and semantic understanding. From a single user cue, such as a click or bounding box, Click2Graph segments and tracks the subject across time, autonomously discovers interacting objects, and predicts  triplets to form a temporally consistent scene graph. Our framework introduces two key components: a Dynamic Interaction Discovery Module that generates subject-conditioned object prompts, and a Semantic Classification Head that performs joint entity and predicate reasoning. Experiments on the OpenPVSG benchmark demonstrate that Click2Graph establishes a strong foundation for user-guided PVSG, showing how human prompting can be combined with panoptic grounding and relational inference to enable controllable and interpretable video scene understanding.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PuzzlePoles: Cylindrical Fiducial Markers Based on the PuzzleBoard Pattern</title>
<link>https://arxiv.org/abs/2511.19448</link>
<guid>https://arxiv.org/abs/2511.19448</guid>
<content:encoded><![CDATA[
<div> Keywords: PuzzlePole, fiducial markers, localization, pose estimation, autonomous systems<br /><br />Summary:  
Reliable perception is critical for autonomous systems, particularly for tasks such as calibration and localization that depend on robust visual markers. The article introduces the PuzzlePole, a novel type of fiducial marker based on the PuzzleBoard calibration pattern. Unlike traditional planar markers, the PuzzlePole is cylindrical, allowing reliable recognition and pose estimation from a full 360° viewing angle. This design leverages the unique combinatorial structure of the PuzzleBoard pattern, which enhances both localization accuracy and orientation estimation. Additionally, the PuzzlePole demonstrates strong robustness against occlusions, a common challenge in dynamic environments. The versatility of the design makes it suitable for a wide range of use cases in autonomous systems, including robot navigation, simultaneous localization and mapping (SLAM), and tangible user interfaces. Overall, the PuzzlePole marker improves the reliability and flexibility of visual markers, facilitating enhanced autonomous system perception and operational performance across diverse scenarios. <div>
arXiv:2511.19448v1 Announce Type: new 
Abstract: Reliable perception of the environment is a key enabler for autonomous systems, where calibration and localization tasks often rely on robust visual markers. We introduce the PuzzlePole, a new type of fiducial markers derived from the recently proposed PuzzleBoard calibration pattern. The PuzzlePole is a cylindrical marker, enabling reliable recognition and pose estimation from 360{\deg} viewing direction. By leveraging the unique combinatorial structure of the PuzzleBoard pattern, PuzzlePoles provide a high accuracy in localization and orientation while being robust to occlusions. The design offers flexibility for deployment in diverse autonomous systems scenarios, ranging from robot navigation and SLAM to tangible interfaces.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Reward Modeling for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2511.19458</link>
<guid>https://arxiv.org/abs/2511.19458</guid>
<content:encoded><![CDATA[
<div> Keywords: Personalized reward model, Text-to-image generation, User preferences, Prompt optimization, PIGBench  

<br /><br />Summary:  
This paper introduces PIGReward, a novel personalized reward model designed to evaluate text-to-image (T2I) generation based on individual user preferences. Unlike traditional evaluation methods that apply general reward functions or similarity metrics, PIGReward dynamically generates user-conditioned evaluation dimensions, employing chain-of-thought (CoT) reasoning to better capture personal visual tastes. To overcome the challenge of limited user data, it uses a self-bootstrapping strategy that builds rich user contexts from minimal reference inputs, enabling effective personalization without extensive user-specific training. Beyond merely assessing image quality, PIGReward also provides personalized feedback to optimize user prompts, thereby aligning generated images more closely with each user’s intent. The authors further propose PIGBench, a benchmark dataset that records diverse user-specific interpretations of shared prompts, facilitating per-user preference evaluation. Extensive experiments demonstrate that PIGReward outperforms existing evaluation methods in both accuracy and interpretability, offering a robust and scalable foundation for personalized T2I evaluation and optimization. Collectively, these contributions mark a significant advancement toward generating text-to-image outputs that better reflect individual user alignment and preferences. <div>
arXiv:2511.19458v1 Announce Type: new 
Abstract: Recent text-to-image (T2I) models generate semantically coherent images from textual prompts, yet evaluating how well they align with individual user preferences remains an open challenge. Conventional evaluation methods, general reward functions or similarity-based metrics, fail to capture the diversity and complexity of personal visual tastes. In this work, we present PIGReward, a personalized reward model that dynamically generates user-conditioned evaluation dimensions and assesses images through CoT reasoning. To address the scarcity of user data, PIGReward adopt a self-bootstrapping strategy that reasons over limited reference data to construct rich user contexts, enabling personalization without user-specific training. Beyond evaluation, PIGReward provides personalized feedback that drives user-specific prompt optimization, improving alignment between generated images and individual intent. We further introduce PIGBench, a per-user preference benchmark capturing diverse visual interpretations of shared prompts. Extensive experiments demonstrate that PIGReward surpasses existing methods in both accuracy and interpretability, establishing a scalable and reasoning-based foundation for personalized T2I evaluation and optimization. Taken together, our findings highlight PIGReward as a robust steptoward individually aligned T2I generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SG-OIF: A Stability-Guided Online Influence Framework for Reliable Vision Data</title>
<link>https://arxiv.org/abs/2511.19466</link>
<guid>https://arxiv.org/abs/2511.19466</guid>
<content:encoded><![CDATA[
<div> Influence functions, Deep learning, Online estimation, Algorithmic stability, Noise detection<br /><br />Summary:<br /><br />This paper addresses the challenge of approximating the influence of individual training points on test predictions in deep-learning vision models, a crucial task for identifying noisy or corrupted data. Traditional influence functions, relying on inverse-curvature computations, are computationally expensive and struggle due to the non-stationarity of deep model training. Existing methods attempt to reduce cost using iterative solvers and low-rank approximations but fall short as offline computations lag behind training dynamics and lack confidence calibration, leading to unreliable rankings. To overcome these issues, the authors propose the Stability-Guided Online Influence Framework (SG-OIF), which innovatively incorporates algorithmic stability as a real-time controller. SG-OIF maintains lightweight anchor inverse-Hessian-vector products (IHVPs) employing stochastic Richardson iterations and preconditioned Neumann series, enabling efficient online updates. The framework includes modular curvature backends that adjust per-example influence scores through stability-guided residual thresholds, anomaly gating, and confidence measures, enhancing robustness. Experimental evaluations demonstrate that SG-OIF achieves state-of-the-art performance in noise-label and out-of-distribution detection across multiple datasets with various types of corruption. Notably, the framework achieves 91.1% accuracy in identifying the top 1% influential samples on CIFAR-10 with 20% asymmetric noise and attains 99.8% AUPR on MNIST, indicating its practicality and effectiveness for online influence estimation in deep vision models. <div>
arXiv:2511.19466v1 Announce Type: new 
Abstract: Approximating training-point influence on test predictions is critical for deploying deep-learning vision models, essential for locating noisy data. Though the influence function was proposed for attributing how infinitesimal up-weighting or removal of individual training examples affects model outputs, its implementation is still challenging in deep-learning vision models: inverse-curvature computations are expensive, and training non-stationarity invalidates static approximations. Prior works use iterative solvers and low-rank surrogates to reduce cost, but offline computation lags behind training dynamics, and missing confidence calibration yields fragile rankings that misidentify critical examples. To address these challenges, we introduce a Stability-Guided Online Influence Framework (SG-OIF), the first framework that treats algorithmic stability as a real-time controller, which (i) maintains lightweight anchor IHVPs via stochastic Richardson and preconditioned Neumann; (ii) proposes modular curvature backends to modulate per-example influence scores using stability-guided residual thresholds, anomaly gating, and confidence. Experimental results show that SG-OIF achieves SOTA (State-Of-The-Art) on noise-label and out-of-distribution detection tasks across multiple datasets with various corruption. Notably, our approach achieves 91.1\% accuracy in the top 1\% prediction samples on the CIFAR-10 (20\% asym), and gets 99.8\% AUPR score on MNIST, effectively demonstrating that this framework is a practical controller for online influence estimation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pistachio: Towards Synthetic, Balanced, and Long-Form Video Anomaly Benchmarks</title>
<link>https://arxiv.org/abs/2511.19474</link>
<guid>https://arxiv.org/abs/2511.19474</guid>
<content:encoded><![CDATA[
<div> Video Anomaly Detection, Video Anomaly Understanding, Benchmark, Video Generation, Temporal Narratives<br /><br />Summary:<br /><br />1. The paper addresses limitations in current Video Anomaly Detection (VAD) benchmarks, highlighting their lack of scene diversity, balanced anomaly representation, and temporal complexity required for realistic evaluation.  
2. It notes the shift in the research community toward Video Anomaly Understanding (VAU), which demands deeper semantic and causal reasoning but is challenging to evaluate due to the extensive manual annotation involved.  
3. To tackle these issues, the authors introduce Pistachio, a novel benchmark created entirely via a controlled video generation pipeline that allows precise control over scene composition, anomaly types, and multi-step temporal storylines.  
4. The benchmark benefits from recent advances in video generation technology to produce coherent, long-form (41-second) videos with consistent temporal narratives and minimal human intervention, circumventing biases found in internet-sourced datasets.  
5. Experiments conducted demonstrate Pistachio’s scale, diversity, and complexity, exposing new challenges for existing methods and encouraging future research on dynamic, multi-event anomaly detection and understanding in videos. <div>
arXiv:2511.19474v1 Announce Type: new 
Abstract: Automatically detecting abnormal events in videos is crucial for modern autonomous systems, yet existing Video Anomaly Detection (VAD) benchmarks lack the scene diversity, balanced anomaly coverage, and temporal complexity needed to reliably assess real-world performance. Meanwhile, the community is increasingly moving toward Video Anomaly Understanding (VAU), which requires deeper semantic and causal reasoning but remains difficult to benchmark due to the heavy manual annotation effort it demands. In this paper, we introduce Pistachio, a new VAD/VAU benchmark constructed entirely through a controlled, generation-based pipeline. By leveraging recent advances in video generation models, Pistachio provides precise control over scenes, anomaly types, and temporal narratives, effectively eliminating the biases and limitations of Internet-collected datasets. Our pipeline integrates scene-conditioned anomaly assignment, multi-step storyline generation, and a temporally consistent long-form synthesis strategy that produces coherent 41-second videos with minimal human intervention. Extensive experiments demonstrate the scale, diversity, and complexity of Pistachio, revealing new challenges for existing methods and motivating future research on dynamic and multi-event anomaly understanding.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracking and Segmenting Anything in Any Modality</title>
<link>https://arxiv.org/abs/2511.19475</link>
<guid>https://arxiv.org/abs/2511.19475</guid>
<content:encoded><![CDATA[
<div> Keywords: tracking, segmentation, cross-modal learning, multi-task inference, universal framework  

<br /><br />Summary:  
This paper addresses the challenges in video understanding related to tracking and segmentation tasks, which provide essential positional and temporal associations of objects in video sequences. Although these tasks share similar goals, existing methods often rely on specialized or modality-specific architectures, limiting scalability and generalization across different inputs and tasks. The authors highlight two key challenges neglected in prior work: the distributional gaps across modalities and feature representation gaps across tasks, which obstruct effective knowledge sharing in multi-task and cross-modal learning. To overcome these issues, they propose SATA, a universal framework unifying a wide range of tracking and segmentation subtasks regardless of input modality. Central to SATA is the Decoupled Mixture-of-Expert (DeMoE) mechanism, which separates the learning of shared cross-modal knowledge from task-specific information, enhancing model flexibility and generalizability. Additionally, the Task-aware Multi-object Tracking (TaMOT) pipeline consolidates outputs from multiple tasks into a unified set with calibrated instance IDs, preserving task-specific knowledge during joint training. Experimental results demonstrate that SATA achieves superior performance on 18 diverse and challenging tracking and segmentation benchmarks, providing a promising direction toward truly generalist video understanding models. <div>
arXiv:2511.19475v1 Announce Type: new 
Abstract: Tracking and segmentation play essential roles in video understanding, providing basic positional information and temporal association of objects within video sequences. Despite their shared objective, existing approaches often tackle these tasks using specialized architectures or modality-specific parameters, limiting their generalization and scalability. Recent efforts have attempted to unify multiple tracking and segmentation subtasks from the perspectives of any modality input or multi-task inference. However, these approaches tend to overlook two critical challenges: the distributional gap across different modalities and the feature representation gap across tasks. These issues hinder effective cross-task and cross-modal knowledge sharing, ultimately constraining the development of a true generalist model. To address these limitations, we propose a universal tracking and segmentation framework named SATA, which unifies a broad spectrum of tracking and segmentation subtasks with any modality input. Specifically, a Decoupled Mixture-of-Expert (DeMoE) mechanism is presented to decouple the unified representation learning task into the modeling process of cross-modal shared knowledge and specific information, thus enabling the model to maintain flexibility while enhancing generalization. Additionally, we introduce a Task-aware Multi-object Tracking (TaMOT) pipeline to unify all the task outputs as a unified set of instances with calibrated ID information, thereby alleviating the degradation of task-specific knowledge during multi-task training. SATA demonstrates superior performance on 18 challenging tracking and segmentation benchmarks, offering a novel perspective for more generalizable video understanding.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Determinant Ratio Matrix Approach to Solving 3D Matching and 2D Orthographic Projection Alignment Tasks</title>
<link>https://arxiv.org/abs/2511.19511</link>
<guid>https://arxiv.org/abs/2511.19511</guid>
<content:encoded><![CDATA[
<div> Pose estimation, orthographic projection, determinant ratio matrix, EnP problem, OnP problem<br /><br />Summary:<br /><br />1. The article addresses the problem of pose estimation in computer vision, focusing on determining the relative orientation of a 3D reference object from its rotated versions or their 2D projections.<br />2. It concentrates on the orthographic projection case, specifically the OnP (orthographic pose) problem and the EnP (full 3D pose estimation) problem.<br />3. The authors present a solution to the least squares systems in error-free scenarios of both EnP and OnP problems using the determinant ratio matrix (DRaM) approach.<br />4. For noisy data, they propose a straightforward rotation correction scheme, noting that unlike EnP which can be solved exactly by SVD and quaternion eigensystem methods, the noisy OnP has no known closed-form solution and benefits from DRaM-based methods.<br />5. The work contextualizes existing methods involving QR decomposition and Moore-Penrose pseudoinverses within a broader DRaM solution family, comparing behaviors across EnP and OnP rotation estimation.<br />6. It highlights that the DRaM solutions for exact EnP and OnP problems could have been historically discovered in Gauss’s era and can generalize to pose estimation in any N-dimensional Euclidean space.<br />7. Overall, this research introduces new closed-form solutions for 3D and 2D orthographic pose estimation problems and provides deeper insights into the mathematical structures underlying these tasks. <div>
arXiv:2511.19511v1 Announce Type: new 
Abstract: Pose estimation is a general problem in computer vision with wide applications. The relative orientation of a 3D reference object can be determined from a 3D rotated version of that object, or from a projection of the rotated object to a 2D planar image. This projection can be a perspective projection (the PnP problem) or an orthographic projection (the OnP problem). We restrict our attention here to the OnP problem and the full 3D pose estimation task (the EnP problem). Here we solve the least squares systems for both the error-free EnP and OnP problems in terms of the determinant ratio matrix (DRaM) approach. The noisy-data case can be addressed with a straightforward rotation correction scheme. While the SVD and optimal quaternion eigensystem methods solve the noisy EnP 3D-3D alignment exactly, the noisy 3D-2D orthographic (OnP) task has no known comparable closed form, and can be solved by DRaM-class methods. We note that while previous similar work has been presented in the literature exploiting both the QR decomposition and the Moore-Penrose pseudoinverse transformations, here we place these methods in a larger context that has not previously been fully recognized in the absence of the corresponding DRaM solution. We term this class of solutions as the DRaM family, and conduct comparisons of the behavior of the families of solutions for the EnP and OnP rotation estimation problems. Overall, this work presents both a new solution to the 3D and 2D orthographic pose estimation problems and provides valuable insight into these classes of problems. With hindsight, we are able to show that our DRaM solutions to the exact EnP and OnP problems possess derivations that could have been discovered in the time of Gauss, and in fact generalize to all analogous N-dimensional Euclidean pose estimation problems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single Image to High-Quality 3D Object via Latent Features</title>
<link>https://arxiv.org/abs/2511.19512</link>
<guid>https://arxiv.org/abs/2511.19512</guid>
<content:encoded><![CDATA[
<div> 3D generation, latent features, variational autoencoder, single image input, high-fidelity textures<br /><br />Summary:<br /><br />1. LatentDreamer is a new framework designed to generate 3D objects from single images efficiently and with high detail.<br />2. The approach leverages a pre-trained variational autoencoder (VAE) that maps 3D geometries into latent feature representations, simplifying the generation process.<br />3. Starting from these latent features, LatentDreamer sequentially produces coarse geometries, refines them, and then applies realistic textures to finalize the 3D models.<br />4. The generated 3D objects capture high fidelity and closely resemble the input images.<br />5. The entire generation process is fast, typically completing within 70 seconds, making it practical for real-time or near real-time applications.<br />6. Experimental results demonstrate that LatentDreamer achieves competitive performance with only a small amount of training data compared to contemporary methods.<br />7. This method addresses common challenges faced in automatic 3D generation, such as balancing speed, detail, and fidelity, thereby advancing the capabilities of image-to-3D object generation techniques. <div>
arXiv:2511.19512v1 Announce Type: new 
Abstract: 3D assets are essential in the digital age. While automatic 3D generation, such as image-to-3d, has made significant strides in recent years, it often struggles to achieve fast, detailed, and high-fidelity generation simultaneously. In this work, we introduce LatentDreamer, a novel framework for generating 3D objects from single images. The key to our approach is a pre-trained variational autoencoder that maps 3D geometries to latent features, which greatly reducing the difficulty of 3D generation. Starting from latent features, the pipeline of LatentDreamer generates coarse geometries, refined geometries, and realistic textures sequentially. The 3D objects generated by LatentDreamer exhibit high fidelity to the input images, and the entire generation process can be completed within a short time (typically in 70 seconds). Extensive experiments show that with only a small amount of training, LatentDreamer demonstrates competitive performance compared to contemporary approachs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fewer Tokens, Greater Scaling: Self-Adaptive Visual Bases for Efficient and Expansive Representation Learning</title>
<link>https://arxiv.org/abs/2511.19515</link>
<guid>https://arxiv.org/abs/2511.19515</guid>
<content:encoded><![CDATA[
<div> Keywords: model capacity, visual tokens, semantic complexity, Orthogonal Filtering, ViT models<br /><br />Summary:  
This paper explores the intrinsic link between the capacity of vision models and the minimal number of visual tokens required to effectively represent image semantics. Drawing inspiration from the Minimum Description Length principle, the authors reinterpret image tokens as vectors situated in a visual semantic space. They define an image’s intrinsic semantic complexity as the smallest set of basis vectors needed to fully span this space, providing a theoretical framework to quantify semantic representation. Based on this, the paper introduces Orthogonal Filtering, a lightweight module that clusters redundant tokens dynamically into a compact set of orthogonal bases, reducing token redundancy while preserving semantic information. Extensive experiments across various Vision Transformer (ViT) models reveal a consistent scaling law, where larger models require fewer tokens to span the visual semantic space, suggesting improved token efficiency with model size. Additionally, the authors contribute a new dataset tailored for tasks involving visual long-context scenarios, enabling further research in this domain. Collectively, these contributions offer insights into token efficiency and model scaling, promising advancements in efficient visual representation learning and practical applications in vision transformers. <div>
arXiv:2511.19515v1 Announce Type: new 
Abstract: This paper investigates the fundamental relationship between model capacity and the minimal number of visual tokens required to preserve image semantics. Inspired by the Minimum Description Length principle, we reinterpret image tokens as vectors in a visual semantic space and define the intrinsic semantic complexity of an image as the smallest set of basis vectors needed to span this space. Building on this perspective, we propose Orthogonal Filtering, a lightweight module that adaptively clusters redundant tokens into a compact set of orthogonal bases. Through extensive experiments across a range of ViT models, we reveal a consistent token, model scaling law: larger models require significantly fewer tokens to span visual semantic space. Besides, we also contribute a visual long-context dataset.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Connecting the Dots: Training-Free Visual Grounding via Agentic Reasoning</title>
<link>https://arxiv.org/abs/2511.19516</link>
<guid>https://arxiv.org/abs/2511.19516</guid>
<content:encoded><![CDATA[
<div> Visual grounding, zero-shot learning, multimodal large language models, iterative reasoning, interpretability<br /><br />Summary:<br /><br />1. GroundingAgent is a new framework designed for visual grounding, which links textual queries to specific image regions without requiring any task-specific fine-tuning. <br /><br />2. It addresses limitations in existing methods that depend heavily on annotated data and fine-tuning, which restricts their generalization to unseen or out-of-distribution data.<br /><br />3. The framework uses a novel agentic process involving structured, iterative reasoning that integrates pretrained open-vocabulary object detectors, multimodal large language models (MLLMs), and large language models (LLMs) to refine candidate regions by combining semantic and spatial information.<br /><br />4. GroundingAgent achieves an impressive zero-shot grounding accuracy of 65.1% on standard benchmarks (RefCOCO, RefCOCO+, RefCOCOg) entirely without fine-tuning, demonstrating strong generalization capabilities.<br /><br />5. By replacing MLLM-generated captions with original textual queries, the model’s accuracy at the selection stage rises to around 90%, comparable to supervised approaches and highlighting the importance of large language model reasoning.<br /><br />6. The approach also offers high interpretability by clearly illustrating each step in the reasoning process, giving transparent insights into how decisions are made. <div>
arXiv:2511.19516v1 Announce Type: new 
Abstract: Visual grounding, the task of linking textual queries to specific regions within images, plays a pivotal role in vision-language integration. Existing methods typically rely on extensive task-specific annotations and fine-tuning, limiting their ability to generalize effectively to novel or out-of-distribution scenarios. To address these limitations, we introduce GroundingAgent, a novel agentic visual grounding framework that operates without any task-specific fine-tuning. GroundingAgent employs a structured, iterative reasoning mechanism that integrates pretrained open-vocabulary object detectors, multimodal large language models (MLLMs), and large language models (LLMs) to progressively refine candidate regions through joint semantic and spatial analyses. Remarkably, GroundingAgent achieves an average zero-shot grounding accuracy of 65.1 % on widely-used benchmarks (RefCOCO, RefCOCO+, RefCOCOg), entirely without fine-tuning. Furthermore, by substituting MLLM-generated captions with the original query texts, the accuracy at the selection stage alone reaches approximately 90 %, closely matching supervised performance and underscoring the critical role of LLM reasoning capabilities. GroundingAgent also offers strong interpretability, transparently illustrating each reasoning step and providing clear insights into its decision-making process.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Efficient VLMs: Information-Theoretic Driven Compression via Adaptive Structural Pruning</title>
<link>https://arxiv.org/abs/2511.19518</link>
<guid>https://arxiv.org/abs/2511.19518</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Information Bottleneck, Compression, Attention Head Pruning, Low-Rank Approximation<br /><br />Summary:<br /><br />1. The paper addresses challenges in deploying large vision-language models (VLMs) due to their increased scale and inefficiency. Existing model compression techniques lack theoretical guarantees for preserving information. <br /><br />2. The authors propose InfoPrune, an information-theoretic adaptive structural compression framework for VLMs, rooted in the Information Bottleneck principle to balance task-relevant semantic retention and redundancy removal.<br /><br />3. A novel entropy-based effective rank (eRank) metric is introduced to evaluate the importance of each attention head, alongside the Kolmogorov–Smirnov (KS) distance to quantify divergence between the original and compressed network structures, forming a unified pruning criterion.<br /><br />4. InfoPrune implements two complementary schemes: a training-based attention head pruning guided by minimizing information loss, and a training-free feed-forward network (FFN) compression via adaptive low-rank approximation.<br /><br />5. Extensive experiments on VQAv2, TextVQA, and GQA datasets demonstrate that InfoPrune achieves up to 3.2× reduction in floating-point operations (FLOPs) and 1.8× acceleration with negligible performance drop, offering a theoretically grounded and practical solution for efficient multimodal large model deployment. <div>
arXiv:2511.19518v1 Announce Type: new 
Abstract: Recent advances in vision-language models (VLMs) have shown remarkable performance across multimodal tasks, yet their ever-growing scale poses severe challenges for deployment and efficiency. Existing compression methods often rely on heuristic importance metrics or empirical pruning rules, lacking theoretical guarantees about information preservation. In this work, we propose InfoPrune, an information-theoretic framework for adaptive structural compression of VLMs. Grounded in the Information Bottleneck principle, we formulate pruning as a trade-off between retaining task-relevant semantics and discarding redundant dependencies. To quantify the contribution of each attention head, we introduce an entropy-based effective rank (eRank) and employ the Kolmogorov--Smirnov (KS) distance to measure the divergence between original and compressed structures. This yields a unified criterion that jointly considers structural sparsity and informational efficiency. Building on this foundation, we further design two complementary schemes: (1) a training-based head pruning guided by the proposed information loss objective, and (2) a training-free FFN compression via adaptive low-rank approximation. Extensive experiments on VQAv2, TextVQA, and GQA demonstrate that InfoPrune achieves up to 3.2x FLOP reduction and 1.8x acceleration with negligible performance degradation, establishing a theoretically grounded and practically effective step toward efficient multimodal large models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blinking Beyond EAR: A Stable Eyelid Angle Metric for Driver Drowsiness Detection and Data Augmentation</title>
<link>https://arxiv.org/abs/2511.19519</link>
<guid>https://arxiv.org/abs/2511.19519</guid>
<content:encoded><![CDATA[
<div> driver drowsiness, Eyelid Angle (ELA), blink detection, synthetic datasets, driver monitoring systems<br /><br />Summary:<br /><br />1. The paper introduces the Eyelid Angle (ELA), a novel metric derived from 3D facial landmarks, designed to measure eye openness more reliably than traditional 2D measures like the Eye Aspect Ratio (EAR).<br />2. Unlike binary or 2D estimators, the ELA provides a stable geometric description of eyelid motion that is robust to changes in camera angle, improving blink detection accuracy.<br />3. A blink detection framework leveraging ELA extracts detailed temporal features such as closing, closed, and reopening durations, which correlate with driver drowsiness levels.<br />4. To overcome the limited availability and ethical issues of collecting natural drowsiness data, the authors use ELA signals to animate avatars in Blender 3D, creating synthetic datasets with controllable parameters including noise, viewpoints, and blink dynamics.<br />5. Experimental evaluation using public driver monitoring datasets shows ELA outperforms EAR by reducing variance from viewpoint changes, achieving more accurate blink detection, while synthetic data augmentation improves training diversity for drowsiness recognition tasks.<br /><br />Overall, the ELA metric serves both as a reliable biometric for driver state monitoring and as a foundation for scalable synthetic data generation in advanced driver assistance system development. <div>
arXiv:2511.19519v1 Announce Type: new 
Abstract: Detecting driver drowsiness reliably is crucial for enhancing road safety and supporting advanced driver assistance systems (ADAS). We introduce the Eyelid Angle (ELA), a novel, reproducible metric of eye openness derived from 3D facial landmarks. Unlike conventional binary eye state estimators or 2D measures, such as the Eye Aspect Ratio (EAR), the ELA provides a stable geometric description of eyelid motion that is robust to variations in camera angle. Using the ELA, we design a blink detection framework that extracts temporal characteristics, including the closing, closed, and reopening durations, which are shown to correlate with drowsiness levels. To address the scarcity and risk of collecting natural drowsiness data, we further leverage ELA signals to animate rigged avatars in Blender 3D, enabling the creation of realistic synthetic datasets with controllable noise, camera viewpoints, and blink dynamics. Experimental results in public driver monitoring datasets demonstrate that the ELA offers lower variance under viewpoint changes compared to EAR and achieves accurate blink detection. At the same time, synthetic augmentation expands the diversity of training data for drowsiness recognition. Our findings highlight the ELA as both a reliable biometric measure and a powerful tool for generating scalable datasets in driver state monitoring.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VideoChat-M1: Collaborative Policy Planning for Video Understanding via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.19524</link>
<guid>https://arxiv.org/abs/2511.19524</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent system, video understanding, Collaborative Policy Planning, Multi-Agent Reinforcement Learning, tool invocation<br /><br />Summary: The paper introduces VideoChat-M1, a novel multi-agent system designed to improve video understanding by leveraging tool-augmented Multimodal Large Language Models (MLLMs). Unlike existing approaches that use static, fixed tool invocation policies, VideoChat-M1 implements a Collaborative Policy Planning (CPP) paradigm involving multiple policy agents. The CPP framework encompasses three key processes: Policy Generation, where each agent creates a unique, query-specific tool invocation policy; Policy Execution, in which agents sequentially use tools to analyze video content; and Policy Communication, enabling agents to interact and update their policies dynamically during execution. This collaboration allows agents to refine strategies based on contextual feedback from peers, resulting in more effective responses to user queries. The CPP paradigm is further enhanced by integrating a concise Multi-Agent Reinforcement Learning (MARL) approach, allowing joint optimization of all policy agents. This optimization is driven by rewards from both the final answer quality and the intermediate collaboration process, improving overall system performance. Extensive experiments across eight benchmarks involving four different tasks demonstrate that VideoChat-M1 achieves state-of-the-art results. Notably, it surpasses Gemini 2.5 pro by 3.6% and GPT-4o by 15.6% on the challenging LongVideoBench dataset. <div>
arXiv:2511.19524v1 Announce Type: new 
Abstract: By leveraging tool-augmented Multimodal Large Language Models (MLLMs), multi-agent frameworks are driving progress in video understanding. However, most of them adopt static and non-learnable tool invocation mechanisms, which limit the discovery of diverse clues essential for robust perception and reasoning regarding temporally or spatially complex videos. To address this challenge, we propose a novel Multi-agent system for video understanding, namely VideoChat-M1. Instead of using a single or fixed policy, VideoChat-M1 adopts a distinct Collaborative Policy Planning (CPP) paradigm with multiple policy agents, which comprises three key processes. (1) Policy Generation: Each agent generates its unique tool invocation policy tailored to the user's query; (2) Policy Execution: Each agent sequentially invokes relevant tools to execute its policy and explore the video content; (3) Policy Communication: During the intermediate stages of policy execution, agents interact with one another to update their respective policies. Through this collaborative framework, all agents work in tandem, dynamically refining their preferred policies based on contextual insights from peers to effectively respond to the user's query. Moreover, we equip our CPP paradigm with a concise Multi-Agent Reinforcement Learning (MARL) method. Consequently, the team of policy agents can be jointly optimized to enhance VideoChat-M1's performance, guided by both the final answer reward and intermediate collaborative process feedback. Extensive experiments demonstrate that VideoChat-M1 achieves SOTA performance across eight benchmarks spanning four tasks. Notably, on LongVideoBench, our method outperforms the SOTA model Gemini 2.5 pro by 3.6% and GPT-4o by 15.6%.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perceptual Taxonomy: Evaluating and Guiding Hierarchical Scene Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.19526</link>
<guid>https://arxiv.org/abs/2511.19526</guid>
<content:encoded><![CDATA[
<div> Perceptual Taxonomy, Visual Reasoning, Scene Understanding, Vision-Language Models, Benchmark

<br /><br />Summary:  
This paper introduces Perceptual Taxonomy, a novel structured approach to scene understanding which involves recognizing objects and their spatial relationships before inferring task-relevant properties such as material, affordance, function, and physical attributes to support goal-directed reasoning. It addresses the shortcoming in current vision-language benchmarks that primarily assess surface-level recognition or image-text alignment without evaluating deeper physically grounded visual reasoning. To fill this gap, the authors present a new benchmark consisting of 3173 annotated objects tagged with 84 fine-grained attributes across four property families. The dataset includes 5802 images from both synthetic and real domains, accompanied by 28033 template-based multiple-choice questions spanning four categories: object description, spatial reasoning, property matching, and taxonomy reasoning. Additionally, the benchmark contains 50 expert-crafted questions designed to thoroughly evaluate perceptual taxonomy reasoning abilities. Experimental results reveal that state-of-the-art vision-language models perform well on simple recognition tasks but show a 10 to 20 percent accuracy drop on questions requiring multi-step reasoning over structured attributes, emphasizing current models’ reliance on pattern matching rather than genuine reasoning. The paper also demonstrates that in-context prompting with reasoning examples from simulated scenes significantly improves model performance on challenging real-world and expert questions, indicating the promise of perceptual-taxonomy-guided prompting for advancing structured visual understanding. <div>
arXiv:2511.19526v1 Announce Type: new 
Abstract: We propose Perceptual Taxonomy, a structured process of scene understanding that first recognizes objects and their spatial configurations, then infers task-relevant properties such as material, affordance, function, and physical attributes to support goal-directed reasoning. While this form of reasoning is fundamental to human cognition, current vision-language benchmarks lack comprehensive evaluation of this ability and instead focus on surface-level recognition or image-text alignment.
  To address this gap, we introduce Perceptual Taxonomy, a benchmark for physically grounded visual reasoning. We annotate 3173 objects with four property families covering 84 fine-grained attributes. Using these annotations, we construct a multiple-choice question benchmark with 5802 images across both synthetic and real domains. The benchmark contains 28033 template-based questions spanning four types (object description, spatial reasoning, property matching, and taxonomy reasoning), along with 50 expert-crafted questions designed to evaluate models across the full spectrum of perceptual taxonomy reasoning.
  Experimental results show that leading vision-language models perform well on recognition tasks but degrade by 10 to 20 percent on property-driven questions, especially those requiring multi-step reasoning over structured attributes. These findings highlight a persistent gap in structured visual understanding and the limitations of current models that rely heavily on pattern matching. We also show that providing in-context reasoning examples from simulated scenes improves performance on real-world and expert-curated questions, demonstrating the effectiveness of perceptual-taxonomy-guided prompting.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapRF: Weakly Supervised Online HD Map Construction via NeRF-Guided Self-Training</title>
<link>https://arxiv.org/abs/2511.19527</link>
<guid>https://arxiv.org/abs/2511.19527</guid>
<content:encoded><![CDATA[
<div> Keywords: Autonomous driving, HD map construction, weakly supervised learning, Neural Radiance Fields, self-training<br /><br />Summary:<br /><br />1. The paper introduces MapRF, a weakly supervised framework designed to construct 3D HD maps for autonomous driving using only 2D image labels, addressing the scalability and cost issues inherent in relying on 3D map annotations.<br /><br />2. MapRF leverages a novel Neural Radiance Fields (NeRF) module conditioned on map predictions to reconstruct consistent 3D geometry and semantic information, enabling the generation of high-quality pseudo labels.<br /><br />3. These pseudo labels facilitate a self-training process, where the map network is iteratively refined without requiring additional supervision, thus progressively improving map accuracy.<br /><br />4. To prevent error accumulation during self-training, the method employs a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from the original 2D labels.<br /><br />5. Experimental results on the Argoverse 2 and nuScenes datasets show that MapRF attains about 75% of the performance of fully supervised methods and surpasses several other approaches limited to 2D labels, demonstrating its effectiveness for scalable, cost-efficient online HD map construction in autonomous driving applications. <div>
arXiv:2511.19527v1 Announce Type: new 
Abstract: Autonomous driving systems benefit from high-definition (HD) maps that provide critical information about road infrastructure. The online construction of HD maps offers a scalable approach to generate local maps from on-board sensors. However, existing methods typically rely on costly 3D map annotations for training, which limits their generalization and scalability across diverse driving environments. In this work, we propose MapRF, a weakly supervised framework that learns to construct 3D maps using only 2D image labels. To generate high-quality pseudo labels, we introduce a novel Neural Radiance Fields (NeRF) module conditioned on map predictions, which reconstructs view-consistent 3D geometry and semantics. These pseudo labels are then iteratively used to refine the map network in a self-training manner, enabling progressive improvement without additional supervision. Furthermore, to mitigate error accumulation during self-training, we propose a Map-to-Ray Matching strategy that aligns map predictions with camera rays derived from 2D labels. Extensive experiments on the Argoverse 2 and nuScenes datasets demonstrate that MapRF achieves performance comparable to fully supervised methods, attaining around 75% of the baseline while surpassing several approaches using only 2D labels. This highlights the potential of MapRF to enable scalable and cost-effective online HD map construction for autonomous driving.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vidi2: Large Multimodal Models for Video Understanding and Creation</title>
<link>https://arxiv.org/abs/2511.19529</link>
<guid>https://arxiv.org/abs/2511.19529</guid>
<content:encoded><![CDATA[
<div> Video, spatio-temporal grounding, Vidi2, VUE-STG, video question answering<br /><br />Summary: Video has become the dominant medium for communication and creativity online, driving the need for scalable and high-quality video production tools. The Vidi model series continues to advance video understanding, with the latest iteration, Vidi2, enhancing capabilities in fine-grained spatio-temporal grounding (STG) and expanding into video question answering (Video QA). Vidi2 can process text queries to identify not just the relevant timestamps but also the precise bounding boxes of target objects within those time segments, enabling comprehensive multimodal reasoning. This spatio-temporal grounding ability unlocks advanced applications like plot and character understanding, automatic multi-view switching, and intelligent, composition-aware reframing and cropping. To evaluate STG effectively in real-world contexts, the authors introduce the VUE-STG benchmark, which improves upon existing datasets by covering longer video durations (10s to 30 mins), using noun phrase queries that maintain sentence-level expressiveness, providing manually annotated high-accuracy ground truth for both time ranges and bounding boxes, and applying a refined evaluation metric combining vIoU, tIoU, and vIoU-Intersection measures. Additionally, the previous VUE-TR benchmark is upgraded to VUE-TR-V2 with a more balanced video-length distribution and more user-style queries. Experimental results show that Vidi2 significantly surpasses proprietary systems like Gemini 3 Pro (Preview) and GPT-5 on VUE-TR-V2 and VUE-STG, while delivering competitive performance against similar-scale open-source models on Video QA tasks. <div>
arXiv:2511.19529v1 Announce Type: new 
Abstract: Video has emerged as the primary medium for communication and creativity on the Internet, driving strong demand for scalable, high-quality video production. Vidi models continue to evolve toward next-generation video creation and have achieved state-of-the-art performance in multimodal temporal retrieval (TR). In its second release, Vidi2 advances video understanding with fine-grained spatio-temporal grounding (STG) and extends its capability to video question answering (Video QA), enabling comprehensive multimodal reasoning. Given a text query, Vidi2 can identify not only the corresponding timestamps but also the bounding boxes of target objects within the output time ranges. This end-to-end spatio-temporal grounding capability enables potential applications in complex editing scenarios, such as plot or character understanding, automatic multi-view switching, and intelligent, composition-aware reframing and cropping. To enable comprehensive evaluation of STG in practical settings, we introduce a new benchmark, VUE-STG, which offers four key improvements over existing STG datasets: 1) Video duration: spans from roughly 10s to 30 mins, enabling long-context reasoning; 2) Query format: queries are mostly converted into noun phrases while preserving sentence-level expressiveness; 3) Annotation quality: all ground-truth time ranges and bounding boxes are manually annotated with high accuracy; 4) Evaluation metric: a refined vIoU/tIoU/vIoU-Intersection scheme. In addition, we upgrade the previous VUE-TR benchmark to VUE-TR-V2, achieving a more balanced video-length distribution and more user-style queries. Remarkably, the Vidi2 model substantially outperforms leading proprietary systems, such as Gemini 3 Pro (Preview) and GPT-5, on both VUE-TR-V2 and VUE-STG, while achieving competitive results with popular open-source models with similar scale on video QA benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Domain Generalization of Multimodal LLMs for Global Photovoltaic Assessment</title>
<link>https://arxiv.org/abs/2511.19537</link>
<guid>https://arxiv.org/abs/2511.19537</guid>
<content:encoded><![CDATA[
<div> Keywords: photovoltaic systems, multimodal large language model, cross-domain generalization, satellite imagery, power grid management<br /><br />Summary: The paper addresses challenges in managing the power grid due to the rapid increase of distributed photovoltaic (PV) installations, many of which are undocumented. Traditional computer vision models such as CNNs and U-Nets, widely used to detect and assess PV systems from satellite imagery, require large amounts of labeled data and tend to perform poorly when applied to regions different from those they were trained on. To overcome these limitations, the study explores the use of a multimodal large language model (LLM) capable of integrating detection, localization, and quantification tasks within a unified framework by employing structured prompts and fine-tuning techniques. The model is evaluated across multiple regions to test its ability to generalize under domain shifts, with performance measured by the ΔF1 metric. Results show that this multimodal LLM exhibits the least performance degradation when applied to unseen regions compared to conventional computer vision and transformer-based models. The findings highlight the model’s robustness, scalability, transferability, and interpretability for global photovoltaic system mapping, offering significant advancements for large-scale and reliable PV assessment using satellite imagery. <div>
arXiv:2511.19537v1 Announce Type: new 
Abstract: The rapid expansion of distributed photovoltaic (PV) systems poses challenges for power grid management, as many installations remain undocumented. While satellite imagery provides global coverage, traditional computer vision (CV) models such as CNNs and U-Nets require extensive labeled data and fail to generalize across regions. This study investigates the cross-domain generalization of a multimodal large language model (LLM) for global PV assessment. By leveraging structured prompts and fine-tuning, the model integrates detection, localization, and quantification within a unified schema. Cross-regional evaluation using the $\Delta$F1 metric demonstrates that the proposed model achieves the smallest performance degradation across unseen regions, outperforming conventional CV and transformer baselines. These results highlight the robustness of multimodal LLMs under domain shift and their potential for scalable, transferable, and interpretable global PV mapping.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration</title>
<link>https://arxiv.org/abs/2511.19538</link>
<guid>https://arxiv.org/abs/2511.19538</guid>
<content:encoded><![CDATA[
<div> Keywords: cartographic heritage, map digitization, semantic segmentation, cultural analysis, cartographic signs<br /><br />Summary:<br /><br />1. This thesis explores large-scale methods and datasets to study cartographic heritage from a cultural and historical perspective, addressing the gap between automated content recognition and the semantic-symbolic nature of maps.<br /><br />2. The research aggregates and normalizes a vast dataset of 771,561 map records and 99,715 digitized images from 38 digital catalogs, covering six centuries (1492–1948) and including data on 236,925 contributors.<br /><br />3. The dataset enables detailed analysis of geographic structures and the chronology of global map publication, revealing links between cartographic focus and political dynamics, such as Atlantic maritime charting’s connection to the triangular trade and colonialism.<br /><br />4. The study employs semantic segmentation and object detection models, trained on annotated and synthetic data, to recognize land classes and cartographic signs, showing maps as deliberately composed images emphasizing features through centering and symmetry.<br /><br />5. By encoding over 63 million signs and 25 million fragments into a latent visual space, the research uncovers shifts in cartographic figurations, the local consistency of sign systems, and how collaboration and diffusion are influenced by legitimacy, major actors, and key cities in shaping semiotic traditions. <div>
arXiv:2511.19538v1 Announce Type: new 
Abstract: This thesis presents methods and datasets to investigate cartographic heritage on a large scale and from a cultural perspective. Heritage institutions worldwide have digitized more than one million maps, and automated techniques now enable large-scale recognition and extraction of map content. Yet these methods have engaged little with the history of cartography, or the view that maps are semantic-symbolic systems, and cultural objects reflecting political and epistemic expectations. This work leverages a diverse corpus of 771,561 map records and 99,715 digitized images aggregated from 38 digital catalogs. After normalization, the dataset includes 236,925 contributors and spans six centuries, from 1492 to 1948. These data make it possible to chart geographic structures and the global chronology of map publication. The spatial focus of cartography is analyzed in relation to political dynamics, evidencing links between Atlantic maritime charting, the triangular trade, and colonial expansion. Further results document the progression of national, domestic focus and the impact of military conflicts on publication volumes. The research introduces semantic segmentation techniques and object detection models for the generic recognition of land classes and cartographic signs, trained on annotated data and synthetic images. The analysis of land classes shows that maps are designed images whose framing and composition emphasize features through centering and semantic symmetries. The study of cartographic figuration encodes 63 M signs and 25 M fragments into a latent visual space, revealing figurative shifts such as the replacement of relief hachures by terrain contours and showing that signs tend to form locally consistent systems. Analyses of collaboration and diffusion highlight the role of legitimacy, larger actors, and major cities in the spread of figurative norms and semiotic cultures.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proxy-Free Gaussian Splats Deformation with Splat-Based Surface Estimation</title>
<link>https://arxiv.org/abs/2511.19542</link>
<guid>https://arxiv.org/abs/2511.19542</guid>
<content:encoded><![CDATA[
<div> Gaussian splats, Laplacian operator, surface-aware graph, proxy-free deformation, rendering quality<br /><br />Summary:<br /><br />This paper presents SpLap, a novel deformation method for Gaussian splats (GS) that operates without the need for deformation proxies such as cages or meshes, which often introduce dependency on proxy quality and computational overhead. The core innovation lies in constructing a surface-aware splat graph that better captures the surface structure of the splats by defining neighboring splats based on intersection rather than just center distance. This graph allows the Laplacian operator to produce more plausible deformations that preserve surface details and topology better than prior proxy-based or naive proxy-free methods. Additionally, a Gaussian kernel adaptation technique is introduced to maintain surface integrity and enhance rendering quality after deformation. The effectiveness of SpLap is demonstrated through extensive experiments on 50 challenging 3D objects across datasets including ShapeNet, Objaverse, Sketchfab, and the NeRF-Synthetic dataset. The results show superior performance against both proxy-based and proxy-free baselines, confirming the advantages of the surface-aware graph formulation and kernel adaptation. The method and code are publicly available, facilitating further research and application in 3D shape deformation and rendering. <div>
arXiv:2511.19542v1 Announce Type: new 
Abstract: We introduce SpLap, a proxy-free deformation method for Gaussian splats (GS) based on a Laplacian operator computed from our novel surface-aware splat graph. Existing approaches to GS deformation typically rely on deformation proxies such as cages or meshes, but they suffer from dependency on proxy quality and additional computational overhead. An alternative is to directly apply Laplacian-based deformation techniques by treating splats as point clouds. However, this often fail to properly capture surface information due to lack of explicit structure. To address this, we propose a novel method that constructs a surface-aware splat graph, enabling the Laplacian operator derived from it to support more plausible deformations that preserve details and topology. Our key idea is to leverage the spatial arrangement encoded in splats, defining neighboring splats not merely by the distance between their centers, but by their intersections. Furthermore, we introduce a Gaussian kernel adaptation technique that preserves surface structure under deformation, thereby improving rendering quality after deformation. In our experiments, we demonstrate the superior performance of our method compared to both proxy-based and proxy-free baselines, evaluated on 50 challenging objects from the ShapeNet, Objaverse, and Sketchfab datasets, as well as the NeRF-Synthetic dataset. Code is available at https://github.com/kjae0/SpLap.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think First, Assign Next (ThiFAN-VQA): A Two-stage Chain-of-Thought Framework for Post-Disaster Damage Assessment</title>
<link>https://arxiv.org/abs/2511.19557</link>
<guid>https://arxiv.org/abs/2511.19557</guid>
<content:encoded><![CDATA[
<div> Disaster assessment, Visual question answering, Chain-of-thought prompting, In-context learning, UAV imagery<br /><br />Summary:<br /><br />This article presents ThiFAN-VQA, a novel two-stage reasoning-based framework designed to improve visual question answering (VQA) for post-disaster damage assessment using aerial imagery collected by Unmanned Aerial Vehicles (UAVs). First, it highlights the challenge that existing AI models face due to the high cost and limited diversity of annotated training data, as well as rigid classification approaches that restrict answer flexibility. To overcome these issues, ThiFAN-VQA leverages pre-trained generative models with in-context learning (ICL) and chain-of-thought (CoT) prompting to produce structured reasoning traces, thus enabling interpretable and flexible answers even under limited supervision. A second stage involves an answer selection module that evaluates generated responses to pick the most coherent and contextually accurate answer, improving overall performance and reducing hallucinated or generic outputs. The framework integrates domain-specific prompts and a custom information retrieval system to enhance relevance and adaptability. Experimental evaluation on UAV-based datasets FloodNet and RescueNet-VQA, drawn from flood and hurricane disaster scenarios, demonstrates superior accuracy, interpretability, and adaptability of ThiFAN-VQA, bridging the gap between zero-shot and supervised methods, and providing a robust tool for real-world post-disaster damage assessment tasks. <div>
arXiv:2511.19557v1 Announce Type: new 
Abstract: Timely and accurate assessment of damages following natural disasters is essential for effective emergency response and recovery. Recent AI-based frameworks have been developed to analyze large volumes of aerial imagery collected by Unmanned Aerial Vehicles, providing actionable insights rapidly. However, creating and annotating data for training these models is costly and time-consuming, resulting in datasets that are limited in size and diversity. Furthermore, most existing approaches rely on traditional classification-based frameworks with fixed answer spaces, restricting their ability to provide new information without additional data collection or model retraining. Using pre-trained generative models built on in-context learning (ICL) allows for flexible and open-ended answer spaces. However, these models often generate hallucinated outputs or produce generic responses that lack domain-specific relevance. To address these limitations, we propose ThiFAN-VQA, a two-stage reasoning-based framework for visual question answering (VQA) in disaster scenarios. ThiFAN-VQA first generates structured reasoning traces using chain-of-thought (CoT) prompting and ICL to enable interpretable reasoning under limited supervision. A subsequent answer selection module evaluates the generated responses and assigns the most coherent and contextually accurate answer, effectively improve the model performance. By integrating a custom information retrieval system, domain-specific prompting, and reasoning-guided answer selection, ThiFAN-VQA bridges the gap between zero-shot and supervised methods, combining flexibility with consistency. Experiments on FloodNet and RescueNet-VQA, UAV-based datasets from flood- and hurricane-affected regions, demonstrate that ThiFAN-VQA achieves superior accuracy, interpretability, and adaptability for real-world post-disaster damage assessment tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HunyuanOCR Technical Report</title>
<link>https://arxiv.org/abs/2511.19575</link>
<guid>https://arxiv.org/abs/2511.19575</guid>
<content:encoded><![CDATA[
<div> Keywords: HunyuanOCR, Vision-Language Model, OCR, Reinforcement Learning, Lightweight  

<br /><br />Summary:  
This paper introduces HunyuanOCR, a commercial-grade, open-source Vision-Language Model (VLM) designed specifically for OCR tasks with only 1 billion parameters. The architecture features a Native Vision Transformer (ViT) paired with a lightweight large language model (LLM) connected through an MLP adapter, enabling superior performance over commercial OCR APIs, traditional pipelines, and larger models like Qwen3-VL-4B. It excels in both perception tasks (such as Text Spotting and Parsing) and semantic tasks (including Information Extraction and Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge Small Model Track. HunyuanOCR achieves state-of-the-art results on OCRBench for VLMs under 3B parameters. Key breakthroughs include (1) unifying versatility and efficiency by supporting spotting, parsing, IE, VQA, and translation within a lightweight framework, overcoming the limitations of specialized OCR models and general heavy VLMs; (2) implementing a streamlined end-to-end architecture that removes the need for preprocessing modules such as layout analysis, thereby mitigating error propagation issues and simplifying deployment; and (3) harnessing data-driven approaches alongside Reinforcement Learning strategies, which for the first time demonstrate significant OCR performance improvements. The model is open-sourced on HuggingFace with a high-performance deployment built on vLLM, positioning it as a strong foundation for both research and industry applications. <div>
arXiv:2511.19575v1 Announce Type: new 
Abstract: This paper presents HunyuanOCR, a commercial-grade, open-source, and lightweight (1B parameters) Vision-Language Model (VLM) dedicated to OCR tasks. The architecture comprises a Native Vision Transformer (ViT) and a lightweight LLM connected via an MLP adapter. HunyuanOCR demonstrates superior performance, outperforming commercial APIs, traditional pipelines, and larger models (e.g., Qwen3-VL-4B). Specifically, it surpasses current public solutions in perception tasks (Text Spotting, Parsing) and excels in semantic tasks (IE, Text Image Translation), securing first place in the ICDAR 2025 DIMT Challenge (Small Model Track). Furthermore, it achieves state-of-the-art (SOTA) results on OCRBench among VLMs with fewer than 3B parameters.
  HunyuanOCR achieves breakthroughs in three key aspects: 1) Unifying Versatility and Efficiency: We implement comprehensive support for core capabilities including spotting, parsing, IE, VQA, and translation within a lightweight framework. This addresses the limitations of narrow "OCR expert models" and inefficient "General VLMs". 2) Streamlined End-to-End Architecture: Adopting a pure end-to-end paradigm eliminates dependencies on pre-processing modules (e.g., layout analysis). This fundamentally resolves error propagation common in traditional pipelines and simplifies system deployment. 3) Data-Driven and RL Strategies: We confirm the critical role of high-quality data and, for the first time in the industry, demonstrate that Reinforcement Learning (RL) strategies yield significant performance gains in OCR tasks.
  HunyuanOCR is officially open-sourced on HuggingFace. We also provide a high-performance deployment solution based on vLLM, placing its production efficiency in the top tier. We hope this model will advance frontier research and provide a solid foundation for industrial applications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Unlabeled Scans for NCCT Image Segmentation in Early Stroke Diagnosis: A Semi-Supervised GAN Approach</title>
<link>https://arxiv.org/abs/2511.19576</link>
<guid>https://arxiv.org/abs/2511.19576</guid>
<content:encoded><![CDATA[
<div> Ischemic stroke, Non-contrast CT, Semi-supervised learning, GAN, Early infarct segmentation<br /><br />Summary: Ischemic stroke is a medical emergency where early diagnosis is critical for better patient outcomes. Non-contrast computed tomography (NCCT) is commonly used as the initial imaging method but often fails to detect subtle ischemic changes in the early hyperacute phase, potentially delaying treatment. To overcome this challenge, the study proposes a semi-supervised segmentation technique based on generative adversarial networks (GANs) designed to accurately identify and delineate early ischemic stroke regions in NCCT scans. This approach utilizes an adversarial training framework to effectively learn from a limited set of annotated scans while leveraging a larger collection of unlabeled scans. The model integrates various losses including Dice loss, cross-entropy loss, feature matching loss, and self-training loss, which collectively enhance its ability to detect small or faint early infarcts. Experiments conducted on the publicly available Acute Ischemic Stroke Dataset (AISD) demonstrate that the proposed method improves diagnostic accuracy and reduces dependency on extensive manual annotation. By enhancing detection of early infarct areas, this approach has the potential to support more efficient clinical decision-making and improve patient care in stroke management. <div>
arXiv:2511.19576v1 Announce Type: new 
Abstract: Ischemic stroke is a time-critical medical emergency where rapid diagnosis is essential for improving patient outcomes. Non-contrast computed tomography (NCCT) serves as the frontline imaging tool, yet it often fails to reveal the subtle ischemic changes present in the early, hyperacute phase. This limitation can delay crucial interventions. To address this diagnostic challenge, we introduce a semi-supervised segmentation method using generative adversarial networks (GANs) to accurately delineate early ischemic stroke regions. The proposed method employs an adversarial framework to effectively learn from a limited number of annotated NCCT scans, while simultaneously leveraging a larger pool of unlabeled scans. By employing Dice loss, cross-entropy loss, a feature matching loss and a self-training loss, the model learns to identify and delineate early infarcts, even when they are faint or their size is small. Experiments on the publicly available Acute Ischemic Stroke Dataset (AISD) demonstrate the potential of the proposed method to enhance diagnostic capabilities, reduce the burden of manual annotation, and support more efficient clinical decision-making in stroke care.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiscale Vector-Quantized Variational Autoencoder for Endoscopic Image Synthesis</title>
<link>https://arxiv.org/abs/2511.19578</link>
<guid>https://arxiv.org/abs/2511.19578</guid>
<content:encoded><![CDATA[
<div> Keywords: Wireless Capsule Endoscopy, Synthetic Data Generation, Multiscale Vector Quantized VAE, Medical Image Synthesis, Clinical Decision Support<br /><br />Summary:  
This work addresses the challenge of limited annotated medical imaging data for training deep learning-based Clinical Decision Support (CDS) systems in Gastrointestinal (GI) imaging using Wireless Capsule Endoscopy (WCE). To overcome data scarcity, the authors propose a novel generative methodology based on a multiscale extension of the Vector Quantized Variational Autoencoder, termed Multiscale Vector Quantized Variational Autoencoder (MSVQ-VAE). Unlike prior VAE-based models, MSVQ-VAE uniquely enables seamless introduction of abnormalities into normal WCE images via conditional synthetic image generation. The model is able to incorporate diverse types of abnormalities, such as polyps, vascular lesions, and inflammatory conditions, into the generated images. Experimental evaluation includes classification tasks to assess the usefulness of these synthetic abnormal images for CDS. Results show that classifiers trained on synthetic abnormalities generated by MSVQ-VAE achieve comparable performance to those trained solely on real abnormal images. This demonstrates the practical utility of the proposed synthetic data for enhancing CDS training in GI imaging. The methodology’s design and generality suggest it can be extended to other medical multimedia domains, potentially improving data availability and robustness of AI-driven diagnostic tools across healthcare areas. <div>
arXiv:2511.19578v1 Announce Type: new 
Abstract: Gastrointestinal (GI) imaging via Wireless Capsule Endoscopy (WCE) generates a large number of images requiring manual screening. Deep learning-based Clinical Decision Support (CDS) systems can assist screening, yet their performance relies on the existence of large, diverse, training medical datasets. However, the scarcity of such data, due to privacy constraints and annotation costs, hinders CDS development. Generative machine learning offers a viable solution to combat this limitation. While current Synthetic Data Generation (SDG) methods, such as Generative Adversarial Networks and Variational Autoencoders have been explored, they often face challenges with training stability and capturing sufficient visual diversity, especially when synthesizing abnormal findings. This work introduces a novel VAE-based methodology for medical image synthesis and presents its application for the generation of WCE images. The novel contributions of this work include a) multiscale extension of the Vector Quantized VAE model, named as Multiscale Vector Quantized Variational Autoencoder (MSVQ-VAE); b) unlike other VAE-based SDG models for WCE image generation, MSVQ-VAE is used to seamlessly introduce abnormalities into normal WCE images; c) it enables conditional generation of synthetic images, enabling the introduction of different types of abnormalities into the normal WCE images; d) it performs experiments with a variety of abnormality types, including polyps, vascular and inflammatory conditions. The utility of the generated images for CDS is assessed via image classification. Comparative experiments demonstrate that training a CDS classifier using the abnormal images generated by the proposed methodology yield comparable results with a classifier trained with only real data. The generality of the proposed methodology promises its applicability to various domains related to medical multimedia.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SkillSight: Efficient First-Person Skill Assessment with Gaze</title>
<link>https://arxiv.org/abs/2511.19629</link>
<guid>https://arxiv.org/abs/2511.19629</guid>
<content:encoded><![CDATA[
<div> Keywords: egocentric perception, skill assessment, gaze tracking, power efficiency, AI-supported learning

<br /><br />Summary:  
This paper introduces SkillSight, a novel framework designed for power-efficient skill assessment from first-person data captured by smart glasses. The core hypothesis of the approach is that an individual's skill level can be inferred not only from the video of their activity but also from their gaze patterns, reflecting how they direct attention during task execution. SkillSight employs a two-stage learning framework: initially training a teacher model that jointly analyzes both egocentric video and gaze data to predict skill levels, followed by distilling this knowledge into a student model that relies solely on gaze input during inference. This gaze-only model significantly reduces power consumption by eliminating the need for continuous video processing. Experimental validation across three diverse datasets encompassing cooking, music, and sports domains demonstrates the critical role of gaze in skill evaluation and confirms that SkillSight achieves state-of-the-art prediction accuracy. Notably, the student model achieves comparable performance while using 73 times less power compared to existing approaches, making it highly suitable for real-world, in-the-wild skill learning applications. The results highlight the potential of leveraging gaze data in wearable AI systems to enhance and support physical skill acquisition more efficiently. <div>
arXiv:2511.19629v1 Announce Type: new 
Abstract: Egocentric perception on smart glasses could transform how we learn new skills in the physical world, but automatic skill assessment remains a fundamental technical challenge. We introduce SkillSight for power-efficient skill assessment from first-person data. Central to our approach is the hypothesis that skill level is evident not only in how a person performs an activity (video), but also in how they direct their attention when doing so (gaze). Our two-stage framework first learns to jointly model gaze and egocentric video when predicting skill level, then distills a gaze-only student model. At inference, the student model requires only gaze input, drastically reducing power consumption by eliminating continuous video processing. Experiments on three datasets spanning cooking, music, and sports establish, for the first time, the valuable role of gaze in skill understanding across diverse real-world settings. Our SkillSight teacher model achieves state-of-the-art performance, while our gaze-only student variant maintains high accuracy using 73x less power than competing methods. These results pave the way for in-the-wild AI-supported skill learning.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Utility of Foundation Models for Fast MRI: Vision-Language-Guided Image Reconstruction</title>
<link>https://arxiv.org/abs/2511.19641</link>
<guid>https://arxiv.org/abs/2511.19641</guid>
<content:encoded><![CDATA[
<div> Keywords: vision-language foundation model, undersampled MRI reconstruction, semantic distribution, contrastive learning, multimodal priors<br /><br />Summary:  
1. The study investigates the use of a vision-language foundation model to enhance undersampled MRI reconstruction by leveraging high-level semantic information beyond traditional image priors.  
2. A novel semantic distribution-guided reconstruction framework is developed, which encodes both the MRI image and auxiliary information into semantic features using a pre-trained vision-language model.  
3. The framework employs a contrastive objective to align the reconstructed image features with a target semantic distribution, ensuring consistency with perceptual cues and improving reconstruction quality.  
4. This approach is compatible with various deep learning-based reconstruction methods and flexibly integrates semantic priors from multimodal sources, including image-only and image-language data.  
5. Experiments on knee and brain MRI datasets demonstrate that semantic priors significantly preserve fine anatomical details and yield superior perceptual quality (e.g., lower LPIPS, higher Tenengrad scores) compared to conventional regularization.  
6. Incorporating image-language information further enriches the semantic distribution and provides high-level control over reconstruction attributes.  
7. The contrastive objective consistently guides the reconstructed features toward the desired semantic space while maintaining data fidelity.  
8. The results highlight the potential of vision-language foundation models to improve undersampled MRI reconstruction by optimizing in semantic feature space, advancing both the perceptual quality and interpretability of MRI reconstructions. <div>
arXiv:2511.19641v1 Announce Type: new 
Abstract: Purpose: To investigate whether a vision-language foundation model can enhance undersampled MRI reconstruction by providing high-level contextual information beyond conventional priors. Methods: We proposed a semantic distribution-guided reconstruction framework that uses a pre-trained vision-language foundation model to encode both the reconstructed image and auxiliary information into high-level semantic features. A contrastive objective aligns the reconstructed representation with the target semantic distribution, ensuring consistency with high-level perceptual cues. The proposed objective works with various deep learning-based reconstruction methods and can flexibly incorporate semantic priors from multimodal sources. To test the effectiveness of these semantic priors, we evaluated reconstruction results guided by priors derived from either image-only or image-language auxiliary information. Results: Experiments on knee and brain datasets demonstrate that semantic priors from images preserve fine anatomical structures and achieve superior perceptual quality, as reflected in lower LPIPS values, higher Tenengrad scores, and improved scores in the reader study, compared with conventional regularization. The image-language information further expands the semantic distribution and enables high-level control over reconstruction attributes. Across all evaluations, the contrastive objective consistently guided the reconstructed features toward the desired semantic distributions while maintaining data fidelity, demonstrating the effectiveness of the proposed optimization framework. Conclusion: The study highlights that vision-language foundation models can improve undersampled MRI reconstruction through semantic-space optimization.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating Gigapixel Pathology Images with Large Multimodal Models</title>
<link>https://arxiv.org/abs/2511.19652</link>
<guid>https://arxiv.org/abs/2511.19652</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Multimodal Models, Pathology, Gigapixel Images, Whole-Slide Images, MultiPathQA<br /><br />Summary:<br /><br />1. This study addresses the challenge of applying general-purpose large multimodal models (LMMs) to medical image interpretation in pathology, where the complexity of gigapixel whole-slide images (WSIs) has hindered effective analysis. <br /><br />2. Previous approaches relied on low-resolution thumbnails or random image patches, which limited the performance evaluation and underestimated LMM capability in pathology. <br /><br />3. The authors introduce GIANT (Gigapixel Image Agent for Navigating Tissue), a novel framework that enables LMMs to navigate WSIs iteratively and make coherent, context-aware assessments akin to a pathologist's reasoning process. <br /><br />4. Alongside GIANT, they release MultiPathQA, a new benchmark dataset comprising 934 WSI-level questions across five clinical tasks including cancer diagnosis and open-ended reasoning; notably, 128 questions are authored by professional pathologists requiring direct slide interpretation. <br /><br />5. Experiments demonstrate that GIANT significantly outperforms conventional baseline methods based on thumbnails and patches, reaching or exceeding the performance of specialized pathology models like TITAN and SlideChat. For example, GPT-5 with GIANT achieves 62.5% accuracy on pathologist-authored questions, compared to 43.8% and 37.5% by TITAN and SlideChat respectively. <br /><br />6. The study highlights both strengths and current limitations of foundation LMMs in expert pathology, guiding future enhancements for clinical reasoning in medical image analysis. <div>
arXiv:2511.19652v1 Announce Type: new 
Abstract: Despite being widely used to support clinical care, general-purpose large multimodal models (LMMs) have generally shown poor or inconclusive performance in medical image interpretation, particularly in pathology, where gigapixel images are used. However, prior studies have used either low-resolution thumbnails or random patches, which likely underestimated model performance. Here, we ask whether LMMs can be adapted to reason coherently and accurately in the evaluation of such images. In this study, we introduce Gigapixel Image Agent for Navigating Tissue (GIANT), the first framework that allows LMMs to iteratively navigate whole-slide images (WSIs) like a pathologist. Accompanying GIANT, we release MultiPathQA, a new benchmark, which comprises 934 WSI-level questions, encompassing five clinically-relevant tasks ranging from cancer diagnosis to open-ended reasoning. MultiPathQA also includes 128 questions, authored by two professional pathologists, requiring direct slide interpretation. Using MultiPathQA, we show that our simple agentic system substantially outperforms conventional patch- and thumbnail-based baselines, approaching or surpassing the performance of specialized models trained on millions of images. For example, on pathologist-authored questions, GPT-5 with GIANT achieves 62.5% accuracy, outperforming specialist pathology models such as TITAN (43.8%) and SlideChat (37.5%). Our findings reveal the strengths and limitations of current foundation models and ground future development of LMMs for expert reasoning in pathology.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CodeV: Code with Images for Faithful Visual Reasoning via Tool-Aware Policy Optimization</title>
<link>https://arxiv.org/abs/2511.19661</link>
<guid>https://arxiv.org/abs/2511.19661</guid>
<content:encoded><![CDATA[
<div> Agentic vision-language models, Visual tool-use faithfulness, Tool-Aware Policy Optimization (TAPO), CodeV, Multimodal reasoning benchmarks  

<br /><br />Summary:  
This paper addresses the issue that current agentic vision-language models often achieve high final-answer accuracy despite exhibiting unfaithful visual reasoning, such as invoking image tools on irrelevant regions or ignoring tool outputs. To evaluate this, the authors propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs contain the necessary evidence, revealing a disconnect between accuracy and faithful tool usage in recent models. They introduce CodeV, a code-based visual reasoning agent trained with a novel reinforcement learning method called Tool-Aware Policy Optimization (TAPO). TAPO enhances traditional RL by providing dense, step-wise rewards based directly on visual tool inputs and outputs rather than only chain-of-thought tokens, promoting more reliable supervision and reducing reward hacking. CodeV represents visual tool operations as executable Python code and leverages a two-stage supervised fine-tuning plus RL pipeline to improve both accuracy and faithful tool use on visual search benchmarks. Additionally, CodeV demonstrates strong performance across multimodal reasoning and mathematical problem-solving benchmarks, underlining the importance of explicitly supervising intermediate tool behavior to develop trustworthy and agentic visual reasoning systems. <div>
arXiv:2511.19661v1 Announce Type: new 
Abstract: Agentic vision-language models are increasingly trained to "think with images" by calling image operations. However, we show that high final-answer accuracy often hides unfaithful visual reasoning: models may invoke tools on irrelevant regions or ignore tool outputs entirely, yet still guess the correct answer. In this work, we first propose a faithfulness evaluation protocol that measures whether intermediate visual tool outputs (e.g., crops) actually contain the queried evidence. This reveals that recent visual agents achieve high final-answer accuracy but exhibit low rates of faithful tool-use on visual search benchmarks. We then introduce CodeV, a code-based visual agent trained with Tool-Aware Policy Optimization (TAPO). TAPO is a process-level RL framework that augments GRPO with dense rewards defined directly on visual tool inputs and outputs, rather than on chain-of-thought tokens, making supervision easier to verify and less susceptible to reward hacking. CodeV represents visual tools as executable Python code, and TAPO assigns step-wise rewards based solely on the question and tool output, encouraging both necessary and evidence-consistent tool use. In a two-stage SFT+RL pipeline, CodeV achieves competitive or superior accuracy while substantially increasing faithful tool-use rates on related visual search benchmarks. Beyond visual search, CodeV attains strong performance on a range of multimodal reasoning and math benchmarks, suggesting that explicitly supervising intermediate tool behavior is crucial for building trustworthy, agentic visual reasoning systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OncoVision: Integrating Mammography and Clinical Data through Attention-Driven Multimodal AI for Enhanced Breast Cancer Diagnosis</title>
<link>https://arxiv.org/abs/2511.19667</link>
<guid>https://arxiv.org/abs/2511.19667</guid>
<content:encoded><![CDATA[
<div> Keywords: OncoVision, breast cancer diagnosis, multimodal AI, mammography segmentation, late fusion strategies  

<br /><br />Summary:  
OncoVision is a comprehensive multimodal AI pipeline designed to enhance breast cancer diagnosis by integrating mammography images with clinical data. It employs an attention-based encoder-decoder backbone to accurately and simultaneously segment four critical regions of interest (ROIs) in mammograms: masses, calcifications, axillary findings, and breast tissues. The system robustly predicts ten structured clinical features, including mass morphology, calcification type, ACR breast density, and BI-RADS categories, to support diagnostic decision-making. Two late-fusion strategies have been developed to effectively combine imaging and clinical insights, which improve diagnostic precision and reduce variability among observers. OncoVision is delivered as a secure, user-friendly web application that provides structured reports enhanced with dual-confidence scoring and attention-weighted visualizations, facilitating real-time diagnostic support. This design increases clinician trust and serves as a valuable teaching aid in medical education. Its ease of integration makes it practical for deployment in clinical settings, including resource-limited and underserved regions such as rural South Asia. By combining accurate segmentation with clinical interpretability, OncoVision offers a scalable, equitable solution that enables earlier breast cancer detection and supports timely treatment interventions, ultimately raising the standard of AI-assisted mammography diagnostics. <div>
arXiv:2511.19667v1 Announce Type: new 
Abstract: OncoVision is a multimodal AI pipeline that combines mammography images and clinical data for better breast cancer diagnosis. Employing an attention-based encoder-decoder backbone, it jointly segments four ROIs - masses, calcifications, axillary findings, and breast tissues - with state-of-the-art accuracy and robustly predicts ten structured clinical features: mass morphology, calcification type, ACR breast density, and BI-RADS categories. To fuse imaging and clinical insights, we developed two late-fusion strategies. By utilizing complementary multimodal data, late fusion strategies improve diagnostic precision and reduce inter-observer variability. Operationalized as a secure, user-friendly web application, OncoVision produces structured reports with dual-confidence scoring and attention-weighted visualizations for real-time diagnostic support to improve clinician trust and facilitate medical teaching. It can be easily incorporated into the clinic, making screening available in underprivileged areas around the world, such as rural South Asia. Combining accurate segmentation with clinical intuition, OncoVision raises the bar for AI-based mammography, offering a scalable and equitable solution to detect breast cancer at an earlier stage and enhancing treatment through timely interventions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>INTERLACE: Interleaved Layer Pruning and Efficient Adaptation in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.19676</link>
<guid>https://arxiv.org/abs/2511.19676</guid>
<content:encoded><![CDATA[
<div> INTERLACE, layer pruning, vision-language models, finetuning, redundancy<br /><br />Summary:<br /><br />1. The paper introduces INTERLACE, a novel framework designed to prune redundant layers in vision-language models (VLMs) while preserving their performance through sample-efficient finetuning.  
2. Existing layer pruning methods applied to VLMs typically cause significant performance drops; INTERLACE addresses this by analyzing triplets of consecutive layers to identify local redundancy.  
3. The framework removes the most redundant of the first two layers in each triplet, finetunes the remaining layer to recover lost capacity, and freezes the third layer to act as a stable anchor during finetuning.  
4. This interleaved finetune-freeze approach enables rapid convergence using minimal data after pruning, enhancing training efficiency.  
5. Experimentally, INTERLACE finetunes only a subset of layers on 1% of the FineVision dataset for one epoch, achieving 88.9% average performance retention after dropping 25% of the network, reaching state-of-the-art results.  
6. The authors have made their code publicly accessible on GitHub for further research and development. <div>
arXiv:2511.19676v1 Announce Type: new 
Abstract: We introduce INTERLACE, a novel framework that prunes redundant layers in VLMs while maintaining performance through sample-efficient finetuning. Existing layer pruning methods lead to significant performance drop when applied to VLMs. Instead, we analyze triplets of consecutive layers to identify local redundancy, removing the most redundant of the first two layers, finetune the remaining layer to compensate for the lost capacity, and freeze the third layer to serve as a stable anchor during finetuning. We found that this interleaved finetune-freeze design enables rapid convergence with minimal data after pruning. By finetuning only a subset of layers on just 1% of the FineVision dataset for one epoch, Interlace achieves 88.9% average performance retention after dropping 25% of the network, achieving SOTA performance. Our code is available at: https://github.com/pmadinei/Interlace.git
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndEgo: A Dataset of Industrial Scenarios and Collaborative Work for Egocentric Assistants</title>
<link>https://arxiv.org/abs/2511.19684</link>
<guid>https://arxiv.org/abs/2511.19684</guid>
<content:encoded><![CDATA[
<div> Keywords: IndEgo, multimodal dataset, egocentric, exocentric, collaborative work<br /><br />Summary:  
The paper introduces IndEgo, a comprehensive multimodal dataset designed for industrial tasks such as assembly/disassembly, logistics, inspection and repair, woodworking, and more. It contains 3,460 egocentric recordings totaling about 197 hours and 1,092 exocentric recordings amounting to roughly 97 hours. A significant focus is placed on collaborative work scenarios where two workers jointly carry out complex cognitive and physical tasks. The egocentric data is multimodal, incorporating eye gaze tracking, narration, sound, and motion, which enrich contextual understanding. Detailed annotations are provided, including action labels, summaries, mistake annotations, and narrations, alongside metadata and processed outputs like hand pose estimation, eye gaze data, and semi-dense point clouds. The dataset also includes benchmarks targeting procedural and non-procedural task understanding, mistake detection, and reasoning-based question answering. Baseline evaluations indicate that state-of-the-art multimodal models struggle with tasks on this dataset, underscoring its challenge and potential for advancing research. IndEgo is publicly available for the research community at the Hugging Face dataset repository. <div>
arXiv:2511.19684v1 Announce Type: new 
Abstract: We introduce IndEgo, a multimodal egocentric and exocentric dataset addressing common industrial tasks, including assembly/disassembly, logistics and organisation, inspection and repair, woodworking, and others. The dataset contains 3,460 egocentric recordings (approximately 197 hours), along with 1,092 exocentric recordings (approximately 97 hours). A key focus of the dataset is collaborative work, where two workers jointly perform cognitively and physically intensive tasks. The egocentric recordings include rich multimodal data and added context via eye gaze, narration, sound, motion, and others. We provide detailed annotations (actions, summaries, mistake annotations, narrations), metadata, processed outputs (eye gaze, hand pose, semi-dense point cloud), and benchmarks on procedural and non-procedural task understanding, Mistake Detection, and reasoning-based Question Answering. Baseline evaluations for Mistake Detection, Question Answering and collaborative task understanding show that the dataset presents a challenge for the state-of-the-art multimodal models. Our dataset is available at: https://huggingface.co/datasets/FraunhoferIPK/IndEgo
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CountXplain: Interpretable Cell Counting with Prototype-Based Density Map Estimation</title>
<link>https://arxiv.org/abs/2511.19686</link>
<guid>https://arxiv.org/abs/2511.19686</guid>
<content:encoded><![CDATA[
<div> Keywords: cell counting, interpretability, prototype-based method, density map estimation, biomedical imaging<br /><br />Summary:<br /><br />This paper addresses the challenge of interpretability in deep learning models for cell counting in biomedical imaging. The authors propose a novel prototype-based approach integrated into a density map estimation network, which enables the model to learn representative visual patterns for cells and background artifacts. This design allows the model not only to count cells but also to provide explanations by highlighting image regions most similar to learned prototypes. To validate interpretability, the learned prototypes were evaluated through a survey involving biologists, who confirmed the relevance of the identified visual patterns. Experimental validation was conducted on two public datasets, demonstrating that the proposed method achieves interpretability without sacrificing counting accuracy. By generating visual explanations linked directly to counting decisions, the method increases transparency and trustworthiness, which is crucial for clinical adoption. The approach thus offers a reliable and interpretable tool for cell counting in biomedical settings, potentially accelerating the use of deep learning in critical healthcare applications. Additionally, the authors provide open-source code for their method, encouraging reproducibility and further research in this domain. <div>
arXiv:2511.19686v1 Announce Type: new 
Abstract: Cell counting in biomedical imaging is pivotal for various clinical applications, yet the interpretability of deep learning models in this domain remains a significant challenge. We propose a novel prototype-based method for interpretable cell counting via density map estimation. Our approach integrates a prototype layer into the density estimation network, enabling the model to learn representative visual patterns for both cells and background artifacts. The learned prototypes were evaluated through a survey of biologists, who confirmed the relevance of the visual patterns identified, further validating the interpretability of the model. By generating interpretations that highlight regions in the input image most similar to each prototype, our method offers a clear understanding of how the model identifies and counts cells. Extensive experiments on two public datasets demonstrate that our method achieves interpretability without compromising counting effectiveness. This work provides researchers and clinicians with a transparent and reliable tool for cell counting, potentially increasing trust and accelerating the adoption of deep learning in critical biomedical applications. Code is available at https://github.com/NRT-D4/CountXplain.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RADSeg: Unleashing Parameter and Compute Efficient Zero-Shot Open-Vocabulary Segmentation Using Agglomerative Models</title>
<link>https://arxiv.org/abs/2511.19704</link>
<guid>https://arxiv.org/abs/2511.19704</guid>
<content:encoded><![CDATA[
<div> Open-vocabulary semantic segmentation, zero-shot OVSS, RADIO model, self-correlating attention, RADSeg  

<br /><br />Summary:  
The paper addresses the challenge of open-vocabulary semantic segmentation (OVSS), which is critical for vision and robotics tasks requiring broad semantic understanding. Current methods either depend on limited segmentation training data or utilize zero-shot heuristics with vision-language models like CLIP, often combining multiple large models that lead to high computational and memory costs. This study investigates the agglomerative vision foundation model RADIO for zero-shot OVSS, improving performance along three key dimensions: mean Intersection over Union (mIoU), inference latency, and parameter efficiency. The authors introduce RADSeg, a novel approach that enhances RADIO via self-correlating recursive attention, self-correlating global aggregation, and efficient mask refinement. RADSeg achieves a 6-30% improvement in mIoU on the base Vision Transformer (ViT) class while being 3.95 times faster and using 2.5 times fewer parameters. Remarkably, RADSeg-base with 105 million parameters surpasses previous state-of-the-art approaches that combine much larger vision models (850-1350 million parameters) in accuracy, with substantially lower computational and memory requirements. This work highlights the potential for lightweight, efficient models to achieve competitive performance in zero-shot OVSS tasks. <div>
arXiv:2511.19704v1 Announce Type: new 
Abstract: Open-vocabulary semantic segmentation (OVSS) underpins many vision and robotics tasks that require generalizable semantic understanding. Existing approaches either rely on limited segmentation training data, which hinders generalization, or apply zero-shot heuristics to vision-language models (e.g CLIP), while the most competitive approaches combine multiple models to improve performance at the cost of high computational and memory demands. In this work, we leverage an overlooked agglomerative vision foundation model, RADIO, to improve zero-shot OVSS along three key axes simultaneously: mIoU, latency, and parameter efficiency. We present the first comprehensive study of RADIO for zero-shot OVSS and enhance its performance through self-correlating recursive attention, self-correlating global aggregation, and computationally efficient mask refinement. Our approach, RADSeg, achieves 6-30% mIoU improvement in the base ViT class while being 3.95x faster and using 2.5x fewer parameters. Surprisingly, RADSeg-base (105M) outperforms previous combinations of huge vision models (850-1350M) in mIoU, achieving state-of-the-art accuracy with substantially lower computational and memory cost.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Vision Transformer Depth via Structural Reparameterization</title>
<link>https://arxiv.org/abs/2511.19718</link>
<guid>https://arxiv.org/abs/2511.19718</guid>
<content:encoded><![CDATA[
<div> Transformer, Structural Reparameterization, Vision Transformers, Model Compression, Inference Acceleration  

<br /><br />Summary:  
This paper addresses the computational overhead of Vision Transformers, which primarily arises due to their deep stacked architectures. Existing acceleration techniques typically focus on algorithmic optimizations such as token pruning and speeding up attention mechanisms, but this work explores reducing the number of transformer layers while maintaining representational capacity. The authors propose a branch-based structural reparameterization technique applied during training, which employs parallel branches within transformer blocks. These branches can be systematically merged into a more compact single-path model for inference without approximation errors. The merging process happens at nonlinear module entry points, enabling exact mathematical reparameterization in both feed-forward networks (FFN) and multi-head self-attention (MHSA) modules. When implemented on the ViT-Tiny model, the method successfully reduces the original 12-layer transformer to as few as 3 layers, sustaining classification accuracy on the ImageNet-1K dataset. The compressed models demonstrate up to 37% inference speedup on mobile CPU platforms. Overall, the study challenges the necessity of very deep transformer stacks in vision tasks and opens new avenues for creating more efficient, shallower vision transformers with competitive performance. <div>
arXiv:2511.19718v1 Announce Type: new 
Abstract: The computational overhead of Vision Transformers in practice stems fundamentally from their deep architectures, yet existing acceleration strategies have primarily targeted algorithmic-level optimizations such as token pruning and attention speedup. This leaves an underexplored research question: can we reduce the number of stacked transformer layers while maintaining comparable representational capacity? To answer this, we propose a branch-based structural reparameterization technique that operates during the training phase. Our approach leverages parallel branches within transformer blocks that can be systematically consolidated into streamlined single-path models suitable for inference deployment. The consolidation mechanism works by gradually merging branches at the entry points of nonlinear components, enabling both feed-forward networks (FFN) and multi-head self-attention (MHSA) modules to undergo exact mathematical reparameterization without inducing approximation errors at test time. When applied to ViT-Tiny, the framework successfully reduces the original 12-layer architecture to 6, 4, or as few as 3 layers while maintaining classification accuracy on ImageNet-1K. The resulting compressed models achieve inference speedups of up to 37% on mobile CPU platforms. Our findings suggest that the conventional wisdom favoring extremely deep transformer stacks may be unnecessarily restrictive, and point toward new opportunities for constructing efficient vision transformers.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maritime Small Object Detection from UAVs using Deep Learning with Altitude-Aware Dynamic Tiling</title>
<link>https://arxiv.org/abs/2511.19728</link>
<guid>https://arxiv.org/abs/2511.19728</guid>
<content:encoded><![CDATA[
<div> UAV, small object detection, altitude-aware, dynamic tiling, SAR<br /><br />Summary:<br /><br />Unmanned Aerial Vehicles (UAVs) play a vital role in Search and Rescue (SAR) missions, especially over large maritime areas where they help in monitoring for targets. Detecting small objects from high altitudes remains challenging due to the low object-to-background pixel ratio, which hampers accuracy. To address this, the paper proposes an altitude-aware dynamic tiling method that adjusts the image scaling and adaptively subdivides images into tiles based on UAV altitude. This method aims to enhance small object detection by increasing effective resolution where needed while minimizing unnecessary computations. The approach integrates altitude-dependent scaling with an adaptive tiling factor, balancing detection performance and computational efficiency. Experiments were conducted on the SeaDronesSee dataset using the YOLOv5 object detection model and the Slicing Aided Hyper Inference (SAHI) framework. Results indicate a significant improvement of 38% in Mean Average Precision (mAP) for small objects compared to a baseline method. Additionally, the method achieves over twice the inference speed versus static tiling approaches. Overall, this altitude-aware dynamic tiling strategy enables more efficient and accurate UAV-based SAR operations across varying conditions by improving small object detection without compromising speed. <div>
arXiv:2511.19728v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) are crucial in Search and Rescue (SAR) missions due to their ability to monitor vast maritime areas. However, small objects often remain difficult to detect from high altitudes due to low object-to-background pixel ratios. We propose an altitude-aware dynamic tiling method that scales and adaptively subdivides the image into tiles for enhanced small object detection. By integrating altitude-dependent scaling with an adaptive tiling factor, we reduce unnecessary computation while maintaining detection performance. Tested on the SeaDronesSee dataset [1] with YOLOv5 [2] and Slicing Aided Hyper Inference (SAHI) framework [3], our approach improves Mean Average Precision (mAP) for small objects by 38% compared to a baseline and achieves more than double the inference speed compared to static tiling. This approach enables more efficient and accurate UAV-based SAR operations under diverse conditions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Transferable Optimal Transport via Min-Sliced Transport Plans</title>
<link>https://arxiv.org/abs/2511.19741</link>
<guid>https://arxiv.org/abs/2511.19741</guid>
<content:encoded><![CDATA[
<div> Optimal Transport, min-Sliced Transport Plan, Transferability, Computational Scalability, Point Cloud Alignment<br /><br />Summary:<br /><br />Optimal Transport (OT) is a versatile tool for matching and alignment in computer vision but suffers from high computational costs, limiting its scalability. Recent methods reduce this cost by using slice-based approaches that project data onto one-dimensional subspaces, where OT problems have closed-form solutions. These methods optimize slicers to generate conditional transport plans that minimize transport cost in the original high-dimensional space. However, a key open question remains: can these optimized slicers transfer effectively when applied to new, unseen distribution pairs under distributional shifts? Addressing this, the study focuses on the min-Sliced Transport Plan (min-STP) framework and theoretically demonstrates that optimized slicers are stable and remain effective under small perturbations in data distributions, supporting transferability across related tasks. To make min-STP more scalable, the authors propose a minibatch formulation with statistical accuracy guarantees. Empirical results show that transferable min-STP performs well in one-shot matching scenarios and enables amortized training, benefiting applications such as point cloud alignment and flow-based generative modeling. This work thus provides both theoretical and practical advances for efficient, transferable OT computations in evolving or related data settings. <div>
arXiv:2511.19741v1 Announce Type: new 
Abstract: Optimal Transport (OT) offers a powerful framework for finding correspondences between distributions and addressing matching and alignment problems in various areas of computer vision, including shape analysis, image generation, and multimodal tasks. The computation cost of OT, however, hinders its scalability. Slice-based transport plans have recently shown promise for reducing the computational cost by leveraging the closed-form solutions of 1D OT problems. These methods optimize a one-dimensional projection (slice) to obtain a conditional transport plan that minimizes the transport cost in the ambient space. While efficient, these methods leave open the question of whether learned optimal slicers can transfer to new distribution pairs under distributional shift. Understanding this transferability is crucial in settings with evolving data or repeated OT computations across closely related distributions. In this paper, we study the min-Sliced Transport Plan (min-STP) framework and investigate the transferability of optimized slicers: can a slicer trained on one distribution pair yield effective transport plans for new, unseen pairs? Theoretically, we show that optimized slicers remain close under slight perturbations of the data distributions, enabling efficient transfer across related tasks. To further improve scalability, we introduce a minibatch formulation of min-STP and provide statistical guarantees on its accuracy. Empirically, we demonstrate that the transferable min-STP achieves strong one-shot matching performance and facilitates amortized training for point cloud alignment and flow-based generative modeling.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Foundation Models for Histological Grading in Cutaneous Squamous Cell Carcinoma using PathFMTools</title>
<link>https://arxiv.org/abs/2511.19751</link>
<guid>https://arxiv.org/abs/2511.19751</guid>
<content:encoded><![CDATA[
<div> Keywords: pathology foundation models, whole-slide image, vision-language models, cutaneous squamous cell carcinoma, adaptation strategies  

<br /><br />Summary:  
This paper addresses the challenges of adapting computational pathology foundation models to specific clinical tasks, focusing on whole-slide image (WSI) processing difficulties, feature opacity, and diverse adaptation strategies. To overcome these, the authors introduce PathFMTools, a lightweight and extensible Python package designed to enable efficient execution, analysis, and visualization of pathology foundation models. The study evaluates two state-of-the-art vision-language foundation models, CONCH and MUSK, using PathFMTools on the task of histological grading of cutaneous squamous cell carcinoma (cSCC), a crucial factor in cSCC staging and patient management. Utilizing a cohort of 440 cSCC H&amp;E stained WSIs, multiple adaptation strategies are benchmarked, revealing trade-offs between different prediction approaches. The results validate the feasibility of leveraging foundation model embeddings to train smaller, specialist models tailored to the clinical task. Overall, the findings highlight the promise of pathology foundation models for practical clinical applications and demonstrate that PathFMTools facilitates efficient model analysis and validation, thus promoting further research and deployment in real-world settings. <div>
arXiv:2511.19751v1 Announce Type: new 
Abstract: Despite the promise of computational pathology foundation models, adapting them to specific clinical tasks remains challenging due to the complexity of whole-slide image (WSI) processing, the opacity of learned features, and the wide range of potential adaptation strategies. To address these challenges, we introduce PathFMTools, a lightweight, extensible Python package that enables efficient execution, analysis, and visualization of pathology foundation models. We use this tool to interface with and evaluate two state-of-the-art vision-language foundation models, CONCH and MUSK, on the task of histological grading in cutaneous squamous cell carcinoma (cSCC), a critical criterion that informs cSCC staging and patient management. Using a cohort of 440 cSCC H&amp;E WSIs, we benchmark multiple adaptation strategies, demonstrating trade-offs across prediction approaches and validating the potential of using foundation model embeddings to train small specialist models. These findings underscore the promise of pathology foundation models for real-world clinical applications, with PathFMTools enabling efficient analysis and validation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What You See is (Usually) What You Get: Multimodal Prototype Networks that Abstain from Expensive Modalities</title>
<link>https://arxiv.org/abs/2511.19752</link>
<guid>https://arxiv.org/abs/2511.19752</guid>
<content:encoded><![CDATA[
<div> Species detection, multimodal neural networks, prototype networks, interpretability, cost-aware genetic data<br /><br />Summary:<br /><br />Species detection plays a vital role in monitoring ecosystem health and identifying invasive species, which is crucial for conservation efforts. Multimodal neural networks have increasingly been used for species identification, but they suffer from two main drawbacks: lack of interpretability due to their black-box nature and the high cost and invasiveness of collecting genetic data. The paper addresses these challenges by extending prototype networks (ProtoPNets), known for their interpretability, to a multimodal and cost-aware framework. Their approach ensembles prototypes from multiple modalities (e.g., images and genetic data), assigning weights to control the influence of each modality on the prediction. A key innovation is the system’s ability to selectively use expensive genetic data only when necessary for fine-grained classification, while relying primarily on more abundant, non-invasive image data for clearer cases. This selective use of costly data reduces the need for invasive specimen capture or destruction. Experimental results show that this method achieves accuracy comparable to models using both modalities consistently, demonstrating an intelligent balance between cost efficiency and predictive performance while maintaining interpretability. <div>
arXiv:2511.19752v1 Announce Type: new 
Abstract: Species detection is important for monitoring the health of ecosystems and identifying invasive species, serving a crucial role in guiding conservation efforts. Multimodal neural networks have seen increasing use for identifying species to help automate this task, but they have two major drawbacks. First, their black-box nature prevents the interpretability of their decision making process. Second, collecting genetic data is often expensive and requires invasive procedures, often necessitating researchers to capture or kill the target specimen. We address both of these problems by extending prototype networks (ProtoPNets), which are a popular and interpretable alternative to traditional neural networks, to the multimodal, cost-aware setting. We ensemble prototypes from each modality, using an associated weight to determine how much a given prediction relies on each modality. We further introduce methods to identify cases for which we do not need the expensive genetic information to make confident predictions. We demonstrate that our approach can intelligently allocate expensive genetic data for fine-grained distinctions while using abundant image data for clearer visual classifications and achieving comparable accuracy to models that consistently use both modalities.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision--Language Enhanced Foundation Model for Semi-supervised Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.19759</link>
<guid>https://arxiv.org/abs/2511.19759</guid>
<content:encoded><![CDATA[
<div> Keywords: semi-supervised learning, medical image segmentation, vision-language models, VESSA, pseudo-labels<br /><br />Summary:  
This work proposes VESSA, a Vision-Language Enhanced Semi-supervised Segmentation Assistant that integrates foundation-level visual-semantic knowledge into semi-supervised learning (SSL) for medical image segmentation. The method operates in two stages: Stage 1 involves training VESSA as a reference-guided segmentation assistant using a template bank of gold-standard exemplars, allowing it to perform visual feature matching and generate structured prompts for a SAM2-inspired mask decoder. This enables learning from limited annotated data by extracting representative semantic and spatial cues. In Stage 2, VESSA is incorporated into an advanced SSL framework where it dynamically interacts with the student model. As the student's predictions improve, they are used as prompts for VESSA to produce higher-quality pseudo-labels, enhancing guidance further. Extensive experiments across multiple datasets and domains demonstrate that the VESSA-augmented SSL approach significantly improves segmentation accuracy, particularly in scenarios with extremely limited annotations. The results consistently outperform state-of-the-art baselines, confirming the effectiveness of combining vision-language understanding with semi-supervised segmentation in medical imaging. <div>
arXiv:2511.19759v1 Announce Type: new 
Abstract: Semi-supervised learning (SSL) has emerged as an effective paradigm for medical image segmentation, reducing the reliance on extensive expert annotations. Meanwhile, vision-language models (VLMs) have demonstrated strong generalization and few-shot capabilities across diverse visual domains. In this work, we integrate VLM-based segmentation into semi-supervised medical image segmentation by introducing a Vision-Language Enhanced Semi-supervised Segmentation Assistant (VESSA) that incorporates foundation-level visual-semantic understanding into SSL frameworks. Our approach consists of two stages. In Stage 1, the VLM-enhanced segmentation foundation model VESSA is trained as a reference-guided segmentation assistant using a template bank containing gold-standard exemplars, simulating learning from limited labeled data. Given an input-template pair, VESSA performs visual feature matching to extract representative semantic and spatial cues from exemplar segmentations, generating structured prompts for a SAM2-inspired mask decoder to produce segmentation masks. In Stage 2, VESSA is integrated into a state-of-the-art SSL framework, enabling dynamic interaction with the student model: as student predictions become more refined, they are fed back to VESSA as prompts, allowing it to generate higher-quality pseudo-labels and stronger guidance. Extensive experiments across multiple segmentation datasets and domains show that VESSA-augmented SSL significantly enhances segmentation accuracy, outperforming state-of-the-art baselines under extremely limited annotation conditions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Storage-Efficient Feature for 3D Concrete Defect Segmentation to Replace Normal Vector</title>
<link>https://arxiv.org/abs/2511.19760</link>
<guid>https://arxiv.org/abs/2511.19760</guid>
<content:encoded><![CDATA[
<div> Keywords: point cloud reconstruction, relative angle, concrete surface defects, entropy-based feature evaluation, PointNet++<br /><br />Summary:  
This study addresses the limitations of image-based damage detection methods, which are vulnerable to background noise, by focusing on point cloud reconstruction. It introduces a novel single-dimensional feature called the relative angle, defined as the angle between the normal vector of a point and the average normal vector of its parent point cloud, capturing directionality information critical for identifying concrete surface defects. The research employs entropy-based feature evaluation to demonstrate that the relative angle feature effectively filters out redundant data in undamaged sections while preserving essential information in damaged areas. Models trained and tested using PointNet++ with relative angle features achieve similar performance compared to those using traditional normal vectors. Importantly, the use of relative angle leads to a 27.6% reduction in data storage and an 83% compression of input channels. This significant data compression facilitates the processing of larger batches on hardware with limited resources, without needing any architectural changes to existing models. Overall, the relative angle feature offers an efficient and compact alternative for 3D damage reconstruction tasks, enabling more scalable and resource-friendly implementations. <div>
arXiv:2511.19760v1 Announce Type: new 
Abstract: Point cloud reconstruction of damage offers an effective solution to image-based methods vulnerable to background noise, yet its application is constrained by the high volume of 3D data. This study proposes a new feature, relative angle, computed as the angle between the normal vector of a point and the average normal vector of its parent point cloud. This single-dimensional feature provides directionality information equivalent to normal vectors for concrete surface defect characteristics. Through entropy-based feature evaluation, this study demonstrates the ability of relative angle to filter out redundant information in undamaged sections while retaining effective information in damaged sections. By training and testing with PointNet++, models based on the relative angles achieved similar performance to that of models based on normal vectors while delivering 27.6% storage reduction and 83% input channel compression. This novel feature has the potential to enable larger-batch execution on resource-constrained hardware without the necessity of architectural modifications to models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lightweight Transformer Framework for Weakly Supervised Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.19765</link>
<guid>https://arxiv.org/abs/2511.19765</guid>
<content:encoded><![CDATA[
<div> Weakly supervised semantic segmentation, SegFormer, boundary-aware loss, uncertainty-guided refiner, multi-scale fusion<br /><br />Summary:<br /><br />This paper addresses the challenge of weakly supervised semantic segmentation (WSSS), which involves learning dense segmentation masks from noisy and incomplete cues. The authors revisit the SegFormer decoder and propose three key enhancements forming their method called CrispFormer. First, they introduce a boundary branch that uses a lightweight edge detection head combined with a boundary-aware loss to effectively supervise thin object contours, improving boundary precision. Second, an uncertainty-guided refiner is employed to estimate per-pixel aleatoric uncertainty; this information is leveraged to weight the loss function and control a residual correction applied to segmentation logits, helping to mitigate the impact of noisy labels. Third, a dynamic multi-scale fusion layer replaces the conventional static concatenation of features with a spatial softmax gating that selectively fuses multi-resolution information, which can be optionally modulated by uncertainty estimates. These synergistic changes lead to a single-pass decoder that preserves sharp object boundaries, adaptively handles multi-scale features per location, and is robust against weak supervision label noise. Integrated within a standard WSSS training pipeline involving seed generation, student learning, and EMA relabeling, CrispFormer consistently outperforms SegFormer baselines in boundary F-score, recall of small objects, and mean Intersection over Union (mIoU) with minimal extra computational cost. The approach is simple to implement, compatible with SegFormer variants, and provides a reproducible route to higher-fidelity segmentation masks using only image-level labels. <div>
arXiv:2511.19765v1 Announce Type: new 
Abstract: Weakly supervised semantic segmentation (WSSS) must learn dense masks from noisy, under-specified cues. We revisit the SegFormer decoder and show that three small, synergistic changes make weak supervision markedly more effective-without altering the MiT backbone or relying on heavy post-processing. Our method, CrispFormer, augments the decoder with: (1) a boundary branch that supervises thin object contours using a lightweight edge head and a boundary-aware loss; (2) an uncertainty-guided refiner that predicts per-pixel aleatoric uncertainty and uses it to weight losses and gate a residual correction of the segmentation logits; and (3) a dynamic multi-scale fusion layer that replaces static concatenation with spatial softmax gating over multi-resolution features, optionally modulated by uncertainty. The result is a single-pass model that preserves crisp boundaries, selects appropriate scales per location, and resists label noise from weak cues. Integrated into a standard WSSS pipeline (seed, student, and EMA relabeling), CrispFormer consistently improves boundary F-score, small-object recall, and mIoU over SegFormer baselines trained on the same seeds, while adding minimal compute. Our decoder-centric formulation is simple to implement, broadly compatible with existing SegFormer variants, and offers a reproducible path to higher-fidelity masks from image-level supervision.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering</title>
<link>https://arxiv.org/abs/2511.19768</link>
<guid>https://arxiv.org/abs/2511.19768</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Embodied Question Answering, Frontier Oscillations, Step-level Calibration, Prune-Then-Plan<br /><br />Summary:<br /><br />This paper addresses the challenges faced by large vision-language models (VLMs) in embodied question answering (EQA) tasks, particularly the problem of frontier oscillations—unstable back-and-forth navigation caused by VLM overconfidence and miscalibration. To overcome these inefficiencies, the authors propose a novel framework called Prune-Then-Plan. This approach first prunes unlikely frontier choices using a pruning method inspired by the Holm-Bonferroni procedure, thereby eliminating unreliable candidates. The decision-making then defers to a coverage-based planner, which ensures more stable and interpretable navigation actions. By separating overconfident predictions from final navigation decisions, the method achieves step-level calibration that better aligns with human judgment and reduces erratic movements. When integrated with the 3D-Mem embodied question answering framework, Prune-Then-Plan significantly improves performance, with relative gains of up to 49% in visually grounded Success weighted by Path Length (SPL) and 33% in LLM-Match metrics compared to existing baselines. Additionally, the method enhances scene coverage given the same exploration budget and demonstrates effectiveness on both OpenEQA and EXPRESS-Bench datasets. Overall, this work provides a simple but effective solution for stabilizing EQA agent exploration and improving answer quality through calibrated step-level decision-making. <div>
arXiv:2511.19768v1 Announce Type: new 
Abstract: Large vision-language models (VLMs) have improved embodied question answering (EQA) agents by providing strong semantic priors for open-vocabulary reasoning. However, when used directly for step-level exploration, VLMs often exhibit frontier oscillations, unstable back-and-forth movements caused by overconfidence and miscalibration, leading to inefficient navigation and degraded answer quality. We propose Prune-Then-Plan, a simple and effective framework that stabilizes exploration through step-level calibration. Instead of trusting raw VLM scores, our method prunes implausible frontier choices using a Holm-Bonferroni inspired pruning procedure and then delegates final decisions to a coverage-based planner. This separation converts overconfident predictions into conservative, interpretable actions by relying on human-level judgments to calibrate the step-level behavior of VLMs. Integrated into the 3D-Mem EQA framework, our approach achieves relative improvements of up to 49% and 33% in visually grounded SPL and LLM-Match metrics respectively over baselines. Overall, our method achieves better scene coverage under equal exploration budgets on both OpenEQA and EXPRESS-Bench datasets.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Attention, One Scale: Phase-Aligned Rotary Positional Embeddings for Mixed-Resolution Diffusion Transformer</title>
<link>https://arxiv.org/abs/2511.19778</link>
<guid>https://arxiv.org/abs/2511.19778</guid>
<content:encoded><![CDATA[
<div> Rotary Positional Embeddings, Diffusion Transformers, Cross-Resolution Phase-Aligned Attention, mixed-resolution denoising, phase aliasing  

<br /><br />Summary:  
1. The paper identifies a core failure mode in Diffusion Transformers (DiTs) when applying linear interpolation on rotary positional embeddings (RoPE) for mixed-resolution denoising tasks.  
2. When tokens from different spatial grids are mixed, the attention mechanism degrades due to structural issues caused by linear coordinate remapping, which forces attention heads to compare RoPE phases sampled at incompatible rates.  
3. This discrepancy leads to phase aliasing, destabilizing the score landscape and causing sharp, periodic phase selectivity in pretrained DiTs, resulting in blur, artifacts, or complete collapse even from minor inconsistencies.  
4. To resolve this, the authors propose Cross-Resolution Phase-Aligned Attention (CRPA), a training-free, drop-in modification to the RoPE index mapping that expresses all Q/K positions on the query’s stride, ensuring consistent phase increments across physical distances.  
5. CRPA restores the phase patterns crucial for DiT performance, uniformly stabilizes all attention heads and layers, is fully compatible with pretrained models, and demonstrates superior performance in high-fidelity, efficient mixed-resolution image and video generation compared to previous state-of-the-art methods. <div>
arXiv:2511.19778v1 Announce Type: new 
Abstract: We identify a core failure mode that occurs when using the usual linear interpolation on rotary positional embeddings (RoPE) for mixed-resolution denoising with Diffusion Transformers. When tokens from different spatial grids are mixed, the attention mechanism collapses. The issue is structural. Linear coordinate remapping forces a single attention head to compare RoPE phases sampled at incompatible rates, creating phase aliasing that destabilizes the score landscape. Pretrained DiTs are especially brittle-many heads exhibit extremely sharp, periodic phase selectivity-so even tiny cross-rate inconsistencies reliably cause blur, artifacts, or full collapse.
  To this end, our main contribution is Cross-Resolution Phase-Aligned Attention (CRPA), a training-free drop-in fix that eliminates this failure at its source. CRPA modifies only the RoPE index map for each attention call: all Q/K positions are expressed on the query's stride so that equal physical distances always induce identical phase increments. This restores the precise phase patterns that DiTs rely on. CRPA is fully compatible with pretrained DiTs, stabilizes all heads and layers uniformly. We demonstrate that CRPA enables high-fidelity and efficient mixed-resolution generation, outperforming previous state-of-the-art methods on image and video generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reading Between the Lines: Abstaining from VLM-Generated OCR Errors via Latent Representation Probes</title>
<link>https://arxiv.org/abs/2511.19806</link>
<guid>https://arxiv.org/abs/2511.19806</guid>
<content:encoded><![CDATA[
<div> Keywords: Visual Language Models, Uncertainty Estimation, Abstention, Latent Representation Probing, Scene Text Visual Question Answering<br /><br />Summary:  
1. The paper addresses the critical need for Visual Language Models (VLMs) to abstain from answering when uncertain, especially in safety-critical tasks like Scene Text Visual Question Answering (STVQA) where OCR errors can have serious consequences.  
2. Existing abstention methods are inadequate as they depend on miscalibrated output probabilities or semantic agreement, which are not suitable for OCR-related tasks.  
3. The authors propose that uncertainty signals are more effectively found in the internal latent representations of VLMs rather than their final outputs.  
4. They introduce Latent Representation Probing (LRP), which involves training lightweight probes on hidden states or attention mechanisms within the model to detect uncertainty.  
5. Three different probe designs are explored: concatenating hidden representations across all layers, aggregating attention over visual tokens, and ensembling single-layer probes through majority voting.  
6. Experiments on four benchmarks that span image and video data demonstrate that LRP improves abstention accuracy by 7.6% compared to the best existing baselines.  
7. Analysis shows that probes generalize well across different sources of uncertainty and datasets, and that the most informative signals come from intermediate model layers rather than the final output layers.  
8. This work provides a principled framework for creating deployment-ready AI systems by leveraging confidence signals from internal model states rather than relying on unreliable output probabilities. <div>
arXiv:2511.19806v1 Announce Type: new 
Abstract: As VLMs are deployed in safety-critical applications, their ability to abstain from answering when uncertain becomes crucial for reliability, especially in Scene Text Visual Question Answering (STVQA) tasks. For example, OCR errors like misreading "50 mph" as "60 mph" could cause severe traffic accidents. This leads us to ask: Can VLMs know when they can't see? Existing abstention methods suggest pessimistic answers: they either rely on miscalibrated output probabilities or require semantic agreement unsuitable for OCR tasks. However, this failure may indicate we are looking in the wrong place: uncertainty signals could be hidden in VLMs' internal representations.
  Building on this insight, we propose Latent Representation Probing (LRP): training lightweight probes on hidden states or attention patterns. We explore three probe designs: concatenating representations across all layers, aggregating attention over visual tokens, and ensembling single layer probes by majority vote. Experiments on four benchmarks across image and video modalities show LRP improves abstention accuracy by 7.6\% over best baselines. Our analysis reveals: probes generalize across various uncertainty sources and datasets, and optimal signals emerge from intermediate rather than final layers. This establishes a principled framework for building deployment-ready AI systems by detecting confidence signals from internal states rather than unreliable outputs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Generation of Diverse and High-Fidelity Images via Prompt Semantic Space Optimization</title>
<link>https://arxiv.org/abs/2511.19811</link>
<guid>https://arxiv.org/abs/2511.19811</guid>
<content:encoded><![CDATA[
<div> Keywords: Image diversity, text-to-image diffusion, Token-Prompt embedding Space Optimization, mode collapse, generative quality<br /><br />Summary:<br /><br />1. Text-to-image diffusion models often struggle with low image diversity, tending to generate repetitive outputs that limit creative exploration and usefulness in downstream applications.<br />2. This low diversity is largely caused by generation collapsing towards strong modes in the learned data distribution, resulting in sampling redundancy.<br />3. Previous methods to enhance diversity, including noise resampling, prompt rewriting, and steering-based guidance, have issues such as collapsing to dominant modes again or introducing distortions that reduce image quality.<br />4. The authors propose a novel module called Token-Prompt embedding Space Optimization (TPSO), which is training-free and model-agnostic. It uses learnable parameters to explore underrepresented regions of the token embedding space, mitigating mode collapse.<br />5. TPSO applies a global semantic constraint at the prompt level to regulate distribution shifts, preserving image quality and high fidelity.<br />6. Experimental results on MS-COCO and three different diffusion model backbones demonstrate that TPSO greatly enhances generative diversity, boosting baseline diversity scores from 1.10 to 4.18, without compromising image quality.<br />7. The authors plan to release the code upon acceptance of the work. <div>
arXiv:2511.19811v1 Announce Type: new 
Abstract: Image diversity remains a fundamental challenge for text-to-image diffusion models. Low-diversity models tend to generate repetitive outputs, increasing sampling redundancy and hindering both creative exploration and downstream applications. A primary cause is that generation often collapses toward a strong mode in the learned distribution. Existing attempts to improve diversity, such as noise resampling, prompt rewriting, or steering-based guidance, often still collapse to dominant modes or introduce distortions that degrade image quality. In light of this, we propose Token-Prompt embedding Space Optimization (TPSO), a training-free and model-agnostic module. TPSO introduces learnable parameters to explore underrepresented regions of the token embedding space, reducing the tendency of the model to repeatedly generate samples from strong modes of the learned distribution. At the same time, the prompt-level space provides a global semantic constraint that regulates distribution shifts, preventing quality degradation while maintaining high fidelity. Extensive experiments on MS-COCO and three diffusion backbones show that TPSO significantly enhances generative diversity, improving baseline performance from 1.10 to 4.18 points, without sacrificing image quality. Code will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception</title>
<link>https://arxiv.org/abs/2511.19820</link>
<guid>https://arxiv.org/abs/2511.19820</guid>
<content:encoded><![CDATA[
<div> CropVLM, Vision-Language Models, fine-grained image understanding, reinforcement learning, dynamic zooming<br /><br />Summary:<br /><br />Vision-Language Models (VLMs) frequently face difficulties with tasks needing detailed image comprehension, such as scene-text recognition and document analysis, primarily due to their limited perception and issues with visual fragmentation. To overcome these challenges, the paper introduces CropVLM, an external enhancement method that allows VLMs to "zoom in" dynamically on important regions of an image, thus improving their focus on fine details without altering the original model. CropVLM is trained via reinforcement learning, notably without relying on human-labeled bounding boxes or costly synthetic evaluations, making it a low-cost yet effective solution. Once trained, CropVLM can be integrated with various VLMs, both open-source and proprietary, enhancing their performance on tasks that require high-resolution image interpretation. A key advantage is that this approach does not require any modification or fine-tuning of the base VLM, thereby preventing issues like catastrophic forgetting. The method shows substantial gains especially on benchmarks that are considered out-of-domain for the underlying VLM, highlighting its robustness and generalization capability. Overall, CropVLM provides a practical and scalable means of boosting VLM performance on fine-grained vision-language tasks. <div>
arXiv:2511.19820v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReDirector: Creating Any-Length Video Retakes with Rotary Camera Encoding</title>
<link>https://arxiv.org/abs/2511.19827</link>
<guid>https://arxiv.org/abs/2511.19827</guid>
<content:encoded><![CDATA[
<div> ReDirector, RoPE, Rotary Camera Encoding, camera trajectories, video retake generation

<br /><br />Summary:  
This paper introduces ReDirector, a novel method designed for camera-controlled video retake generation in dynamically captured videos of variable length. It addresses a common misuse of Rotary Positional Encoding (RoPE) in previous works by properly aligning the spatiotemporal positions between the input video and the target retake, enhancing positional consistency. The authors propose a new concept called Rotary Camera Encoding (RoCE), which applies a camera-conditioned phase shift to RoPE. This innovation effectively captures and integrates multi-view relationships both within single videos and across paired input and retake videos. By incorporating camera conditions into RoPE, ReDirector generalizes well to out-of-distribution camera trajectories and different video lengths, improving robustness and adaptability. Experimental results demonstrate substantial improvements in dynamic object localization accuracy and static background preservation, making the generated retakes more geometrically consistent. Additionally, the method shows enhanced camera controllability and better video quality across a variety of camera movement patterns and video durations. Overall, ReDirector advances the state of the art in video retake generation by tightly coupling camera parameters with positional encoding for dynamic scene understanding and synthesis. <div>
arXiv:2511.19827v1 Announce Type: new 
Abstract: We present ReDirector, a novel camera-controlled video retake generation method for dynamically captured variable-length videos. In particular, we rectify a common misuse of RoPE in previous works by aligning the spatiotemporal positions of the input video and the target retake. Moreover, we introduce Rotary Camera Encoding (RoCE), a camera-conditioned RoPE phase shift that captures and integrates multi-view relationships within and across the input and target videos. By integrating camera conditions into RoPE, our method generalizes to out-of-distribution camera trajectories and video lengths, yielding improved dynamic object localization and static background preservation. Extensive experiments further demonstrate significant improvements in camera controllability, geometric consistency, and video quality across various trajectories and lengths.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Model Aided Birt-Hogg-Dube Syndrome Diagnosis with Multimodal Retrieval-Augmented Generation</title>
<link>https://arxiv.org/abs/2511.19834</link>
<guid>https://arxiv.org/abs/2511.19834</guid>
<content:encoded><![CDATA[
<div> Birt-Hogg-Dube syndrome, Deep learning, Diffuse Cystic Lung Diseases, Multimodal Large Language Models, Retrieval-augmented generation<br /><br />Summary:  
This study addresses challenges in diagnosing Birt-Hogg-Dube syndrome (BHD) from CT imaging, mainly caused by limited clinical samples and low differentiation among types of Diffuse Cystic Lung Diseases (DCLDs). To tackle these issues, the authors propose BHD-RAG, a novel multimodal retrieval-augmented generation framework that integrates domain-specific knowledge and clinical precedents with Multimodal Large Language Models (MLLMs). The framework consists of three key components: (1) a specialized agent that generates detailed imaging manifestation descriptions of CT images, building a rich multimodal corpus of DCLD cases; (2) a cosine similarity-based retriever that identifies relevant image-description pairs for new query images; and (3) an MLLM that synthesizes the retrieved evidence alongside imaging data to produce accurate BHD diagnoses. Validation of BHD-RAG was conducted on a dataset encompassing four types of DCLDs, demonstrating superior diagnostic accuracy compared to baseline models. Moreover, the system generates evidence-based descriptions that closely align with expert radiological interpretations, enhancing trustworthiness and interpretability. Overall, BHD-RAG shows promise in improving BHD diagnosis by effectively combining imaging data, expert knowledge, and advanced language modeling techniques to address challenges inherent in rare lung disease detection. <div>
arXiv:2511.19834v1 Announce Type: new 
Abstract: Deep learning methods face dual challenges of limited clinical samples and low inter-class differentiation among Diffuse Cystic Lung Diseases (DCLDs) in advancing Birt-Hogg-Dube syndrome (BHD) diagnosis via Computed Tomography (CT) imaging. While Multimodal Large Language Models (MLLMs) demonstrate diagnostic potential fo such rare diseases, the absence of domain-specific knowledge and referable radiological features intensify hallucination risks. To address this problem, we propose BHD-RAG, a multimodal retrieval-augmented generation framework that integrates DCLD-specific expertise and clinical precedents with MLLMs to improve BHD diagnostic accuracy. BHDRAG employs: (1) a specialized agent generating imaging manifestation descriptions of CT images to construct a multimodal corpus of DCLDs cases. (2) a cosine similarity-based retriever pinpointing relevant imagedescription pairs for query images, and (3) an MLLM synthesizing retrieved evidence with imaging data for diagnosis. BHD-RAG is validated on the dataset involving four types of DCLDs, achieving superior accuracy and generating evidence-based descriptions closely aligned with expert insights.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation</title>
<link>https://arxiv.org/abs/2511.19835</link>
<guid>https://arxiv.org/abs/2511.19835</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Transformers, attention sparsity, Rectified SpaAttn, pooling rectification, video generation<br /><br />Summary: Diffusion Transformers have become a leading technique in video generation but suffer from high latency due to the quadratic complexity of attention computation. Attention sparsity attempts to reduce this computational burden by concentrating on critical tokens and ignoring non-critical ones; however, existing sparse attention methods often cause significant performance drops because they induce systematic biases in attention allocation. These biases include overemphasis on critical tokens, exaggerating their attention weights, and complete neglect of non-critical tokens, resulting in loss of relevant information. To solve these problems, the authors propose Rectified SpaAttn, which aligns sparse attention maps with full attention references to correct these biases. For critical tokens, they introduce Isolated-Pooling Attention Reallocation, a strategy to compute accurate rectification factors by redistributing multimodal pooled weights proportionally. For non-critical tokens, they propose Gain-Aware Pooling Rectification, which carefully balances attention gains recovered from query-key pooling with the introduced pooling errors, ensuring that the net gain is positive. Additionally, they optimize Rectified SpaAttn with a custom kernel implemented in Triton, achieving speedups of up to 3.33 times on HunyuanVideo and 2.08 times on Wan 2.1 datasets, all while maintaining high video generation quality. The method and code are publicly available at the given GitHub repository. <div>
arXiv:2511.19835v1 Announce Type: new 
Abstract: Diffusion Transformers dominate video generation, but the quadratic complexity of attention computation introduces substantial latency. Attention sparsity reduces computational costs by focusing on critical tokens while ignoring non-critical tokens. However, existing methods suffer from severe performance degradation. In this paper, we revisit attention sparsity and reveal that existing methods induce systematic biases in attention allocation: (1) excessive focus on critical tokens amplifies their attention weights; (2) complete neglect of non-critical tokens causes the loss of relevant attention weights. To address these issues, we propose Rectified SpaAttn, which rectifies attention allocation with implicit full attention reference, thereby enhancing the alignment between sparse and full attention maps. Specifically: (1) for critical tokens, we show that their bias is proportional to the sparse attention weights, with the ratio governed by the amplified weights. Accordingly, we propose Isolated-Pooling Attention Reallocation, which calculates accurate rectification factors by reallocating multimodal pooled weights. (2) for non-critical tokens, recovering attention weights from the pooled query-key yields attention gains but also introduces pooling errors. Therefore, we propose Gain-Aware Pooling Rectification, which ensures that the rectified gain consistently surpasses the induced error. Moreover, we customize and integrate the Rectified SpaAttn kernel using Triton, achieving up to 3.33 and 2.08 times speedups on HunyuanVideo and Wan 2.1, respectively, while maintaining high generation quality. We release Rectified SpaAttn as open-source at https://github.com/BienLuky/Rectified-SpaAttn .
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models</title>
<link>https://arxiv.org/abs/2511.19836</link>
<guid>https://arxiv.org/abs/2511.19836</guid>
<content:encoded><![CDATA[
<div> Keywords: World Generation Models, 4DWorldBench, Physical Realism, Multimodal Evaluation, Adaptive Conditioning  

<br /><br />Summary:  
This paper introduces World Generation Models as a new paradigm for developing multimodal intelligence systems that generate realistic, dynamic, and physically consistent 3D and 4D worlds from images, videos, or text. Unlike traditional 2D visual generation, these models emphasize coherence across spatial, temporal, physical, and instruction-based dimensions, enabling advanced applications such as virtual reality, autonomous driving, embodied intelligence, and content creation. To address the lack of unified evaluation methods, the authors propose 4DWorldBench, a benchmark assessing four core dimensions: Perceptual Quality, Condition-4D Alignment, Physical Realism, and 4D Consistency. The benchmark supports tasks including Image-to-3D/4D, Video-to-4D, and Text-to-3D/4D generation. A novel adaptive conditioning mechanism integrates multiple input modalities by mapping all conditions into a unified textual space during evaluation, allowing for a comprehensive assessment using a combination of large language models (LLM-as-judge), multimodal large language models (MLLM-as-judge), and traditional network-based metrics. Preliminary human studies confirm that this adaptive tool selection aligns better with human subjective judgments. Overall, 4DWorldBench offers a consistent and holistic framework to objectively evaluate and advance the transition from simple visual generation to complex world generation systems. The project website is available at https://yeppp27.github.io/4DWorldBench.github.io/. <div>
arXiv:2511.19836v1 Announce Type: new 
Abstract: World Generation Models are emerging as a cornerstone of next-generation multimodal intelligence systems. Unlike traditional 2D visual generation, World Models aim to construct realistic, dynamic, and physically consistent 3D/4D worlds from images, videos, or text. These models not only need to produce high-fidelity visual content but also maintain coherence across space, time, physics, and instruction control, enabling applications in virtual reality, autonomous driving, embodied intelligence, and content creation. However, prior benchmarks emphasize different evaluation dimensions and lack a unified assessment of world-realism capability. To systematically evaluate World Models, we introduce the 4DWorldBench, which measures models across four key dimensions: Perceptual Quality, Condition-4D Alignment, Physical Realism, and 4D Consistency. The benchmark covers tasks such as Image-to-3D/4D, Video-to-4D, Text-to-3D/4D. Beyond these, we innovatively introduce adaptive conditioning across multiple modalities, which not only integrates but also extends traditional evaluation paradigms. To accommodate different modality-conditioned inputs, we map all modality conditions into a unified textual space during evaluation, and further integrate LLM-as-judge, MLLM-as-judge, and traditional network-based methods. This unified and adaptive design enables more comprehensive and consistent evaluation of alignment, physical realism, and cross-modal coherence. Preliminary human studies further demonstrate that our adaptive tool selection achieves closer agreement with subjective human judgments. We hope this benchmark will serve as a foundation for objective comparisons and improvements, accelerating the transition from "visual generation" to "world generation." Our project can be found at https://yeppp27.github.io/4DWorldBench.github.io/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum</title>
<link>https://arxiv.org/abs/2511.19846</link>
<guid>https://arxiv.org/abs/2511.19846</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision foundation models, catastrophic forgetting, Interleaved Multi-Domain Identity Curriculum, multi-task learning, unified embedding space<br /><br />Summary:<br /><br />1. The paper addresses the challenge of catastrophic forgetting in fine-tuned vision foundation models when they are adapted for specific tasks like face and person recognition. <br />2. The authors propose the Interleaved Multi-Domain Identity Curriculum (IMIC), which employs a gradient-coupled, interleaved training schedule to simultaneously fine-tune a single backbone on four distinct tasks: object recognition, face recognition (from both high- and low-quality images), and whole-body person recognition. <br />3. IMIC was evaluated on three foundation models: DINOv3, CLIP, and EVA-02, with EVA-02 and CLIP performing comparably to domain-specific expert models across all four tasks. <br />4. Their multi-task models even demonstrated higher accuracy than humans when handling face, body, and object datasets concurrently. <br />5. Importantly, the approach preserves the out-of-distribution generalization capabilities of foundation models, a key practical advantage. <br />6. Analysis revealed that the best IMIC variants (based on EVA-02) develop a unified embedding space where representations of different tasks are linearly separable but feature-sharing across tasks is extensive. <br />7. Remarkably, fewer than 100 principal components derived from one task’s embeddings could support high performance on all other tasks, indicating strong cross-task feature reuse and efficiency. <div>
arXiv:2511.19846v1 Announce Type: new 
Abstract: Vision foundation models can perform generalized object classification in zero-shot mode, and face/person recognition when they are fine-tuned. However, fine-tuned models suffer from catastrophic forgetting. We create models that perform four tasks (object recognition, face recognition from high- and low-quality images, and person recognition from whole-body images) in a single embedding space -- without incurring substantial catastrophic forgetting. To accomplish this, we introduce two variants of the Interleaved Multi-Domain Identity Curriculum (IMIC): a gradient-coupled, interleaving training schedule that fine-tunes a foundation backbone simultaneously on all four tasks. The IMIC method proved effective with three foundation model bases: DINOv3, CLIP, and EVA-02. Two of these (EVA-02 and CLIP) performed comparably with domain experts on all four tasks concurrently and were more accurate than humans at multi-tasking across face, body, and object datasets. Further, we demonstrate that our approach does not substantially harm out-of-distribution generalization, thus maintaining a key property of foundation models. Analysis of the most accurate model variants (EVA-02 + IMIC A and B) showed linearly separable representations of the four tasks in the unified embedding space, but with substantial sharing of features across tasks. Fewer than 100 PCs calculated from any one task could perform all other tasks with nearly zero performance degradation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DOGE: Differentiable Bezier Graph Optimization for Road Network Extraction</title>
<link>https://arxiv.org/abs/2511.19850</link>
<guid>https://arxiv.org/abs/2511.19850</guid>
<content:encoded><![CDATA[
<div> Keywords: B\'ezier Graph, road network extraction, differentiable rendering, topology optimization, vector maps<br /><br />Summary:<br /><br />1. The article addresses automatic extraction of road networks from aerial images, highlighting the limitations of current polyline-based methods in capturing curvilinear road geometry.<br />2. It introduces the B\'ezier Graph, a novel differentiable parametric curve-based representation that better models the inherent curved nature of roads.<br />3. A major challenge is the scarcity of vector ground-truth data for training, which the authors overcome by framing the problem as a global optimization task over the B\'ezier Graph.<br />4. The proposed framework, DOGE, learns the parametric B\'ezier Graph directly from segmentation masks instead of relying on curve ground-truth.<br />5. DOGE integrates two key modules: DiffAlign, which refines geometric accuracy through differentiable rendering, and TopoAdapt, which improves topology using discrete operators.<br />6. The method achieves state-of-the-art results on large-scale benchmarks such as SpaceNet and CityScale.<br />7. This approach sets a new paradigm for generating high-fidelity vector maps of road networks, promising enhancements in mapping technologies.<br />8. The authors plan to release their code and related datasets to foster further research and application. <div>
arXiv:2511.19850v1 Announce Type: new 
Abstract: Automatic extraction of road networks from aerial imagery is a fundamental task, yet prevailing methods rely on polylines that struggle to model curvilinear geometry. We maintain that road geometry is inherently curve-based and introduce the B\'ezier Graph, a differentiable parametric curve-based representation. The primary obstacle to this representation is to obtain the difficult-to-construct vector ground-truth (GT). We sidestep this bottleneck by reframing the task as a global optimization problem over the B\'ezier Graph. Our framework, DOGE, operationalizes this paradigm by learning a parametric B\'ezier Graph directly from segmentation masks, eliminating the need for curve GT. DOGE holistically optimizes the graph by alternating between two complementary modules: DiffAlign continuously optimizes geometry via differentiable rendering, while TopoAdapt uses discrete operators to refine its topology. Our method sets a new state-of-the-art on the large-scale SpaceNet and CityScale benchmarks, presenting a new paradigm for generating high-fidelity vector maps of road networks. We will release our code and related data.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAvatar: Soft Binding and Temporal Density Control for Monocular 3D Head Avatars Reconstruction</title>
<link>https://arxiv.org/abs/2511.19854</link>
<guid>https://arxiv.org/abs/2511.19854</guid>
<content:encoded><![CDATA[
arXiv:2511.19854v1 Announce Type: new 
Abstract: Reconstructing high-fidelity and animatable 3D head avatars from monocular videos remains a challenging yet essential task. Existing methods based on 3D Gaussian Splatting typically bind Gaussians to mesh triangles and model deformations solely via Linear Blend Skinning, which results in rigid motion and limited expressiveness. Moreover, they lack specialized strategies to handle frequently occluded regions (e.g., mouth interiors, eyelids). To address these limitations, we propose STAvatar, which consists of two key components: (1) a UV-Adaptive Soft Binding framework that leverages both image-based and geometric priors to learn per-Gaussian feature offsets within the UV space. This UV representation supports dynamic resampling, ensuring full compatibility with Adaptive Density Control (ADC) and enhanced adaptability to shape and textural variations. (2) a Temporal ADC strategy, which first clusters structurally similar frames to facilitate more targeted computation of the densification criterion. It further introduces a novel fused perceptual error as clone criterion to jointly capture geometric and textural discrepancies, encouraging densification in regions requiring finer details. Extensive experiments on four benchmark datasets demonstrate that STAvatar achieves state-of-the-art reconstruction performance, especially in capturing fine-grained details and reconstructing frequently occluded regions. The code will be publicly available.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal-Visual Semantic Alignment: A Unified Architecture for Transferring Spatial Priors from Vision Models to Zero-Shot Temporal Tasks</title>
<link>https://arxiv.org/abs/2511.19856</link>
<guid>https://arxiv.org/abs/2511.19856</guid>
<content:encoded><![CDATA[
arXiv:2511.19856v1 Announce Type: new 
Abstract: Large Multimodal Models (LMMs) have achieved remarkable progress in aligning and generating content across text and image modalities. However, the potential of using non-visual, continuous sequential, as a conditioning signal for high-fidelity image generation remains largely unexplored. Furthermore, existing methods that convert series into "pseudo-images" for temporal forecasting fail to establish semantic-level alignment. In this paper, we propose TimeArtist, a temporal-visual conversion framework that pioneers semantic-level alignment between time series fluctuations and visual concepts. It pioneers a "warmup-align" paradigm: first, a dual-autoencoder and shared quantizer are self-supervised trained on large-scale datasets to learn modality-shared representations. Then, the encoders and quantizer are frozen, and a projection is introduced to align temporal and visual samples at the representation level. TimeArtist establishes a versatile cross-modal framework, enabling high-quality, diverse image generation directly from time series, while capturing temporal fluctuation patterns to render images as styles transfer. Extensive experiments show that TimeArtist achieves satisfactory performance in image generation metrics, while also attaining superior results in zero-shot temporal tasks. Our work establishes a new paradigm for cross-modal generation, bridging the gap between temporal dynamics and visual semantics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GigaWorld-0: World Models as Data Engine to Empower Embodied AI</title>
<link>https://arxiv.org/abs/2511.19861</link>
<guid>https://arxiv.org/abs/2511.19861</guid>
<content:encoded><![CDATA[
arXiv:2511.19861v1 Announce Type: new 
Abstract: World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization</title>
<link>https://arxiv.org/abs/2511.19878</link>
<guid>https://arxiv.org/abs/2511.19878</guid>
<content:encoded><![CDATA[
arXiv:2511.19878v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChessMamba: Structure-Aware Interleaving of State Spaces for Change Detection in Remote Sensing Images</title>
<link>https://arxiv.org/abs/2511.19882</link>
<guid>https://arxiv.org/abs/2511.19882</guid>
<content:encoded><![CDATA[
arXiv:2511.19882v1 Announce Type: new 
Abstract: Change detection (CD) in multitemporal remote sensing imagery presents significant challenges for fine-grained recognition, owing to heterogeneity and spatiotemporal misalignment. However, existing methodologies based on vision transformers or state-space models typically disrupt local structural consistency during temporal serialization, obscuring discriminative cues under misalignment and hindering reliable change localization. To address this, we introduce ChessMamba, a structure-aware framework leveraging interleaved state-space modeling for robust CD with multi-temporal inputs. ChessMamba integrates a SpatialMamba encoder with a lightweight cross-source interaction module, featuring two key innovations: (i) Chessboard interleaving with snake scanning order, which serializes multi-temporal features into a unified sequence within a single forward pass, thereby shortening interaction paths and enabling direct comparison for accurate change localization; and (ii) Structure-aware fusion via multi-dilated convolutions, selectively capturing center-and-corner neighborhood contexts within each mono-temporal. Comprehensive evaluations on three CD tasks, including binary CD, semantic CD and multimodal building damage assessment, demonstrate that ChessMamba effectively fuses heterogeneous features and achieves substantial accuracy improvements over state-of-the-art methods.The relevant code will be available at: github.com/DingLei14/ChessMamba.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distilling Cross-Modal Knowledge via Feature Disentanglement</title>
<link>https://arxiv.org/abs/2511.19887</link>
<guid>https://arxiv.org/abs/2511.19887</guid>
<content:encoded><![CDATA[
arXiv:2511.19887v1 Announce Type: new 
Abstract: Knowledge distillation (KD) has proven highly effective for compressing large models and enhancing the performance of smaller ones. However, its effectiveness diminishes in cross-modal scenarios, such as vision-to-language distillation, where inconsistencies in representation across modalities lead to difficult knowledge transfer. To address this challenge, we propose frequency-decoupled cross-modal knowledge distillation, a method designed to decouple and balance knowledge transfer across modalities by leveraging frequency-domain features. We observed that low-frequency features exhibit high consistency across different modalities, whereas high-frequency features demonstrate extremely low cross-modal similarity. Accordingly, we apply distinct losses to these features: enforcing strong alignment in the low-frequency domain and introducing relaxed alignment for high-frequency features. We also propose a scale consistency loss to address distributional shifts between modalities, and employ a shared classifier to unify feature spaces. Extensive experiments across multiple benchmark datasets show our method substantially outperforms traditional KD and state-of-the-art cross-modal KD approaches. Code is available at https://github.com/Johumliu/FD-CMKD.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiMT: A Multi-task Liver Image Benchmark Dataset</title>
<link>https://arxiv.org/abs/2511.19889</link>
<guid>https://arxiv.org/abs/2511.19889</guid>
<content:encoded><![CDATA[
arXiv:2511.19889v1 Announce Type: new 
Abstract: Computer-aided diagnosis (CAD) technology can assist clinicians in evaluating liver lesions and intervening with treatment in time. Although CAD technology has advanced in recent years, the application scope of existing datasets remains relatively limited, typically supporting only single tasks, which has somewhat constrained the development of CAD technology. To address the above limitation, in this paper, we construct a multi-task liver dataset (LiMT) used for liver and tumor segmentation, multi-label lesion classification, and lesion detection based on arterial phase-enhanced computed tomography (CT), potentially providing an exploratory solution that is able to explore the correlation between tasks and does not need to worry about the heterogeneity between task-specific datasets during training. The dataset includes CT volumes from 150 different cases, comprising four types of liver diseases as well as normal cases. Each volume has been carefully annotated and calibrated by experienced clinicians. This public multi-task dataset may become a valuable resource for the medical imaging research community in the future. In addition, this paper not only provides relevant baseline experimental results but also reviews existing datasets and methods related to liver-related tasks. Our dataset is available at https://drive.google.com/drive/folders/1l9HRK13uaOQTNShf5pwgSz3OTanWjkag?usp=sharing.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeriSciQA: An Auto-Verified Dataset for Scientific Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.19899</link>
<guid>https://arxiv.org/abs/2511.19899</guid>
<content:encoded><![CDATA[
arXiv:2511.19899v1 Announce Type: new 
Abstract: Large Vision-Language Models (LVLMs) show promise for scientific applications, yet open-source models still struggle with Scientific Visual Question Answering (SVQA), namely answering questions about figures from scientific papers. A key bottleneck lies in the lack of public, large-scale, high-quality SVQA datasets. Although recent work uses LVLMs to synthesize data at scale, we identify systematic errors in their resulting QA pairs, stemming from LVLMs' inherent limitations and information asymmetry between figures and text. To address these challenges, we propose a verification-centric Generate-then-Verify framework that first generates QA pairs with figure-associated textual context, then applies cross-modal consistency checks against figures along with auxiliary filters to eliminate erroneous pairs. We instantiate this framework to curate VeriSciQA, a dataset of 20,351 QA pairs spanning 20 scientific domains and 12 figure types. VeriSciQA poses a challenging benchmark for open-source models, with a substantial accuracy gap between the leading open-source models (64%) and a proprietary model (82%). Moreover, models fine-tuned on VeriSciQA achieve consistent improvements on SVQA benchmarks, with performance gains that scale with data size and surpass models trained on existing datasets. Human evaluation further validates the superior correctness of VeriSciQA. Together, these evidences demonstrate that continued data expansion by our scalable framework can further advance SVQA capability in the open-source community.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Agent0-VL: Exploring Self-Evolving Agent for Tool-Integrated Vision-Language Reasoning</title>
<link>https://arxiv.org/abs/2511.19900</link>
<guid>https://arxiv.org/abs/2511.19900</guid>
<content:encoded><![CDATA[
arXiv:2511.19900v1 Announce Type: new 
Abstract: Vision-language agents have achieved remarkable progress in a variety of multimodal reasoning tasks; however, their learning remains constrained by the limitations of human-annotated supervision. Recent self-rewarding approaches attempt to overcome this constraint by allowing models to act as their own critics or reward providers. Yet, purely text-based self-evaluation struggles to verify complex visual reasoning steps and often suffers from evaluation hallucinations. To address these challenges, inspired by recent advances in tool-integrated reasoning, we propose Agent0-VL, a self-evolving vision-language agent that achieves continual improvement with tool-integrated reasoning. Agent0-VL incorporates tool usage not only into reasoning but also into self-evaluation and self-repair, enabling the model to introspect, verify, and refine its reasoning through evidence-grounded analysis. It unifies two synergistic roles within a single LVLM: a Solver that performs multi-turn tool-integrated reasoning, and a Verifier that generates structured feedback and fine-grained self-rewards through tool-grounded critique. These roles interact through a Self-Evolving Reasoning Cycle, where tool-based verification and reinforcement learning jointly align the reasoning and evaluation distributions for stable self-improvement. Through this zero-external-reward evolution, Agent0-VL aligns its reasoning and verification behaviors without any human annotation or external reward models, achieving continual self-improvement. Experiments on geometric problem solving and visual scientific analysis show that Agent0-VL achieves an 12.5% improvement over the base model. Our code is available at \href{https://github.com/aiming-lab/Agent0/Agent0-VL}{this https URL}.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MHB: Multimodal Handshape-aware Boundary Detection for Continuous Sign Language Recognition</title>
<link>https://arxiv.org/abs/2511.19907</link>
<guid>https://arxiv.org/abs/2511.19907</guid>
<content:encoded><![CDATA[
arXiv:2511.19907v1 Announce Type: new 
Abstract: This paper presents a multimodal approach for continuous sign recognition that first uses machine learning to detect the start and end frames of signs in videos of American Sign Language (ASL) sentences, and then recognizes the segmented signs. For improved robustness, we use 3D skeletal features extracted from sign language videos to capture the convergence of sign properties and their dynamics, which tend to cluster at sign boundaries. Another focus of this work is the incorporation of information from 3D handshape for boundary detection. To detect handshapes normally expected at the beginning and end of signs, we pretrain a handshape classifier for 87 linguistically defined canonical handshape categories using a dataset that we created by integrating and normalizing several existing datasets. A multimodal fusion module is then used to unify the pretrained sign video segmentation framework and the handshape classification models. Finally, the estimated boundaries are used for sign recognition, where the recognition model is trained on a large database containing both citation-form isolated signs and signs pre-segmented (based on manual annotations) from continuous signing, as such signs often differ in certain respects. We evaluate our method on the ASLLRP corpus and demonstrate significant improvements over previous work.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Motion Marionette: Rethinking Rigid Motion Transfer via Prior Guidance</title>
<link>https://arxiv.org/abs/2511.19909</link>
<guid>https://arxiv.org/abs/2511.19909</guid>
<content:encoded><![CDATA[
arXiv:2511.19909v1 Announce Type: new 
Abstract: We present Motion Marionette, a zero-shot framework for rigid motion transfer from monocular source videos to single-view target images. Previous works typically employ geometric, generative, or simulation priors to guide the transfer process, but these external priors introduce auxiliary constraints that lead to trade-offs between generalizability and temporal consistency. To address these limitations, we propose guiding the motion transfer process through an internal prior that exclusively captures the spatial-temporal transformations and is shared between the source video and any transferred target video. Specifically, we first lift both the source video and the target image into a unified 3D representation space. Motion trajectories are then extracted from the source video to construct a spatial-temporal (SpaT) prior that is independent of object geometry and semantics, encoding relative spatial variations over time. This prior is further integrated with the target object to synthesize a controllable velocity field, which is subsequently refined using Position-Based Dynamics to mitigate artifacts and enhance visual coherence. The resulting velocity field can be flexibly employed for efficient video production. Empirical results demonstrate that Motion Marionette generalizes across diverse objects, produces temporally consistent videos that align well with the source motion, and supports controllable video generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.19912</link>
<guid>https://arxiv.org/abs/2511.19912</guid>
<content:encoded><![CDATA[
arXiv:2511.19912v1 Announce Type: new 
Abstract: Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coupled Physics-Gated Adaptation: Spatially Decoding Volumetric Photochemical Conversion in Complex 3D-Printed Objects</title>
<link>https://arxiv.org/abs/2511.19913</link>
<guid>https://arxiv.org/abs/2511.19913</guid>
<content:encoded><![CDATA[
arXiv:2511.19913v1 Announce Type: new 
Abstract: We present a framework that pioneers the prediction of photochemical conversion in complex three-dimensionally printed objects, introducing a challenging new computer vision task: predicting dense, non-visual volumetric physical properties from 3D visual data. This approach leverages the largest-ever optically printed 3D specimen dataset, comprising a large family of parametrically designed complex minimal surface structures that have undergone terminal chemical characterisation. Conventional vision models are ill-equipped for this task, as they lack an inductive bias for the coupled, non-linear interactions of optical physics (diffraction, absorption) and material physics (diffusion, convection) that govern the final chemical state. To address this, we propose Coupled Physics-Gated Adaptation (C-PGA), a novel multimodal fusion architecture. Unlike standard concatenation, C-PGA explicitly models physical coupling by using sparse geometrical and process parameters (e.g., surface transport, print layer height) as a Query to dynamically gate and adapt the dense visual features via feature-wise linear modulation (FiLM). This mechanism spatially modulates dual 3D visual streams-extracted by parallel 3D-CNNs processing raw projection stacks and their diffusion-diffraction corrected counterparts allowing the model to recalibrate its visual perception based on the physical context. This approach offers a breakthrough in virtual chemical characterisation, eliminating the need for traditional post-print measurements and enabling precise control over the chemical conversion state.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scale Where It Matters: Training-Free Localized Scaling for Diffusion Models</title>
<link>https://arxiv.org/abs/2511.19917</link>
<guid>https://arxiv.org/abs/2511.19917</guid>
<content:encoded><![CDATA[
arXiv:2511.19917v1 Announce Type: new 
Abstract: Diffusion models have become the dominant paradigm in text-to-image generation, and test-time scaling (TTS) further improves quality by allocating more computation during inference. However, existing TTS methods operate at the full-image level, overlooking the fact that image quality is often spatially heterogeneous. This leads to unnecessary computation on already satisfactory regions and insufficient correction of localized defects. In this paper, we explore a new direction - Localized TTS - that adaptively resamples defective regions while preserving high-quality regions, thereby substantially reducing the search space. This paradigm poses two central challenges: accurately localizing defects and maintaining global consistency. We propose LoTTS, the first fully training-free framework for localized TTS. For defect localization, LoTTS contrasts cross- and self-attention signals under quality-aware prompts (e.g., high-quality vs. low-quality) to identify defective regions, and then refines them into coherent masks. For consistency, LoTTS perturbs only defective regions and denoises them locally, ensuring that corrections remain confined while the rest of the image remains undisturbed. Extensive experiments on SD2.1, SDXL, and FLUX demonstrate that LoTTS achieves state-of-the-art performance: it consistently improves both local quality and global fidelity, while reducing GPU cost by 2-4x compared to Best-of-N sampling. These findings establish localized TTS as a promising new direction for scaling diffusion models at inference time.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybriDLA: Hybrid Generation for Document Layout Analysis</title>
<link>https://arxiv.org/abs/2511.19919</link>
<guid>https://arxiv.org/abs/2511.19919</guid>
<content:encoded><![CDATA[
arXiv:2511.19919v1 Announce Type: new 
Abstract: Conventional document layout analysis (DLA) traditionally depends on empirical priors or a fixed set of learnable queries executed in a single forward pass. While sufficient for early-generation documents with a small, predetermined number of regions, this paradigm struggles with contemporary documents, which exhibit diverse element counts and increasingly complex layouts. To address challenges posed by modern documents, we present HybriDLA, a novel generative framework that unifies diffusion and autoregressive decoding within a single layer. The diffusion component iteratively refines bounding-box hypotheses, whereas the autoregressive component injects semantic and contextual awareness, enabling precise region prediction even in highly varied layouts. To further enhance detection quality, we design a multi-scale feature-fusion encoder that captures both fine-grained and high-level visual cues. This architecture elevates performance to 83.5% mean Average Precision (mAP). Extensive experiments on the DocLayNet and M$^6$Doc benchmarks demonstrate that HybriDLA sets a state-of-the-art performance, outperforming previous approaches. All data and models will be made publicly available at https://yufanchen96.github.io/projects/HybriDLA.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Image Search Algorithms Fusing Visual Large Models</title>
<link>https://arxiv.org/abs/2511.19920</link>
<guid>https://arxiv.org/abs/2511.19920</guid>
<content:encoded><![CDATA[
arXiv:2511.19920v1 Announce Type: new 
Abstract: Fine-grained image retrieval, which aims to find images containing specific object components and assess their detailed states, is critical in fields like security and industrial inspection. However, conventional methods face significant limitations: manual features (e.g., SIFT) lack robustness; deep learning-based detectors (e.g., YOLO) can identify component presence but cannot perform state-specific retrieval or zero-shot search; Visual Large Models (VLMs) offer semantic and zero-shot capabilities but suffer from poor spatial grounding and high computational cost, making them inefficient for direct retrieval. To bridge these gaps, this paper proposes DetVLM, a novel intelligent image search framework that synergistically fuses object detection with VLMs. The framework pioneers a search-enhancement paradigm via a two-stage pipeline: a YOLO detector first conducts efficient, high-recall component-level screening to determine component presence; then, a VLM acts as a recall-enhancement unit, performing secondary verification for components missed by the detector. This architecture directly enables two advanced capabilities: 1) State Search: Guided by task-specific prompts, the VLM refines results by verifying component existence and executing sophisticated state judgments (e.g., "sun visor lowered"), allowing retrieval based on component state. 2) Zero-shot Search: The framework leverages the VLM's inherent zero-shot capability to recognize and retrieve images containing unseen components or attributes (e.g., "driver wearing a mask") without any task-specific training. Experiments on a vehicle component dataset show DetVLM achieves a state-of-the-art overall retrieval accuracy of 94.82\%, significantly outperforming detection-only baselines. It also attains 94.95\% accuracy in zero-shot search for driver mask-wearing and over 90\% average accuracy in state search tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding</title>
<link>https://arxiv.org/abs/2511.19923</link>
<guid>https://arxiv.org/abs/2511.19923</guid>
<content:encoded><![CDATA[
arXiv:2511.19923v1 Announce Type: new 
Abstract: Vision Language Models (VLMs) have recently shown significant advancements in video understanding, especially in feature alignment, event reasoning, and instruction-following tasks. However, their capability for counterfactual reasoning, inferring alternative outcomes under hypothetical conditions, remains underexplored. This capability is essential for robust video understanding, as it requires identifying underlying causal structures and reasoning about unobserved possibilities, rather than merely recognizing observed patterns. To systematically evaluate this capability, we introduce CounterVQA, a video-based benchmark featuring three progressive difficulty levels that assess different aspects of counterfactual reasoning. Through comprehensive evaluation of both state-of-the-art open-source and closed-source models, we uncover a substantial performance gap: while these models achieve reasonable accuracy on simple counterfactual questions, performance degrades significantly on complex multi-hop causal chains. To address these limitations, we develop a post-training method, CFGPT, that enhances a model's visual counterfactual reasoning ability by distilling its counterfactual reasoning capability from the language modality, yielding consistent improvements across all CounterVQA difficulty levels. Dataset and code will be further released.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Token Pruning and Discriminative Selective Attention for Transformer Tracking</title>
<link>https://arxiv.org/abs/2511.19928</link>
<guid>https://arxiv.org/abs/2511.19928</guid>
<content:encoded><![CDATA[
arXiv:2511.19928v1 Announce Type: new 
Abstract: One-stream Transformer-based trackers have demonstrated remarkable performance by concatenating template and search region tokens, thereby enabling joint attention across all tokens. However, enabling an excessive proportion of background search tokens to attend to the target template tokens weakens the tracker's discriminative capability. Several token pruning methods have been proposed to mitigate background interference; however, they often remove tokens near the target, leading to the loss of essential contextual information and degraded tracking performance. Moreover, the presence of distractors within the search tokens further reduces the tracker's ability to accurately identify the target. To address these limitations, we propose CPDATrack, a novel tracking framework designed to suppress interference from background and distractor tokens while enhancing computational efficiency. First, a learnable module is integrated between two designated encoder layers to estimate the probability of each search token being associated with the target. Based on these estimates, less-informative background tokens are pruned from the search region while preserving the contextual cues surrounding the target. To further suppress background interference, a discriminative selective attention mechanism is employed that fully blocks search-to-template attention in the early layers. In the subsequent encoder layers, high-probability target tokens are selectively extracted from a localized region to attend to the template tokens, thereby reducing the influence of background and distractor tokens. The proposed CPDATrack achieves state-of-the-art performance across multiple benchmarks, particularly on GOT-10k, where it attains an average overlap of 75.1 percent.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image Diffusion Models Exhibit Emergent Temporal Propagation in Videos</title>
<link>https://arxiv.org/abs/2511.19936</link>
<guid>https://arxiv.org/abs/2511.19936</guid>
<content:encoded><![CDATA[
arXiv:2511.19936v1 Announce Type: new 
Abstract: Image diffusion models, though originally developed for image generation, implicitly capture rich semantic structures that enable various recognition and localization tasks beyond synthesis. In this work, we investigate their self-attention maps can be reinterpreted as semantic label propagation kernels, providing robust pixel-level correspondences between relevant image regions. Extending this mechanism across frames yields a temporal propagation kernel that enables zero-shot object tracking via segmentation in videos. We further demonstrate the effectiveness of test-time optimization strategies-DDIM inversion, textual inversion, and adaptive head weighting-in adapting diffusion features for robust and consistent label propagation. Building on these findings, we introduce DRIFT, a framework for object tracking in videos leveraging a pretrained image diffusion model with SAM-guided mask refinement, achieving state-of-the-art zero-shot performance on standard video object segmentation benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Resolution Editing is All You Need for High-Resolution Editing</title>
<link>https://arxiv.org/abs/2511.19945</link>
<guid>https://arxiv.org/abs/2511.19945</guid>
<content:encoded><![CDATA[
arXiv:2511.19945v1 Announce Type: new 
Abstract: High-resolution content creation is rapidly emerging as a central challenge in both the vision and graphics communities. While images serve as the most fundamental modality for visual expression, content generation that aligns with the user intent requires effective, controllable high-resolution image manipulation mechanisms. However, existing approaches remain limited to low-resolution settings, typically supporting only up to 1K resolution. In this work, we introduce the task of high-resolution image editing and propose a test-time optimization framework to address it. Our method performs patch-wise optimization on high-resolution source images, followed by a fine-grained detail transfer module and a novel synchronization strategy to maintain consistency across patches. Extensive experiments show that our method produces high-quality edits, facilitating the way toward high-resolution content creation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervise Less, See More: Training-free Nuclear Instance Segmentation with Prototype-Guided Prompting</title>
<link>https://arxiv.org/abs/2511.19953</link>
<guid>https://arxiv.org/abs/2511.19953</guid>
<content:encoded><![CDATA[
arXiv:2511.19953v1 Announce Type: new 
Abstract: Accurate nuclear instance segmentation is a pivotal task in computational pathology, supporting data-driven clinical insights and facilitating downstream translational applications. While large vision foundation models have shown promise for zero-shot biomedical segmentation, most existing approaches still depend on dense supervision and computationally expensive fine-tuning. Consequently, training-free methods present a compelling research direction, yet remain largely unexplored. In this work, we introduce SPROUT, a fully training- and annotation-free prompting framework for nuclear instance segmentation. SPROUT leverages histology-informed priors to construct slide-specific reference prototypes that mitigate domain gaps. These prototypes progressively guide feature alignment through a partial optimal transport scheme. The resulting foreground and background features are transformed into positive and negative point prompts, enabling the Segment Anything Model (SAM) to produce precise nuclear delineations without any parameter updates. Extensive experiments across multiple histopathology benchmarks demonstrate that SPROUT achieves competitive performance without supervision or retraining, establishing a novel paradigm for scalable, training-free nuclear instance segmentation in pathology.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GFT-GCN: Privacy-Preserving 3D Face Mesh Recognition with Spectral Diffusion</title>
<link>https://arxiv.org/abs/2511.19958</link>
<guid>https://arxiv.org/abs/2511.19958</guid>
<content:encoded><![CDATA[
arXiv:2511.19958v1 Announce Type: new 
Abstract: 3D face recognition offers a robust biometric solution by capturing facial geometry, providing resilience to variations in illumination, pose changes, and presentation attacks. Its strong spoof resistance makes it suitable for high-security applications, but protecting stored biometric templates remains critical. We present GFT-GCN, a privacy-preserving 3D face recognition framework that combines spectral graph learning with diffusion-based template protection. Our approach integrates the Graph Fourier Transform (GFT) and Graph Convolutional Networks (GCN) to extract compact, discriminative spectral features from 3D face meshes. To secure these features, we introduce a spectral diffusion mechanism that produces irreversible, renewable, and unlinkable templates. A lightweight client-server architecture ensures that raw biometric data never leaves the client device. Experiments on the BU-3DFE and FaceScape datasets demonstrate high recognition accuracy and strong resistance to reconstruction attacks. Results show that GFT-GCN effectively balances privacy and performance, offering a practical solution for secure 3D face authentication.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaEye: A Size-Agnostic Visual Encoder with Causal Sequential Processing</title>
<link>https://arxiv.org/abs/2511.19963</link>
<guid>https://arxiv.org/abs/2511.19963</guid>
<content:encoded><![CDATA[
arXiv:2511.19963v1 Announce Type: new 
Abstract: Despite decades of progress, a truly input-size agnostic visual encoder-a fundamental characteristic of human vision-has remained elusive. We address this limitation by proposing \textbf{MambaEye}, a novel, causal sequential encoder that leverages the low complexity and causal-process based pure Mamba2 backbone. Unlike previous Mamba-based vision encoders that often employ bidirectional processing, our strictly unidirectional approach preserves the inherent causality of State Space Models, enabling the model to generate a prediction at any point in its input sequence. A core innovation is our use of relative move embedding, which encodes the spatial shift between consecutive patches, providing a strong inductive bias for translation invariance and making the model inherently adaptable to arbitrary image resolutions and scanning patterns. To achieve this, we introduce a novel diffusion-inspired loss function that provides dense, step-wise supervision, training the model to build confidence as it gathers more visual evidence. We demonstrate that MambaEye exhibits robust performance across a wide range of image resolutions, especially at higher resolutions such as $1536^2$ on the ImageNet-1K classification task. This feat is achieved while maintaining linear time and memory complexity relative to the number of patches.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiCoGen: Hierarchical Compositional Text-to-Image Generation in Diffusion Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.19965</link>
<guid>https://arxiv.org/abs/2511.19965</guid>
<content:encoded><![CDATA[
arXiv:2511.19965v1 Announce Type: new 
Abstract: Recent advances in diffusion models have demonstrated impressive capability in generating high-quality images for simple prompts. However, when confronted with complex prompts involving multiple objects and hierarchical structures, existing models struggle to accurately follow instructions, leading to issues such as concept omission, confusion, and poor compositionality. To address these limitations, we propose a Hierarchical Compositional Generative framework (HiCoGen) built upon a novel Chain of Synthesis (CoS) paradigm. Instead of monolithic generation, HiCoGen first leverages a Large Language Model (LLM) to decompose complex prompts into minimal semantic units. It then synthesizes these units iteratively, where the image generated in each step provides crucial visual context for the next, ensuring all textual concepts are faithfully constructed into the final scene. To further optimize this process, we introduce a reinforcement learning (RL) framework. Crucially, we identify that the limited exploration of standard diffusion samplers hinders effective RL. We theoretically prove that sample diversity is maximized by concentrating stochasticity in the early generation stages and, based on this insight, propose a novel Decaying Stochasticity Schedule to enhance exploration. Our RL algorithm is then guided by a hierarchical reward mechanism that jointly evaluates the image at the global, subject, and relationship levels. We also construct HiCoPrompt, a new text-to-image benchmark with hierarchical prompts for rigorous evaluation. Experiments show our approach significantly outperforms existing methods in both concept coverage and compositional accuracy.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction</title>
<link>https://arxiv.org/abs/2511.19971</link>
<guid>https://arxiv.org/abs/2511.19971</guid>
<content:encoded><![CDATA[
arXiv:2511.19971v1 Announce Type: new 
Abstract: Reconstructing dynamic 4D scenes is challenging, as it requires robust disentanglement of dynamic objects from the static background. While 3D foundation models like VGGT provide accurate 3D geometry, their performance drops markedly when moving objects dominate. Existing 4D approaches often rely on external priors, heavy post-optimization, or require fine-tuning on 4D datasets. In this paper, we propose VGGT4D, a training-free framework that extends the 3D foundation model VGGT for robust 4D scene reconstruction. Our approach is motivated by the key finding that VGGT's global attention layers already implicitly encode rich, layer-wise dynamic cues. To obtain masks that decouple static and dynamic elements, we mine and amplify global dynamic cues via gram similarity and aggregate them across a temporal window. To further sharpen mask boundaries, we introduce a refinement strategy driven by projection gradient. We then integrate these precise masks into VGGT's early-stage inference, effectively mitigating motion interference in both pose estimation and geometric reconstruction. Across six datasets, our method achieves superior performance in dynamic object segmentation, camera pose estimation, and dense reconstruction. It also supports single-pass inference on sequences longer than 500 frames.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Reasoning in Large Multimodal Models via Activation Replay</title>
<link>https://arxiv.org/abs/2511.19972</link>
<guid>https://arxiv.org/abs/2511.19972</guid>
<content:encoded><![CDATA[
arXiv:2511.19972v1 Announce Type: new 
Abstract: Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach to incentivizing reasoning capability in Large Multimodal Models (LMMs), while the underlying mechanisms behind this post-training paradigm are poorly understood. We begin by exploring how input activations are affected by RLVR through the perspective of logit lens. Our systematic investigations across multiple post-trained LMMs suggest that RLVR shifts low-entropy activations unexpectedly, while high-entropy ones are less affected. We further demonstrate that such phenomena are associated with LMM reasoning by controlled experiments, suggesting a potentially beneficial role of modulating low-entropy activations. To this end, we propose Activation Replay, a novel simple yet effective training-free approach that boosts multimodal reasoning of post-trained LMMs without requiring expensive policy optimization. Our design involves manipulation of visual tokens at test time, replaying low-entropy activations from the input context of base LMMs to regulating the RLVR counterparts. Activation Replay triggers better reasoning across diverse scenarios, including mathematics, o3-like visual agents, and video reasoning. We further show that Activation Replay boosts Pass@K and mitigates narrower reasoning coverage of RLVR. Our design is compared against alternative choices, such as replaying high-entropy activations instead of low-entropy ones, or direct cross-model intervention instead of manipulating input tokens, demonstrating the superiority of our implementation. Codes will be made publicly available.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EmoFeedback2: Reinforcement of Continuous Emotional Image Generation via LVLM-based Reward and Textual Feedback</title>
<link>https://arxiv.org/abs/2511.19982</link>
<guid>https://arxiv.org/abs/2511.19982</guid>
<content:encoded><![CDATA[
arXiv:2511.19982v1 Announce Type: new 
Abstract: Continuous emotional image generation (C-EICG) is emerging rapidly due to its ability to produce images aligned with both user descriptions and continuous emotional values. However, existing approaches lack emotional feedback from generated images, limiting the control of emotional continuity. Additionally, their simple alignment between emotions and naively generated texts fails to adaptively adjust emotional prompts according to image content, leading to insufficient emotional fidelity. To address these concerns, we propose a novel generation-understanding-feedback reinforcement paradigm (EmoFeedback2) for C-EICG, which exploits the reasoning capability of the fine-tuned large vision-language model (LVLM) to provide reward and textual feedback for generating high-quality images with continuous emotions. Specifically, we introduce an emotion-aware reward feedback strategy, where the LVLM evaluates the emotional values of generated images and computes the reward against target emotions, guiding the reinforcement fine-tuning of the generative model and enhancing the emotional continuity of images. Furthermore, we design a self-promotion textual feedback framework, in which the LVLM iteratively analyzes the emotional content of generated images and adaptively produces refinement suggestions for the next-round prompt, improving the emotional fidelity with fine-grained content. Extensive experimental results demonstrate that our approach effectively generates high-quality images with the desired emotions, outperforming existing state-of-the-art methods in our custom dataset. The code and dataset will be released soon.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SONIC: Spectral Optimization of Noise for Inpainting with Consistency</title>
<link>https://arxiv.org/abs/2511.19985</link>
<guid>https://arxiv.org/abs/2511.19985</guid>
<content:encoded><![CDATA[
arXiv:2511.19985v1 Announce Type: new 
Abstract: We propose a novel training-free method for inpainting with off-the-shelf text-to-image models. While guidance-based methods in theory allow generic models to be used for inverse problems such as inpainting, in practice, their effectiveness is limited, leading to the necessity of specialized inpainting-specific models. In this work, we argue that the missing ingredient for training-free inpainting is the optimization (guidance) of the initial seed noise. We propose to optimize the initial seed noise to approximately match the unmasked parts of the data - with as few as a few tens of optimization steps. We then apply conventional training-free inpainting methods on top of our optimized initial seed noise. Critically, we propose two core ideas to effectively implement this idea: (i) to avoid the costly unrolling required to relate the initial noise and the generated outcome, we perform linear approximation; and (ii) to stabilize the optimization, we optimize the initial seed noise in the spectral domain. We demonstrate the effectiveness of our method on various inpainting tasks, outperforming the state of the art. Project page: https://ubc-vision.github.io/sonic/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GazeProphetV2: Head-Movement-Based Gaze Prediction Enabling Efficient Foveated Rendering on Mobile VR</title>
<link>https://arxiv.org/abs/2511.19988</link>
<guid>https://arxiv.org/abs/2511.19988</guid>
<content:encoded><![CDATA[
arXiv:2511.19988v1 Announce Type: new 
Abstract: Predicting gaze behavior in virtual reality environments remains a significant challenge with implications for rendering optimization and interface design. This paper introduces a multimodal approach to VR gaze prediction that combines temporal gaze patterns, head movement data, and visual scene information. By leveraging a gated fusion mechanism with cross-modal attention, the approach learns to adaptively weight gaze history, head movement, and scene content based on contextual relevance. Evaluations using a dataset spanning 22 VR scenes with 5.3M gaze samples demonstrate improvements in predictive accuracy when combining modalities compared to using individual data streams alone. The results indicate that integrating past gaze trajectories with head orientation and scene content enhances prediction accuracy across 1-3 future frames. Cross-scene generalization testing shows consistent performance with 93.1% validation accuracy and temporal consistency in predicted gaze trajectories. These findings contribute to understanding attention mechanisms in virtual environments while suggesting potential applications in rendering optimization, interaction design, and user experience evaluation. The approach represents a step toward more efficient virtual reality systems that can anticipate user attention patterns without requiring expensive eye tracking hardware.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniRefiner: Reinforcement-Guided Local Diffusion Refinement</title>
<link>https://arxiv.org/abs/2511.19990</link>
<guid>https://arxiv.org/abs/2511.19990</guid>
<content:encoded><![CDATA[
arXiv:2511.19990v1 Announce Type: new 
Abstract: Reference-guided image generation has progressed rapidly, yet current diffusion models still struggle to preserve fine-grained visual details when refining a generated image using a reference. This limitation arises because VAE-based latent compression inherently discards subtle texture information, causing identity- and attribute-specific cues to vanish. Moreover, post-editing approaches that amplify local details based on existing methods often produce results inconsistent with the original image in terms of lighting, texture, or shape. To address this, we introduce \ourMthd{}, a detail-aware refinement framework that performs two consecutive stages of reference-driven correction to enhance pixel-level consistency. We first adapt a single-image diffusion editor by fine-tuning it to jointly ingest the draft image and the reference image, enabling globally coherent refinement while maintaining structural fidelity. We then apply reinforcement learning to further strengthen localized editing capability, explicitly optimizing for detail accuracy and semantic consistency. Extensive experiments demonstrate that \ourMthd{} significantly improves reference alignment and fine-grained detail preservation, producing faithful and visually coherent edits that surpass both open-source and commercial models on challenging reference-guided restoration benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CREward: A Type-Specific Creativity Reward Model</title>
<link>https://arxiv.org/abs/2511.19995</link>
<guid>https://arxiv.org/abs/2511.19995</guid>
<content:encoded><![CDATA[
arXiv:2511.19995v1 Announce Type: new 
Abstract: Creativity is a complex phenomenon. When it comes to representing and assessing creativity, treating it as a single undifferentiated quantity would appear naive and underwhelming. In this work, we learn the \emph{first type-specific creativity reward model}, coined CREward, which spans three creativity ``axes," geometry, material, and texture, to allow us to view creativity through the lens of the image formation pipeline. To build our reward model, we first conduct a human benchmark evaluation to capture human perception of creativity for each type across various creative images. We then analyze the correlation between human judgments and predictions by large vision-language models (LVLMs), confirming that LVLMs exhibit strong alignment with human perception. Building on this observation, we collect LVLM-generated labels to train our CREward model that is applicable to both evaluation and generation of creative images. We explore three applications of CREward: creativity assessment, explainable creativity, and creative sample acquisition for both human design inspiration and guiding creative generation through low-rank adaptation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Feasibility of Hijacking MLLMs' Decision Chain via One Perturbation</title>
<link>https://arxiv.org/abs/2511.20002</link>
<guid>https://arxiv.org/abs/2511.20002</guid>
<content:encoded><![CDATA[
arXiv:2511.20002v1 Announce Type: new 
Abstract: Conventional adversarial attacks focus on manipulating a single decision of neural networks. However, real-world models often operate in a sequence of decisions, where an isolated mistake can be easily corrected, but cascading errors can lead to severe risks.
  This paper reveals a novel threat: a single perturbation can hijack the whole decision chain. We demonstrate the feasibility of manipulating a model's outputs toward multiple, predefined outcomes, such as simultaneously misclassifying "non-motorized lane" signs as "motorized lane" and "pedestrian" as "plastic bag".
  To expose this threat, we introduce Semantic-Aware Universal Perturbations (SAUPs), which induce varied outcomes based on the semantics of the inputs. We overcome optimization challenges by developing an effective algorithm, which searches for perturbations in normalized space with a semantic separation strategy. To evaluate the practical threat of SAUPs, we present RIST, a new real-world image dataset with fine-grained semantic annotations. Extensive experiments on three multimodal large language models demonstrate their vulnerability, achieving a 70% attack success rate when controlling five distinct targets using just an adversarial frame.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pedestrian Crossing Intention Prediction Using Multimodal Fusion Network</title>
<link>https://arxiv.org/abs/2511.20008</link>
<guid>https://arxiv.org/abs/2511.20008</guid>
<content:encoded><![CDATA[
arXiv:2511.20008v1 Announce Type: new 
Abstract: Pedestrian crossing intention prediction is essential for the deployment of autonomous vehicles (AVs) in urban environments. Ideal prediction provides AVs with critical environmental cues, thereby reducing the risk of pedestrian-related collisions. However, the prediction task is challenging due to the diverse nature of pedestrian behavior and its dependence on multiple contextual factors. This paper proposes a multimodal fusion network that leverages seven modality features from both visual and motion branches, aiming to effectively extract and integrate complementary cues across different modalities. Specifically, motion and visual features are extracted from the raw inputs using multiple Transformer-based extraction modules. Depth-guided attention module leverages depth information to guide attention towards salient regions in another modality through comprehensive spatial feature interactions. To account for the varying importance of different modalities and frames, modality attention and temporal attention are designed to selectively emphasize informative modalities and effectively capture temporal dependencies. Extensive experiments on the JAAD dataset validate the effectiveness of the proposed network, achieving superior performance compared to the baseline methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Context Fusion Transformer for Pedestrian Crossing Intention Prediction in Urban Environments</title>
<link>https://arxiv.org/abs/2511.20011</link>
<guid>https://arxiv.org/abs/2511.20011</guid>
<content:encoded><![CDATA[
arXiv:2511.20011v1 Announce Type: new 
Abstract: Pedestrian crossing intention prediction is essential for autonomous vehicles to improve pedestrian safety and reduce traffic accidents. However, accurate pedestrian intention prediction in urban environments remains challenging due to the multitude of factors affecting pedestrian behavior. In this paper, we propose a multi-context fusion Transformer (MFT) that leverages diverse numerical contextual attributes across four key dimensions, encompassing pedestrian behavior context, environmental context, pedestrian localization context and vehicle motion context, to enable accurate pedestrian intention prediction. MFT employs a progressive fusion strategy, where mutual intra-context attention enables reciprocal interactions within each context, thereby facilitating feature sequence fusion and yielding a context token as a context-specific representation. This is followed by mutual cross-context attention, which integrates features across contexts with a global CLS token serving as a compact multi-context representation. Finally, guided intra-context attention refines context tokens within each context through directed interactions, while guided cross-context attention strengthens the global CLS token to promote multi-context fusion via guided information propagation, yielding deeper and more efficient integration. Experimental results validate the superiority of MFT over state-of-the-art methods, achieving accuracy rates of 73%, 93%, and 90% on the JAADbeh, JAADall, and PIE datasets, respectively. Extensive ablation studies are further conducted to investigate the effectiveness of the network architecture and contribution of different input context. Our code is open-source: https://github.com/ZhongHang0307/Multi-Context-Fusion-Transformer.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACIT: Attention-Guided Cross-Modal Interaction Transformer for Pedestrian Crossing Intention Prediction</title>
<link>https://arxiv.org/abs/2511.20020</link>
<guid>https://arxiv.org/abs/2511.20020</guid>
<content:encoded><![CDATA[
arXiv:2511.20020v1 Announce Type: new 
Abstract: Predicting pedestrian crossing intention is crucial for autonomous vehicles to prevent pedestrian-related collisions. However, effectively extracting and integrating complementary cues from different types of data remains one of the major challenges. This paper proposes an attention-guided cross-modal interaction Transformer (ACIT) for pedestrian crossing intention prediction. ACIT leverages six visual and motion modalities, which are grouped into three interaction pairs: (1) Global semantic map and global optical flow, (2) Local RGB image and local optical flow, and (3) Ego-vehicle speed and pedestrian's bounding box. Within each visual interaction pair, a dual-path attention mechanism enhances salient regions within the primary modality through intra-modal self-attention and facilitates deep interactions with the auxiliary modality (i.e., optical flow) via optical flow-guided attention. Within the motion interaction pair, cross-modal attention is employed to model the cross-modal dynamics, enabling the effective extraction of complementary motion features. Beyond pairwise interactions, a multi-modal feature fusion module further facilitates cross-modal interactions at each time step. Furthermore, a Transformer-based temporal feature aggregation module is introduced to capture sequential dependencies. Experimental results demonstrate that ACIT outperforms state-of-the-art methods, achieving accuracy rates of 70% and 89% on the JAADbeh and JAADall datasets, respectively. Extensive ablation studies are further conducted to investigate the contribution of different modules of ACIT.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.20022</link>
<guid>https://arxiv.org/abs/2511.20022</guid>
<content:encoded><![CDATA[
arXiv:2511.20022v1 Announce Type: new 
Abstract: Recent advancements in multimodal large language models (MLLMs) have shown strong understanding of driving scenes, drawing interest in their application to autonomous driving. However, high-level reasoning in safety-critical scenarios, where avoiding one traffic risk can create another, remains a major challenge. Such reasoning is often infeasible with only a single front view and requires a comprehensive view of the environment, which we achieve through multi-view inputs. We define Safety-Critical Reasoning as a new task that leverages multi-view inputs to address this challenge. Then, we distill Safety-Critical Reasoning into two stages: first resolve the immediate risk, then mitigate the decision-induced downstream risks. To support this, we introduce WaymoQA, a dataset of 35,000 human-annotated question-answer pairs covering complex, high-risk driving scenarios. The dataset includes multiple-choice and open-ended formats across both image and video modalities. Experiments reveal that existing MLLMs underperform in safety-critical scenarios compared to normal scenes, but fine-tuning with WaymoQA significantly improves their reasoning ability, highlighting the effectiveness of our dataset in developing safer and more reasoning-capable driving agents.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAM-MI: A Mask-Injected Framework for Enhancing Open-Vocabulary Semantic Segmentation with SAM</title>
<link>https://arxiv.org/abs/2511.20027</link>
<guid>https://arxiv.org/abs/2511.20027</guid>
<content:encoded><![CDATA[
arXiv:2511.20027v1 Announce Type: new 
Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment and recognize objects universally. Trained on extensive high-quality segmentation data, the segment anything model (SAM) has demonstrated remarkable universal segmentation capabilities, offering valuable support for OVSS. Although previous methods have made progress in leveraging SAM for OVSS, there are still some challenges: (1) SAM's tendency to over-segment and (2) hard combinations between fixed masks and labels. This paper introduces a novel mask-injected framework, SAM-MI, which effectively integrates SAM with OVSS models to address these challenges. Initially, SAM-MI employs a Text-guided Sparse Point Prompter to sample sparse prompts for SAM instead of previous dense grid-like prompts, thus significantly accelerating the mask generation process. The framework then introduces Shallow Mask Aggregation (SMAgg) to merge partial masks to mitigate the SAM's over-segmentation issue. Finally, Decoupled Mask Injection (DMI) incorporates SAM-generated masks for guidance at low-frequency and high-frequency separately, rather than directly combining them with labels. Extensive experiments on multiple benchmarks validate the superiority of SAM-MI. Notably, the proposed method achieves a 16.7% relative improvement in mIoU over Grounded-SAM on the MESS benchmark, along with a 1.6$\times$ speedup. We hope SAM-MI can serve as an alternative methodology to effectively equip the OVSS model with SAM.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tell Model Where to Look: Mitigating Hallucinations in MLLMs by Vision-Guided Attention</title>
<link>https://arxiv.org/abs/2511.20032</link>
<guid>https://arxiv.org/abs/2511.20032</guid>
<content:encoded><![CDATA[
arXiv:2511.20032v1 Announce Type: new 
Abstract: Visual attention serves as the primary mechanism through which MLLMs interpret visual information; however, its limited localization capability often leads to hallucinations. We observe that although MLLMs can accurately extract visual semantics from visual tokens, they fail to fully leverage this advantage during subsequent inference. To address this limitation, we propose Vision-Guided Attention (VGA), a training-free method that first constructs precise visual grounding by exploiting the semantic content of visual tokens, and then uses this grounding to guide the model's focus toward relevant visual regions. In image captioning, VGA further refines this guidance dynamically during generation by suppressing regions that have already been described. In VGA, each token undergoes only a single forward pass, introducing a negligible latency overhead of just 4.36\%. In addition, VGA is fully compatible with efficient attention implementations such as FlashAttention. Extensive experiments across diverse MLLMs and multiple hallucination benchmarks demonstrate that VGA achieves state-of-the-art dehallucination performance. Further analysis confirms that explicit visual guidance plays a crucial role in enhancing the visual understanding capabilities of MLLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clair Obscur: an Illumination-Aware Method for Real-World Image Vectorization</title>
<link>https://arxiv.org/abs/2511.20034</link>
<guid>https://arxiv.org/abs/2511.20034</guid>
<content:encoded><![CDATA[
arXiv:2511.20034v1 Announce Type: new 
Abstract: Image vectorization aims to convert raster images into editable, scalable vector representations while preserving visual fidelity. Existing vectorization methods struggle to represent complex real-world images, often producing fragmented shapes at the cost of semantic conciseness. In this paper, we propose COVec, an illumination-aware vectorization framework inspired by the Clair-Obscur principle of light-shade contrast. COVec is the first to introduce intrinsic image decomposition in the vector domain, separating an image into albedo, shade, and light layers in a unified vector representation. A semantic-guided initialization and two-stage optimization refine these layers with differentiable rendering. Experiments on various datasets demonstrate that COVec achieves higher visual fidelity and significantly improved editability compared to existing methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MFM-point: Multi-scale Flow Matching for Point Cloud Generation</title>
<link>https://arxiv.org/abs/2511.20041</link>
<guid>https://arxiv.org/abs/2511.20041</guid>
<content:encoded><![CDATA[
arXiv:2511.20041v1 Announce Type: new 
Abstract: In recent years, point cloud generation has gained significant attention in 3D generative modeling. Among existing approaches, point-based methods directly generate point clouds without relying on other representations such as latent features, meshes, or voxels. These methods offer low training cost and algorithmic simplicity, but often underperform compared to representation-based approaches. In this paper, we propose MFM-Point, a multi-scale Flow Matching framework for point cloud generation that substantially improves the scalability and performance of point-based methods while preserving their simplicity and efficiency. Our multi-scale generation algorithm adopts a coarse-to-fine generation paradigm, enhancing generation quality and scalability without incurring additional training or inference overhead. A key challenge in developing such a multi-scale framework lies in preserving the geometric structure of unordered point clouds while ensuring smooth and consistent distributional transitions across resolutions. To address this, we introduce a structured downsampling and upsampling strategy that preserves geometry and maintains alignment between coarse and fine resolutions. Our experimental results demonstrate that MFM-Point achieves best-in-class performance among point-based methods and challenges the best representation-based methods. In particular, MFM-point demonstrates strong results in multi-category and high-resolution generation tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History-Augmented Contrastive Meta-Learning for Unsupervised Blind Super-Resolution of Planetary Remote Sensing Images</title>
<link>https://arxiv.org/abs/2511.20045</link>
<guid>https://arxiv.org/abs/2511.20045</guid>
<content:encoded><![CDATA[
arXiv:2511.20045v1 Announce Type: new 
Abstract: Planetary remote sensing images are affected by diverse and unknown degradations caused by imaging environments and hardware constraints. These factors limit image quality and hinder supervised blind super-resolution due to the lack of ground-truth images. This work presents History-Augmented Contrastive Blind Super-Resolution (HACBSR), an unsupervised framework for blind super-resolution that operates without ground-truth images and external kernel priors. HACBSR comprises two components: (1) a contrastive kernel sampling mechanism with kernel similarity control to mitigate distribution bias from Gaussian sampling, and (2) a history-augmented contrastive learning that uses historical models to generate negative samples to enable less greedy optimization and to induce strong convexity without ground-truth. A convergence analysis of the history-augmented contrastive learning is given in the Appendix. To support evaluation in planetary applications, we introduce Ceres-50, a dataset with diverse geological features simulated degradation patterns. Experiments show that HACBSR achieves competitive performance compared with state-of-the-art unsupervised methods across multiple upscaling factors. The code is available at https://github.com/2333repeat/HACBSR, and the dataset is available at https://github.com/2333repeat/Ceres-50.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeLightMono: Enhancing Self-Supervised Monocular Depth Estimation in Endoscopy by Decoupling Uneven Illumination</title>
<link>https://arxiv.org/abs/2511.20058</link>
<guid>https://arxiv.org/abs/2511.20058</guid>
<content:encoded><![CDATA[
arXiv:2511.20058v1 Announce Type: new 
Abstract: Self-supervised monocular depth estimation serves as a key task in the development of endoscopic navigation systems. However, performance degradation persists due to uneven illumination inherent in endoscopic images, particularly in low-intensity regions. Existing low-light enhancement techniques fail to effectively guide the depth network. Furthermore, solutions from other fields, like autonomous driving, require well-lit images, making them unsuitable and increasing data collection burdens. To this end, we present DeLight-Mono - a novel self-supervised monocular depth estimation framework with illumination decoupling. Specifically, endoscopic images are represented by a designed illumination-reflectance-depth model, and are decomposed with auxiliary networks. Moreover, a self-supervised joint-optimizing framework with novel losses leveraging the decoupled components is proposed to mitigate the effects of uneven illumination on depth estimation. The effectiveness of the proposed methods was rigorously verified through extensive comparisons and an ablation study performed on two public datasets.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLaTEC: Frequency-Disentangled Latent Triplanes for Efficient Compression of LiDAR Point Clouds</title>
<link>https://arxiv.org/abs/2511.20065</link>
<guid>https://arxiv.org/abs/2511.20065</guid>
<content:encoded><![CDATA[
arXiv:2511.20065v1 Announce Type: new 
Abstract: Point cloud compression methods jointly optimize bitrates and reconstruction distortion. However, balancing compression ratio and reconstruction quality is difficult because low-frequency and high-frequency components contribute differently at the same resolution. To address this, we propose FLaTEC, a frequency-aware compression model that enables the compression of a full scan with high compression ratios. Our approach introduces a frequency-aware mechanism that decouples low-frequency structures and high-frequency textures, while hybridizing latent triplanes as a compact proxy for point cloud. Specifically, we convert voxelized embeddings into triplane representations to reduce sparsity, computational cost, and storage requirements. We then devise a frequency-disentangling technique that extracts compact low-frequency content while collecting high-frequency details across scales. The decoupled low-frequency and high-frequency components are stored in binary format. During decoding, full-spectrum signals are progressively recovered via a modulation block. Additionally, to compensate for the loss of 3D correlation, we introduce an efficient frequency-based attention mechanism that fosters local connectivity and outputs arbitrary resolution points. Our method achieves state-of-the-art rate-distortion performance and outperforms the standard codecs by 78\% and 94\% in BD-rate on both SemanticKITTI and Ford datasets.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRADA: Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images</title>
<link>https://arxiv.org/abs/2511.20068</link>
<guid>https://arxiv.org/abs/2511.20068</guid>
<content:encoded><![CDATA[
arXiv:2511.20068v1 Announce Type: new 
Abstract: Autoregressive (AR) image generation has recently emerged as a powerful paradigm for image synthesis. Leveraging the generation principle of large language models, they allow for efficiently generating deceptively real-looking images, further increasing the need for reliable detection methods. However, to date there is a lack of work specifically targeting the detection of images generated by AR image generators. In this work, we present PRADA (Probability-Ratio-Based Attribution and Detection of Autoregressive-Generated Images), a simple and interpretable approach that can reliably detect AR-generated images and attribute them to their respective source model. The key idea is to inspect the ratio of a model's conditional and unconditional probability for the autoregressive token sequence representing a given image. Whenever an image is generated by a particular model, its probability ratio shows unique characteristics which are not present for images generated by other models or real images. We exploit these characteristics for threshold-based attribution and detection by calibrating a simple, model-specific score function. Our experimental evaluation shows that PRADA is highly effective against eight class-to-image and four text-to-image models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Procedural-aware Video Representations through State-Grounded Hierarchy Unfolding</title>
<link>https://arxiv.org/abs/2511.20073</link>
<guid>https://arxiv.org/abs/2511.20073</guid>
<content:encoded><![CDATA[
arXiv:2511.20073v1 Announce Type: new 
Abstract: Learning procedural-aware video representations is a key step towards building agents that can reason about and execute complex tasks. Existing methods typically address this problem by aligning visual content with textual descriptions at the task and step levels to inject procedural semantics into video representations. However, due to their high level of abstraction, 'task' and 'step' descriptions fail to form a robust alignment with the concrete, observable details in visual data. To address this, we introduce 'states', i.e., textual snapshots of object configurations, as a visually-grounded semantic layer that anchors abstract procedures to what a model can actually see. We formalize this insight in a novel Task-Step-State (TSS) framework, where tasks are achieved via steps that drive transitions between observable states. To enforce this structure, we propose a progressive pre-training strategy that unfolds the TSS hierarchy, forcing the model to ground representations in states while associating them with steps and high-level tasks. Extensive experiments on the COIN and CrossTask datasets show that our method outperforms baseline models on multiple downstream tasks, including task recognition, step recognition, and next step prediction. Ablation studies show that introducing state supervision is a key driver of performance gains across all tasks. Additionally, our progressive pretraining strategy proves more effective than standard joint training, as it better enforces the intended hierarchical structure.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Adaptive Local Denoising for CEST Imaging</title>
<link>https://arxiv.org/abs/2511.20081</link>
<guid>https://arxiv.org/abs/2511.20081</guid>
<content:encoded><![CDATA[
arXiv:2511.20081v1 Announce Type: new 
Abstract: Chemical Exchange Saturation Transfer (CEST) MRI enables molecular-level visualization of low-concentration metabolites by leveraging proton exchange dynamics. However, its clinical translation is hindered by inherent challenges: spatially varying noise arising from hardware limitations, and complex imaging protocols introduce heteroscedasticity in CEST data, perturbing the accuracy of quantitative contrast mapping such as amide proton transfer (APT) imaging. Traditional denoising methods are not designed for this complex noise and often alter the underlying information that is critical for biomedical analysis. To overcome these limitations, we propose a new Blind Adaptive Local Denoising (BALD) method. BALD exploits the self-similar nature of CEST data to derive an adaptive variance-stabilizing transform that equalizes the noise distributions across CEST pixels without prior knowledge of noise characteristics. Then, BALD performs two-stage denoising on a linear transformation of data to disentangle molecular signals from noise. A local SVD decomposition is used as a linear transform to prevent spatial and spectral denoising artifacts. We conducted extensive validation experiments on multiple phantoms and \textit{in vivo} CEST scans. In these experiments, BALD consistently outperformed state-of-the-art CEST denoisers in both denoising metrics and downstream tasks such as molecular concentration maps estimation and cancer detection.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Visual Anomaly Detection via Concept Bottleneck Models</title>
<link>https://arxiv.org/abs/2511.20088</link>
<guid>https://arxiv.org/abs/2511.20088</guid>
<content:encoded><![CDATA[
arXiv:2511.20088v1 Announce Type: new 
Abstract: In recent years, Visual Anomaly Detection (VAD) has gained significant attention due to its ability to identify anomalous images using only normal images during training. Many VAD models work without supervision but are still able to provide visual explanations by highlighting the anomalous regions within an image. However, although these visual explanations can be helpful, they lack a direct and semantically meaningful interpretation for users. To address this limitation, we propose extending Concept Bottleneck Models (CBMs) to the VAD setting. By learning meaningful concepts, the network can provide human-interpretable descriptions of anomalies, offering a novel and more insightful way to explain them. Our contributions are threefold: (i) we develop a Concept Dataset to support research on CBMs for VAD; (ii) we improve the CBM architecture to generate both concept-based and visual explanations, bridging semantic and localization interpretability; and (iii) we introduce a pipeline for synthesizing artificial anomalies, preserving the VAD paradigm of minimizing dependence on rare anomalous samples. Our approach, Concept-Aware Visual Anomaly Detection (CONVAD), achieves performance comparable to classic VAD methods while providing richer, concept-driven explanations that enhance interpretability and trust in VAD systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WPT: World-to-Policy Transfer via Online World Model Distillation</title>
<link>https://arxiv.org/abs/2511.20095</link>
<guid>https://arxiv.org/abs/2511.20095</guid>
<content:encoded><![CDATA[
arXiv:2511.20095v1 Announce Type: new 
Abstract: Recent years have witnessed remarkable progress in world models, which primarily aim to capture the spatio-temporal correlations between an agent's actions and the evolving environment. However, existing approaches often suffer from tight runtime coupling or depend on offline reward signals, resulting in substantial inference overhead or hindering end-to-end optimization. To overcome these limitations, we introduce WPT, a World-to-Policy Transfer training paradigm that enables online distillation under the guidance of an end-to-end world model. Specifically, we develop a trainable reward model that infuses world knowledge into a teacher policy by aligning candidate trajectories with the future dynamics predicted by the world model. Subsequently, we propose policy distillation and world reward distillation to transfer the teacher's reasoning ability into a lightweight student policy, enhancing planning performance while preserving real-time deployability. Extensive experiments on both open-loop and closed-loop benchmarks show that our WPT achieves state-of-the-art performance with a simple policy architecture: it attains a 0.11 collision rate (open-loop) and achieves a 79.23 driving score (closed-loop) surpassing both world-model-based and imitation-learning methods in accuracy and safety. Moreover, the student sustains up to 4.9x faster inference, while retaining most of the gains.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring State-of-the-art models for Early Detection of Forest Fires</title>
<link>https://arxiv.org/abs/2511.20096</link>
<guid>https://arxiv.org/abs/2511.20096</guid>
<content:encoded><![CDATA[
arXiv:2511.20096v1 Announce Type: new 
Abstract: There have been many recent developments in the use of Deep Learning Neural Networks for fire detection. In this paper, we explore an early warning system for detection of forest fires. Due to the lack of sizeable datasets and models tuned for this task, existing methods suffer from missed detection. In this work, we first propose a dataset for early identification of forest fires through visual analysis. Unlike existing image corpuses that contain images of wide-spread fire, our dataset consists of multiple instances of smoke plumes and fire that indicates the initiation of fire. We obtained this dataset synthetically by utilising game simulators such as Red Dead Redemption 2. We also combined our dataset with already published images to obtain a more comprehensive set. Finally, we compared image classification and localisation methods on the proposed dataset. More specifically we used YOLOv7 (You Only Look Once) and different models of detection transformer.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi Head Attention Enhanced Inception v3 for Cardiomegaly Detection</title>
<link>https://arxiv.org/abs/2511.20101</link>
<guid>https://arxiv.org/abs/2511.20101</guid>
<content:encoded><![CDATA[
arXiv:2511.20101v1 Announce Type: new 
Abstract: The healthcare industry has been revolutionized significantly by novel imaging technologies, not just in the diagnosis of cardiovascular diseases but also by the visualization of structural abnormalities like cardiomegaly. This article explains an integrated approach to the use of deep learning tools and attention mechanisms for automatic detection of cardiomegaly using X-ray images. The initiation of the project is grounded on a strong Data Collection phase and gathering the data of annotated X-ray images of various types. Then, while the Preprocessing module fine-tunes image quality, it is feasible to utilize the best out of the data quality in the proposed system. In our proposed system, the process is a CNN configuration leveraging the inception V3 model as one of the key blocks. Besides, we also employ a multilayer attention mechanism to enhance the strength. The most important feature of the method is the multi-head attention mechanism that can learn features automatically. By exact selective focusing on only some regions of input, the model can thus identify cardiomegaly in a sensitive manner. Attention rating is calculated, duplicated, and applied to enhance representation of main data, and therefore there is a successful diagnosis. The Evaluation stage will be extremely strict and it will thoroughly evaluate the model based on such measures as accuracy and precision. This will validate that the model can identify cardiomegaly and will also show the clinical significance of this method. The model has accuracy of 95.6, precision of 95.2, recall of 96.2, sensitivity of 95.7, specificity of 96.1 and an Area Under Curve(AUC) of 96.0 and their respective graphs are plotted for visualisation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LungEvaty: A Scalable, Open-Source Transformer-based Deep Learning Model for Lung Cancer Risk Prediction in LDCT Screening</title>
<link>https://arxiv.org/abs/2511.20116</link>
<guid>https://arxiv.org/abs/2511.20116</guid>
<content:encoded><![CDATA[
arXiv:2511.20116v1 Announce Type: new 
Abstract: Lung cancer risk estimation is gaining increasing importance as more countries introduce population-wide screening programs using low-dose CT (LDCT). As imaging volumes grow, scalable methods that can process entire lung volumes efficiently are essential to tap into the full potential of these large screening datasets. Existing approaches either over-rely on pixel-level annotations, limiting scalability, or analyze the lung in fragments, weakening performance. We present LungEvaty, a fully transformer-based framework for predicting 1-6 year lung cancer risk from a single LDCT scan. The model operates on whole-lung inputs, learning directly from large-scale screening data to capture comprehensive anatomical and pathological cues relevant for malignancy risk. Using only imaging data and no region supervision, LungEvaty matches state-of-the-art performance, refinable by an optional Anatomically Informed Attention Guidance (AIAG) loss that encourages anatomically focused attention. In total, LungEvaty was trained on more than 90,000 CT scans, including over 28,000 for fine-tuning and 6,000 for evaluation. The framework offers a simple, data-efficient, and fully open-source solution that provides an extensible foundation for future research in longitudinal and multimodal lung cancer risk prediction.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers</title>
<link>https://arxiv.org/abs/2511.20123</link>
<guid>https://arxiv.org/abs/2511.20123</guid>
<content:encoded><![CDATA[
arXiv:2511.20123v1 Announce Type: new 
Abstract: Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language Models for Automated 3D PET/CT Report Generation</title>
<link>https://arxiv.org/abs/2511.20145</link>
<guid>https://arxiv.org/abs/2511.20145</guid>
<content:encoded><![CDATA[
arXiv:2511.20145v1 Announce Type: new 
Abstract: Positron emission tomography/computed tomography (PET/CT) is essential in oncology, yet the rapid expansion of scanners has outpaced the availability of trained specialists, making automated PET/CT report generation (PETRG) increasingly important for reducing clinical workload. Compared with structural imaging (e.g., X-ray, CT, and MRI), functional PET poses distinct challenges: metabolic patterns vary with tracer physiology, and whole-body 3D contextual information is required rather than local-region interpretation. To advance PETRG, we propose PETRG-3D, an end-to-end 3D dual-branch framework that separately encodes PET and CT volumes and incorporates style-adaptive prompts to mitigate inter-hospital variability in reporting practices. We construct PETRG-Lym, a multi-center lymphoma dataset collected from four hospitals (824 reports w/ 245,509 paired PET/CT slices), and construct AutoPET-RG-Lym, a publicly accessible PETRG benchmark derived from open imaging data but equipped with new expert-written, clinically validated reports (135 cases). To assess clinical utility, we introduce PETRG-Score, a lymphoma-specific evaluation protocol that jointly measures metabolic and structural findings across curated anatomical regions. Experiments show that PETRG-3D substantially outperforms existing methods on both natural language metrics (e.g., +31.49\% ROUGE-L) and clinical efficacy metrics (e.g., +8.18\% PET-All), highlighting the benefits of volumetric dual-modality modeling and style-aware prompting. Overall, this work establishes a foundation for future PET/CT-specific models emphasizing disease-aware reasoning and clinically reliable evaluation. Codes, models, and AutoPET-RG-Lym will be released.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Convolution and Frequency State Space Network for Image Compression</title>
<link>https://arxiv.org/abs/2511.20151</link>
<guid>https://arxiv.org/abs/2511.20151</guid>
<content:encoded><![CDATA[
arXiv:2511.20151v1 Announce Type: new 
Abstract: Learned image compression (LIC) has recently benefited from Transformer based and state space model (SSM) based architectures. Convolutional neural networks (CNNs) effectively capture local high frequency details, whereas Transformers and SSMs provide strong long range modeling capabilities but may cause structural information loss or ignore frequency characteristics that are crucial for compression. In this work we propose HCFSSNet, a Hybrid Convolution and Frequency State Space Network for LIC. HCFSSNet uses CNNs to extract local high frequency structures and introduces a Vision Frequency State Space (VFSS) block that models long range low frequency information. The VFSS block combines an Omni directional Neighborhood State Space (VONSS) module, which scans features horizontally, vertically and diagonally, with an Adaptive Frequency Modulation Module (AFMM) that applies content adaptive weighting of discrete cosine transform frequency components for more efficient bit allocation. To further reduce redundancy in the entropy model, we integrate AFMM with a Swin Transformer to form a Frequency Swin Transformer Attention Module (FSTAM) for frequency aware side information modeling. Experiments on the Kodak, Tecnick and CLIC Professional Validation datasets show that HCFSSNet achieves competitive rate distortion performance compared with recent SSM based codecs such as MambaIC, while using significantly fewer parameters. On Kodak, Tecnick and CLIC, HCFSSNet reduces BD rate over the VTM anchor by 18.06, 24.56 and 22.44 percent, respectively, providing an efficient and interpretable hybrid architecture for future learned image compression systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Restora-Flow: Mask-Guided Image Restoration with Flow Matching</title>
<link>https://arxiv.org/abs/2511.20152</link>
<guid>https://arxiv.org/abs/2511.20152</guid>
<content:encoded><![CDATA[
arXiv:2511.20152v1 Announce Type: new 
Abstract: Flow matching has emerged as a promising generative approach that addresses the lengthy sampling times associated with state-of-the-art diffusion models and enables a more flexible trajectory design, while maintaining high-quality image generation. This capability makes it suitable as a generative prior for image restoration tasks. Although current methods leveraging flow models have shown promising results in restoration, some still suffer from long processing times or produce over-smoothed results. To address these challenges, we introduce Restora-Flow, a training-free method that guides flow matching sampling by a degradation mask and incorporates a trajectory correction mechanism to enforce consistency with degraded inputs. We evaluate our approach on both natural and medical datasets across several image restoration tasks involving a mask-based degradation, i.e., inpainting, super-resolution and denoising. We show superior perceptual quality and processing time compared to diffusion and flow matching-based reference methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alzheimers Disease Progression Prediction Based on Manifold Mapping of Irregularly Sampled Longitudinal Data</title>
<link>https://arxiv.org/abs/2511.20154</link>
<guid>https://arxiv.org/abs/2511.20154</guid>
<content:encoded><![CDATA[
arXiv:2511.20154v1 Announce Type: new 
Abstract: The uncertainty of clinical examinations frequently leads to irregular observation intervals in longitudinal imaging data, posing challenges for modeling disease progression.Most existing imaging-based disease prediction models operate in Euclidean space, which assumes a flat representation of data and fails to fully capture the intrinsic continuity and nonlinear geometric structure of irregularly sampled longitudinal images. To address the challenge of modeling Alzheimers disease (AD) progression from irregularly sampled longitudinal structural Magnetic Resonance Imaging (sMRI) data, we propose a Riemannian manifold mapping, a Time-aware manifold Neural ordinary differential equation, and an Attention-based riemannian Gated recurrent unit (R-TNAG) framework. Our approach first projects features extracted from high-dimensional sMRI into a manifold space to preserve the intrinsic geometry of disease progression. On this representation, a time-aware Neural Ordinary Differential Equation (TNODE) models the continuous evolution of latent states between observations, while an Attention-based Riemannian Gated Recurrent Unit (ARGRU) adaptively integrates historical and current information to handle irregular intervals. This joint design improves temporal consistency and yields robust AD trajectory prediction under irregular sampling.Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art models in both disease status prediction and cognitive score regression. Ablation studies verify the contributions of each module, highlighting their complementary roles in enhancing predictive accuracy. Moreover, the model exhibits stable performance across varying sequence lengths and missing data rates, indicating strong temporal generalizability. Cross-dataset validation further confirms its robustness and applicability in diverse clinical settings.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Map-World: Masked Action planning and Path-Integral World Model for Autonomous Driving</title>
<link>https://arxiv.org/abs/2511.20156</link>
<guid>https://arxiv.org/abs/2511.20156</guid>
<content:encoded><![CDATA[
arXiv:2511.20156v1 Announce Type: new 
Abstract: Motion planning for autonomous driving must handle multiple plausible futures while remaining computationally efficient. Recent end-to-end systems and world-model-based planners predict rich multi-modal trajectories, but typically rely on handcrafted anchors or reinforcement learning to select a single best mode for training and control. This selection discards information about alternative futures and complicates optimization. We propose MAP-World, a prior-free multi-modal planning framework that couples masked action planning with a path-weighted world model. The Masked Action Planning (MAP) module treats future ego motion as masked sequence completion: past waypoints are encoded as visible tokens, future waypoints are represented as mask tokens, and a driving-intent path provides a coarse scaffold. A compact latent planning state is expanded into multiple trajectory queries with injected noise, yielding diverse, temporally consistent modes without anchor libraries or teacher policies. A lightweight world model then rolls out future BEV semantics conditioned on each candidate trajectory. During training, semantic losses are computed as an expectation over modes, using trajectory probabilities as discrete path weights, so the planner learns from the full distribution of plausible futures instead of a single selected path. On NAVSIM, our method matches anchor-based approaches and achieves state-of-the-art performance among world-model-based methods, while avoiding reinforcement learning and maintaining real-time inference latency.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SKEL-CF: Coarse-to-Fine Biomechanical Skeleton and Surface Mesh Recovery</title>
<link>https://arxiv.org/abs/2511.20157</link>
<guid>https://arxiv.org/abs/2511.20157</guid>
<content:encoded><![CDATA[
arXiv:2511.20157v1 Announce Type: new 
Abstract: Parametric 3D human models such as SMPL have driven significant advances in human pose and shape estimation, yet their simplified kinematics limit biomechanical realism. The recently proposed SKEL model addresses this limitation by re-rigging SMPL with an anatomically accurate skeleton. However, estimating SKEL parameters directly remains challenging due to limited training data, perspective ambiguities, and the inherent complexity of human articulation. We introduce SKEL-CF, a coarse-to-fine framework for SKEL parameter estimation. SKEL-CF employs a transformer-based encoder-decoder architecture, where the encoder predicts coarse camera and SKEL parameters, and the decoder progressively refines them in successive layers. To ensure anatomically consistent supervision, we convert the existing SMPL-based dataset 4DHuman into a SKEL-aligned version, 4DHuman-SKEL, providing high-quality training data for SKEL estimation. In addition, to mitigate depth and scale ambiguities, we explicitly incorporate camera modeling into the SKEL-CF pipeline and demonstrate its importance across diverse viewpoints. Extensive experiments validate the effectiveness of the proposed design. On the challenging MOYO dataset, SKEL-CF achieves 85.0 MPJPE / 51.4 PA-MPJPE, significantly outperforming the previous SKEL-based state-of-the-art HSMR (104.5 / 79.6). These results establish SKEL-CF as a scalable and anatomically faithful framework for human motion analysis, bridging the gap between computer vision and biomechanics. Our implementation is available on the project page: https://pokerman8.github.io/SKEL-CF/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harmonious Parameter Adaptation in Continual Visual Instruction Tuning for Safety-Aligned MLLMs</title>
<link>https://arxiv.org/abs/2511.20158</link>
<guid>https://arxiv.org/abs/2511.20158</guid>
<content:encoded><![CDATA[
arXiv:2511.20158v1 Announce Type: new 
Abstract: While continual visual instruction tuning (CVIT) has shown promise in adapting multimodal large language models (MLLMs), existing studies predominantly focus on models without safety alignment. This critical oversight ignores the fact that real-world MLLMs inherently require such mechanisms to mitigate potential risks. In this work, we shift our focus to CVIT for safety-aligned MLLMs and observe that during continual adaptation, the model not only suffers from task forgetting but also exhibits degradation in its safety. Achieving a harmonious balance between safety and task performance remains a crucial challenge. To address this, we propose Harmonious Parameter Adaptation (HPA), a post-training framework composed of focusing-based parameter partition, harmoniously balanced parameter selection, and orthogonal parameter adjustment. Specifically, HPA partitions parameters into two types based on their focus on safety or task performance, and selects the focused ones to preserve from a balanced perspective. In addition, HPA imposes orthogonality constraints on parameter updates to further alleviate catastrophic forgetting. Extensive experiments on the CVIT benchmark and safety evaluation datasets demonstrate that HPA better maintains high safety and mitigates forgetting than existing baselines.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>While recognizing actions, LMMs struggle to detect core interaction events</title>
<link>https://arxiv.org/abs/2511.20162</link>
<guid>https://arxiv.org/abs/2511.20162</guid>
<content:encoded><![CDATA[
arXiv:2511.20162v1 Announce Type: new 
Abstract: Large multi-modal models (LMMs) show increasing performance in realistic visual tasks for images and, more recently, for videos. For example, given a video sequence, such models are able to describe in detail objects, the surroundings and dynamic actions. In this study, we explored the extent to which these models ground their semantic understanding in the actual visual input. Specifically, given sequences of hands interacting with objects, we asked models when and where the interaction begins or ends. For this purpose, we introduce a first of its kind, large-scale dataset with more than 20K annotated interactions on videos from the Something-Something-V2 dataset. 250 AMTurk human annotators labeled core interaction events, particularly when and where objects and agents become attached ('contact') or detached ('release'). We asked two LMMs (Qwen-2.5VL and GPT-4o) to locate these events in short videos, each with a single event. The results show that although the models can reliably name the target objects, identify the action and provide coherent reasoning, they consistently fail to identify the frame where the interaction begins or ends and cannot localize the event within the scene. Our findings suggest that in struggling to pinpoint the moment and location of physical contact that defines the interaction, the models lack the perceptual grounding required for deeper understanding of dynamic scenes.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADNet: A Large-Scale and Extensible Multi-Domain Benchmark for Anomaly Detection Across 380 Real-World Categories</title>
<link>https://arxiv.org/abs/2511.20169</link>
<guid>https://arxiv.org/abs/2511.20169</guid>
<content:encoded><![CDATA[
arXiv:2511.20169v1 Announce Type: new 
Abstract: Anomaly detection (AD) aims to identify defects using normal-only training data. Existing anomaly detection benchmarks (e.g., MVTec-AD with 15 categories) cover only a narrow range of categories, limiting the evaluation of cross-context generalization and scalability. We introduce ADNet, a large-scale, multi-domain benchmark comprising 380 categories aggregated from 49 publicly available datasets across Electronics, Industry, Agrifood, Infrastructure, and Medical domains. The benchmark includes a total of 196,294 RGB images, consisting of 116,192 normal samples for training and 80,102 test images, of which 60,311 are anomalous. All images are standardized with MVTec-style pixel-level annotations and structured text descriptions spanning both spatial and visual attributes, enabling multimodal anomaly detection tasks. Extensive experiments reveal a clear scalability challenge: existing state-of-the-art methods achieve 90.6% I-AUROC in one-for-one settings but drop to 78.5% when scaling to all 380 categories in a multi-class setting. To address this, we propose Dinomaly-m, a context-guided Mixture-of-Experts extension of Dinomaly that expands decoder capacity without increasing inference cost. It achieves 83.2% I-AUROC and 93.1% P-AUROC, demonstrating superior performance over existing approaches. ADNet is designed as a standardized and extensible benchmark, supporting the community in expanding anomaly detection datasets across diverse domains and providing a scalable foundation for future anomaly detection foundation models. Dataset: https://grainnet.github.io/ADNet
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realizing Fully-Integrated, Low-Power, Event-Based Pupil Tracking with Neuromorphic Hardware</title>
<link>https://arxiv.org/abs/2511.20175</link>
<guid>https://arxiv.org/abs/2511.20175</guid>
<content:encoded><![CDATA[
arXiv:2511.20175v1 Announce Type: new 
Abstract: Eye tracking is fundamental to numerous applications, yet achieving robust, high-frequency tracking with ultra-low power consumption remains challenging for wearable platforms. While event-based vision sensors offer microsecond resolution and sparse data streams, they have lacked fully integrated, low-power processing solutions capable of real-time inference. In this work, we present the first battery-powered, wearable pupil-center-tracking system with complete on-device integration, combining event-based sensing and neuromorphic processing on the commercially available Speck2f system-on-chip with lightweight coordinate decoding on a low-power microcontroller. Our solution features a novel uncertainty-quantifying spiking neural network with gated temporal decoding, optimized for strict memory and bandwidth constraints, complemented by systematic deployment mechanisms that bridge the reality gap. We validate our system on a new multi-user dataset and demonstrate a wearable prototype with dual neuromorphic devices achieving robust binocular pupil tracking at 100 Hz with an average power consumption below 5 mW per eye. Our work demonstrates that end-to-end neuromorphic computing enables practical, always-on eye tracking for next-generation energy-efficient wearable systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis</title>
<link>https://arxiv.org/abs/2511.20186</link>
<guid>https://arxiv.org/abs/2511.20186</guid>
<content:encoded><![CDATA[
arXiv:2511.20186v1 Announce Type: new 
Abstract: Foundation video generation models such as WAN 2.2 exhibit strong text- and image-conditioned synthesis abilities but remain constrained to the same-view generation setting. In this work, we introduce Exo2EgoSyn, an adaptation of WAN 2.2 that unlocks Exocentric-to-Egocentric(Exo2Ego) cross-view video synthesis. Our framework consists of three key modules. Ego-Exo View Alignment(EgoExo-Align) enforces latent-space alignment between exocentric and egocentric first-frame representations, reorienting the generative space from the given exo view toward the ego view. Multi-view Exocentric Video Conditioning (MultiExoCon) aggregates multi-view exocentric videos into a unified conditioning signal, extending WAN2.2 beyond its vanilla single-image or text conditioning. Furthermore, Pose-Aware Latent Injection (PoseInj) injects relative exo-to-ego camera pose information into the latent state, guiding geometry-aware synthesis across viewpoints. Together, these modules enable high-fidelity ego view video generation from third-person observations without retraining from scratch. Experiments on ExoEgo4D validate that Exo2EgoSyn significantly improves Ego2Exo synthesis, paving the way for scalable cross-view video generation with foundation models. Source code and models will be released publicly.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SFA: Scan, Focus, and Amplify toward Guidance-aware Answering for Video TextVQA</title>
<link>https://arxiv.org/abs/2511.20190</link>
<guid>https://arxiv.org/abs/2511.20190</guid>
<content:encoded><![CDATA[
arXiv:2511.20190v1 Announce Type: new 
Abstract: Video text-based visual question answering (Video TextVQA) task aims to answer questions about videos by leveraging the visual text appearing within the videos. This task poses significant challenges, requiring models to accurately perceive and comprehend scene text that varies in scale, orientation, and clarity across frames, while effectively integrating temporal and semantic context to generate precise answers. Moreover, the model must identify question-relevant textual cues and filter out redundant or irrelevant information to ensure answering is guided by the most relevant and informative cues. To address these challenges, we propose SFA, a training-free framework and the first Video-LLM-based method tailored for Video TextVQA, motivated by the human process of answering questions. By adaptively scanning video frames, selectively focusing on key regions, and directly amplifying them, SFA effectively guides the Video-LLM's attention toward essential cues, enabling it to generate more accurate answers. SFA achieves new state-of-the-art results across several public Video TextVQA datasets and surpasses previous methods by a substantial margin, demonstrating its effectiveness and generalizability.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering</title>
<link>https://arxiv.org/abs/2511.20201</link>
<guid>https://arxiv.org/abs/2511.20201</guid>
<content:encoded><![CDATA[
arXiv:2511.20201v1 Announce Type: new 
Abstract: We propose GHR-VQA, Graph-guided Hierarchical Relational Reasoning for Video Question Answering (Video QA), a novel human-centric framework that incorporates scene graphs to capture intricate human-object interactions within video sequences. Unlike traditional pixel-based methods, each frame is represented as a scene graph and human nodes across frames are linked to a global root, forming the video-level graph and enabling cross-frame reasoning centered on human actors. The video-level graphs are then processed by Graph Neural Networks (GNNs), transforming them into rich, context-aware embeddings for efficient processing. Finally, these embeddings are integrated with question features in a hierarchical network operating across different abstraction levels, enhancing both local and global understanding of video content. This explicit human-rooted structure enhances interpretability by decomposing actions into human-object interactions and enables a more profound understanding of spatiotemporal dynamics. We validate our approach on the Action Genome Question Answering (AGQA) dataset, achieving significant performance improvements, including a 7.3% improvement in object-relation reasoning over the state of the art.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust 3D Brain MRI Inpainting with Random Masking Augmentation</title>
<link>https://arxiv.org/abs/2511.20202</link>
<guid>https://arxiv.org/abs/2511.20202</guid>
<content:encoded><![CDATA[
arXiv:2511.20202v1 Announce Type: new 
Abstract: The ASNR-MICCAI BraTS-Inpainting Challenge was established to mitigate dataset biases that limit deep learning models in the quantitative analysis of brain tumor MRI. This paper details our submission to the 2025 challenge, a novel deep learning framework for synthesizing healthy tissue in 3D scans. The core of our method is a U-Net architecture trained to inpaint synthetically corrupted regions, enhanced with a random masking augmentation strategy to improve generalization. Quantitative evaluation confirmed the efficacy of our approach, yielding an SSIM of 0.873$\pm$0.004, a PSNR of 24.996$\pm$4.694, and an MSE of 0.005$\pm$0.087 on the validation set. On the final online test set, our method achieved an SSIM of 0.919$\pm$0.088, a PSNR of 26.932$\pm$5.057, and an RMSE of 0.052$\pm$0.026. This performance secured first place in the BraTS-Inpainting 2025 challenge and surpassed the winning solutions from the 2023 and 2024 competitions on the official leaderboard.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation</title>
<link>https://arxiv.org/abs/2511.20211</link>
<guid>https://arxiv.org/abs/2511.20211</guid>
<content:encoded><![CDATA[
arXiv:2511.20211v1 Announce Type: new 
Abstract: Generative models have excelled in RGB synthesis, but real-world applications require RGBA manipulation. This has led to a fragmented landscape: specialized, single-task models handle alpha but lack versatility, while unified multi-task frameworks are confined to the RGB domain. To bridge this critical gap, we propose OmniAlpha, the first unified, multi-task generative framework for sequence-to-sequence RGBA image generation and editing. Its architecture features MSRoPE-BiL, a novel RoPE method with a bi-directionally extendable layer axis for its Diffusion Transformer (DiT) backbone, enabling the concurrent processing of multiple input and target RGBA layers. To power this framework, we introduce AlphaLayers, a new dataset of 1,000 high-quality, multi-layer triplets, built via a novel automated synthesis and filter pipeline. Jointly training OmniAlpha on this dataset across a comprehensive suite of 21 diverse tasks, extensive experiments demonstrate that our unified approach consistently outperforms strong, specialized baselines. Most notably, OmniAlpha achieves a dramatic 84.8% relative reduction in SAD for mask-free matting on AIM-500 and wins over 90% of human preferences in layer-conditioned completion. Our work proves that a unified, multi-task model can learn a superior shared representation for RGBA, paving the way for more powerful, layer-aware generative systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-guided Controllable Diffusion for Realistic Camouflage Images Generation</title>
<link>https://arxiv.org/abs/2511.20218</link>
<guid>https://arxiv.org/abs/2511.20218</guid>
<content:encoded><![CDATA[
arXiv:2511.20218v1 Announce Type: new 
Abstract: Camouflage Images Generation (CIG) is an emerging research area that focuses on synthesizing images in which objects are harmoniously blended and exhibit high visual consistency with their surroundings. Existing methods perform CIG by either fusing objects into specific backgrounds or outpainting the surroundings via foreground object-guided diffusion. However, they often fail to obtain natural results because they overlook the logical relationship between camouflaged objects and background environments. To address this issue, we propose CT-CIG, a Controllable Text-guided Camouflage Images Generation method that produces realistic and logically plausible camouflage images. Leveraging Large Visual Language Models (VLM), we design a Camouflage-Revealing Dialogue Mechanism (CRDM) to annotate existing camouflage datasets with high-quality text prompts. Subsequently, the constructed image-prompt pairs are utilized to finetune Stable Diffusion, incorporating a lightweight controller to guide the location and shape of camouflaged objects for enhanced camouflage scene fitness. Moreover, we design a Frequency Interaction Refinement Module (FIRM) to capture high-frequency texture features, facilitating the learning of complex camouflage patterns. Extensive experiments, including CLIPScore evaluation and camouflage effectiveness assessment, demonstrate the semantic alignment of our generated text prompts and CT-CIG's ability to produce photorealistic camouflage images.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patch-Level Glioblastoma Subregion Classification with a Contrastive Learning-Based Encoder</title>
<link>https://arxiv.org/abs/2511.20221</link>
<guid>https://arxiv.org/abs/2511.20221</guid>
<content:encoded><![CDATA[
arXiv:2511.20221v1 Announce Type: new 
Abstract: The significant molecular and pathological heterogeneity of glioblastoma, an aggressive brain tumor, complicates diagnosis and patient stratification. While traditional histopathological assessment remains the standard, deep learning offers a promising path toward objective and automated analysis of whole slide images. For the BraTS-Path 2025 Challenge, we developed a method that fine-tunes a pre-trained Vision Transformer (ViT) encoder with a dedicated classification head on the official training dataset. Our model's performance on the online validation set, evaluated via the Synapse platform, yielded a Matthews Correlation Coefficient (MCC) of 0.7064 and an F1-score of 0.7676. On the final test set, the model achieved an MCC of 0.6509 and an F1-score of 0.5330, which secured our team second place in the BraTS-Pathology 2025 Challenge. Our results establish a solid baseline for ViT-based histopathological analysis, and future efforts will focus on bridging the performance gap observed on the unseen validation data.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs</title>
<link>https://arxiv.org/abs/2511.20223</link>
<guid>https://arxiv.org/abs/2511.20223</guid>
<content:encoded><![CDATA[
arXiv:2511.20223v1 Announce Type: new 
Abstract: Adversarial attacks have evolved from simply disrupting predictions on conventional task-specific models to the more complex goal of manipulating image semantics on Large Vision-Language Models (LVLMs). However, existing methods struggle with controllability and fail to precisely manipulate the semantics of specific concepts in the image. We attribute this limitation to semantic entanglement in the patch-token representations on which adversarial attacks typically operate: global context aggregated by self-attention in the vision encoder dominates individual patch features, making them unreliable handles for precise local semantic manipulation. Our systematic investigation reveals a key insight: value features (V) computed within the transformer attention block serve as much more precise handles for manipulation. We show that V suppresses global-context channels, allowing it to retain high-entropy, disentangled local semantic information. Building on this discovery, we propose V-Attack, a novel method designed for precise local semantic attacks. V-Attack targets the value features and introduces two core components: (1) a Self-Value Enhancement module to refine V's intrinsic semantic richness, and (2) a Text-Guided Value Manipulation module that leverages text prompts to locate source concept and optimize it toward a target concept. By bypassing the entangled patch features, V-Attack achieves highly effective semantic control. Extensive experiments across diverse LVLMs, including LLaVA, InternVL, DeepseekVL and GPT-4o, show that V-Attack improves the attack success rate by an average of 36% over state-of-the-art methods, exposing critical vulnerabilities in modern visual-language understanding. Our code and data are available https://github.com/Summu77/V-Attack.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HistoSpeckle-Net: Mutual Information-Guided Deep Learning for high-fidelity reconstruction of complex OrganAMNIST images via perturbed Multimode Fibers</title>
<link>https://arxiv.org/abs/2511.20245</link>
<guid>https://arxiv.org/abs/2511.20245</guid>
<content:encoded><![CDATA[
arXiv:2511.20245v1 Announce Type: new 
Abstract: Existing deep learning methods in multimode fiber (MMF) imaging often focus on simpler datasets, limiting their applicability to complex, real-world imaging tasks. These models are typically data-intensive, a challenge that becomes more pronounced when dealing with diverse and complex images. In this work, we propose HistoSpeckle-Net, a deep learning architecture designed to reconstruct structurally rich medical images from MMF speckles. To build a clinically relevant dataset, we develop an optical setup that couples laser light through a spatial light modulator (SLM) into an MMF, capturing output speckle patterns corresponding to input OrganAMNIST images. Unlike previous MMF imaging approaches, which have not considered the underlying statistics of speckles and reconstructed images, we introduce a distribution-aware learning strategy. We employ a histogram-based mutual information loss to enhance model robustness and reduce reliance on large datasets. Our model includes a histogram computation unit that estimates smooth marginal and joint histograms for calculating mutual information loss. It also incorporates a unique Three-Scale Feature Refinement Module, which leads to multiscale Structural Similarity Index Measure (SSIM) loss computation. Together, these two loss functions enhance both the structural fidelity and statistical alignment of the reconstructed images. Our experiments on the complex OrganAMNIST dataset demonstrate that HistoSpeckle-Net achieves higher fidelity than baseline models such as U-Net and Pix2Pix. It gives superior performance even with limited training samples and across varying fiber bending conditions. By effectively reconstructing complex anatomical features with reduced data and under fiber perturbations, HistoSpeckle-Net brings MMF imaging closer to practical deployment in real-world clinical environments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uplifting Table Tennis: A Robust, Real-World Application for 3D Trajectory and Spin Estimation</title>
<link>https://arxiv.org/abs/2511.20250</link>
<guid>https://arxiv.org/abs/2511.20250</guid>
<content:encoded><![CDATA[
arXiv:2511.20250v1 Announce Type: new 
Abstract: Obtaining the precise 3D motion of a table tennis ball from standard monocular videos is a challenging problem, as existing methods trained on synthetic data struggle to generalize to the noisy, imperfect ball and table detections of the real world. This is primarily due to the inherent lack of 3D ground truth trajectories and spin annotations for real-world video. To overcome this, we propose a novel two-stage pipeline that divides the problem into a front-end perception task and a back-end 2D-to-3D uplifting task. This separation allows us to train the front-end components with abundant 2D supervision from our newly created TTHQ dataset, while the back-end uplifting network is trained exclusively on physically-correct synthetic data. We specifically re-engineer the uplifting model to be robust to common real-world artifacts, such as missing detections and varying frame rates. By integrating a ball detector and a table keypoint detector, our approach transforms a proof-of-concept uplifting method into a practical, robust, and high-performing end-to-end application for 3D table tennis trajectory and spin analysis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PromptMoG: Enhancing Diversity in Long-Prompt Image Generation via Prompt Embedding Mixture-of-Gaussian Sampling</title>
<link>https://arxiv.org/abs/2511.20251</link>
<guid>https://arxiv.org/abs/2511.20251</guid>
<content:encoded><![CDATA[
arXiv:2511.20251v1 Announce Type: new 
Abstract: Recent advances in text-to-image (T2I) generation have achieved remarkable visual outcomes through large-scale rectified flow models. However, how these models behave under long prompts remains underexplored. Long prompts encode rich content, spatial, and stylistic information that enhances fidelity but often suppresses diversity, leading to repetitive and less creative outputs. In this work, we systematically study this fidelity-diversity dilemma and reveal that state-of-the-art models exhibit a clear drop in diversity as prompt length increases. To enable consistent evaluation, we introduce LPD-Bench, a benchmark designed for assessing both fidelity and diversity in long-prompt generation. Building on our analysis, we develop a theoretical framework that increases sampling entropy through prompt reformulation and propose a training-free method, PromptMoG, which samples prompt embeddings from a Mixture-of-Gaussians in the embedding space to enhance diversity while preserving semantics. Extensive experiments on four state-of-the-art models, SD3.5-Large, Flux.1-Krea-Dev, CogView4, and Qwen-Image, demonstrate that PromptMoG consistently improves long-prompt generation diversity without semantic drifting.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zoo3D: Zero-Shot 3D Object Detection at Scene Level</title>
<link>https://arxiv.org/abs/2511.20253</link>
<guid>https://arxiv.org/abs/2511.20253</guid>
<content:encoded><![CDATA[
arXiv:2511.20253v1 Announce Type: new 
Abstract: 3D object detection is fundamental for spatial understanding. Real-world environments demand models capable of recognizing diverse, previously unseen objects, which remains a major limitation of closed-set methods. Existing open-vocabulary 3D detectors relax annotation requirements but still depend on training scenes, either as point clouds or images. We take this a step further by introducing Zoo3D, the first training-free 3D object detection framework. Our method constructs 3D bounding boxes via graph clustering of 2D instance masks, then assigns semantic labels using a novel open-vocabulary module with best-view selection and view-consensus mask generation. Zoo3D operates in two modes: the zero-shot Zoo3D$_0$, which requires no training at all, and the self-supervised Zoo3D$_1$, which refines 3D box prediction by training a class-agnostic detector on Zoo3D$_0$-generated pseudo labels. Furthermore, we extend Zoo3D beyond point clouds to work directly with posed and even unposed images. Across ScanNet200 and ARKitScenes benchmarks, both Zoo3D$_0$ and Zoo3D$_1$ achieve state-of-the-art results in open-vocabulary 3D object detection. Remarkably, our zero-shot Zoo3D$_0$ outperforms all existing self-supervised methods, hence demonstrating the power and adaptability of training-free, off-the-shelf approaches for real-world 3D understanding. Code is available at https://github.com/col14m/zoo3d .
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XiCAD: Camera Activation Detection in the Da Vinci Xi User Interface</title>
<link>https://arxiv.org/abs/2511.20254</link>
<guid>https://arxiv.org/abs/2511.20254</guid>
<content:encoded><![CDATA[
arXiv:2511.20254v1 Announce Type: new 
Abstract: Purpose: Robot-assisted minimally invasive surgery relies on endoscopic video as the sole intraoperative visual feedback. The DaVinci Xi system overlays a graphical user interface (UI) that indicates the state of each robotic arm, including the activation of the endoscope arm. Detecting this activation provides valuable metadata such as camera movement information, which can support downstream surgical data science tasks including tool tracking, skill assessment, or camera control automation.
  Methods: We developed a lightweight pipeline based on a ResNet18 convolutional neural network to automatically identify the position of the camera tile and its activation state within the DaVinci Xi UI. The model was fine-tuned on manually annotated data from the SurgToolLoc dataset and evaluated across three public datasets comprising over 70,000 frames.
  Results: The model achieved F1-scores between 0.993 and 1.000 for the binary detection of active cameras and correctly localized the camera tile in all cases without false multiple-camera detections.
  Conclusion: The proposed pipeline enables reliable, real-time extraction of camera activation metadata from surgical videos, facilitating automated preprocessing and analysis for diverse downstream applications. All code, trained models, and annotations are publicly available.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Image as Its Own Reward: Reinforcement Learning with Adversarial Reward for Image Generation</title>
<link>https://arxiv.org/abs/2511.20256</link>
<guid>https://arxiv.org/abs/2511.20256</guid>
<content:encoded><![CDATA[
arXiv:2511.20256v1 Announce Type: new 
Abstract: A reliable reward function is essential for reinforcement learning (RL) in image generation. Most current RL approaches depend on pre-trained preference models that output scalar rewards to approximate human preferences. However, these rewards often fail to capture human perception and are vulnerable to reward hacking, where higher scores do not correspond to better images. To address this, we introduce Adv-GRPO, an RL framework with an adversarial reward that iteratively updates both the reward model and the generator. The reward model is supervised using reference images as positive samples and can largely avoid being hacked. Unlike KL regularization that constrains parameter updates, our learned reward directly guides the generator through its visual outputs, leading to higher-quality images. Moreover, while optimizing existing reward functions can alleviate reward hacking, their inherent biases remain. For instance, PickScore may degrade image quality, whereas OCR-based rewards often reduce aesthetic fidelity. To address this, we take the image itself as a reward, using reference images and vision foundation models (e.g., DINO) to provide rich visual rewards. These dense visual signals, instead of a single scalar, lead to consistent gains across image quality, aesthetics, and task-specific metrics. Finally, we show that combining reference samples with foundation-model rewards enables distribution transfer and flexible style customization. In human evaluation, our method outperforms Flow-GRPO and SD3, achieving 70.0% and 72.4% win rates in image quality and aesthetics, respectively. Code and models have been released.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization</title>
<link>https://arxiv.org/abs/2511.20258</link>
<guid>https://arxiv.org/abs/2511.20258</guid>
<content:encoded><![CDATA[
arXiv:2511.20258v1 Announce Type: new 
Abstract: Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Image Classification with Discrete Diffusion Classification Modeling</title>
<link>https://arxiv.org/abs/2511.20263</link>
<guid>https://arxiv.org/abs/2511.20263</guid>
<content:encoded><![CDATA[
arXiv:2511.20263v1 Announce Type: new 
Abstract: Image classification is a well-studied task in computer vision, and yet it remains challenging under high-uncertainty conditions, such as when input images are corrupted or training data are limited. Conventional classification approaches typically train models to directly predict class labels from input images, but this might lead to suboptimal performance in such scenarios. To address this issue, we propose Discrete Diffusion Classification Modeling (DiDiCM), a novel framework that leverages a diffusion-based procedure to model the posterior distribution of class labels conditioned on the input image. DiDiCM supports diffusion-based predictions either on class probabilities or on discrete class labels, providing flexibility in computation and memory trade-offs. We conduct a comprehensive empirical study demonstrating the superior performance of DiDiCM over standard classifiers, showing that a few diffusion iterations achieve higher classification accuracy on the ImageNet dataset compared to baselines, with accuracy gains increasing as the task becomes more challenging. We release our code at https://github.com/omerb01/didicm .
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRL-Guided Neural Batch Sampling for Semi-Supervised Pixel-Level Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.20270</link>
<guid>https://arxiv.org/abs/2511.20270</guid>
<content:encoded><![CDATA[
arXiv:2511.20270v1 Announce Type: new 
Abstract: Anomaly detection in industrial visual inspection is challenging due to the scarcity of defective samples. Most existing methods rely on unsupervised reconstruction using only normal data, often resulting in overfitting and poor detection of subtle defects. We propose a semi-supervised deep reinforcement learning framework that integrates a neural batch sampler, an autoencoder, and a predictor. The RL-based sampler adaptively selects informative patches by balancing exploration and exploitation through a composite reward. The autoencoder generates loss profiles highlighting abnormal regions, while the predictor performs segmentation in the loss-profile space. This interaction enables the system to effectively learn both normal and defective patterns with limited labeled data. Experiments on the MVTec AD dataset demonstrate that our method achieves higher accuracy and better localization of subtle anomalies than recent state-of-the-art approaches while maintaining low complexity, yielding an average improvement of 0.15 in F1_max and 0.06 in AUC, with a maximum gain of 0.37 in F1_max in the best case.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.20272</link>
<guid>https://arxiv.org/abs/2511.20272</guid>
<content:encoded><![CDATA[
arXiv:2511.20272v1 Announce Type: new 
Abstract: While Multimodal Large Language Models (MLLMs) have become adept at recognizing objects, they often lack the intuitive, human-like understanding of the world's underlying physical and social principles. This high-level vision-grounded semantics, which we term visual knowledge, forms a bridge between perception and reasoning, yet remains an underexplored area in current MLLMs. To systematically evaluate this capability, we present VKnowU, a comprehensive benchmark featuring 1,680 questions in 1,249 videos, covering 8 core types of visual knowledge spanning both world-centric (e.g., intuitive physics) and human-centric (e.g., subjective intentions). Evaluation of 23 SOTA MLLMs reveals that leading models still fall short of human performance, with particularly notable gaps in the world-centric. To bridge this gap, we introduce a new dataset, VKnowQA, and VideoKnow+, a baseline model that explicitly incorporates visual knowledge into MLLMs. VideoKnow+ follows a structured See-Think-Answer paradigm and adopts reinforcement learning with visual knowledge reward, achieving a +3.7% improvement on VKnowU and consistent gains on MVBench, Video-MME, and MMVU. Our work highlights visual knowledge as a missing cornerstone for developing more generalizable MLLMs that can not only see but also truly understand our physical and social worlds.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis</title>
<link>https://arxiv.org/abs/2511.20274</link>
<guid>https://arxiv.org/abs/2511.20274</guid>
<content:encoded><![CDATA[
arXiv:2511.20274v1 Announce Type: new 
Abstract: Until recently, the general corpus of CLIP-type fundamental models has widely explored either the retrieval of short descriptions or the classification of objects in the scene as SINGLE-object image classification task. The same holds for retrieving the image embedding (image retrieval task) given a text prompt. However, real-world scene images exhibit rich compositional structure involving multiple objects and actions. The latest methods in the CLIP-based literature improve class-level discrimination by mining harder negative image-text pairs and by refining permanent text prompts, often using LLMs. However, these improvements remain confined to predefined class lists and do not explicitly model relational or compositional structure. PyramidCLIP partially addresses this gap by aligning global and local visual features, yet it still lacks explicit modeling of inter-object relations. Hence, to further leverage this aspect for scene analysis, the proposed ScenarioCLIP model accepts input texts, grounded relations, and input images, along with focused regions highlighting relations. The proposed model is pretrained on curated scenario data, and finetuned for specialized downstream tasks, such as cross-modal retrieval and fine-grained visual understanding tasks. To address the lack of domain-specific datasets, we generate a novel dataset by extending image-text pairs from existing diverse indoor and outdoor scenario datasets that are publicly available. We used a pipeline of existing language models to ground action, object, and relations, filled by manual and automatic curation. We established a comprehensive benchmark for several scenario-based tasks and compared it with many baseline methods. ScenarioCLIP demonstrates robust zero-shot and finetune performance on various domain-specific tasks. Our code and dataset are available at https://github.com/scenario-clip/ScenarioCLIP
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAPointMamba: Domain Adaptive Point Mamba for Point Cloud Completion</title>
<link>https://arxiv.org/abs/2511.20278</link>
<guid>https://arxiv.org/abs/2511.20278</guid>
<content:encoded><![CDATA[
arXiv:2511.20278v1 Announce Type: new 
Abstract: Domain adaptive point cloud completion (DA PCC) aims to narrow the geometric and semantic discrepancies between the labeled source and unlabeled target domains. Existing methods either suffer from limited receptive fields or quadratic complexity due to using CNNs or vision Transformers. In this paper, we present the first work that studies the adaptability of State Space Models (SSMs) in DA PCC and find that directly applying SSMs to DA PCC will encounter several challenges: directly serializing 3D point clouds into 1D sequences often disrupts the spatial topology and local geometric features of the target domain. Besides, the overlook of designs in the learning domain-agnostic representations hinders the adaptation performance. To address these issues, we propose a novel framework, DAPointMamba for DA PCC, that exhibits strong adaptability across domains and has the advantages of global receptive fields and efficient linear complexity. It has three novel modules. In particular, Cross-Domain Patch-Level Scanning introduces patch-level geometric correspondences, enabling effective local alignment. Cross-Domain Spatial SSM Alignment further strengthens spatial consistency by modulating patch features based on cross-domain similarity, effectively mitigating fine-grained structural discrepancies. Cross-Domain Channel SSM Alignment actively addresses global semantic gaps by interleaving and aligning feature channels. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our DAPointMamba outperforms state-of-the-art methods with less computational complexity and inference latency.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SelfMOTR: Revisiting MOTR with Self-Generating Detection Priors</title>
<link>https://arxiv.org/abs/2511.20279</link>
<guid>https://arxiv.org/abs/2511.20279</guid>
<content:encoded><![CDATA[
arXiv:2511.20279v1 Announce Type: new 
Abstract: Despite progress toward end-to-end tracking with transformer architectures, poor detection performance and the conflict between detection and association in a joint architecture remain critical concerns. Recent approaches aim to mitigate these issues by (i) employing advanced denoising or label assignment strategies, or (ii) incorporating detection priors from external object detectors via distillation or anchor proposal techniques. Inspired by the success of integrating detection priors and by the key insight that MOTR-like models are secretly strong detection models, we introduce SelfMOTR, a novel tracking transformer that relies on self-generated detection priors. Through extensive analysis and ablation studies, we uncover and demonstrate the hidden detection capabilities of MOTR-like models, and present a practical set of tools for leveraging them effectively. On DanceTrack, SelfMOTR achieves strong performance, competing with recent state-of-the-art end-to-end tracking methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement</title>
<link>https://arxiv.org/abs/2511.20280</link>
<guid>https://arxiv.org/abs/2511.20280</guid>
<content:encoded><![CDATA[
arXiv:2511.20280v1 Announce Type: new 
Abstract: Recent progress in video generation has led to impressive visual quality, yet current models still struggle to produce results that align with real-world physical principles. To this end, we propose an iterative self-refinement framework that leverages large language models and vision-language models to provide physics-aware guidance for video generation. Specifically, we introduce a multimodal chain-of-thought (MM-CoT) process that refines prompts based on feedback from physical inconsistencies, progressively enhancing generation quality. This method is training-free and plug-and-play, making it readily applicable to a wide range of video generation models. Experiments on the PhyIQ benchmark show that our method improves the Physics-IQ score from 56.31 to 62.38. We hope this work serves as a preliminary exploration of physics-consistent video generation and may offer insights for future research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to the Feature: Explaining Video Classifiers with Video Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2511.20295</link>
<guid>https://arxiv.org/abs/2511.20295</guid>
<content:encoded><![CDATA[
arXiv:2511.20295v1 Announce Type: new 
Abstract: Counterfactual explanations (CFEs) are minimal and semantically meaningful modifications of the input of a model that alter the model predictions. They highlight the decisive features the model relies on, providing contrastive interpretations for classifiers. State-of-the-art visual counterfactual explanation methods are designed to explain image classifiers. The generation of CFEs for video classifiers remains largely underexplored. For the counterfactual videos to be useful, they have to be physically plausible, temporally coherent, and exhibit smooth motion trajectories. Existing CFE image-based methods, designed to explain image classifiers, lack the capacity to generate temporally coherent, smooth and physically plausible video CFEs. To address this, we propose Back To The Feature (BTTF), an optimization framework that generates video CFEs. Our method introduces two novel features, 1) an optimization scheme to retrieve the initial latent noise conditioned by the first frame of the input video, 2) a two-stage optimization strategy to enable the search for counterfactual videos in the vicinity of the input video. Both optimization processes are guided solely by the target classifier, ensuring the explanation is faithful. To accelerate convergence, we also introduce a progressive optimization strategy that incrementally increases the number of denoising steps. Extensive experiments on video datasets such as Shape-Moving (motion classification), MEAD (emotion classification), and NTU RGB+D (action classification) show that our BTTF effectively generates valid, visually similar and realistic counterfactual videos that provide concrete insights into the classifier's decision-making mechanism.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting Lipschitz-constrained network for multiple-in-one sparse-view CT reconstruction</title>
<link>https://arxiv.org/abs/2511.20296</link>
<guid>https://arxiv.org/abs/2511.20296</guid>
<content:encoded><![CDATA[
arXiv:2511.20296v1 Announce Type: new 
Abstract: Despite significant advancements in deep learning-based sparse-view computed tomography (SVCT) reconstruction algorithms, these methods still encounter two primary limitations: (i) It is challenging to explicitly prove that the prior networks of deep unfolding algorithms satisfy Lipschitz constraints due to their empirically designed nature. (ii) The substantial storage costs of training a separate model for each setting in the case of multiple views hinder practical clinical applications. To address these issues, we elaborate an explicitly provable Lipschitz-constrained network, dubbed LipNet, and integrate an explicit prompt module to provide discriminative knowledge of different sparse sampling settings, enabling the treatment of multiple sparse view configurations within a single model. Furthermore, we develop a storage-saving deep unfolding framework for multiple-in-one SVCT reconstruction, termed PromptCT, which embeds LipNet as its prior network to ensure the convergence of its corresponding iterative algorithm. In simulated and real data experiments, PromptCT outperforms benchmark reconstruction algorithms in multiple-in-one SVCT reconstruction, achieving higher-quality reconstructions with lower storage costs. On the theoretical side, we explicitly demonstrate that LipNet satisfies boundary property, further proving its Lipschitz continuity and subsequently analyzing the convergence of the proposed iterative algorithms. The data and code are publicly available at https://github.com/shibaoshun/PromptCT.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrossEarth-Gate: Fisher-Guided Adaptive Tuning Engine for Efficient Adaptation of Cross-Domain Remote Sensing Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.20302</link>
<guid>https://arxiv.org/abs/2511.20302</guid>
<content:encoded><![CDATA[
arXiv:2511.20302v1 Announce Type: new 
Abstract: In Remote Sensing (RS), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key approach to activate the generalizable representation ability of foundation models for downstream tasks. However, existing specialized PEFT methods often fail when applied to large-scale Earth observation tasks, as they are unable to fully handle the multifaceted and unpredictable domain gaps (\eg, spatial, semantic, and frequency shifts) inherent in RS data. To overcome this, we propose CrossEarth-Gate, which introduces two primary contributions. First, we establish a comprehensive RS module toolbox to address multifaceted domain gaps, comprising spatial, semantic, and frequency modules. Second, we develop a Fisher-guided adaptive selection mechanism that operates on this toolbox. This selection is guided by Fisher Information to quantify each module's importance by measuring its contribution to the task-specific gradient flow. It dynamically activates only the most critical modules at the appropriate layers, guiding the gradient flow to maximize adaptation effectiveness and efficiency. Comprehensive experiments validate the efficacy and generalizability of our method, where CrossEarth-Gate achieves state-of-the-art performance across 16 cross-domain benchmarks for RS semantic segmentation. The code of the work will be released.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TaCo: Capturing Spatio-Temporal Semantic Consistency in Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2511.20306</link>
<guid>https://arxiv.org/abs/2511.20306</guid>
<content:encoded><![CDATA[
arXiv:2511.20306v1 Announce Type: new 
Abstract: Remote sensing change detection (RSCD) aims to identify surface changes across bi-temporal satellite images. Most previous methods rely solely on mask supervision, which effectively guides spatial localization but provides limited constraints on the temporal semantic transitions. Consequently, they often produce spatially coherent predictions while still suffering from unresolved semantic inconsistencies. To address this limitation, we propose TaCo, a spatio-temporal semantic consistent network, which enriches the existing mask-supervised framework with a spatio-temporal semantic joint constraint. TaCo conceptualizes change as a semantic transition between bi-temporal states, in which one temporal feature representation can be derived from the other via dedicated transition features. To realize this, we introduce a Text-guided Transition Generator that integrates textual semantics with bi-temporal visual features to construct the cross-temporal transition features. In addition, we propose a spatio-temporal semantic joint constraint consisting of bi-temporal reconstruct constraints and a transition constraint: the former enforces alignment between reconstructed and original features, while the latter enhances discrimination for changes. This design can yield substantial performance gains without introducing any additional computational overhead during inference. Extensive experiments on six public datasets, spanning both binary and semantic change detection tasks, demonstrate that TaCo consistently achieves SOTA performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TReFT: Taming Rectified Flow Models For One-Step Image Translation</title>
<link>https://arxiv.org/abs/2511.20307</link>
<guid>https://arxiv.org/abs/2511.20307</guid>
<content:encoded><![CDATA[
arXiv:2511.20307v1 Announce Type: new 
Abstract: Rectified Flow (RF) models have advanced high-quality image and video synthesis via optimal transport theory. However, when applied to image-to-image translation, they still depend on costly multi-step denoising, hindering real-time applications. Although the recent adversarial training paradigm, CycleGAN-Turbo, works in pretrained diffusion models for one-step image translation, we find that directly applying it to RF models leads to severe convergence issues. In this paper, we analyze these challenges and propose TReFT, a novel method to Tame Rectified Flow models for one-step image Translation. Unlike previous works, TReFT directly uses the velocity predicted by pretrained DiT or UNet as output-a simple yet effective design that tackles the convergence issues under adversarial training with one-step inference. This design is mainly motivated by a novel observation that, near the end of the denoising process, the velocity predicted by pretrained RF models converges to the vector from origin to the final clean image, a property we further justify through theoretical analysis. When applying TReFT to large pretrained RF models such as SD3.5 and FLUX, we introduce memory-efficient latent cycle-consistency and identity losses during training, as well as lightweight architectural simplifications for faster inference. Pretrained RF models finetuned with TReFT achieve performance comparable to sota methods across multiple image translation datasets while enabling real-time inference.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IrisNet: Infrared Image Status Awareness Meta Decoder for Infrared Small Targets Detection</title>
<link>https://arxiv.org/abs/2511.20319</link>
<guid>https://arxiv.org/abs/2511.20319</guid>
<content:encoded><![CDATA[
arXiv:2511.20319v1 Announce Type: new 
Abstract: Infrared Small Target Detection (IRSTD) faces significant challenges due to low signal-to-noise ratios, complex backgrounds, and the absence of discernible target features. While deep learning-based encoder-decoder frameworks have advanced the field, their static pattern learning suffers from pattern drift across diverse scenarios (\emph{e.g.}, day/night variations, sky/maritime/ground domains), limiting robustness. To address this, we propose IrisNet, a novel meta-learned framework that dynamically adapts detection strategies to the input infrared image status. Our approach establishes a dynamic mapping between infrared image features and entire decoder parameters via an image-to-decoder transformer. More concretely, we represent the parameterized decoder as a structured 2D tensor preserving hierarchical layer correlations and enable the transformer to model inter-layer dependencies through self-attention while generating adaptive decoding patterns via cross-attention. To further enhance the perception ability of infrared images, we integrate high-frequency components to supplement target-position and scene-edge information. Experiments on NUDT-SIRST, NUAA-SIRST, and IRSTD-1K datasets demonstrate the superiority of our IrisNet, achieving state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AD-R1: Closed-Loop Reinforcement Learning for End-to-End Autonomous Driving with Impartial World Models</title>
<link>https://arxiv.org/abs/2511.20325</link>
<guid>https://arxiv.org/abs/2511.20325</guid>
<content:encoded><![CDATA[
arXiv:2511.20325v1 Announce Type: new 
Abstract: End-to-end models for autonomous driving hold the promise of learning complex behaviors directly from sensor data, but face critical challenges in safety and handling long-tail events. Reinforcement Learning (RL) offers a promising path to overcome these limitations, yet its success in autonomous driving has been elusive. We identify a fundamental flaw hindering this progress: a deep seated optimistic bias in the world models used for RL. To address this, we introduce a framework for post-training policy refinement built around an Impartial World Model. Our primary contribution is to teach this model to be honest about danger. We achieve this with a novel data synthesis pipeline, Counterfactual Synthesis, which systematically generates a rich curriculum of plausible collisions and off-road events. This transforms the model from a passive scene completer into a veridical forecaster that remains faithful to the causal link between actions and outcomes. We then integrate this Impartial World Model into our closed-loop RL framework, where it serves as an internal critic. During refinement, the agent queries the critic to ``dream" of the outcomes for candidate actions. We demonstrate through extensive experiments, including on a new Risk Foreseeing Benchmark, that our model significantly outperforms baselines in predicting failures. Consequently, when used as a critic, it enables a substantial reduction in safety violations in challenging simulations, proving that teaching a model to dream of danger is a critical step towards building truly safe and intelligent autonomous agents.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Motion Perception of Binocular Vision Target with PID-CNN</title>
<link>https://arxiv.org/abs/2511.20332</link>
<guid>https://arxiv.org/abs/2511.20332</guid>
<content:encoded><![CDATA[
arXiv:2511.20332v1 Announce Type: new 
Abstract: This article trained a network for perceiving three-dimensional motion information of binocular vision target, which can provide real-time three-dimensional coordinate, velocity, and acceleration, and has a basic spatiotemporal perception capability. Understood the ability of neural networks to fit nonlinear problems from the perspective of PID. Considered a single-layer neural network as using a second-order difference equation and a nonlinearity to describe a local problem. Multilayer networks gradually transform the raw representation to the desired representation through multiple such combinations. Analysed some reference principles for designing neural networks. Designed a relatively small PID convolutional neural network, with a total of 17 layers and 413 thousand parameters. Implemented a simple but practical feature reuse method by concatenation and pooling. The network was trained and tested using the simulated randomly moving ball datasets, and the experimental results showed that the prediction accuracy was close to the upper limit that the input image resolution can represent. Analysed the experimental results and errors, as well as the existing shortcomings and possible directions for improvement. Finally, discussed the advantages of high-dimensional convolution in improving computational efficiency and feature space utilization. As well as the potential advantages of using PID information to implement memory and attention mechanisms.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShelfRectNet: Single View Shelf Image Rectification with Homography Estimation</title>
<link>https://arxiv.org/abs/2511.20335</link>
<guid>https://arxiv.org/abs/2511.20335</guid>
<content:encoded><![CDATA[
arXiv:2511.20335v1 Announce Type: new 
Abstract: Estimating homography from a single image remains a challenging yet practically valuable task, particularly in domains like retail, where only one viewpoint is typically available for shelf monitoring and product alignment. In this paper, we present a deep learning framework that predicts a 4-point parameterized homography matrix to rectify shelf images captured from arbitrary angles. Our model leverages a ConvNeXt-based backbone for enhanced feature representation and adopts normalized coordinate regression for improved stability. To address data scarcity and promote generalization, we introduce a novel augmentation strategy by modeling and sampling synthetic homographies. Our method achieves a mean corner error of 1.298 pixels on the test set. When compared with both classical computer vision and deep learning-based approaches, our method demonstrates competitive performance in both accuracy and inference speed. Together, these results establish our approach as a robust and efficient solution for realworld single-view rectification. To encourage further research in this domain, we will make our dataset, ShelfRectSet, and code publicly available
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMB3R: Accurate Feed-forward Metric-scale 3D Reconstruction with Backend</title>
<link>https://arxiv.org/abs/2511.20343</link>
<guid>https://arxiv.org/abs/2511.20343</guid>
<content:encoded><![CDATA[
arXiv:2511.20343v1 Announce Type: new 
Abstract: We present AMB3R, a multi-view feed-forward model for dense 3D reconstruction on a metric-scale that addresses diverse 3D vision tasks. The key idea is to leverage a sparse, yet compact, volumetric scene representation as our backend, enabling geometric reasoning with spatial compactness. Although trained solely for multi-view reconstruction, we demonstrate that AMB3R can be seamlessly extended to uncalibrated visual odometry (online) or large-scale structure from motion without the need for task-specific fine-tuning or test-time optimization. Compared to prior pointmap-based models, our approach achieves state-of-the-art performance in camera pose, depth, and metric-scale estimation, 3D reconstruction, and even surpasses optimization-based SLAM and SfM methods with dense reconstruction priors on common benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin</title>
<link>https://arxiv.org/abs/2511.20348</link>
<guid>https://arxiv.org/abs/2511.20348</guid>
<content:encoded><![CDATA[
arXiv:2511.20348v1 Announce Type: new 
Abstract: 3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thinking in 360{\deg}: Humanoid Visual Search in the Wild</title>
<link>https://arxiv.org/abs/2511.20351</link>
<guid>https://arxiv.org/abs/2511.20351</guid>
<content:encoded><![CDATA[
arXiv:2511.20351v1 Announce Type: new 
Abstract: Humans rely on the synergistic control of head (cephalomotor) and eye (oculomotor) to efficiently search for visual information in 360{\deg}. However, prior approaches to visual search are limited to a static image, neglecting the physical embodiment and its interaction with the 3D world. How can we develop embodied visual search agents as efficient as humans while bypassing the constraints imposed by real-world hardware? To this end, we propose humanoid visual search where a humanoid agent actively rotates its head to search for objects or paths in an immersive world represented by a 360{\deg} panoramic image. To study visual search in visually-crowded real-world scenarios, we build H* Bench, a new benchmark that moves beyond household scenes to challenging in-the-wild scenes that necessitate advanced visual-spatial reasoning capabilities, such as transportation hubs, large-scale retail spaces, urban streets, and public institutions. Our experiments first reveal that even top-tier proprietary models falter, achieving only ~30% success in object and path search. We then use post-training techniques to enhance the open-source Qwen2.5-VL, increasing its success rate by over threefold for both object search (14.83% to 47.38%) and path search (6.44% to 24.94%). Notably, the lower ceiling of path search reveals its inherent difficulty, which we attribute to the demand for sophisticated spatial commonsense. Our results not only show a promising path forward but also quantify the immense challenge that remains in building MLLM agents that can be seamlessly integrated into everyday human life.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GS-Checker: Tampering Localization for 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2511.20354</link>
<guid>https://arxiv.org/abs/2511.20354</guid>
<content:encoded><![CDATA[
arXiv:2511.20354v1 Announce Type: new 
Abstract: Recent advances in editing technologies for 3D Gaussian Splatting (3DGS) have made it simple to manipulate 3D scenes. However, these technologies raise concerns about potential malicious manipulation of 3D content. To avoid such malicious applications, localizing tampered regions becomes crucial. In this paper, we propose GS-Checker, a novel method for locating tampered areas in 3DGS models. Our approach integrates a 3D tampering attribute into the 3D Gaussian parameters to indicate whether the Gaussian has been tampered. Additionally, we design a 3D contrastive mechanism by comparing the similarity of key attributes between 3D Gaussians to seek tampering cues at 3D level. Furthermore, we introduce a cyclic optimization strategy to refine the 3D tampering attribute, enabling more accurate tampering localization. Notably, our approach does not require expensive 3D labels for supervision. Extensive experimental results demonstrate the effectiveness of our proposed method to locate the tampered 3DGS area.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Passive Perception to Active Memory: A Weakly Supervised Image Manipulation Localization Framework Driven by Coarse-Grained Annotations</title>
<link>https://arxiv.org/abs/2511.20359</link>
<guid>https://arxiv.org/abs/2511.20359</guid>
<content:encoded><![CDATA[
arXiv:2511.20359v1 Announce Type: new 
Abstract: Image manipulation localization (IML) faces a fundamental trade-off between minimizing annotation cost and achieving fine-grained localization accuracy. Existing fully-supervised IML methods depend heavily on dense pixel-level mask annotations, which limits scalability to large datasets or real-world deployment.In contrast, the majority of existing weakly-supervised IML approaches are based on image-level labels, which greatly reduce annotation effort but typically lack precise spatial localization. To address this dilemma, we propose BoxPromptIML, a novel weakly-supervised IML framework that effectively balances annotation cost and localization performance. Specifically, we propose a coarse region annotation strategy, which can generate relatively accurate manipulation masks at lower cost. To improve model efficiency and facilitate deployment, we further design an efficient lightweight student model, which learns to perform fine-grained localization through knowledge distillation from a fixed teacher model based on the Segment Anything Model (SAM). Moreover, inspired by the human subconscious memory mechanism, our feature fusion module employs a dual-guidance strategy that actively contextualizes recalled prototypical patterns with real-time observational cues derived from the input. Instead of passive feature extraction, this strategy enables a dynamic process of knowledge recollection, where long-term memory is adapted to the specific context of the current image, significantly enhancing localization accuracy and robustness. Extensive experiments across both in-distribution and out-of-distribution datasets show that BoxPromptIML outperforms or rivals fully-supervised models, while maintaining strong generalization, low annotation cost, and efficient deployment characteristics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VGGTFace: Topologically Consistent Facial Geometry Reconstruction in the Wild</title>
<link>https://arxiv.org/abs/2511.20366</link>
<guid>https://arxiv.org/abs/2511.20366</guid>
<content:encoded><![CDATA[
arXiv:2511.20366v1 Announce Type: new 
Abstract: Reconstructing topologically consistent facial geometry is crucial for the digital avatar creation pipelines. Existing methods either require tedious manual efforts, lack generalization to in-the-wild data, or are constrained by the limited expressiveness of 3D Morphable Models. To address these limitations, we propose VGGTFace, an automatic approach that innovatively applies the 3D foundation model, \emph{i.e.} VGGT, for topologically consistent facial geometry reconstruction from in-the-wild multi-view images captured by everyday users. Our key insight is that, by leveraging VGGT, our method naturally inherits strong generalization ability and expressive power from its large-scale training and point map representation. However, it is unclear how to reconstruct a topologically consistent mesh from VGGT, as the topology information is missing in its prediction. To this end, we augment VGGT with Pixel3DMM for injecting topology information via pixel-aligned UV values. In this manner, we convert the pixel-aligned point map of VGGT to a point cloud with topology. Tailored to this point cloud with known topology, we propose a novel Topology-Aware Bundle Adjustment strategy to fuse them, where we construct a Laplacian energy for the Bundle Adjustment objective. Our method achieves high-quality reconstruction in 10 seconds for 16 views on a single NVIDIA RTX 4090. Experiments demonstrate state-of-the-art results on benchmarks and impressive generalization to in-the-wild data. Code is available at https://github.com/grignarder/vggtface.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FREE: Uncertainty-Aware Autoregression for Parallel Diffusion Transformers</title>
<link>https://arxiv.org/abs/2511.20390</link>
<guid>https://arxiv.org/abs/2511.20390</guid>
<content:encoded><![CDATA[
arXiv:2511.20390v1 Announce Type: new 
Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art generation quality but require long sequential denoising trajectories, leading to high inference latency. Recent speculative inference methods enable lossless parallel sampling in U-Net-based diffusion models via a drafter-verifier scheme, but their acceleration is limited on DiTs due to insufficient draft accuracy during verification. To address this limitation, we analyze the DiTs' feature dynamics and find the features of the final transformer layer (top-block) exhibit strong temporal consistency and rich semantic abstraction. Based on this insight, we propose FREE, a novel framework that employs a lightweight drafter to perform feature-level autoregression with parallel verification, guaranteeing lossless acceleration with theoretical and empirical support. Meanwhile, prediction variance (uncertainty) of DiTs naturally increases in later denoising steps, reducing acceptance rates under speculative sampling. To mitigate this effect, we further introduce an uncertainty-guided relaxation strategy, forming FREE (relax), which dynamically adjusts the acceptance probability in response to uncertainty levels. Experiments on ImageNet-$512^2$ show that FREE achieves up to $1.86 \times$ acceleration, and FREE (relax) further reaches $2.25 \times$ speedup while maintaining high perceptual and quantitative fidelity in generation quality.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Training-Free Approach for Multi-ID Customization via Attention Adjustment and Spatial Control</title>
<link>https://arxiv.org/abs/2511.20401</link>
<guid>https://arxiv.org/abs/2511.20401</guid>
<content:encoded><![CDATA[
arXiv:2511.20401v1 Announce Type: new 
Abstract: Multi-ID customization is an interesting topic in computer vision and attracts considerable attention recently. Given the ID images of multiple individuals, its purpose is to generate a customized image that seamlessly integrates them while preserving their respective identities. Compared to single-ID customization, multi-ID customization is much more difficult and poses two major challenges. First, since the multi-ID customization model is trained to reconstruct an image from the cropped person regions, it often encounters the copy-paste issue during inference, leading to lower quality. Second, the model also suffers from inferior text controllability. The generated result simply combines multiple persons into one image, regardless of whether it is aligned with the input text. In this work, we propose MultiID to tackle this challenging task in a training-free manner. Since the existing single-ID customization models have less copy-paste issue, our key idea is to adapt these models to achieve multi-ID customization. To this end, we present an ID-decoupled cross-attention mechanism, injecting distinct ID embeddings into the corresponding image regions and thus generating multi-ID outputs. To enhance the generation controllability, we introduce three critical strategies, namely the local prompt, depth-guided spatial control, and extended self-attention, making the results more consistent with the text prompts and ID images. We also carefully build a benchmark, called IDBench, for evaluation. The extensive qualitative and quantitative results demonstrate the effectiveness of MultiID in solving the aforementioned two challenges. Its performance is comparable or even better than the training-based multi-ID customization methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-Free Timestep Distillation via Continuous-Time Consistency with Trajectory-Sampled Pairs</title>
<link>https://arxiv.org/abs/2511.20410</link>
<guid>https://arxiv.org/abs/2511.20410</guid>
<content:encoded><![CDATA[
arXiv:2511.20410v1 Announce Type: new 
Abstract: Timestep distillation is an effective approach for improving the generation efficiency of diffusion models. The Consistency Model (CM), as a trajectory-based framework, demonstrates significant potential due to its strong theoretical foundation and high-quality few-step generation. Nevertheless, current continuous-time consistency distillation methods still rely heavily on training data and computational resources, hindering their deployment in resource-constrained scenarios and limiting their scalability to diverse domains. To address this issue, we propose Trajectory-Backward Consistency Model (TBCM), which eliminates the dependence on external training data by extracting latent representations directly from the teacher model's generation trajectory. Unlike conventional methods that require VAE encoding and large-scale datasets, our self-contained distillation paradigm significantly improves both efficiency and simplicity. Moreover, the trajectory-extracted samples naturally bridge the distribution gap between training and inference, thereby enabling more effective knowledge transfer. Empirically, TBCM achieves 6.52 FID and 28.08 CLIP scores on MJHQ-30k under one-step generation, while reducing training time by approximately 40% compared to Sana-Sprint and saving a substantial amount of GPU memory, demonstrating superior efficiency without sacrificing quality. We further reveal the diffusion-generation space discrepancy in continuous-time consistency distillation and analyze how sampling strategies affect distillation performance, offering insights for future distillation research. GitHub Link: https://github.com/hustvl/TBCM.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MajutsuCity: Language-driven Aesthetic-adaptive City Generation with Controllable 3D Assets and Layouts</title>
<link>https://arxiv.org/abs/2511.20415</link>
<guid>https://arxiv.org/abs/2511.20415</guid>
<content:encoded><![CDATA[
arXiv:2511.20415v1 Announce Type: new 
Abstract: Generating realistic 3D cities is fundamental to world models, virtual reality, and game development, where an ideal urban scene must satisfy both stylistic diversity, fine-grained, and controllability. However, existing methods struggle to balance the creative flexibility offered by text-based generation with the object-level editability enabled by explicit structural representations. We introduce MajutsuCity, a natural language-driven and aesthetically adaptive framework for synthesizing structurally consistent and stylistically diverse 3D urban scenes. MajutsuCity represents a city as a composition of controllable layouts, assets, and materials, and operates through a four-stage pipeline. To extend controllability beyond initial generation, we further integrate MajutsuAgent, an interactive language-grounded editing agent} that supports five object-level operations. To support photorealistic and customizable scene synthesis, we also construct MajutsuDataset, a high-quality multimodal dataset} containing 2D semantic layouts and height maps, diverse 3D building assets, and curated PBR materials and skyboxes, each accompanied by detailed annotations. Meanwhile, we develop a practical set of evaluation metrics, covering key dimensions such as structural consistency, scene complexity, material fidelity, and lighting atmosphere. Extensive experiments demonstrate MajutsuCity reduces layout FID by 83.7% compared with CityDreamer and by 20.1% over CityCraft. Our method ranks first across all AQS and RDR scores, outperforming existing methods by a clear margin. These results confirm MajutsuCity as a new state-of-the-art in geometric fidelity, stylistic adaptability, and semantic controllability for 3D city generation. We expect our framework can inspire new avenues of research in 3D city generation. Our dataset and code will be released at https://github.com/LongHZ140516/MajutsuCity.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StableTrack: Stabilizing Multi-Object Tracking on Low-Frequency Detections</title>
<link>https://arxiv.org/abs/2511.20418</link>
<guid>https://arxiv.org/abs/2511.20418</guid>
<content:encoded><![CDATA[
arXiv:2511.20418v1 Announce Type: new 
Abstract: Multi-object tracking (MOT) is one of the most challenging tasks in computer vision, where it is important to correctly detect objects and associate these detections across frames. Current approaches mainly focus on tracking objects in each frame of a video stream, making it almost impossible to run the model under conditions of limited computing resources. To address this issue, we propose StableTrack, a novel approach that stabilizes the quality of tracking on low-frequency detections. Our method introduces a new two-stage matching strategy to improve the cross-frame association between low-frequency detections. We propose a novel Bbox-Based Distance instead of the conventional Mahalanobis distance, which allows us to effectively match objects using the Re-ID model. Furthermore, we integrate visual tracking into the Kalman Filter and the overall tracking pipeline. Our method outperforms current state-of-the-art trackers in the case of low-frequency detections, achieving $\textit{11.6%}$ HOTA improvement at $\textit{1}$ Hz on MOT17-val, while keeping up with the best approaches on the standard MOT17, MOT20, and DanceTrack benchmarks with full-frequency detections.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block Cascading: Training Free Acceleration of Block-Causal Video Models</title>
<link>https://arxiv.org/abs/2511.20426</link>
<guid>https://arxiv.org/abs/2511.20426</guid>
<content:encoded><![CDATA[
arXiv:2511.20426v1 Announce Type: new 
Abstract: Block-causal video generation faces a stark speed-quality trade-off: small 1.3B models manage only 16 FPS while large 14B models crawl at 4.5 FPS, forcing users to choose between responsiveness and quality. Block Cascading significantly mitigates this trade-off through training-free parallelization. Our key insight: future video blocks do not need fully denoised current blocks to begin generation. By starting block generation with partially denoised context from predecessors, we transform sequential pipelines into parallel cascades where multiple blocks denoise simultaneously. With 5 GPUs exploiting temporal parallelism, we achieve ~2x acceleration across all model scales: 1.3B models accelerate from 16 to 30 FPS, 14B models from 4.5 to 12.5 FPS. Beyond inference speed, Block Cascading eliminates overhead from KV-recaching (of ~200ms) during context switches for interactive generation. Extensive evaluations validated against multiple block-causal pipelines demonstrate no significant loss in generation quality when switching from block-causal to Block Cascading pipelines for inference. Project Page: https://hmrishavbandy.github.io/block_cascading_page/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BRIC: Bridging Kinematic Plans and Physical Control at Test Time</title>
<link>https://arxiv.org/abs/2511.20431</link>
<guid>https://arxiv.org/abs/2511.20431</guid>
<content:encoded><![CDATA[
arXiv:2511.20431v1 Announce Type: new 
Abstract: We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Object-Centric Vision Token Pruning for Vision Language Models</title>
<link>https://arxiv.org/abs/2511.20439</link>
<guid>https://arxiv.org/abs/2511.20439</guid>
<content:encoded><![CDATA[
arXiv:2511.20439v1 Announce Type: new 
Abstract: In Vision Language Models (VLMs), vision tokens are quantity-heavy yet information-dispersed compared with language tokens, thus consume too much unnecessary computation. Pruning redundant vision tokens for high VLM inference efficiency has been continuously studied but all existing methods resort to indirect and non-guaranteed ways. We propose OC-VTP, a direct and guaranteed approach to select the most representative vision tokens for high-efficiency yet accuracy-preserving VLM inference. Our OC-VTP requires merely light-weight pre-training of a small object-centric vision token pruner, which can then be inserted into existing VLMs, without fine-tuning of any models on any datasets. It is gauranteed that the most representative vision tokens are kept by minimizing the error in reconstructing the original unpruned tokens from the selected ones. Across any vision pruning ratios, i.e., inference efficiency, our OC-VTP consistently helps mainstream VLMs to preserve the highest inference accuracy. Our pruning also demonstrates interesting interpretability. Our codes are available at https://github.com/GarryLarry010131/OC-VTP.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Generate Human-Human-Object Interactions from Textual Descriptions</title>
<link>https://arxiv.org/abs/2511.20446</link>
<guid>https://arxiv.org/abs/2511.20446</guid>
<content:encoded><![CDATA[
arXiv:2511.20446v1 Announce Type: new 
Abstract: The way humans interact with each other, including interpersonal distances, spatial configuration, and motion, varies significantly across different situations. To enable machines to understand such complex, context-dependent behaviors, it is essential to model multiple people in relation to the surrounding scene context. In this paper, we present a novel research problem to model the correlations between two people engaged in a shared interaction involving an object. We refer to this formulation as Human-Human-Object Interactions (HHOIs). To overcome the lack of dedicated datasets for HHOIs, we present a newly captured HHOIs dataset and a method to synthesize HHOI data by leveraging image generative models. As an intermediary, we obtain individual human-object interaction (HOIs) and human-human interaction (HHIs) from the HHOIs, and with these data, we train an text-to-HOI and text-to-HHI model using score-based diffusion model. Finally, we present a unified generative framework that integrates the two individual model, capable of synthesizing complete HHOIs in a single advanced sampling process. Our method extends HHOI generation to multi-human settings, enabling interactions involving more than two individuals. Experimental results show that our method generates realistic HHOIs conditioned on textual descriptions, outperforming previous approaches that focus only on single-human HOIs. Furthermore, we introduce multi-human motion generation involving objects as an application of our framework.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search</title>
<link>https://arxiv.org/abs/2511.20460</link>
<guid>https://arxiv.org/abs/2511.20460</guid>
<content:encoded><![CDATA[
arXiv:2511.20460v1 Announce Type: new 
Abstract: With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8\% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20%~44% in speed.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STARFlow-V: End-to-End Video Generative Modeling with Normalizing Flow</title>
<link>https://arxiv.org/abs/2511.20462</link>
<guid>https://arxiv.org/abs/2511.20462</guid>
<content:encoded><![CDATA[
arXiv:2511.20462v1 Announce Type: new 
Abstract: Normalizing flows (NFs) are end-to-end likelihood-based generative models for continuous data, and have recently regained attention with encouraging progress on image generation. Yet in the video generation domain, where spatiotemporal complexity and computational cost are substantially higher, state-of-the-art systems almost exclusively rely on diffusion-based models. In this work, we revisit this design space by presenting STARFlow-V, a normalizing flow-based video generator with substantial benefits such as end-to-end learning, robust causal prediction, and native likelihood estimation. Building upon the recently proposed STARFlow, STARFlow-V operates in the spatiotemporal latent space with a global-local architecture which restricts causal dependencies to a global latent space while preserving rich local within-frame interactions. This eases error accumulation over time, a common pitfall of standard autoregressive diffusion model generation. Additionally, we propose flow-score matching, which equips the model with a light-weight causal denoiser to improve the video generation consistency in an autoregressive fashion. To improve the sampling efficiency, STARFlow-V employs a video-aware Jacobi iteration scheme that recasts inner updates as parallelizable iterations without breaking causality. Thanks to the invertible structure, the same model can natively support text-to-video, image-to-video as well as video-to-video generation tasks. Empirically, STARFlow-V achieves strong visual fidelity and temporal consistency with practical sampling throughput relative to diffusion-based baselines. These results present the first evidence, to our knowledge, that NFs are capable of high-quality autoregressive video generation, establishing them as a promising research direction for building world models. Code and generated samples are available at https://github.com/apple/ml-starflow.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dance Style Classification using Laban-Inspired and Frequency-Domain Motion Features</title>
<link>https://arxiv.org/abs/2511.20469</link>
<guid>https://arxiv.org/abs/2511.20469</guid>
<content:encoded><![CDATA[
arXiv:2511.20469v1 Announce Type: new 
Abstract: Dance is an essential component of human culture and serves as a tool for conveying emotions and telling stories. Identifying and distinguishing dance genres based on motion data is a complex problem in human activity recognition, as many styles share similar poses, gestures, and temporal motion patterns. This work presents a lightweight framework for classifying dance styles that determines motion characteristics based on pose estimates extracted from videos. We propose temporal-spatial descriptors inspired by Laban Movement Analysis. These features capture local joint dynamics such as velocity, acceleration, and angular movement of the upper body, enabling a structured representation of spatial coordination. To further encode rhythmic and periodic aspects of movement, we integrate Fast Fourier Transform features that characterize movement patterns in the frequency domain. The proposed approach achieves robust classification of different dance styles with low computational effort, as complex model architectures are not required, and shows that interpretable motion representations can effectively capture stylistic nuances.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Deep Learning Framework for Assistive Perception: Gaze, Affect, and Speaker Identification</title>
<link>https://arxiv.org/abs/2511.20474</link>
<guid>https://arxiv.org/abs/2511.20474</guid>
<content:encoded><![CDATA[
arXiv:2511.20474v1 Announce Type: new 
Abstract: Developing comprehensive assistive technologies requires the seamless integration of visual and auditory perception. This research evaluates the feasibility of a modular architecture inspired by core functionalities of perceptive systems like 'Smart Eye.' We propose and benchmark three independent sensing modules: a Convolutional Neural Network (CNN) for eye state detection (drowsiness/attention), a deep CNN for facial expression recognition, and a Long Short-Term Memory (LSTM) network for voice-based speaker identification. Utilizing the Eyes Image, FER2013, and customized audio datasets, our models achieved accuracies of 93.0%, 97.8%, and 96.89%, respectively. This study demonstrates that lightweight, domain-specific models can achieve high fidelity on discrete tasks, establishing a validated foundation for future real-time, multimodal integration in resource-constrained assistive devices.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Informed Loss Function for Boundary-Consistent and Robust Artery Segmentation in DSA Sequences</title>
<link>https://arxiv.org/abs/2511.20501</link>
<guid>https://arxiv.org/abs/2511.20501</guid>
<content:encoded><![CDATA[
arXiv:2511.20501v1 Announce Type: new 
Abstract: Accurate extraction and segmentation of the cerebral arteries from digital subtraction angiography (DSA) sequences is essential for developing reliable clinical management models of complex cerebrovascular diseases. Conventional loss functions often rely solely on pixel-wise overlap, overlooking the geometric and physical consistency of vascular boundaries, which can lead to fragmented or unstable vessel predictions. To overcome this limitation, we propose a novel \textit{Physics-Informed Loss} (PIL) that models the interaction between the predicted and ground-truth boundaries as an elastic process inspired by dislocation theory in materials physics. This formulation introduces a physics-based regularization term that enforces smooth contour evolution and structural consistency, allowing the network to better capture fine vascular geometry. The proposed loss is integrated into several segmentation architectures, including U-Net, U-Net++, SegFormer, and MedFormer, and evaluated on two public benchmarks: DIAS and DSCA. Experimental results demonstrate that PIL consistently outperforms conventional loss functions such as Cross-Entropy, Dice, Active Contour, and Surface losses, achieving superior sensitivity, F1 score, and boundary coherence. These findings confirm that the incorporation of physics-based boundary interactions into deep neural networks improves both the precision and robustness of vascular segmentation in dynamic angiographic imaging. The implementation of the proposed method is publicly available at https://github.com/irfantahir301/Physicsis_loss.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DesignPref: Capturing Personal Preferences in Visual Design Generation</title>
<link>https://arxiv.org/abs/2511.20513</link>
<guid>https://arxiv.org/abs/2511.20513</guid>
<content:encoded><![CDATA[
arXiv:2511.20513v1 Announce Type: new 
Abstract: Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs</title>
<link>https://arxiv.org/abs/2511.20515</link>
<guid>https://arxiv.org/abs/2511.20515</guid>
<content:encoded><![CDATA[
arXiv:2511.20515v1 Announce Type: new 
Abstract: Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation</title>
<link>https://arxiv.org/abs/2511.20520</link>
<guid>https://arxiv.org/abs/2511.20520</guid>
<content:encoded><![CDATA[
arXiv:2511.20520v1 Announce Type: new 
Abstract: Recent unified models integrate understanding experts (e.g., LLMs) with generative experts (e.g., diffusion models), achieving strong multimodal performance. However, recent advanced methods such as BAGEL and LMFusion follow the Mixture-of-Transformers (MoT) paradigm, adopting a symmetric design that mirrors one expert to another for convenient initialization and fusion, which remains suboptimal due to inherent modality discrepancies. In this work, we propose HBridge, an asymmetric H-shaped architecture that enables heterogeneous experts to optimally leverage pretrained priors from their respective modality domains. Unlike prior dense fusion strategies that straightforwardly connect all layers between experts via shared attention, HBridge selectively bridges intermediate layers, reducing over 40% attention sharing, which improves efficiency and enhances generation quality. Shallow and deep layers, which capture modality-specific representations, are decoupled, while mid-layer bridging promotes semantic alignment. To further strengthen cross-modal coherence, we introduce semantic reconstruction tokens that explicitly guide the generative expert to reconstruct visual semantic tokens of the target image. Extensive experiments across multiple benchmarks demonstrate the effectiveness and superior performance of HBridge, establishing a new paradigm for unified multimodal generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos</title>
<link>https://arxiv.org/abs/2511.20525</link>
<guid>https://arxiv.org/abs/2511.20525</guid>
<content:encoded><![CDATA[
arXiv:2511.20525v1 Announce Type: new 
Abstract: We introduce Mistake Attribution (MATT), a task for fine-grained understanding of human mistakes in egocentric video. Unlike prior mistake understanding work, which lacks fine-grained output, MATT concretely attributes mistakes to the input instruction text or the attempt video. MATT determines what part of the instruction is violated (semantic role), when the deviation becomes irreversible (the Point-of-No-Return, PNR), and where the mistake appears in the PNR frame. We develop MisEngine, a data engine that automatically constructs attribution-rich mistake samples from existing datasets and inherits their annotations. Applied to large egocentric corpora, MisEngine yields EPIC-KITCHENS-M and Ego4D-M, two datasets that are up to two orders of magnitude larger than prior mistake datasets. We then present MisFormer, a unified attention-based model for mistake attribution across semantic (what), temporal (when), and spatial (where) dimensions, trained using MisEngine supervision. Experiments on our new datasets and prior benchmarks show that MisFormer outperforms strong video-language, temporal localization, hand-object interaction, and mistake-detection baselines.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Monitoring of Cultural Heritage Artifacts Using Semantic Segmentation</title>
<link>https://arxiv.org/abs/2511.20541</link>
<guid>https://arxiv.org/abs/2511.20541</guid>
<content:encoded><![CDATA[
arXiv:2511.20541v1 Announce Type: new 
Abstract: This paper addresses the critical need for automated crack detection in the preservation of cultural heritage through semantic segmentation. We present a comparative study of U-Net architectures, using various convolutional neural network (CNN) encoders, for pixel-level crack identification on statues and monuments. A comparative quantitative evaluation is performed on the test set of the OmniCrack30k dataset [1] using popular segmentation metrics including Mean Intersection over Union (mIoU), Dice coefficient, and Jaccard index. This is complemented by an out-of-distribution qualitative evaluation on an unlabeled test set of real-world cracked statues and monuments. Our findings provide valuable insights into the capabilities of different CNN- based encoders for fine-grained crack segmentation. We show that the models exhibit promising generalization capabilities to unseen cultural heritage contexts, despite never having been explicitly trained on images of statues or monuments.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>New York Smells: A Large Multimodal Dataset for Olfaction</title>
<link>https://arxiv.org/abs/2511.20544</link>
<guid>https://arxiv.org/abs/2511.20544</guid>
<content:encoded><![CDATA[
arXiv:2511.20544v1 Announce Type: new 
Abstract: While olfaction is central to how animals perceive the world, this rich chemical sensory modality remains largely inaccessible to machines. One key bottleneck is the lack of diverse, multimodal olfactory training data collected in natural settings. We present New York Smells, a large dataset of paired image and olfactory signals captured ``in the wild.'' Our dataset contains 7,000 smell-image pairs from 3,500 distinct objects across indoor and outdoor environments, with approximately 70$\times$ more objects than existing olfactory datasets. Our benchmark has three tasks: cross-modal smell-to-image retrieval, recognizing scenes, objects, and materials from smell alone, and fine-grained discrimination between grass species. Through experiments on our dataset, we find that visual data enables cross-modal olfactory representation learning, and that our learned olfactory representations outperform widely-used hand-crafted features.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.20549</link>
<guid>https://arxiv.org/abs/2511.20549</guid>
<content:encoded><![CDATA[
arXiv:2511.20549v1 Announce Type: new 
Abstract: Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward</title>
<link>https://arxiv.org/abs/2511.20561</link>
<guid>https://arxiv.org/abs/2511.20561</guid>
<content:encoded><![CDATA[
arXiv:2511.20561v1 Announce Type: new 
Abstract: Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysChoreo: Physics-Controllable Video Generation with Part-Aware Semantic Grounding</title>
<link>https://arxiv.org/abs/2511.20562</link>
<guid>https://arxiv.org/abs/2511.20562</guid>
<content:encoded><![CDATA[
arXiv:2511.20562v1 Announce Type: new 
Abstract: While recent video generation models have achieved significant visual fidelity, they often suffer from the lack of explicit physical controllability and plausibility. To address this, some recent studies attempted to guide the video generation with physics-based rendering. However, these methods face inherent challenges in accurately modeling complex physical properties and effectively control ling the resulting physical behavior over extended temporal sequences. In this work, we introduce PhysChoreo, a novel framework that can generate videos with diverse controllability and physical realism from a single image. Our method consists of two stages: first, it estimates the static initial physical properties of all objects in the image through part-aware physical property reconstruction. Then, through temporally instructed and physically editable simulation, it synthesizes high-quality videos with rich dynamic behaviors and physical realism. Experimental results show that PhysChoreo can generate videos with rich behaviors and physical realism, outperforming state-of-the-art methods on multiple evaluation metrics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reason-then-Describe Instruction Interpreter for Controllable Video Generation</title>
<link>https://arxiv.org/abs/2511.20563</link>
<guid>https://arxiv.org/abs/2511.20563</guid>
<content:encoded><![CDATA[
arXiv:2511.20563v1 Announce Type: new 
Abstract: Diffusion Transformers have significantly improved video fidelity and temporal coherence, however, practical controllability remains limited. Concise, ambiguous, and compositionally complex user inputs contrast with the detailed prompts used in training, yielding an intent-output mismatch. We propose ReaDe, a universal, model-agnostic interpreter that converts raw instructions into precise, actionable specifications for downstream video generators. ReaDe follows a reason-then-describe paradigm: it first analyzes the user request to identify core requirements and resolve ambiguities, then produces detailed guidance that enables faithful, controllable generation. We train ReaDe via a two-stage optimization: (i) reasoning-augmented supervision imparts analytic parsing with stepwise traces and dense captions, and (ii) a multi-dimensional reward assigner enables stable, feedback-driven refinement for natural-style captions. Experiments across single- and multi-condition scenarios show consistent gains in instruction fidelity, caption accuracy, and downstream video quality, with strong generalization to reasoning-intensive and unseen inputs. ReaDe offers a practical route to aligning controllable video generation with accurately interpreted user intent. Project Page: https://sqwu.top/ReaDe/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DINO-Tok: Adapting DINO for Visual Tokenizers</title>
<link>https://arxiv.org/abs/2511.20565</link>
<guid>https://arxiv.org/abs/2511.20565</guid>
<content:encoded><![CDATA[
arXiv:2511.20565v1 Announce Type: new 
Abstract: Recent advances in visual generation have highlighted the rise of Latent Generative Models (LGMs), which rely on effective visual tokenizers to bridge pixels and semantics. However, existing tokenizers are typically trained from scratch and struggle to balance semantic representation and reconstruction fidelity, particularly in high-dimensional latent spaces. In this work, we introduce DINO-Tok, a DINO-based visual tokenizer that unifies hierarchical representations into an information-complete latent space. By integrating shallow features that retain fine-grained details with deep features encoding global semantics, DINO-Tok effectively bridges pretrained representations and visual generation. We further analyze the challenges of vector quantization (VQ) in this high-dimensional space, where key information is often lost and codebook collapse occurs. We thus propose a global PCA reweighting mechanism to stabilize VQ and preserve essential information across dimensions. On ImageNet 256$\times$256, DINO-Tok achieves state-of-the-art reconstruction performance, reaching 28.54 PSNR for autoencoding and 23.98 PSNR for VQ-based modeling, significantly outperforming prior tokenizers and comparable to billion-level data trained models (such as Hunyuan and Wan). These results demonstrate that adapting powerful pretrained vision models like DINO for tokenization enables semantically aligned and high-fidelity latent representations, enabling next-generation visual generative models. Code will be publicly available at https://github.com/MKJia/DINO-Tok.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VQ-VA World: Towards High-Quality Visual Question-Visual Answering</title>
<link>https://arxiv.org/abs/2511.20573</link>
<guid>https://arxiv.org/abs/2511.20573</guid>
<content:encoded><![CDATA[
arXiv:2511.20573v1 Announce Type: new 
Abstract: This paper studies Visual Question-Visual Answering (VQ-VA): generating an image, rather than text, in response to a visual question -- an ability that has recently emerged in proprietary systems such as NanoBanana and GPT-Image. To also bring this capability to open-source models, we introduce VQ-VA World, a data-centric framework built around an agentic pipeline for large-scale, targeted data construction. Leveraging web-scale deployment, this pipeline crawls a massive amount of ~1.8M high-quality, interleaved image-text samples for model training. For evaluation, we further release IntelligentBench, a human-curated benchmark that systematically assesses VQ-VA along the aspects of world knowledge, design knowledge, and reasoning. Training with VQ-VA World data yields strong empirical gains: it helps LightFusion attain 53.06 on IntelligentBench, substantially surpassing the best prior open-source baselines (i.e., 7.78 from vanilla LightFusion; 1.94 from UniWorld-V1), and significantly narrowing the gap toward leading proprietary systems (e.g., 81.67 from NanoBanana; 82.64 from GPT-Image). By releasing the full suite of model weights, datasets, and pipelines, we hope to stimulate future research on VQ-VA.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Consistency Critic: Correcting Inconsistencies in Generated Images via Reference-Guided Attentive Alignment</title>
<link>https://arxiv.org/abs/2511.20614</link>
<guid>https://arxiv.org/abs/2511.20614</guid>
<content:encoded><![CDATA[
arXiv:2511.20614v1 Announce Type: new 
Abstract: Previous works have explored various customized generation tasks given a reference image, but they still face limitations in generating consistent fine-grained details. In this paper, our aim is to solve the inconsistency problem of generated images by applying a reference-guided post-editing approach and present our ImageCritic. We first construct a dataset of reference-degraded-target triplets obtained via VLM-based selection and explicit degradation, which effectively simulates the common inaccuracies or inconsistencies observed in existing generation models. Furthermore, building on a thorough examination of the model's attention mechanisms and intrinsic representations, we accordingly devise an attention alignment loss and a detail encoder to precisely rectify inconsistencies. ImageCritic can be integrated into an agent framework to automatically detect inconsistencies and correct them with multi-round and local editing in complex scenarios. Extensive experiments demonstrate that ImageCritic can effectively resolve detail-related issues in various customized generation scenarios, providing significant improvements over existing methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Performance of Deep Learning Models in Whole-body Dynamic 3D Posture Prediction During Load-reaching Activities</title>
<link>https://arxiv.org/abs/2511.20615</link>
<guid>https://arxiv.org/abs/2511.20615</guid>
<content:encoded><![CDATA[
arXiv:2511.20615v1 Announce Type: new 
Abstract: This study aimed to explore the application of deep neural networks for whole-body human posture prediction during dynamic load-reaching activities. Two time-series models were trained using bidirectional long short-term memory (BLSTM) and transformer architectures. The dataset consisted of 3D full-body plug-in gait dynamic coordinates from 20 normal-weight healthy male individuals each performing 204 load-reaching tasks from different load positions while adapting various lifting and handling techniques. The model inputs consisted of the 3D position of the hand-load position, lifting (stoop, full-squat and semi-squat) and handling (one- and two-handed) techniques, body weight and height, and the 3D coordinate data of the body posture from the first 25% of the task duration. These inputs were used by the models to predict body coordinates during the remaining 75% of the task period. Moreover, a novel method was proposed to improve the accuracy of the previous and present posture prediction networks by enforcing constant body segment lengths through the optimization of a new cost function. The results indicated that the new cost function decreased the prediction error of the models by approximately 8% and 21% for the arm and leg models, respectively. We indicated that utilizing the transformer architecture, with a root-mean-square-error of 47.0 mm, exhibited ~58% more accurate long-term performance than the BLSTM-based model. This study merits the use of neural networks that capture time series dependencies in 3D motion frames, providing a unique approach for understanding and predict motion dynamics during manual material handling activities.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI</title>
<link>https://arxiv.org/abs/2511.20620</link>
<guid>https://arxiv.org/abs/2511.20620</guid>
<content:encoded><![CDATA[
arXiv:2511.20620v1 Announce Type: new 
Abstract: Reproducible closed-loop evaluation remains a major bottleneck in Embodied AI such as visual navigation. A promising path forward is high-fidelity simulation that combines photorealistic sensor rendering with geometrically grounded interaction in complex, open-world urban environments. Although recent video-3DGS methods ease open-world scene capturing, they are still unsuitable for benchmarking due to large visual and geometric sim-to-real gaps. To address these challenges, we introduce Wanderland, a real-to-sim framework that features multi-sensor capture, reliable reconstruction, accurate geometry, and robust view synthesis. Using this pipeline, we curate a diverse dataset of indoor-outdoor urban scenes and systematically demonstrate how image-only pipelines scale poorly, how geometry quality impacts novel view synthesis, and how all of these adversely affect navigation policy learning and evaluation reliability. Beyond serving as a trusted testbed for embodied navigation, Wanderland's rich raw sensor data further allows benchmarking of 3D reconstruction and novel view synthesis models. Our work establishes a new foundation for reproducible research in open-world embodied AI. Project website is at https://ai4ce.github.io/wanderland/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ShapeGen: Towards High-Quality 3D Shape Synthesis</title>
<link>https://arxiv.org/abs/2511.20624</link>
<guid>https://arxiv.org/abs/2511.20624</guid>
<content:encoded><![CDATA[
arXiv:2511.20624v1 Announce Type: new 
Abstract: Inspired by generative paradigms in image and video, 3D shape generation has made notable progress, enabling the rapid synthesis of high-fidelity 3D assets from a single image. However, current methods still face challenges, including the lack of intricate details, overly smoothed surfaces, and fragmented thin-shell structures. These limitations leave the generated 3D assets still one step short of meeting the standards favored by artists. In this paper, we present ShapeGen, which achieves high-quality image-to-3D shape generation through 3D representation and supervision improvements, resolution scaling up, and the advantages of linear transformers. These advancements allow the generated assets to be seamlessly integrated into 3D pipelines, facilitating their widespread adoption across various applications. Through extensive experiments, we validate the impact of these improvements on overall performance. Ultimately, thanks to the synergistic effects of these enhancements, ShapeGen achieves a significant leap in image-to-3D generation, establishing a new state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models</title>
<link>https://arxiv.org/abs/2511.20629</link>
<guid>https://arxiv.org/abs/2511.20629</guid>
<content:encoded><![CDATA[
arXiv:2511.20629v1 Announce Type: new 
Abstract: Reinforcement learning from human feedback (RLHF) with reward models has advanced alignment of generative models to human aesthetic and perceptual preferences. However, jointly optimizing multiple rewards often incurs an alignment tax, improving one dimension while degrading others. To address this, we introduce two complementary methods: MapReduce LoRA and Reward-aware Token Embedding (RaTE). MapReduce LoRA trains preference-specific LoRA experts in parallel and iteratively merges them to refine a shared base model; RaTE learns reward-specific token embeddings that compose at inference for flexible preference control. Experiments on Text-to-Image generation (Stable Diffusion 3.5 Medium and FLUX.1-dev) show improvements of 36.1%, 4.6%, and 55.7%, and 32.7%, 4.3%, and 67.1% on GenEval, PickScore, and OCR, respectively. On Text-to-Video generation (HunyuanVideo), visual and motion quality improve by 48.1% and 90.0%, respectively. On the language task, Helpful Assistant, with Llama-2 7B, helpful and harmless improve by 43.4% and 136.7%, respectively. Our framework sets a new state-of-the-art multi-preference alignment recipe across modalities.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation</title>
<link>https://arxiv.org/abs/2511.20635</link>
<guid>https://arxiv.org/abs/2511.20635</guid>
<content:encoded><![CDATA[
arXiv:2511.20635v1 Announce Type: new 
Abstract: Pre-trained video models learn powerful priors for generating high-quality, temporally coherent content. While these models excel at temporal coherence, their dynamics are often constrained by the continuous nature of their training data. We hypothesize that by injecting the rich and unconstrained content diversity from image data into this coherent temporal framework, we can generate image sets that feature both natural transitions and a far more expansive dynamic range. To this end, we introduce iMontage, a unified framework designed to repurpose a powerful video model into an all-in-one image generator. The framework consumes and produces variable-length image sets, unifying a wide array of image generation and editing tasks. To achieve this, we propose an elegant and minimally invasive adaptation strategy, complemented by a tailored data curation process and training paradigm. This approach allows the model to acquire broad image manipulation capabilities without corrupting its invaluable original motion priors. iMontage excels across several mainstream many-in-many-out tasks, not only maintaining strong cross-image contextual consistency but also generating scenes with extraordinary dynamics that surpass conventional scopes. Find our homepage at: https://kr1sjfu.github.io/iMontage-web/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionV2V: Editing Motion in a Video</title>
<link>https://arxiv.org/abs/2511.20640</link>
<guid>https://arxiv.org/abs/2511.20640</guid>
<content:encoded><![CDATA[
arXiv:2511.20640v1 Announce Type: new 
Abstract: While generative video models have achieved remarkable fidelity and consistency, applying these capabilities to video editing remains a complex challenge. Recent research has explored motion controllability as a means to enhance text-to-video generation or image animation; however, we identify precise motion control as a promising yet under-explored paradigm for editing existing videos. In this work, we propose modifying video motion by directly editing sparse trajectories extracted from the input. We term the deviation between input and output trajectories a "motion edit" and demonstrate that this representation, when coupled with a generative backbone, enables powerful video editing capabilities. To achieve this, we introduce a pipeline for generating "motion counterfactuals", video pairs that share identical content but distinct motion, and we fine-tune a motion-conditioned video diffusion architecture on this dataset. Our approach allows for edits that start at any timestamp and propagate naturally. In a four-way head-to-head user study, our model achieves over 65 percent preference against prior work. Please see our project page: https://ryanndagreat.github.io/MotionV2V
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition</title>
<link>https://arxiv.org/abs/2511.20641</link>
<guid>https://arxiv.org/abs/2511.20641</guid>
<content:encoded><![CDATA[
arXiv:2511.20641v1 Announce Type: new 
Abstract: Long-tailed multi-label visual recognition poses a significant challenge, as images typically contain multiple labels with highly imbalanced class distributions, leading to biased models that favor head classes while underperforming on tail classes. Recent efforts have leveraged pre-trained vision-language models, such as CLIP, alongside long-tailed learning techniques to exploit rich visual-textual priors for improved performance. However, existing methods often derive semantic inter-class relationships directly from imbalanced datasets, resulting in unreliable correlations for tail classes due to data scarcity. Moreover, CLIP's zero-shot paradigm is optimized for single-label image-text matching, making it suboptimal for multi-label tasks. To address these issues, we propose the correlation adaptation prompt network (CAPNET), a novel end-to-end framework that explicitly models label correlations from CLIP's textual encoder. The framework incorporates a graph convolutional network for label-aware propagation and learnable soft prompts for refined embeddings. It utilizes a distribution-balanced Focal loss with class-aware re-weighting for optimized training under imbalance. Moreover, it improves generalization through test-time ensembling and realigns visual-textual modalities using parameter-efficient fine-tuning to avert overfitting on tail classes without compromising head class performance. Extensive experiments and ablation studies on benchmarks including VOC-LT, COCO-LT, and NUS-WIDE demonstrate that CAPNET achieves substantial improvements over state-of-the-art methods, validating its effectiveness for real-world long-tailed multi-label visual recognition.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concept-Aware Batch Sampling Improves Language-Image Pretraining</title>
<link>https://arxiv.org/abs/2511.20643</link>
<guid>https://arxiv.org/abs/2511.20643</guid>
<content:encoded><![CDATA[
arXiv:2511.20643v1 Announce Type: new 
Abstract: What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Language Memory for Spatial Reasoning</title>
<link>https://arxiv.org/abs/2511.20644</link>
<guid>https://arxiv.org/abs/2511.20644</guid>
<content:encoded><![CDATA[
arXiv:2511.20644v1 Announce Type: new 
Abstract: Spatial reasoning is a critical capability for intelligent robots, yet current vision-language models (VLMs) still fall short of human-level performance in video-based spatial reasoning. This gap mainly stems from two challenges: a semantic-geometric misalignment that prevents consistent 3D understanding, and the absence of persistent memory to retain 3D representation and understanding over time. To address these limitations, we present VLM$^2$, a Vision-Language Model with persistent Memory for spatial reasoning with a view-consistent, 3D-aware representation purely from 2D video. Specifically, to enhance long-horizon reasoning, we incorporate a dual-memory module, consisting of a working memory that operates as a sliding window to focus on immediate context, and an episodic memory that consolidates and stores critical long-term information. This design enables efficient and long-horizon spatial reasoning with a fixed computational cost. Extensive experiments on multiple benchmarks show that VLM$^2$ achieves state-of-the-art performance among video-only models, significantly advancing the frontier of visual-spatial intelligence.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PixelDiT: Pixel Diffusion Transformers for Image Generation</title>
<link>https://arxiv.org/abs/2511.20645</link>
<guid>https://arxiv.org/abs/2511.20645</guid>
<content:encoded><![CDATA[
arXiv:2511.20645v1 Announce Type: new 
Abstract: Latent-space modeling has been the standard for Diffusion Transformers (DiTs). However, it relies on a two-stage pipeline where the pretrained autoencoder introduces lossy reconstruction, leading to error accumulation while hindering joint optimization. To address these issues, we propose PixelDiT, a single-stage, end-to-end model that eliminates the need for the autoencoder and learns the diffusion process directly in the pixel space. PixelDiT adopts a fully transformer-based architecture shaped by a dual-level design: a patch-level DiT that captures global semantics and a pixel-level DiT that refines texture details, enabling efficient training of a pixel-space diffusion model while preserving fine details. Our analysis reveals that effective pixel-level token modeling is essential to the success of pixel diffusion. PixelDiT achieves 1.61 FID on ImageNet 256x256, surpassing existing pixel generative models by a large margin. We further extend PixelDiT to text-to-image generation and pretrain it at the 1024x1024 resolution in pixel space. It achieves 0.74 on GenEval and 83.5 on DPG-bench, approaching the best latent diffusion models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-Aware Multi-Task Learning with Cross-View Correlations for Dense Scene Understanding</title>
<link>https://arxiv.org/abs/2511.20646</link>
<guid>https://arxiv.org/abs/2511.20646</guid>
<content:encoded><![CDATA[
arXiv:2511.20646v1 Announce Type: new 
Abstract: This paper addresses the challenge of training a single network to jointly perform multiple dense prediction tasks, such as segmentation and depth estimation, i.e., multi-task learning (MTL). Current approaches mainly capture cross-task relations in the 2D image space, often leading to unstructured features lacking 3D-awareness. We argue that 3D-awareness is vital for modeling cross-task correlations essential for comprehensive scene understanding. We propose to address this problem by integrating correlations across views, i.e., cost volume, as geometric consistency in the MTL network. Specifically, we introduce a lightweight Cross-view Module (CvM), shared across tasks, to exchange information across views and capture cross-view correlations, integrated with a feature from MTL encoder for multi-task predictions. This module is architecture-agnostic and can be applied to both single and multi-view data. Extensive results on NYUv2 and PASCAL-Context demonstrate that our method effectively injects geometric consistency into existing MTL methods to improve performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diverse Video Generation with Determinantal Point Process-Guided Policy Optimization</title>
<link>https://arxiv.org/abs/2511.20647</link>
<guid>https://arxiv.org/abs/2511.20647</guid>
<content:encoded><![CDATA[
arXiv:2511.20647v1 Announce Type: new 
Abstract: While recent text-to-video (T2V) diffusion models have achieved impressive quality and prompt alignment, they often produce low-diversity outputs when sampling multiple videos from a single text prompt. We tackle this challenge by formulating it as a set-level policy optimization problem, with the goal of training a policy that can cover the diverse range of plausible outcomes for a given prompt. To address this, we introduce DPP-GRPO, a novel framework for diverse video generation that combines Determinantal Point Processes (DPPs) and Group Relative Policy Optimization (GRPO) theories to enforce explicit reward on diverse generations. Our objective turns diversity into an explicit signal by imposing diminishing returns on redundant samples (via DPP) while supplies groupwise feedback over candidate sets (via GRPO). Our framework is plug-and-play and model-agnostic, and encourages diverse generations across visual appearance, camera motions, and scene structure without sacrificing prompt fidelity or perceptual quality. We implement our method on WAN and CogVideoX, and show that our method consistently improves video diversity on state-of-the-art benchmarks such as VBench, VideoScore, and human preference studies. Moreover, we release our code and a new benchmark dataset of 30,000 diverse prompts to support future research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight</title>
<link>https://arxiv.org/abs/2511.20648</link>
<guid>https://arxiv.org/abs/2511.20648</guid>
<content:encoded><![CDATA[
arXiv:2511.20648v1 Announce Type: new 
Abstract: To act in the world, a model must name what it sees and know where it is in 3D. Today's vision-language models (VLMs) excel at open-ended 2D description and grounding, yet multi-object 3D detection remains largely missing from the VLM toolbox. We present LocateAnything3D, a VLM-native recipe that casts 3D detection as a next-token prediction problem. The key is a short, explicit Chain-of-Sight (CoS) sequence that mirrors how human reason from images: find an object in 2D, then infer its distance, size, and pose. The decoder first emits 2D detections as a visual chain-of-thought, then predicts 3D boxes under an easy-to-hard curriculum: across objects, a near-to-far order reduces early ambiguity and matches ego-centric utility; within each object, a center-from-camera, dimensions, and rotation factorization ranks information by stability and learnability. This VLM-native interface preserves open-vocabulary and visual-prompting capability without specialized heads. On the challenging Omni3D benchmark, our model achieves state-of-the-art results, with 49.89 AP_3D, surpassing the previous best by +15.51 absolute improvement even when the baseline is given ground-truth 2D boxes. It also generalizes zero-shot to held-out categories with strong robustness. By turning 3D detection into a disciplined next-token problem, LocateAnything3D offers a practical foundation for models to perceive in 3D.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout</title>
<link>https://arxiv.org/abs/2511.20649</link>
<guid>https://arxiv.org/abs/2511.20649</guid>
<content:encoded><![CDATA[
arXiv:2511.20649v1 Announce Type: new 
Abstract: Current autoregressive video diffusion models are constrained by three core bottlenecks: (i) the finite temporal horizon imposed by the base model's 3D Rotary Positional Embedding (3D-RoPE), (ii) slow prompt responsiveness in maintaining fine-grained action control during long-form rollouts, and (iii) the inability to realize discontinuous cinematic transitions within a single generation stream. We introduce $\infty$-RoPE, a unified inference-time framework that addresses all three limitations through three interconnected components: Block-Relativistic RoPE, KV Flush, and RoPE Cut. Block-Relativistic RoPE reformulates temporal encoding as a moving local reference frame, where each newly generated latent block is rotated relative to the base model's maximum frame horizon while earlier blocks are rotated backward to preserve relative temporal geometry. This relativistic formulation eliminates fixed temporal positions, enabling continuous video generation far beyond the base positional limits. To obtain fine-grained action control without re-encoding, KV Flush renews the KV cache by retaining only two latent frames, the global sink and the last generated latent frame, thereby ensuring immediate prompt responsiveness. Finally, RoPE Cut introduces controlled discontinuities in temporal RoPE coordinates, enabling multi-cut scene transitions within a single continuous rollout. Together, these components establish $\infty$-RoPE as a training-free foundation for infinite-horizon, controllable, and cinematic video diffusion. Comprehensive experiments show that $\infty$-RoPE consistently surpasses previous autoregressive models in overall VBench scores.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedROV: Towards Real-Time Open-Vocabulary Detection Across Diverse Medical Imaging Modalities</title>
<link>https://arxiv.org/abs/2511.20650</link>
<guid>https://arxiv.org/abs/2511.20650</guid>
<content:encoded><![CDATA[
arXiv:2511.20650v1 Announce Type: new 
Abstract: Traditional object detection models in medical imaging operate within a closed-set paradigm, limiting their ability to detect objects of novel labels. Open-vocabulary object detection (OVOD) addresses this limitation but remains underexplored in medical imaging due to dataset scarcity and weak text-image alignment. To bridge this gap, we introduce MedROV, the first Real-time Open Vocabulary detection model for medical imaging. To enable open-vocabulary learning, we curate a large-scale dataset, Omnis, with 600K detection samples across nine imaging modalities and introduce a pseudo-labeling strategy to handle missing annotations from multi-source datasets. Additionally, we enhance generalization by incorporating knowledge from a large pre-trained foundation model. By leveraging contrastive learning and cross-modal representations, MedROV effectively detects both known and novel structures. Experimental results demonstrate that MedROV outperforms the previous state-of-the-art foundation model for medical image detection with an average absolute improvement of 40 mAP50, and surpasses closed-set detectors by more than 3 mAP50, while running at 70 FPS, setting a new benchmark in medical detection. Our source code, dataset, and trained model are available at https://github.com/toobatehreem/MedROV.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RubricRL: Simple Generalizable Rewards for Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2511.20651</link>
<guid>https://arxiv.org/abs/2511.20651</guid>
<content:encoded><![CDATA[
arXiv:2511.20651v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has recently emerged as a promising approach for aligning text-to-image generative models with human preferences. A key challenge, however, lies in designing effective and interpretable rewards. Existing methods often rely on either composite metrics (e.g., CLIP, OCR, and realism scores) with fixed weights or a single scalar reward distilled from human preference models, which can limit interpretability and flexibility. We propose RubricRL, a simple and general framework for rubric-based reward design that offers greater interpretability, composability, and user control. Instead of using a black-box scalar signal, RubricRL dynamically constructs a structured rubric for each prompt--a decomposable checklist of fine-grained visual criteria such as object correctness, attribute accuracy, OCR fidelity, and realism--tailored to the input text. Each criterion is independently evaluated by a multimodal judge (e.g., o4-mini), and a prompt-adaptive weighting mechanism emphasizes the most relevant dimensions. This design not only produces interpretable and modular supervision signals for policy optimization (e.g., GRPO or PPO), but also enables users to directly adjust which aspects to reward or penalize. Experiments with an autoregressive text-to-image model demonstrate that RubricRL improves prompt faithfulness, visual detail, and generalizability, while offering a flexible and extensible foundation for interpretable RL alignment across text-to-image architectures.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Path Knowledge-Augmented Contrastive Alignment Network for Spatially Resolved Transcriptomics</title>
<link>https://arxiv.org/abs/2511.17685</link>
<guid>https://arxiv.org/abs/2511.17685</guid>
<content:encoded><![CDATA[
arXiv:2511.17685v1 Announce Type: cross 
Abstract: Spatial Transcriptomics (ST) is a technology that measures gene expression profiles within tissue sections while retaining spatial context. It reveals localized gene expression patterns and tissue heterogeneity, both of which are essential for understanding disease etiology. However, its high cost has driven efforts to predict spatial gene expression from whole slide images. Despite recent advancements, current methods still face significant limitations, such as under-exploitation of high-level biological context, over-reliance on exemplar retrievals, and inadequate alignment of heterogeneous modalities. To address these challenges, we propose DKAN, a novel Dual-path Knowledge-Augmented contrastive alignment Network that predicts spatially resolved gene expression by integrating histopathological images and gene expression profiles through a biologically informed approach. Specifically, we introduce an effective gene semantic representation module that leverages the external gene database to provide additional biological insights, thereby enhancing gene expression prediction. Further, we adopt a unified, one-stage contrastive learning paradigm, seamlessly combining contrastive learning and supervised learning to eliminate reliance on exemplars, complemented with an adaptive weighting mechanism. Additionally, we propose a dual-path contrastive alignment module that employs gene semantic features as dynamic cross-modal coordinators to enable effective heterogeneous feature integration. Through extensive experiments across three public ST datasets, DKAN demonstrates superior performance over state-of-the-art models, establishing a new benchmark for spatial gene expression prediction and offering a powerful tool for advancing biological and clinical research.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation</title>
<link>https://arxiv.org/abs/2511.18525</link>
<guid>https://arxiv.org/abs/2511.18525</guid>
<content:encoded><![CDATA[
arXiv:2511.18525v1 Announce Type: cross 
Abstract: We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not Quite Anything: Overcoming SAMs Limitations for 3D Medical Imaging</title>
<link>https://arxiv.org/abs/2511.19471</link>
<guid>https://arxiv.org/abs/2511.19471</guid>
<content:encoded><![CDATA[
arXiv:2511.19471v1 Announce Type: cross 
Abstract: Foundation segmentation models such as SAM and SAM-2 perform well on natural images but struggle with brain MRIs where structures like the caudate and thalamus lack sharp boundaries and have low contrast. Rather than fine tune these models (for example MedSAM), we propose a compositional alternative where the foundation model output is treated as an additional input channel and passed alongside the MRI to highlight regions of interest.
  We generate SAM-2 prompts by using a lightweight 3D U-Net that was previously trained on MRI segmentation. The U-Net may have been trained on a different dataset, so its guesses are often imprecise but usually in the correct region. The edges of the resulting foundation model guesses are smoothed to improve alignment with the MRI. We also test prompt free segmentation using DINO attention maps in the same framework.
  This has-a architecture avoids modifying foundation weights and adapts to domain shift without retraining the foundation model. It reaches about 96 percent volume accuracy on basal ganglia segmentation, which is sufficient for our study of longitudinal volume change. The approach is fast, label efficient, and robust to out of distribution scans. We apply it to study inflammation linked changes in sudden onset pediatric OCD.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Stage Deep Learning Framework with PKCP-MixUp Augmentation for Pediatric Liver Tumor Diagnosis Using Multi-Phase Contrast-Enhanced CT</title>
<link>https://arxiv.org/abs/2511.19478</link>
<guid>https://arxiv.org/abs/2511.19478</guid>
<content:encoded><![CDATA[
arXiv:2511.19478v1 Announce Type: cross 
Abstract: Pediatric liver tumors are one of the most common solid tumors in pediatrics, with differentiation of benign or malignant status and pathological classification critical for clinical treatment. While pathological examination is the gold standard, the invasive biopsy has notable limitations: the highly vascular pediatric liver and fragile tumor tissue raise complication risks such as bleeding; additionally, young children with poor compliance require anesthesia for biopsy, increasing medical costs or psychological trauma. Although many efforts have been made to utilize AI in clinical settings, most researchers have overlooked its importance in pediatric liver tumors. To establish a non-invasive examination procedure, we developed a multi-stage deep learning (DL) framework for automated pediatric liver tumor diagnosis using multi-phase contrast-enhanced CT. Two retrospective and prospective cohorts were enrolled. We established a novel PKCP-MixUp data augmentation method to address data scarcity and class imbalance. We also trained a tumor detection model to extract ROIs, and then set a two-stage diagnosis pipeline with three backbones with ROI-masked images. Our tumor detection model has achieved high performance (mAP=0.871), and the first stage classification model between benign and malignant tumors reached an excellent performance (AUC=0.989). Final diagnosis models also exhibited robustness, including benign subtype classification (AUC=0.915) and malignant subtype classification (AUC=0.979). We also conducted multi-level comparative analyses, such as ablation studies on data and training pipelines, as well as Shapley-Value and CAM interpretability analyses. This framework fills the pediatric-specific DL diagnostic gap, provides actionable insights for CT phase selection and model design, and paves the way for precise, accessible pediatric liver tumor diagnosis.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection</title>
<link>https://arxiv.org/abs/2511.19499</link>
<guid>https://arxiv.org/abs/2511.19499</guid>
<content:encoded><![CDATA[
arXiv:2511.19499v1 Announce Type: cross 
Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shortcut Invariance: Targeted Jacobian Regularization in Disentangled Latent Space</title>
<link>https://arxiv.org/abs/2511.19525</link>
<guid>https://arxiv.org/abs/2511.19525</guid>
<content:encoded><![CDATA[
arXiv:2511.19525v1 Announce Type: cross 
Abstract: Deep neural networks are prone to learning shortcuts, spurious and easily learned correlations in training data that cause severe failures in out-of-distribution (OOD) generalization. A dominant line of work seeks robustness by learning a robust representation, often explicitly partitioning the latent space into core and spurious components; this approach can be complex, brittle, and difficult to scale. We take a different approach, instead of a robust representation, we learn a robust function. We present a simple and effective training method that renders the classifier functionally invariant to shortcut signals. Our method operates within a disentangled latent space, which is essential as it isolates spurious and core features into distinct dimensions. This separation enables the identification of candidate shortcut features by their strong correlation with the label, used as a proxy for semantic simplicity. The classifier is then desensitized to these features by injecting targeted, anisotropic latent noise during training. We analyze this as targeted Jacobian regularization, which forces the classifier to ignore spurious features and rely on more complex, core semantic signals. The result is state-of-the-art OOD performance on established shortcut learning benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhysDNet: Physics-Guided Decomposition Network of Side-Scan Sonar Imagery</title>
<link>https://arxiv.org/abs/2511.19539</link>
<guid>https://arxiv.org/abs/2511.19539</guid>
<content:encoded><![CDATA[
arXiv:2511.19539v1 Announce Type: cross 
Abstract: Side-scan sonar (SSS) imagery is widely used for seafloor mapping and underwater remote sensing, yet the measured intensity is strongly influenced by seabed reflectivity, terrain elevation, and acoustic path loss. This entanglement makes the imagery highly view-dependent and reduces the robustness of downstream analysis. In this letter, we present PhysDNet, a physics-guided multi-branch network that decouples SSS images into three interpretable fields: seabed reflectivity, terrain elevation, and propagation loss. By embedding the Lambertian reflection model, PhysDNet reconstructs sonar intensity from these components, enabling self-supervised training without ground-truth annotations. Experiments show that the decomposed representations preserve stable geological structures, capture physically consistent illumination and attenuation, and produce reliable shadow maps. These findings demonstrate that physics-guided decomposition provides a stable and interpretable domain for SSS analysis, improving both physical consistency and downstream tasks such as registration and shadow interpretation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPQR: A Standardized Benchmark for Modern Safety Alignment Methods in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.19558</link>
<guid>https://arxiv.org/abs/2511.19558</guid>
<content:encoded><![CDATA[
arXiv:2511.19558v1 Announce Type: cross 
Abstract: Text-to-image diffusion models can emit copyrighted, unsafe, or private content. Safety alignment aims to suppress specific concepts, yet evaluations seldom test whether safety persists under benign downstream fine-tuning routinely applied after deployment (e.g., LoRA personalization, style/domain adapters). We study the stability of current safety methods under benign fine-tuning and observe frequent breakdowns. As true safety alignment must withstand even benign post-deployment adaptations, we introduce the SPQR benchmark (Safety-Prompt adherence-Quality-Robustness). SPQR is a single-scored metric that provides a standardized and reproducible framework to evaluate how well safety-aligned diffusion models preserve safety, utility, and robustness under benign fine-tuning, by reporting a single leaderboard score to facilitate comparisons. We conduct multilingual, domain-specific, and out-of-distribution analyses, along with category-wise breakdowns, to identify when safety alignment fails after benign fine-tuning, ultimately showcasing SPQR as a concise yet comprehensive benchmark for T2I safety alignment techniques for T2I models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport</title>
<link>https://arxiv.org/abs/2511.19561</link>
<guid>https://arxiv.org/abs/2511.19561</guid>
<content:encoded><![CDATA[
arXiv:2511.19561v1 Announce Type: cross 
Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Massively Multitask World Models for Continuous Control</title>
<link>https://arxiv.org/abs/2511.19584</link>
<guid>https://arxiv.org/abs/2511.19584</guid>
<content:encoded><![CDATA[
arXiv:2511.19584v1 Announce Type: cross 
Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fara-7B: An Efficient Agentic Model for Computer Use</title>
<link>https://arxiv.org/abs/2511.19663</link>
<guid>https://arxiv.org/abs/2511.19663</guid>
<content:encoded><![CDATA[
arXiv:2511.19663v1 Announce Type: cross 
Abstract: Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Selective Disk Bispectrum and Its Inversion, with Application to Multi-Reference Alignment</title>
<link>https://arxiv.org/abs/2511.19706</link>
<guid>https://arxiv.org/abs/2511.19706</guid>
<content:encoded><![CDATA[
arXiv:2511.19706v1 Announce Type: cross 
Abstract: In many computer vision and shape analysis tasks, practitioners are interested in learning from the shape of the object in an image, while disregarding the object's orientation. To this end, it is valuable to define a rotation-invariant representation of images, retaining all information about that image, but disregarding the way an object is rotated in the frame. To be practical for learning tasks, this representation must be computationally efficient for large datasets and invertible, so the representation can be visualized in image space. To this end, we present the selective disk bispectrum: a fast, rotation-invariant representation for image shape analysis. While the translational bispectrum has long been used as a translational invariant representation for 1-D and 2-D signals, its extension to 2-D (disk) rotational invariance on images has been hindered by the absence of an invertible formulation and its cubic complexity. In this work, we derive an explicit inverse for the disk bispectrum, which allows us to define a "selective" disk bispectrum, which only uses the minimal number of coefficients needed for faithful shape recovery. We show that this representation enables multi-reference alignment for rotated images-a task previously intractable for disk bispectrum methods. These results establish the disk bispectrum as a practical and theoretically grounded tool for learning on rotation-invariant shape data.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Agentic Reinforcement Learning for Tool-Integrated Reasoning in VLMs</title>
<link>https://arxiv.org/abs/2511.19773</link>
<guid>https://arxiv.org/abs/2511.19773</guid>
<content:encoded><![CDATA[
arXiv:2511.19773v1 Announce Type: cross 
Abstract: While recent vision-language models (VLMs) demonstrate strong image understanding, their ability to "think with images", i.e., to reason through multi-step visual interactions, remains limited. We introduce VISTA-Gym, a scalable training environment for incentivizing tool-integrated visual reasoning capabilities in VLMs. VISTA-Gym unifies diverse real-world multimodal reasoning tasks (7 tasks from 13 datasets in total) with a standardized interface for visual tools (e.g., grounding, parsing), executable interaction loops, verifiable feedback signals, and efficient trajectory logging, enabling visual agentic reinforcement learning at scale. While recent VLMs exhibit strong text-only reasoning, both proprietary and open-source models still struggle with tool selection, invocation, and coordination. With VISTA-Gym, we train VISTA-R1 to interleave tool-use with agentic reasoning via multi-turn trajectory sampling and end-to-end reinforcement learning. Extensive experiments across 11 public reasoning-intensive VQA benchmarks show that VISTA-R1-8B outperforms state-of-the-art baselines with similar sizes by 9.51%-18.72%, demonstrating VISTA-Gym as an effective training ground to unlock the tool-integrated reasoning capabilities for VLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Terminal Velocity Matching</title>
<link>https://arxiv.org/abs/2511.19797</link>
<guid>https://arxiv.org/abs/2511.19797</guid>
<content:encoded><![CDATA[
arXiv:2511.19797v1 Announce Type: cross 
Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It Hears, It Sees too: Multi-Modal LLM for Depression Detection By Integrating Visual Understanding into Audio Language Models</title>
<link>https://arxiv.org/abs/2511.19877</link>
<guid>https://arxiv.org/abs/2511.19877</guid>
<content:encoded><![CDATA[
arXiv:2511.19877v1 Announce Type: cross 
Abstract: Depression is one of the most prevalent mental health disorders globally. In recent years, multi-modal data, such as speech, video, and transcripts, has been increasingly used to develop AI-assisted depression assessment systems. Large language models have further advanced this field due to their strong language understanding and generalization capabilities. However, conventional LLMs remain text-centric and cannot process the rich non-verbal cues found in audio and visual modalities, which are critical components in mental health evaluation. While multi-modal LLMs offer a promising direction, few are tailored for psychological applications. In this study, we propose a novel multi-modal LLM framework for depression detection. Our approach augments an audio language model with visual understanding and aligns audio-visual features at the timestamp level. This fine-grained alignment improves modeling of temporal dynamics across modalities while reducing the need for extensive training data and computational resources. Experiments on the DAIC-WoZ dataset demonstrate that our model outperforms both single-modality approaches and previous multi-modal methods. Moreover, the proposed framework can be extended to incorporate additional physiological signals, paving the way for broader clinical applications beyond mental health.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frequency Bias Matters: Diving into Robust and Generalized Deep Image Forgery Detection</title>
<link>https://arxiv.org/abs/2511.19886</link>
<guid>https://arxiv.org/abs/2511.19886</guid>
<content:encoded><![CDATA[
arXiv:2511.19886v1 Announce Type: cross 
Abstract: As deep image forgery powered by AI generative models, such as GANs, continues to challenge today's digital world, detecting AI-generated forgeries has become a vital security topic. Generalizability and robustness are two critical concerns of a forgery detector, determining its reliability when facing unknown GANs and noisy samples in an open world. Although many studies focus on improving these two properties, the root causes of these problems have not been fully explored, and it is unclear if there is a connection between them. Moreover, despite recent achievements in addressing these issues from image forensic or anti-forensic aspects, a universal method that can contribute to both sides simultaneously remains practically significant yet unavailable. In this paper, we provide a fundamental explanation of these problems from a frequency perspective. Our analysis reveals that the frequency bias of a DNN forgery detector is a possible cause of generalization and robustness issues. Based on this finding, we propose a two-step frequency alignment method to remove the frequency discrepancy between real and fake images, offering double-sided benefits: it can serve as a strong black-box attack against forgery detectors in the anti-forensic context or, conversely, as a universal defense to improve detector reliability in the forensic context. We also develop corresponding attack and defense implementations and demonstrate their effectiveness, as well as the effect of the frequency alignment method, in various experimental settings involving twelve detectors, eight forgery models, and five metrics.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DLADiff: A Dual-Layer Defense Framework against Fine-Tuning and Zero-Shot Customization of Diffusion Models</title>
<link>https://arxiv.org/abs/2511.19910</link>
<guid>https://arxiv.org/abs/2511.19910</guid>
<content:encoded><![CDATA[
arXiv:2511.19910v1 Announce Type: cross 
Abstract: With the rapid advancement of diffusion models, a variety of fine-tuning methods have been developed, enabling high-fidelity image generation with high similarity to the target content using only 3 to 5 training images. More recently, zero-shot generation methods have emerged, capable of producing highly realistic outputs from a single reference image without altering model weights. However, technological advancements have also introduced significant risks to facial privacy. Malicious actors can exploit diffusion model customization with just a few or even one image of a person to create synthetic identities nearly identical to the original identity. Although research has begun to focus on defending against diffusion model customization, most existing defense methods target fine-tuning approaches and neglect zero-shot generation defenses. To address this issue, this paper proposes Dual-Layer Anti-Diffusion (DLADiff) to defense both fine-tuning methods and zero-shot methods. DLADiff contains a dual-layer protective mechanism. The first layer provides effective protection against unauthorized fine-tuning by leveraging the proposed Dual-Surrogate Models (DSUR) mechanism and Alternating Dynamic Fine-Tuning (ADFT), which integrates adversarial training with the prior knowledge derived from pre-fine-tuned models. The second layer, though simple in design, demonstrates strong effectiveness in preventing image generation through zero-shot methods. Extensive experimental results demonstrate that our method significantly outperforms existing approaches in defending against fine-tuning of diffusion models and achieves unprecedented performance in protecting against zero-shot generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices</title>
<link>https://arxiv.org/abs/2511.19986</link>
<guid>https://arxiv.org/abs/2511.19986</guid>
<content:encoded><![CDATA[
arXiv:2511.19986v1 Announce Type: cross 
Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Redefining Radar Segmentation: Simultaneous Static-Moving Segmentation and Ego-Motion Estimation using Radar Point Clouds</title>
<link>https://arxiv.org/abs/2511.20003</link>
<guid>https://arxiv.org/abs/2511.20003</guid>
<content:encoded><![CDATA[
arXiv:2511.20003v1 Announce Type: cross 
Abstract: Conventional radar segmentation research has typically focused on learning category labels for different moving objects. Although fundamental differences between radar and optical sensors lead to differences in the reliability of predicting accurate and consistent category labels, a review of common radar perception tasks in automotive reveals that determining whether an object is moving or static is a prerequisite for most tasks. To fill this gap, this study proposes a neural network based solution that can simultaneously segment static and moving objects from radar point clouds. Furthermore, since the measured radial velocity of static objects is correlated with the motion of the radar, this approach can also estimate the instantaneous 2D velocity of the moving platform or vehicle (ego motion). However, despite performing dual tasks, the proposed method employs very simple yet effective building blocks for feature extraction: multi layer perceptrons (MLPs) and recurrent neural networks (RNNs). In addition to being the first of its kind in the literature, the proposed method also demonstrates the feasibility of extracting the information required for the dual task directly from unprocessed point clouds, without the need for cloud aggregation, Doppler compensation, motion compensation, or any other intermediate signal processing steps. To measure its performance, this study introduces a set of novel evaluation metrics and tests the proposed method using a challenging real world radar dataset, RadarScenes. The results show that the proposed method not only performs well on the dual tasks, but also has broad application potential in other radar perception tasks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting</title>
<link>https://arxiv.org/abs/2511.20004</link>
<guid>https://arxiv.org/abs/2511.20004</guid>
<content:encoded><![CDATA[
arXiv:2511.20004v1 Announce Type: cross 
Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents</title>
<link>https://arxiv.org/abs/2511.20216</link>
<guid>https://arxiv.org/abs/2511.20216</guid>
<content:encoded><![CDATA[
arXiv:2511.20216v1 Announce Type: cross 
Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation</title>
<link>https://arxiv.org/abs/2511.20330</link>
<guid>https://arxiv.org/abs/2511.20330</guid>
<content:encoded><![CDATA[
arXiv:2511.20330v1 Announce Type: cross 
Abstract: Interactive articulated manipulation requires long-horizon, multi-step interactions with appliances while maintaining physical consistency. Existing vision-language and diffusion-based policies struggle to generalize across parts, instances, and categories. We first introduce ArtiBench, a five-level benchmark covering kitchen, storage, office, and tool environments. ArtiBench enables structured evaluation from cross-part and cross-instance variation to long-horizon multi-object tasks, revealing the core generalization challenges of articulated object manipulation. Building on this benchmark, we propose ArtiBrain, a modular framework that unifies high-level reasoning with adaptive low-level control. ArtiBrain uses a VLM-based Task Reasoner (GPT-4.1) to decompose and validate subgoals, and employs a Hybrid Controller that combines geometry-aware keyframe execution with affordance-guided diffusion for precise and interpretable manipulation. An Affordance Memory Bank continually accumulates successful execution episodes and propagates part-level actionable affordances to unseen articulated parts and configurations. Extensive experiments on ArtiBench show that our ArtiBrain significantly outperforms state-of-the-art multimodal and diffusion-based methods in robustness and generalization. Code and dataset will be released upon acceptance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning</title>
<link>https://arxiv.org/abs/2511.20422</link>
<guid>https://arxiv.org/abs/2511.20422</guid>
<content:encoded><![CDATA[
arXiv:2511.20422v1 Announce Type: cross 
Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Development of a fully deep learning model to improve the reproducibility of sector classification systems for predicting unerupted maxillary canine likelihood of impaction</title>
<link>https://arxiv.org/abs/2511.20493</link>
<guid>https://arxiv.org/abs/2511.20493</guid>
<content:encoded><![CDATA[
arXiv:2511.20493v1 Announce Type: cross 
Abstract: Objectives. The aim of the present study was to develop a fully deep learning model to reduce the intra- and inter-operator reproducibility of sector classification systems for predicting unerupted maxillary canine likelihood of impaction. Methods. Three orthodontists (Os) and three general dental practitioners (GDPs) classified the position of unerupted maxillary canines on 306 radiographs (T0) according to the three different sector classification systems (5-, 4-, and 3-sector classification system). The assessment was repeated after four weeks (T1). Intra- and inter-observer agreement were evaluated with Cohen's K and Fleiss K, and between group differences with a z-test. The same radiographs were tested on different artificial intelligence (AI) models, pre-trained on an extended dataset of 1,222 radiographs. The best-performing model was identified based on its sensitivity and precision. Results. The 3-sector system was found to be the classification method with highest reproducibility, with an agreement (Cohen's K values) between observations (T0 versus T1) for each examiner ranged from 0.80 to 0.92, and an overall agreement of 0.85 [95% confidence interval (CI) = 0.83-0.87]. The overall inter-observer agreement (Fleiss K) ranged from 0.69 to 0.7. The educational background did not affect either intra- or inter-observer agreement (p>0.05). DenseNet121 proved to be the best-performing model in allocating impacted canines in the three different classes, with an overall accuracy of 76.8%. Conclusion. AI models can be designed to automatically classify the position of unerupted maxillary canines.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.20531</link>
<guid>https://arxiv.org/abs/2511.20531</guid>
<content:encoded><![CDATA[
arXiv:2511.20531v1 Announce Type: cross 
Abstract: Visual Language Models (VLMs) are powerful generative tools but often produce factually in- accurate outputs due to a lack of robust reason- ing capabilities. While extensive research has been conducted on integrating external knowl- edge for reasoning in large language models (LLMs), such efforts remain underexplored in VLMs, where the challenge is compounded by the need to bridge multiple modalities seam- lessly. This work introduces a framework for knowledge-guided reasoning in VLMs, leverag- ing structured knowledge graphs for multi-hop verification using image-captioning task to il- lustrate our framework. Our approach enables systematic reasoning across multiple steps, in- cluding visual entity recognition, knowledge graph traversal, and fact-based caption refine- ment. We evaluate the framework using hi- erarchical, triple-based and bullet-point based knowledge representations, analyzing their ef- fectiveness in factual accuracy and logical infer- ence. Empirical results show that our approach improves factual accuracy by approximately 31% on preliminary experiments on a curated dataset of mixtures from Google Landmarks v2, Conceptual captions and Coco captions re- vealing key insights into reasoning patterns and failure modes. This work demonstrates the po- tential of integrating external knowledge for advancing reasoning in VLMs, paving the way for more reliable and knowledgable multimodal systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Diffusion Inversion Requires Understanding the Latent Space</title>
<link>https://arxiv.org/abs/2511.20592</link>
<guid>https://arxiv.org/abs/2511.20592</guid>
<content:encoded><![CDATA[
arXiv:2511.20592v1 Announce Type: cross 
Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pok\'emon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization of Sums of Bivariate Functions: An Introduction to Relaxation-Based Methods for the Case of Finite Domains</title>
<link>https://arxiv.org/abs/2511.20607</link>
<guid>https://arxiv.org/abs/2511.20607</guid>
<content:encoded><![CDATA[
arXiv:2511.20607v1 Announce Type: cross 
Abstract: We study the optimization of functions with $n>2$ arguments that have a representation as a sum of several functions that have only $2$ of the $n$ arguments each, termed sums of bivariates, on finite domains. The complexity of optimizing sums of bivariates is shown to be NP-equivalent and it is shown that there exists free lunch in the optimization of sums of bivariates. Based on measure-valued extensions of the objective function, so-called relaxations, $\ell^2$-approximation, and entropy-regularization, we derive several tractable problem formulations solvable with linear programming, coordinate ascent as well as with closed-form solutions. The limits of applying tractable versions of such relaxations to sums of bivariates are investigated using general results for reconstructing measures from their bivariate marginals. Experiments in which the derived algorithms are applied to random functions, vertex coloring, and signal reconstruction problems provide insights into qualitatively different function classes that can be modeled as sums of bivariates.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Image Stitching Using Depth Maps</title>
<link>https://arxiv.org/abs/2202.06276</link>
<guid>https://arxiv.org/abs/2202.06276</guid>
<content:encoded><![CDATA[
arXiv:2202.06276v3 Announce Type: replace 
Abstract: Natural image stitching aims to create a single, natural-looking mosaic from overlapped images that capture the same 3D scene from different viewing positions. Challenges inevitably arise when the scene is non-planar and captured by handheld cameras since parallax is non-negligible in such cases. In this paper, we propose a novel image stitching method using depth maps, which generates accurate alignment mosaics against parallax. Firstly, we construct a robust fitting method to filter out the outliers in feature matches and estimate the epipolar geometry between input images. Then, we utilize epipolar geometry to establish pixel-to-pixel correspondences between the input images and render the warped images using the proposed optimal warping. In the rendering stage, we introduce several modules to solve the mapping artifacts in the warping results and generate the final mosaic. Experimental results on three challenging datasets demonstrate that the depth maps of input images enable our method to provide much more accurate alignment in the overlapping region and view-consistent results in the non-overlapping region. We believe our method will continue to work under the rapid progress of monocular depth estimation. The source code is available at https://github.com/tlliao/NIS_depths.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Learning Paradigm for Facial Expression Recognition</title>
<link>https://arxiv.org/abs/2209.15402</link>
<guid>https://arxiv.org/abs/2209.15402</guid>
<content:encoded><![CDATA[
arXiv:2209.15402v3 Announce Type: replace 
Abstract: Due to the subjective crowdsourcing annotations and the inherent inter-class similarity of facial expressions, the real-world Facial Expression Recognition (FER) datasets usually exhibit ambiguous annotation. To simplify the learning paradigm, most previous methods convert ambiguous annotation results into precise one-hot annotations and train FER models in an end-to-end supervised manner. In this paper, we rethink the existing training paradigm and propose that it is better to use weakly supervised strategies to train FER models with original ambiguous annotation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SD-MVS: Segmentation-Driven Deformation Multi-View Stereo with Spherical Refinement and EM optimization</title>
<link>https://arxiv.org/abs/2401.06385</link>
<guid>https://arxiv.org/abs/2401.06385</guid>
<content:encoded><![CDATA[
arXiv:2401.06385v2 Announce Type: replace 
Abstract: In this paper, we introduce Segmentation-Driven Deformation Multi-View Stereo (SD-MVS), a method that can effectively tackle challenges in 3D reconstruction of textureless areas. We are the first to adopt the Segment Anything Model (SAM) to distinguish semantic instances in scenes and further leverage these constraints for pixelwise patch deformation on both matching cost and propagation. Concurrently, we propose a unique refinement strategy that combines spherical coordinates and gradient descent on normals and pixelwise search interval on depths, significantly improving the completeness of reconstructed 3D model. Furthermore, we adopt the Expectation-Maximization (EM) algorithm to alternately optimize the aggregate matching cost and hyperparameters, effectively mitigating the problem of parameters being excessively dependent on empirical tuning. Evaluations on the ETH3D high-resolution multi-view stereo benchmark and the Tanks and Temples dataset demonstrate that our method can achieve state-of-the-art results with less time consumption.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMT: Effective Global Framework for Multi-Camera Multi-Target Tracking</title>
<link>https://arxiv.org/abs/2407.01007</link>
<guid>https://arxiv.org/abs/2407.01007</guid>
<content:encoded><![CDATA[
arXiv:2407.01007v2 Announce Type: replace 
Abstract: Multi-Camera Multi-Target (MCMT) tracking aims to locate and associate the same targets across multiple camera views. Existing methods typically adopt a two-stage framework, involving single-camera tracking followed by inter-camera tracking. However, in this paradigm, multi-view information is used only to recover missed matches in the first stage, providing a limited contribution to overall tracking. To address this issue, we propose GMT, a global MCMT tracking framework that jointly exploits intra-view and inter-view cues for tracking. Specifically, instead of assigning trajectories independently for each view, we integrate the same historical targets across different views as global trajectories, thereby reformulating the two-stage tracking as a unified global-level trajectory-target association process. We introduce a Cross-View Feature Consistency Enhancement (CFCE) module to align visual and spatial features across views, providing a consistent feature space for global trajectory modeling. With these aligned features, the Global Trajectory Association (GTA) module associates new detections with existing global trajectories, enabling direct use of multi-view information. Compared to the two-stage framework, GMT achieves significant improvements on existing datasets, with gains of up to 21.3 percent in CVMA and 17.2 percent in CVIDF1. Furthermore, we introduce VisionTrack, a high-quality, large-scale MCMT dataset providing significantly greater diversity than existing datasets. Our code and dataset will be released.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence</title>
<link>https://arxiv.org/abs/2407.16655</link>
<guid>https://arxiv.org/abs/2407.16655</guid>
<content:encoded><![CDATA[
arXiv:2407.16655v3 Announce Type: replace 
Abstract: Recent advancements in video generation have primarily leveraged diffusion models for short-duration content. However, these approaches often fall short in modeling complex narratives and maintaining character consistency over extended periods, which is essential for long-form video production like movies. We propose MovieDreamer, a novel hierarchical framework that integrates the strengths of autoregressive models with diffusion-based rendering to pioneer long-duration video generation with intricate plot progressions and high visual fidelity. Our approach utilizes autoregressive models for global narrative coherence, predicting sequences of visual tokens that are subsequently transformed into high-quality video frames through diffusion rendering. This method is akin to traditional movie production processes, where complex stories are factorized down into manageable scene capturing. Further, we employ a multimodal script that enriches scene descriptions with detailed character information and visual style, enhancing continuity and character identity across scenes. We present extensive experiments across various movie genres, demonstrating that our approach not only achieves superior visual and narrative quality but also effectively extends the duration of generated content significantly beyond current capabilities. Homepage: https://aim-uofa.github.io/MovieDreamer/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E$^{3}$NeRF: Efficient Event-Enhanced Neural Radiance Fields from Blurry Images</title>
<link>https://arxiv.org/abs/2408.01840</link>
<guid>https://arxiv.org/abs/2408.01840</guid>
<content:encoded><![CDATA[
arXiv:2408.01840v2 Announce Type: replace 
Abstract: Neural Radiance Fields (NeRF) achieves impressive novel view rendering performance by learning implicit 3D representation from sparse view images. However, it is difficult to reconstruct a sharp NeRF from blurry input that often occurs in the wild. To solve this problem, we propose a novel Efficient Event-Enhanced NeRF (E$^{3}$NeRF), reconstructing sharp NeRF by utilizing both blurry images and corresponding event streams. A blur rendering loss and an event rendering loss are introduced, which guide the NeRF training via modeling the physical image motion blur process and event generation process, respectively. To improve the efficiency of the framework, we further leverage the latent spatial-temporal blur information in the event stream to evenly distribute training over temporal blur and focus training on spatial blur. Moreover, a camera pose estimation framework for real-world data is built with the guidance of the events, generalizing the method to more practical applications. Compared to previous image-based and event-based NeRF works, our framework makes more profound use of the internal relationship between events and images. Extensive experiments on both synthetic data and real-world data demonstrate that E\textsuperscript{3}NeRF can effectively learn a sharp NeRF from blurry images, especially for high-speed non-uniform motion and low-light scenes.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenScan: A Benchmark for Generalized Open-Vocabulary 3D Scene Understanding</title>
<link>https://arxiv.org/abs/2408.11030</link>
<guid>https://arxiv.org/abs/2408.11030</guid>
<content:encoded><![CDATA[
arXiv:2408.11030v4 Announce Type: replace 
Abstract: Open-vocabulary 3D scene understanding (OV-3D) aims to localize and classify novel objects beyond the closed set of object classes. However, existing approaches and benchmarks primarily focus on the open vocabulary problem within the context of object classes, which is insufficient in providing a holistic evaluation to what extent a model understands the 3D scene. In this paper, we introduce a more challenging task called Generalized Open-Vocabulary 3D Scene Understanding (GOV-3D) to explore the open vocabulary problem beyond object classes. It encompasses an open and diverse set of generalized knowledge, expressed as linguistic queries of fine-grained and object-specific attributes. To this end, we contribute a new benchmark named \textit{OpenScan}, which consists of 3D object attributes across eight representative linguistic aspects, including affordance, property, and material. We further evaluate state-of-the-art OV-3D methods on our OpenScan benchmark and discover that these methods struggle to comprehend the abstract vocabularies of the GOV-3D task, a challenge that cannot be addressed simply by scaling up object classes during training. We highlight the limitations of existing methodologies and explore promising directions to overcome the identified shortcomings.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?</title>
<link>https://arxiv.org/abs/2411.10979</link>
<guid>https://arxiv.org/abs/2411.10979</guid>
<content:encoded><![CDATA[
arXiv:2411.10979v5 Announce Type: replace 
Abstract: The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts. We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations. VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compositions and offers insights into areas for further improvement. The leaderboard and evaluation code are available at https://yunlong10.github.io/VidComposition/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes</title>
<link>https://arxiv.org/abs/2412.05700</link>
<guid>https://arxiv.org/abs/2412.05700</guid>
<content:encoded><![CDATA[
arXiv:2412.05700v2 Announce Type: replace 
Abstract: Recent advancements in high-fidelity dynamic scene reconstruction have leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene representation. However, to make these methods viable for real-time applications such as AR/VR, gaming, and rendering on low-power devices, substantial reductions in memory usage and improvements in rendering efficiency are required. While many state-of-the-art methods prioritize lightweight implementations, they struggle in handling {scenes with complex motions or long sequences}. In this work, we introduce Temporally Compressed 3D Gaussian Splatting (TC3DGS), a novel technique designed specifically to effectively compress dynamic 3D Gaussian representations. TC3DGS selectively prunes Gaussians based on their temporal relevance and employs gradient-aware mixed-precision quantization to dynamically compress Gaussian parameters. In addition, TC3DGS exploits an adapted version of the Ramer-Douglas-Peucker algorithm to further reduce storage by interpolating Gaussian trajectories across frames. Our experiments on multiple datasets demonstrate that TC3DGS achieves up to 67$\times$ compression with minimal or no degradation in visual quality. More results and videos are provided in the supplementary. Project Page: https://ahmad-jarrar.github.io/tc-3dgs/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DVP-MVS: Synergize Depth-Edge and Visibility Prior for Multi-View Stereo</title>
<link>https://arxiv.org/abs/2412.11578</link>
<guid>https://arxiv.org/abs/2412.11578</guid>
<content:encoded><![CDATA[
arXiv:2412.11578v3 Announce Type: replace 
Abstract: Patch deformation-based methods have recently exhibited substantial effectiveness in multi-view stereo, due to the incorporation of deformable and expandable perception to reconstruct textureless areas. However, such approaches typically focus on exploring correlative reliable pixels to alleviate match ambiguity during patch deformation, but ignore the deformation instability caused by mistaken edge-skipping and visibility occlusion, leading to potential estimation deviation. To remedy the above issues, we propose DVP-MVS, which innovatively synergizes depth-edge aligned and cross-view prior for robust and visibility-aware patch deformation. Specifically, to avoid unexpected edge-skipping, we first utilize Depth Anything V2 followed by the Roberts operator to initialize coarse depth and edge maps respectively, both of which are further aligned through an erosion-dilation strategy to generate fine-grained homogeneous boundaries for guiding patch deformation. In addition, we reform view selection weights as visibility maps and restore visible areas by cross-view depth reprojection, then regard them as cross-view prior to facilitate visibility-aware patch deformation. Finally, we improve propagation and refinement with multi-view geometry consistency by introducing aggregated visible hemispherical normals based on view selection and local projection depth differences based on epipolar lines, respectively. Extensive evaluations on ETH3D and Tanks & Temples benchmarks demonstrate that our method can achieve state-of-the-art performance with excellent robustness and generalization.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Personalized Generative Low-light Image Denoising and Enhancement</title>
<link>https://arxiv.org/abs/2412.14327</link>
<guid>https://arxiv.org/abs/2412.14327</guid>
<content:encoded><![CDATA[
arXiv:2412.14327v3 Announce Type: replace 
Abstract: Modern cameras' performance in low-light conditions remains suboptimal due to fundamental limitations in photon shot noise and sensor read noise. Generative image restoration methods have shown promising results compared to traditional approaches, but they suffer from hallucinatory content generation when the signal-to-noise ratio (SNR) is low. Leveraging the availability of personalized photo galleries of the users, we introduce Diffusion-based Personalized Generative Denoising (DiffPGD), a new approach that builds a customized diffusion model for individual users. Our key innovation lies in the development of an identity-consistent physical buffer that extracts the physical attributes of the person from the gallery. This ID-consistent physical buffer serves as a robust prior that can be seamlessly integrated into the diffusion model to restore degraded images without the need for fine-tuning. Over a wide range of low-light testing scenarios, we show that DiffPGD achieves superior image denoising and enhancement performance compared to existing diffusion-based denoising approaches. Our project page can be found at \href{https://genai-restore.github.io/DiffPGD/}{\textcolor{purple}{\textbf{https://genai-restore.github.io/DiffPGD/}}}.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LiHi-GS: LiDAR-Supervised Gaussian Splatting for Highway Driving Scene Reconstruction</title>
<link>https://arxiv.org/abs/2412.15447</link>
<guid>https://arxiv.org/abs/2412.15447</guid>
<content:encoded><![CDATA[
arXiv:2412.15447v3 Announce Type: replace 
Abstract: Photorealistic 3D scene reconstruction plays an important role in autonomous driving, enabling the generation of novel data from existing datasets to simulate safety-critical scenarios and expand training data without additional acquisition costs. Gaussian Splatting (GS) facilitates real-time, photorealistic rendering with an explicit 3D Gaussian representation of the scene, providing faster processing and more intuitive scene editing than the implicit Neural Radiance Fields (NeRFs). While extensive GS research has yielded promising advancements in autonomous driving applications, they overlook two critical aspects: First, existing methods mainly focus on low-speed and feature-rich urban scenes and ignore the fact that highway scenarios play a significant role in autonomous driving. Second, while LiDARs are commonplace in autonomous driving platforms, existing methods learn primarily from images and use LiDAR only for initial estimates or without precise sensor modeling, thus missing out on leveraging the rich depth information LiDAR offers and limiting the ability to synthesize LiDAR data. In this paper, we propose a novel GS method for dynamic scene synthesis and editing with improved scene reconstruction through LiDAR supervision and support for LiDAR rendering. Unlike prior works that are tested mostly on urban datasets, to the best of our knowledge, we are the first to focus on the more challenging and highly relevant highway scenes for autonomous driving, with sparse sensor views and monotone backgrounds. Visit our project page at: https://umautobots.github.io/lihi_gs
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation</title>
<link>https://arxiv.org/abs/2412.21059</link>
<guid>https://arxiv.org/abs/2412.21059</guid>
<content:encoded><![CDATA[
arXiv:2412.21059v3 Announce Type: replace 
Abstract: Visual generative models have achieved remarkable progress in synthesizing photorealistic images and videos, yet aligning their outputs with human preferences across critical dimensions remains a persistent challenge. Though reinforcement learning from human feedback offers promise for preference alignment, existing reward models for visual generation face limitations, including black-box scoring without interpretability and potentially resultant unexpected biases. We present VisionReward, a general framework for learning human visual preferences in both image and video generation. Specifically, we employ a hierarchical visual assessment framework to capture fine-grained human preferences, and leverages linear weighting to enable interpretable preference learning. Furthermore, we propose a multi-dimensional consistent strategy when using VisionReward as a reward model during preference optimization for visual generation. Experiments show that VisionReward can significantly outperform existing image and video reward models on both machine metrics and human evaluation. Notably, VisionReward surpasses VideoScore by 17.2% in preference prediction accuracy, and text-to-video models with VisionReward achieve a 31.6% higher pairwise win rate compared to the same models using VideoScore. All code and datasets are provided at https://github.com/THUDM/VisionReward.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI for Cel-Animation: A Survey</title>
<link>https://arxiv.org/abs/2501.06250</link>
<guid>https://arxiv.org/abs/2501.06250</guid>
<content:encoded><![CDATA[
arXiv:2501.06250v5 Announce Type: replace 
Abstract: Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, challenges like visual consistency, stylistic coherence, and ethical considerations persist. Additionally, this paper explores future directions and advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment</title>
<link>https://arxiv.org/abs/2501.17690</link>
<guid>https://arxiv.org/abs/2501.17690</guid>
<content:encoded><![CDATA[
arXiv:2501.17690v4 Announce Type: replace 
Abstract: We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness</title>
<link>https://arxiv.org/abs/2502.17159</link>
<guid>https://arxiv.org/abs/2502.17159</guid>
<content:encoded><![CDATA[
arXiv:2502.17159v4 Announce Type: replace 
Abstract: Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter-efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, few methods are dedicated to efficient merging, and existing methods designed for full fine-tuning merging fail under efficient merging. To address the issue, we analyze from low-rank decomposition and reveal that direction robustness during merging is crucial for merging efficient modules. We furthermore uncover that compensating for the gap between stark singular values contributes to direction robustness. Therefore, we propose RobustMerge, a training-free parameter-efficient merging method with complementary parameter adaptation to maintain direction robustness. Specifically, we (1) prune parameters and scale coefficients from inter-parameter relation for singular values to maintain direction stability away from task interference, and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certify the outstanding performance and generalizability of our method. Additional studies and extensive analyses further showcase the effectiveness. Code is available at https://github.com/AuroraZengfh/RobustMerge.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Two-Stage Referring-by-Tracking in Referring Multi-Object Tracking: Make it Strong Again</title>
<link>https://arxiv.org/abs/2503.07516</link>
<guid>https://arxiv.org/abs/2503.07516</guid>
<content:encoded><![CDATA[
arXiv:2503.07516v4 Announce Type: replace 
Abstract: Referring Multi-Object Tracking (RMOT) aims to track multiple objects specified by natural language expressions in videos. With the recent significant progress of one-stage methods, the two-stage Referring-by-Tracking (RBT) paradigm has gradually lost its popularity. However, its lower training cost and flexible incremental deployment remain irreplaceable. Rethinking existing two-stage RBT frameworks, we identify two fundamental limitations: the overly heuristic feature construction and fragile correspondence modeling. To address these issues, we propose FlexHook, a novel two-stage RBT framework. In FlexHook, the proposed Conditioning Hook (C-Hook) redefines the feature construction by a sampling-based strategy and language-conditioned cue injection. Then, we introduce a Pairwise Correspondence Decoder (PCD) that replaces CLIP-based similarity matching with active correspondence modeling, yielding a more flexible and robust strategy. Extensive experiments on multiple benchmarks (Refer-KITTI/v2, Refer-Dance, and LaMOT) demonstrate that FlexHook becomes the first two-stage RBT approach to comprehensively outperform current state-of-the-art methods. Code can be found in the Supplementary Materials.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embodied Crowd Counting</title>
<link>https://arxiv.org/abs/2503.08367</link>
<guid>https://arxiv.org/abs/2503.08367</guid>
<content:encoded><![CDATA[
arXiv:2503.08367v2 Announce Type: replace 
Abstract: Occlusion is one of the fundamental challenges in crowd counting. In the community, various data-driven approaches have been developed to address this issue, yet their effectiveness is limited. This is mainly because most existing crowd counting datasets on which the methods are trained are based on passive cameras, restricting their ability to fully sense the environment. Recently, embodied navigation methods have shown significant potential in precise object detection in interactive scenes. These methods incorporate active camera settings, holding promise in addressing the fundamental issues in crowd counting. However, most existing methods are designed for indoor navigation, showing unknown performance in analyzing complex object distribution in large scale scenes, such as crowds. Besides, most existing embodied navigation datasets are indoor scenes with limited scale and object quantity, preventing them from being introduced into dense crowd analysis. Based on this, a novel task, Embodied Crowd Counting (ECC), is proposed. We first build up an interactive simulator, Embodied Crowd Counting Dataset (ECCD), which enables large scale scenes and large object quantity. A prior probability distribution that approximates realistic crowd distribution is introduced to generate crowds. Then, a zero-shot navigation method (ZECC) is proposed. This method contains a MLLM driven coarse-to-fine navigation mechanism, enabling active Z-axis exploration, and a normal-line-based crowd distribution analysis method for fine counting. Experimental results against baselines show that the proposed method achieves the best trade-off between counting accuracy and navigation cost.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FaVChat: Hierarchical Prompt-Query Guided Facial Video Understanding with Data-Efficient GRPO</title>
<link>https://arxiv.org/abs/2503.09158</link>
<guid>https://arxiv.org/abs/2503.09158</guid>
<content:encoded><![CDATA[
arXiv:2503.09158v3 Announce Type: replace 
Abstract: Multi-modal large language models (MLLMs) have shown strong capability in video understanding but still struggle with fine-grained visual comprehension, as pure visual encoders often lose subtle cues essential for precise reasoning. To address this limitation, we propose FaVChat, a Video-MLLM specifically designed for fine-grained facial understanding. FaVChat introduces a multi-level prompt-guided feature extraction mechanism that progressively captures task-relevant information from three complementary stages: low-level transformer layers for textures and motion, medium-level learnable queries for discriminative regions, and high-level adaptive feature weighting for semantic alignment. These enriched features are dynamically fused and fed into the LLM to enable more accurate fine-grained reasoning. To further enhance the model's ability to capture fine-grained facial attributes and maximize the utility of limited data, we propose Date-Efficient GRPO, a novel data-efficient reinforcement learning (RL) algorithm that maximizes the utility of each training sample through per-instance utility estimation and dynamic lifecycle scheduling. Extensive zero-shot evaluations across emotion recognition, explainable reasoning, and textual expression analysis demonstrate that FaVChat achieves finer-grained understanding, stronger accuracy, and better generalization than existing Video-MLLMs, even when trained with only 10K RL samples.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dream-IF: Dynamic Relative EnhAnceMent for Image Fusion</title>
<link>https://arxiv.org/abs/2503.10109</link>
<guid>https://arxiv.org/abs/2503.10109</guid>
<content:encoded><![CDATA[
arXiv:2503.10109v2 Announce Type: replace 
Abstract: Image fusion aims to integrate comprehensive information from images acquired through multiple sources. However, images captured by diverse sensors often encounter various degradations that can negatively affect fusion quality. Traditional fusion methods generally treat image enhancement and fusion as separate processes, overlooking the inherent correlation between them; notably, the dominant regions in one modality of a fused image often indicate areas where the other modality might benefit from enhancement. Inspired by this observation, we introduce the concept of dominant regions for image enhancement and present a Dynamic Relative EnhAnceMent framework for Image Fusion (Dream-IF). This framework quantifies the relative dominance of each modality across different layers and leverages this information to facilitate reciprocal cross-modal enhancement. By integrating the relative dominance derived from image fusion, our approach supports not only image restoration but also a broader range of image enhancement applications. Furthermore, we employ prompt-based encoding to capture degradation-specific details, which dynamically steer the restoration process and promote coordinated enhancement in both multi-modal image fusion and image enhancement scenarios. Extensive experimental results demonstrate that Dream-IF consistently outperforms its counterparts. The code is publicly available.\footnote{ https://github.com/jehovahxu/Dream-IF
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stitch-a-Demo: Video Demonstrations from Multistep Descriptions</title>
<link>https://arxiv.org/abs/2503.13821</link>
<guid>https://arxiv.org/abs/2503.13821</guid>
<content:encoded><![CDATA[
arXiv:2503.13821v2 Announce Type: replace 
Abstract: When obtaining visual illustrations from text descriptions, today's methods take a description with a single text context - a caption, or an action description - and retrieve or generate the matching visual context. However, prior work does not permit visual illustration of multistep descriptions, e.g. a cooking recipe or a gardening instruction manual, and simply handling each step description in isolation would result in an incoherent demonstration. We propose Stitch-a-Demo, a novel retrieval-based method to assemble a video demonstration from a multistep description. The resulting video contains clips, possibly from different sources, that accurately reflect all the step descriptions, while being visually coherent. We formulate a training pipeline that creates large-scale weakly supervised data containing diverse procedures and injects hard negatives that promote both correctness and coherence. Validated on in-the-wild instructional videos, Stitch-a-Demo achieves state-of-the-art performance, with gains up to 29% as well as dramatic wins in a human preference study.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Panoramic Distortion-Aware Tokenization for Person Detection and Localization in Overhead Fisheye Images</title>
<link>https://arxiv.org/abs/2503.14228</link>
<guid>https://arxiv.org/abs/2503.14228</guid>
<content:encoded><![CDATA[
arXiv:2503.14228v3 Announce Type: replace 
Abstract: Person detection in overhead fisheye images is challenging due to person rotation and small persons. Prior work has mainly addressed person rotation, leaving the small-person problem underexplored. We remap fisheye images to equirectangular panoramas to handle rotation and exploit panoramic geometry to handle small persons more effectively. Conventional detection methods tend to favor larger persons because they dominate the attention maps, causing smaller persons to be missed. In hemispherical equirectangular panoramas, we find that apparent person height decreases approximately linearly with the vertical angle near the top of the image. Using this finding, we introduce panoramic distortion-aware tokenization to enhance the detection of small persons. This tokenization procedure divides panoramic features using self-similar figures that enable the determination of optimal divisions without gaps, and we leverage the maximum significance values in each tile of the token groups to preserve the significance areas of smaller persons. We propose a transformer-based person detection and localization method that combines panoramic-image remapping and the tokenization procedure. Extensive experiments demonstrated that our method outperforms conventional methods on large-scale datasets.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExDDV: A New Dataset for Explainable Deepfake Detection in Video</title>
<link>https://arxiv.org/abs/2503.14421</link>
<guid>https://arxiv.org/abs/2503.14421</guid>
<content:encoded><![CDATA[
arXiv:2503.14421v2 Announce Type: replace 
Abstract: The ever growing realism and quality of generated videos makes it increasingly harder for humans to spot deepfake content, who need to rely more and more on automatic deepfake detectors. However, deepfake detectors are also prone to errors, and their decisions are not explainable, leaving humans vulnerable to deepfake-based fraud and misinformation. To this end, we introduce ExDDV, the first dataset and benchmark for Explainable Deepfake Detection in Video. ExDDV comprises around 5.4K real and deepfake videos that are manually annotated with text descriptions (to explain the artifacts) and clicks (to point out the artifacts). We evaluate a number of vision-language models on ExDDV, performing experiments with various fine-tuning and in-context learning strategies. Our results show that text and click supervision are both required to develop robust explainable models for deepfake videos, which are able to localize and describe the observed artifacts. Our novel dataset and code to reproduce the results are available at https://github.com/vladhondru25/ExDDV.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Efficient Fuse-and-Refine for Feed-Forward 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2503.14698</link>
<guid>https://arxiv.org/abs/2503.14698</guid>
<content:encoded><![CDATA[
arXiv:2503.14698v2 Announce Type: replace 
Abstract: Recent advances in feed-forward 3D Gaussian Splatting have led to rapid improvements in efficient scene reconstruction from sparse views. However, most existing approaches construct Gaussian primitives directly aligned with the pixels in one or more of the input images. This leads to redundancies in the representation when input views overlap and constrains the position of the primitives to lie along the input rays without full flexibility in 3D space. Moreover, these pixel-aligned approaches do not naturally generalize to dynamic scenes, where effectively leveraging temporal information requires resolving both redundant and newly appearing content across frames. To address these limitations, we introduce a novel Fuse-and-Refine module that enhances existing feed-forward models by merging and refining the primitives in a canonical 3D space. At the core of our method is an efficient hybrid Splat-Voxel representation: from an initial set of pixel-aligned Gaussian primitives, we aggregate local features into a coarse-to-fine voxel hierarchy, and then use a sparse voxel transformer to process these voxel features and generate refined Gaussian primitives. By fusing and refining an arbitrary number of inputs into a consistent set of primitives, our representation effectively reduces redundancy and naturally adapts to temporal frames, enabling history-aware online reconstruction of dynamic scenes. Our approach achieves state-of-the-art performance in both static and streaming scene reconstructions while running at interactive rates (15 fps with 350ms delay) on a single H100 GPU.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLIP-IT: CLIP-based Pairing for Histology Images Classification</title>
<link>https://arxiv.org/abs/2504.16181</link>
<guid>https://arxiv.org/abs/2504.16181</guid>
<content:encoded><![CDATA[
arXiv:2504.16181v5 Announce Type: replace 
Abstract: Multimodal learning has shown promise in medical imaging, combining complementary modalities like images and text. Vision-language models (VLMs) capture rich diagnostic cues but often require large paired datasets and prompt- or text-based inference, limiting their practicality due to annotation cost, privacy, and compute demands. Crucially, available free unpaired external text, like pathology reports, can still provide complementary diagnostic cues if semantically relevant content is retrievable per image. To address this, we introduce CLIP-IT, a novel framework that relies on rich unpaired text reports. Specifically, CLIP-IT uses a CLIP model pre-trained on histology image-text pairs from a separate dataset to retrieve the most relevant unpaired textual report for each image in the downstream unimodal dataset. These reports, sourced from the same disease domain and tissue type, form pseudo-pairs that reflect shared clinical semantics rather than exact alignment. Knowledge from these texts is distilled into the vision model during training, while LoRA-based adaptation mitigates the semantic gap between unaligned modalities. At inference, only the vision model is used, keeping overhead low while still benefiting from multimodal training without requiring paired data in the downstream dataset. Experiments on histology image datasets confirm that CLIP-IT consistently improves classification accuracy over both unimodal and multimodal CLIP-based baselines in most cases, without the burden of per-dataset paired annotation or inference-time complexity.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Convolutional Neural Networks for Rice Grain Classification: An Explainable AI Approach</title>
<link>https://arxiv.org/abs/2505.05513</link>
<guid>https://arxiv.org/abs/2505.05513</guid>
<content:encoded><![CDATA[
arXiv:2505.05513v4 Announce Type: replace 
Abstract: Rice is an essential staple food worldwide that is important in promoting international trade, economic growth, and nutrition. Asian countries such as China, India, Pakistan, Thailand, Vietnam, and Indonesia are notable for their significant contribution to the cultivation and utilization of rice. These nations are also known for cultivating different rice grains, including short and long grains. These sizes are further classified as basmati, jasmine, kainat saila, ipsala, arborio, etc., catering to diverse culinary preferences and cultural traditions. For both local and international trade, inspecting and maintaining the quality of rice grains to satisfy customers and preserve a country's reputation is necessary. Manual quality check and classification is quite a laborious and time-consuming process. It is also highly prone to mistakes. Therefore, an automatic solution must be proposed for the effective and efficient classification of different varieties of rice grains. This research paper presents an automatic framework based on a convolutional neural network (CNN) for classifying different varieties of rice grains. We evaluated the proposed model based on performance metrics such as accuracy, recall, precision, and F1-Score. The CNN model underwent rigorous training and validation, achieving a remarkable accuracy rate and a perfect area under each class's Receiver Operating Characteristic (ROC) curve. The confusion matrix analysis confirmed the model's effectiveness in distinguishing between the different rice varieties, indicating minimal misclassifications. Additionally, the integration of explainability techniques such as LIME (Local Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive exPlanations) provided valuable insights into the model's decision-making process, revealing how specific features of the rice grains influenced classification outcomes.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration</title>
<link>https://arxiv.org/abs/2505.11895</link>
<guid>https://arxiv.org/abs/2505.11895</guid>
<content:encoded><![CDATA[
arXiv:2505.11895v2 Announce Type: replace 
Abstract: Recent unified multi-modal encoders align a wide range of modalities into a shared representation space, enabling diverse cross-modal tasks. Despite their impressive capabilities, the robustness of these models under adversarial perturbations remains underexplored, which is a critical concern for safety-sensitive applications. In this work, we present the first comprehensive study of adversarial vulnerability in unified multi-modal encoders. We find that even mild adversarial perturbations lead to substantial performance drops across all modalities. Non-visual inputs, such as audio and point clouds, are especially fragile, while visual inputs like images and videos also degrade significantly. To address this, we propose an efficient adversarial calibration framework that improves robustness across modalities without modifying pretrained encoders or semantic centers, ensuring compatibility with existing foundation models. Our method introduces modality-specific projection heads trained solely on adversarial examples, while keeping the backbone and embeddings frozen. We explore three training objectives: fixed-center cross-entropy, clean-to-adversarial L2 alignment, and clean-adversarial InfoNCE, and we introduce a regularization strategy to ensure modality-consistent alignment under attack. Experiments on six modalities and three Bind-style models show that our method improves adversarial robustness by up to 47.3 percent at epsilon = 4/255, while preserving or even improving clean zero-shot and retrieval performance with less than 1 percent trainable parameters.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.13344</link>
<guid>https://arxiv.org/abs/2505.13344</guid>
<content:encoded><![CDATA[
arXiv:2505.13344v2 Announce Type: replace 
Abstract: We propose RoPECraft, a training-free video motion transfer method for diffusion transformers that operates solely by modifying their rotary positional embeddings (RoPE). We first extract dense optical flow from a reference video, and utilize the resulting motion offsets to warp the complex-exponential tensors of RoPE, effectively encoding motion into the generation process. These embeddings are then further optimized during denoising time steps via trajectory alignment between the predicted and target velocities using a flow-matching objective. To keep the output faithful to the text prompt and prevent duplicate generations, we incorporate a regularization term based on the phase components of the reference video's Fourier transform, projecting the phase angles onto a smooth manifold to suppress high-frequency artifacts. Experiments on benchmarks reveal that RoPECraft outperforms all recently published methods, both qualitatively and quantitatively.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Panoptic Captioning: An Equivalence Bridge for Image and Text</title>
<link>https://arxiv.org/abs/2505.16334</link>
<guid>https://arxiv.org/abs/2505.16334</guid>
<content:encoded><![CDATA[
arXiv:2505.16334v3 Announce Type: replace 
Abstract: This work introduces panoptic captioning, a novel task striving to seek the minimum text equivalent of images, which has broad potential applications. We take the first step towards panoptic captioning by formulating it as a task of generating a comprehensive textual description for an image, which encapsulates all entities, their respective locations and attributes, relationships among entities, as well as global image state. Through an extensive evaluation, our work reveals that state-of-the-art Multi-modal Large Language Models (MLLMs) have limited performance in solving panoptic captioning. To address this, we propose an effective data engine named PancapEngine to produce high-quality data and a novel method named PancapChain to improve panoptic captioning. Specifically, our PancapEngine first detects diverse categories of entities in images by an elaborate detection suite, and then generates required panoptic captions using entity-aware prompts. Additionally, our PancapChain explicitly decouples the challenging panoptic captioning task into multiple stages and generates panoptic captions step by step. More importantly, we contribute a comprehensive metric named PancapScore and a human-curated test set for reliable model evaluation. Experiments show that our PancapChain-13B model can beat state-of-the-art open-source MLLMs like InternVL-2.5-78B and even surpass proprietary models like GPT-4o and Gemini-2.0-Pro, demonstrating the effectiveness of our data engine and method. Project page: https://visual-ai.github.io/pancap/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M2SVid: End-to-End Inpainting and Refinement for Monocular-to-Stereo Video Conversion</title>
<link>https://arxiv.org/abs/2505.16565</link>
<guid>https://arxiv.org/abs/2505.16565</guid>
<content:encoded><![CDATA[
arXiv:2505.16565v2 Announce Type: replace 
Abstract: We tackle the problem of monocular-to-stereo video conversion and propose a novel architecture for inpainting and refinement of the warped right view obtained by depth-based reprojection of the input left view. We extend the Stable Video Diffusion (SVD) model to utilize the input left video, the warped right video, and the disocclusion masks as conditioning input to generate a high-quality right camera view. In order to effectively exploit information from neighboring frames for inpainting, we modify the attention layers in SVD to compute full attention for discoccluded pixels. Our model is trained to generate the right view video in an end-to-end manner without iterative diffusion steps by minimizing image space losses to ensure high-quality generation. Our approach outperforms previous state-of-the-art methods, being ranked best 2.6x more often than the second-place method in a user study, while being 6x faster.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TK-Mamba: Marrying KAN With Mamba for Text-Driven 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2505.18525</link>
<guid>https://arxiv.org/abs/2505.18525</guid>
<content:encoded><![CDATA[
arXiv:2505.18525v2 Announce Type: replace 
Abstract: 3D medical image segmentation is important for clinical diagnosis and treatment but faces challenges from high-dimensional data and complex spatial dependencies. Traditional single-modality networks, such as CNNs and Transformers, are often limited by computational inefficiency and constrained contextual modeling in 3D settings. To alleviate these limitations, we propose TK-Mamba, a multimodal framework that fuses the linear-time Mamba with Kolmogorov-Arnold Networks (KAN) to form an efficient hybrid backbone. Our approach is characterized by two primary technical contributions. Firstly, we introduce the novel 3D-Group-Rational KAN (3D-GR-KAN), which marks the first application of KAN in 3D medical imaging, providing a superior and computationally efficient nonlinear feature transformation crucial for complex volumetric structures. Secondly, we devise a dual-branch text-driven strategy using Pubmedclip's embeddings. This strategy significantly enhances segmentation robustness and accuracy by simultaneously capturing inter-organ semantic relationships to mitigate label inconsistencies and aligning image features with anatomical texts. By combining this advanced backbone and vision-language knowledge, TK-Mamba offers a unified and scalable solution for both multi-organ and tumor segmentation. Experiments on multiple datasets demonstrate that our framework achieves state-of-the-art performance in both organ and tumor segmentation tasks, surpassing existing methods in both accuracy and efficiency. Our code is publicly available at https://github.com/yhy-whu/TK-Mamba
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Localizing Knowledge in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2505.18832</link>
<guid>https://arxiv.org/abs/2505.18832</guid>
<content:encoded><![CDATA[
arXiv:2505.18832v2 Announce Type: replace 
Abstract: Understanding how knowledge is distributed across the layers of generative models is crucial for improving interpretability, controllability, and adaptation. While prior work has explored knowledge localization in UNet-based architectures, Diffusion Transformer (DiT)-based models remain underexplored in this context. In this paper, we propose a model- and knowledge-agnostic method to localize where specific types of knowledge are encoded within the DiT blocks. We evaluate our method on state-of-the-art DiT-based models, including PixArt-alpha, FLUX, and SANA, across six diverse knowledge categories. We show that the identified blocks are both interpretable and causally linked to the expression of knowledge in generated outputs. Building on these insights, we apply our localization framework to two key applications: model personalization and knowledge unlearning. In both settings, our localized fine-tuning approach enables efficient and targeted updates, reducing computational cost, improving task-specific performance, and better preserving general model behavior with minimal interference to unrelated or surrounding content. Overall, our findings offer new insights into the internal structure of DiTs and introduce a practical pathway for more interpretable, efficient, and controllable model editing.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Limited-Angle CT Reconstruction Through Diffusion-Based Sinogram Completion</title>
<link>https://arxiv.org/abs/2505.19385</link>
<guid>https://arxiv.org/abs/2505.19385</guid>
<content:encoded><![CDATA[
arXiv:2505.19385v2 Announce Type: replace 
Abstract: Limited Angle Computed Tomography (LACT) often faces significant challenges due to missing angular information. Unlike previous methods that operate in the image domain, we propose a new method that focuses on sinogram inpainting. We leverage MR-SDEs, a variant of diffusion models that characterize the diffusion process with mean-reverting stochastic differential equations, to fill in missing angular data at the projection level. Furthermore, by combining distillation with constraining the output of the model using the pseudo-inverse of the inpainting matrix, the diffusion process is accelerated and done in a step, enabling efficient and accurate sinogram completion. A subsequent post-processing module back-projects the inpainted sinogram into the image domain and further refines the reconstruction, effectively suppressing artifacts while preserving critical structural details. Quantitative experimental results demonstrate that the proposed method achieves state-of-the-art performance in both perceptual and fidelity quality, offering a promising solution for LACT reconstruction in scientific and clinical applications.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness</title>
<link>https://arxiv.org/abs/2505.20426</link>
<guid>https://arxiv.org/abs/2505.20426</guid>
<content:encoded><![CDATA[
arXiv:2505.20426v5 Announce Type: replace 
Abstract: Understanding perspective is fundamental to human visual perception, yet the extent to which multimodal large language models (MLLMs) internalize perspective geometry remains unclear. We introduce MMPerspective, the first benchmark specifically designed to systematically evaluate MLLMs' understanding of perspective through 10 carefully crafted tasks across three complementary dimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark comprises 2,711 real-world and synthetic image instances with 5,083 question-answer pairs that probe key capabilities, such as vanishing point perception and counting, perspective type reasoning, line relationship understanding in 3D space, invariance to perspective-preserving transformations, etc. Through a comprehensive evaluation of 43 state-of-the-art MLLMs, we uncover significant limitations: while models demonstrate competence on surface-level perceptual tasks, they struggle with compositional reasoning and maintaining spatial consistency under perturbations. Our analysis further reveals intriguing patterns between model architecture, scale, and perspective capabilities, highlighting both robustness bottlenecks and the benefits of chain-of-thought prompting. MMPerspective establishes a valuable testbed for diagnosing and advancing spatial understanding in vision-language systems. Resources available at: https://yunlong10.github.io/MMPerspective/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Text-Image-to-Video Generation: A Training-Free Approach to Flexible Visual Conditioning</title>
<link>https://arxiv.org/abs/2505.20629</link>
<guid>https://arxiv.org/abs/2505.20629</guid>
<content:encoded><![CDATA[
arXiv:2505.20629v2 Announce Type: replace 
Abstract: Text-image-to-video (TI2V) generation is a critical problem for controllable video generation using both semantic and visual conditions. Most existing methods typically add visual conditions to text-to-video (T2V) foundation models by finetuning, which is costly in resources and only limited to a few pre-defined conditioning settings. To tackle these constraints, we introduce a unified formulation for TI2V generation with flexible visual conditioning. Furthermore, we propose an innovative training-free approach, dubbed FlexTI2V, that can condition T2V foundation models on an arbitrary amount of images at arbitrary positions. Specifically, we firstly invert the condition images to noisy representation in a latent space. Then, in the denoising process of T2V models, our method uses a novel random patch swapping strategy to incorporate visual features into video representations through local image patches. To balance creativity and fidelity, we use a dynamic control mechanism to adjust the strength of visual conditioning to each video frame. Extensive experiments validate that our method surpasses previous training-free image conditioning methods by a notable margin. Our method can also generalize to both UNet-based and transformer-based architectures.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding</title>
<link>https://arxiv.org/abs/2505.21962</link>
<guid>https://arxiv.org/abs/2505.21962</guid>
<content:encoded><![CDATA[
arXiv:2505.21962v2 Announce Type: replace 
Abstract: While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage for anomaly detection, they face challenges such as dynamic viewpoints, scale variations, and complex scenes. Existing datasets and methods, mainly designed for fixed ground-level views, struggle to adapt to these conditions, leading to significant performance drops in drone-view scenarios. To bridge this gap, we introduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric benchmark dataset for aerial anomaly understanding. This dataset covers various scenarios and environmental conditions, providing high-resolution real-world aerial videos with detailed annotations, including anomaly categories, frame-level timestamps, region-level bounding boxes, and natural language explanations for causal reasoning. Building on this dataset, we propose A2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to aerial anomaly understanding, enabling a deeper understanding of "Where" anomalies occur and "Why" they happen in aerial frames. To this end, A2Seek-R1 first employs a graph-of-thought (GoT)-guided supervised fine-tuning approach to activate the model's latent reasoning capabilities on A2Seek. Then, we introduce Aerial Group Relative Policy Optimization (A-GRPO) to design rule-based reward functions tailored to aerial scenarios. Furthermore, we propose a novel "seeking" mechanism that simulates UAV flight behavior by directing the model's attention to informative regions. Extensive experiments demonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for prediction accuracy and a 13.9% gain in mIoU for anomaly localization, exhibiting strong generalization across complex environments and out-of-distribution scenarios. Our dataset and code are released at https://2-mo.github.io/A2Seek/.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Hierarchical Sparse Transform Coding of 3DGS</title>
<link>https://arxiv.org/abs/2505.22908</link>
<guid>https://arxiv.org/abs/2505.22908</guid>
<content:encoded><![CDATA[
arXiv:2505.22908v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting (3DGS) supports fast, high quality, novel view synthesis but has a heavy memory footprint, making the compression of its model crucial. Current state-of-the-art (SOTA) 3DGS compression methods adopt an anchor-based architecture that pairs the Scaffold-GS representation with conditional entropy coding. However, these methods forego the analysis-synthesis transform, a vital mechanism in visual data compression. As a result, redundancy remains intact in the signal and its removal is left to the entropy coder, which computationally overburdens the entropy coding module, increasing coding latency. Even with added complexity thorough redundancy removal is a task unsuited to an entropy coder. To fix this critical omission, we introduce a Sparsity-guided Hierarchical Transform Coding (SHTC) method, the first study on the end-to-end learned neural transform coding of 3DGS. SHTC applies KLT to decorrelate intra-anchor attributes, followed by quantization and entropy coding, and then compresses KLT residuals with a low-complexity, scene-adaptive neural transform. Aided by the sparsity prior and deep unfolding technique, the learned transform uses only a few trainable parameters, reducing the memory usage. Overall, SHTC achieves an appreciably improved R-D performance and at the same time higher decoding speed over SOTA. Its prior-guided, parameter-efficient design may also inspire low-complexity neural image and video codecs. Our code will be released at https://github.com/hxu160/SHTC_for_3DGS_compression.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Animals Dance (When You're Not Looking)</title>
<link>https://arxiv.org/abs/2505.23738</link>
<guid>https://arxiv.org/abs/2505.23738</guid>
<content:encoded><![CDATA[
arXiv:2505.23738v2 Announce Type: replace 
Abstract: We present a framework for generating music-synchronized, choreography aware animal dance videos. Our framework introduces choreography patterns -- structured sequences of motion beats that define the long-range structure of a dance -- as a novel high-level control signal for dance video generation. These patterns can be automatically estimated from human dance videos. Starting from a few keyframes representing distinct animal poses, generated via text-to-image prompting or GPT-4o, we formulate dance synthesis as a graph optimization problem that seeks the optimal keyframe structure to satisfy a specified choreography pattern of beats. We also introduce an approach for mirrored pose image generation, essential for capturing symmetry in dance. In-between frames are synthesized using an video diffusion model. With as few as six input keyframes, our method can produce up to 30 seconds dance videos across a wide range of animals and music tracks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection</title>
<link>https://arxiv.org/abs/2506.00979</link>
<guid>https://arxiv.org/abs/2506.00979</guid>
<content:encoded><![CDATA[
arXiv:2506.00979v2 Announce Type: replace 
Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) techniques has enabled the creation of high-quality synthetic content, but it also raises significant security concerns. Current detection methods face two major limitations: (1) the lack of multidimensional explainable datasets for generated images and videos. Existing open-source datasets (e.g., WildFake, GenVideo) rely on oversimplified binary annotations, which restrict the explainability and trustworthiness of trained detectors. (2) Prior MLLM-based forgery detectors (e.g., FakeVLM) exhibit insufficiently fine-grained interpretability in their step-by-step reasoning, which hinders reliable localization and explanation. To address these challenges, we introduce Ivy-Fake, the first large-scale multimodal benchmark for explainable AIGC detection. It consists of over 106K richly annotated training samples (images and videos) and 5,000 manually verified evaluation examples, sourced from multiple generative models and real world datasets through a carefully designed pipeline to ensure both diversity and quality. Furthermore, we propose Ivy-xDetector, a reinforcement learning model based on Group Relative Policy Optimization (GRPO), capable of producing explainable reasoning chains and achieving robust performance across multiple synthetic content detection benchmarks. Extensive experiments demonstrate the superiority of our dataset and confirm the effectiveness of our approach. Notably, our method improves performance on GenImage from 86.88% to 96.32%, surpassing prior state-of-the-art methods by a clear margin.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-view Surface Reconstruction Using Normal and Reflectance Cues</title>
<link>https://arxiv.org/abs/2506.04115</link>
<guid>https://arxiv.org/abs/2506.04115</guid>
<content:encoded><![CDATA[
arXiv:2506.04115v2 Announce Type: replace 
Abstract: Achieving high-fidelity 3D surface reconstruction while preserving fine details remains challenging, especially in the presence of materials with complex reflectance properties and without a dense-view setup. In this paper, we introduce a versatile framework that incorporates multi-view normal and optionally reflectance maps into radiance-based surface reconstruction. Our approach employs a pixel-wise joint re-parametrization of reflectance and surface normals, representing them as a vector of radiances under simulated, varying illumination. This formulation enables seamless incorporation into standard surface reconstruction pipelines, such as traditional multi-view stereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined with the latter, our approach achieves state-of-the-art performance on multi-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV, LUCES-MV and Skoltech3D. In particular, our method excels in reconstructing fine-grained details and handling challenging visibility conditions. The present paper is an extended version of the earlier conference paper by Brument et al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024), featuring an accelerated and more robust algorithm as well as a broader empirical evaluation. The code and data relative to this article is available at https://github.com/RobinBruneau/RNb-NeuS2.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HoliSafe: Holistic Safety Benchmarking and Modeling for Vision-Language Model</title>
<link>https://arxiv.org/abs/2506.04704</link>
<guid>https://arxiv.org/abs/2506.04704</guid>
<content:encoded><![CDATA[
arXiv:2506.04704v5 Announce Type: replace 
Abstract: Despite emerging efforts to enhance the safety of Vision-Language Models (VLMs), current approaches face two main shortcomings. 1) Existing safety-tuning datasets and benchmarks only partially consider how image-text interactions can yield harmful content, often overlooking contextually unsafe outcomes from seemingly benign pairs. This narrow coverage leaves VLMs vulnerable to jailbreak attacks in unseen configurations. 2) Prior methods rely primarily on data-centric tuning, with limited architectural innovations to intrinsically strengthen safety. We address these gaps by introducing a holistic safety dataset and benchmark, \textbf{HoliSafe}, that spans all five safe/unsafe image-text combinations, providing a more robust basis for both training and evaluation (HoliSafe-Bench). We further propose a novel modular framework for enhancing VLM safety with a visual guard module (VGM) designed to assess the harmfulness of input images for VLMs. This module endows VLMs with a dual functionality: they not only learn to generate safer responses but can also provide an interpretable harmfulness classification to justify their refusal decisions. A significant advantage of this approach is its modularity; the VGM is designed as a plug-in component, allowing for seamless integration with diverse pre-trained VLMs across various scales. Experiments show that Safe-VLM with VGM, trained on our HoliSafe, achieves state-of-the-art safety performance across multiple VLM benchmarks. Additionally, the HoliSafe-Bench itself reveals critical vulnerabilities in existing VLM models. We hope that HoliSafe and VGM will spur further research into robust and interpretable VLM safety, expanding future avenues for multimodal alignment.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Vision-Language Models for Time Series Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.06836</link>
<guid>https://arxiv.org/abs/2506.06836</guid>
<content:encoded><![CDATA[
arXiv:2506.06836v2 Announce Type: replace 
Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of fields, including healthcare, finance, and sensor-based condition monitoring. Prior methods, which mainly focus on training domain-specific models on numerical data, lack the visual-temporal understanding capacity that human experts have to identify contextual anomalies. To fill this gap, we explore a solution based on vision language models (VLMs). Recent studies have shown the ability of VLMs for visual understanding tasks, yet their direct application to time series has fallen short on both accuracy and efficiency. To harness the power of VLMs for TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening stage built on a relatively lightweight pre-trained vision encoder, which leverages 2D time series representations to accurately localize candidate anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal context and VLM's visual understanding capacity to refine the detection upon the candidates provided by ViT4TS. We show that without any time-series training, VLM4TS outperforms time-series pre-trained and from-scratch baselines in most cases, yielding a 24.6% improvement in F1-max score over the best baseline. Moreover, VLM4TS also consistently outperforms existing language model-based TSAD methods and is on average 36x more efficient in token usage.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orientation Matters: Making 3D Generative Models Orientation-Aligned</title>
<link>https://arxiv.org/abs/2506.08640</link>
<guid>https://arxiv.org/abs/2506.08640</guid>
<content:encoded><![CDATA[
arXiv:2506.08640v2 Announce Type: replace 
Abstract: Humans intuitively perceive object shape and orientation from a single image, guided by strong priors about canonical poses. However, existing 3D generative models often produce misaligned results due to inconsistent training data, limiting their usability in downstream tasks. To address this gap, we introduce the task of orientation-aligned 3D object generation: producing 3D objects from single images with consistent orientations across categories. To facilitate this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two representative 3D generative models based on multi-view diffusion and 3D variational autoencoder frameworks to produce aligned objects that generalize well to unseen objects across various categories. Experimental results demonstrate the superiority of our method over post-hoc alignment approaches. Furthermore, we showcase downstream applications enabled by our aligned object generation, including zero-shot object orientation estimation via analysis-by-synthesis and efficient arrow-based object rotation manipulation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation</title>
<link>https://arxiv.org/abs/2506.23150</link>
<guid>https://arxiv.org/abs/2506.23150</guid>
<content:encoded><![CDATA[
arXiv:2506.23150v2 Announce Type: replace 
Abstract: Single-image-to-3D models typically follow a sequential generation and reconstruction workflow. However, intermediate multi-view images synthesized by pre-trained generation models often lack cross-view consistency (CVC), significantly degrading 3D reconstruction performance. While recent methods attempt to refine CVC by feeding reconstruction results back into the multi-view generator, these approaches struggle with noisy and unstable reconstruction outputs that limit effective CVC improvement. We introduce AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D generation through distribution alignment rather than relying on strict regression losses. Our key insight is to align both generated and reconstructed multi-view distributions toward the ground-truth multi-view distribution, establishing a principled foundation for improved CVC. Observing that generated images exhibit weak CVC while reconstructed images display strong CVC due to explicit rendering, we propose a soft-hard alignment strategy with distinct objectives for generation and reconstruction models. This approach not only enhances generation quality but also dramatically accelerates inference to as few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC, seamlessly integrates various multi-view generation models with 3D reconstruction models. Extensive experiments demonstrate the effectiveness and efficiency of AlignCVC for single-image-to-3D generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fully Supervised Pixel Annotations: Scribble-Driven Weakly-Supervised Framework for Image Manipulation Localization</title>
<link>https://arxiv.org/abs/2507.13018</link>
<guid>https://arxiv.org/abs/2507.13018</guid>
<content:encoded><![CDATA[
arXiv:2507.13018v2 Announce Type: replace 
Abstract: Deep learning-based image manipulation localization (IML) methods have achieved remarkable performance in recent years, but typically rely on large-scale pixel-level annotated datasets. To address the challenge of acquiring high-quality annotations, some recent weakly supervised methods utilize image-level labels to segment manipulated regions. However, the performance is still limited due to insufficient supervision signals. In this study, we explore a form of weak supervision that improves the annotation efficiency and detection performance, namely scribble annotation supervision. We re-annotate mainstream IML datasets with scribble labels and propose the first scribble-based IML (Sc-IML) dataset. Additionally, we propose the first scribble-based weakly supervised IML framework. Specifically, we employ self-supervised training with a structural consistency loss to encourage the model to produce consistent predictions under multi-scale and augmented inputs. In addition, we propose a prior-aware feature modulation module (PFMM) that adaptively integrates prior information from both manipulated and authentic regions for dynamic feature adjustment, further enhancing feature discriminability and prediction consistency in complex scenes. We also propose a gated adaptive fusion module (GAFM) that utilizes gating mechanisms to regulate information flow during feature fusion, guiding the model toward emphasizing potential manipulated regions. Finally, we propose a confidence-aware entropy minimization loss (${\mathcal{L}}_{ {CEM }}$). This loss dynamically regularizes predictions in weakly annotated or unlabeled regions based on model uncertainty, effectively suppressing unreliable predictions. Experimental results show that our method outperforms existing fully supervised approaches in terms of average performance both in-distribution and out-of-distribution.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Early Bird Identifies the Worm: You Can't Beat a Head Start in Long-Term Body Re-ID (ECHO-BID)</title>
<link>https://arxiv.org/abs/2507.17640</link>
<guid>https://arxiv.org/abs/2507.17640</guid>
<content:encoded><![CDATA[
arXiv:2507.17640v2 Announce Type: replace 
Abstract: A wide range of model-based approaches to long-term person re-identification have been proposed. Whether these models perform more accurately than direct domain transfer learning applied to extensively trained large-scale foundation models is not known. We applied domain transfer learning for long-term person re-id to four vision foundation models (CLIP, DINOv2, AIMv2, and EVA-02). Domain-adapted versions of all four models %CLIP-L, DINOv2-L, AIMv2-L, and EVA-02-L surpassed existing state-of-the-art models by a large margin in highly unconstrained viewing environments. Decision score fusion of the four models improved performance over any individual model. Of the individual models, the EVA-02 foundation model provided the best ``head start'' to long-term re-id, surpassing other models on three of the four performance metrics by substantial margins. Accordingly, we introduce $\textbf{E}$va $\textbf{C}$lothes-Change from $\textbf{H}$idden $\textbf{O}$bjects - $\textbf{B}$ody $\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built on the object-pretrained EVA-02 Large backbones. Ablation experiments varying backbone size, scale of object classification pretraining, and transfer learning protocol indicated that model size and the use of a smaller, but more challenging transfer learning protocol are critical features in performance. We conclude that foundation models provide a head start to domain transfer learning and support state-of-the-art performance with modest amounts of domain data. The limited availability of long-term re-id data makes this approach advantageous.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Anomaly Detection with Dual-Branch Prompt Selection</title>
<link>https://arxiv.org/abs/2508.00777</link>
<guid>https://arxiv.org/abs/2508.00777</guid>
<content:encoded><![CDATA[
arXiv:2508.00777v3 Announce Type: replace 
Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects in unseen categories by relying solely on generalizable features rather than requiring any labeled examples of anomalies. However, existing ZSAD methods, whether using fixed or learned prompts, struggle under domain shifts because their training data are derived from limited training domains and fail to generalize to new distributions. In this paper, we introduce PILOT, a framework designed to overcome these challenges through two key innovations: (1) a novel dual-branch prompt learning mechanism that dynamically integrates a pool of learnable prompts with structured semantic attributes, enabling the model to adaptively weight the most relevant anomaly cues for each input image; and (2) a label-free test-time adaptation strategy that updates the learnable prompt parameters using high-confidence pseudo-labels from unlabeled test data. Extensive experiments on 13 industrial and medical benchmarks demonstrate that PILOT achieves state-of-the-art performance in both anomaly detection and localization under domain shift.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models</title>
<link>https://arxiv.org/abs/2508.03356</link>
<guid>https://arxiv.org/abs/2508.03356</guid>
<content:encoded><![CDATA[
arXiv:2508.03356v2 Announce Type: replace 
Abstract: Federated Learning (FL) is an established paradigm for training deep learning models on decentralized data. However, as the size of the models grows, conventional FL approaches often require significant computational resources on client devices, which may not be feasible. We introduce FedPromo, a novel framework that enables efficient adaptation of large-scale foundation models stored on a central server to new domains encountered only by remote clients. Instead of directly training the large model on client devices, FedPromo optimizes lightweight proxy models via FL, significantly reducing computational overhead while maintaining privacy. Our method follows a two-stage process: first, server-side knowledge distillation aligns the representations of a large-scale foundation model (e.g., a transformer) with those of a compact counterpart (e.g., a CNN). Then, the compact model encoder is deployed to client devices, where trainable classifiers are learned locally. These classifiers are subsequently aggregated and seamlessly transferred back to the foundation model, facilitating personalized adaptation without requiring direct access to user data. Through novel regularization strategies, our framework enables decentralized multi-domain learning, balancing performance, privacy, and resource efficiency. Extensive experiments on five image classification benchmarks demonstrate that FedPromo outperforms existing methods while assuming limited-resource clients.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WeatherDiffusion: Controllable Weather Editing in Intrinsic Space</title>
<link>https://arxiv.org/abs/2508.06982</link>
<guid>https://arxiv.org/abs/2508.06982</guid>
<content:encoded><![CDATA[
arXiv:2508.06982v2 Announce Type: replace 
Abstract: We present WeatherDiffusion, a diffusion-based framework for controllable weather editing in intrinsic space. Our framework includes two components based on diffusion priors: an inverse renderer that estimates material properties, scene geometry, and lighting as intrinsic maps from an input image, and a forward renderer that utilizes these geometry and material maps along with a text prompt that describes specific weather conditions to generate a final image. The intrinsic maps enhance controllability compared to traditional pixel-space editing approaches.We propose an intrinsic map-aware attention mechanism that improves spatial correspondence and decomposition quality in large outdoor scenes. For forward rendering, we leverage CLIP-space interpolation of weather prompts to achieve fine-grained weather control. We also introduce a synthetic and a real-world dataset, containing 38k and 18k images under various weather conditions, each with intrinsic map annotations. WeatherDiffusion outperforms state-of-the-art pixel-space editing approaches, weather restoration methods, and rendering-based methods, showing promise for downstream tasks such as autonomous driving, enhancing the robustness of detection and segmentation in challenging weather scenarios.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation</title>
<link>https://arxiv.org/abs/2508.07901</link>
<guid>https://arxiv.org/abs/2508.07901</guid>
<content:encoded><![CDATA[
arXiv:2508.07901v3 Announce Type: replace 
Abstract: Generating high-fidelity human videos that match user-specified identities is important yet challenging in the field of generative AI. Existing methods often rely on an excessive number of training parameters and lack compatibility with other AIGC tools. In this paper, we propose Stand-In, a lightweight and plug-and-play framework for identity preservation in video generation. Specifically, we introduce a conditional image branch into the pre-trained video generation model. Identity control is achieved through restricted self-attentions with conditional position mapping. Thanks to these designs, which greatly preserve the pre-trained prior of the video generation model, our approach is able to outperform other full-parameter training methods in video quality and identity preservation, even with just $\sim$1% additional parameters and only 2000 training pairs. Moreover, our framework can be seamlessly integrated for other tasks, such as subject-driven video generation, pose-referenced video generation, stylization, and face swapping.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeFix: Targeted Model Repair via Controlled Image Generation</title>
<link>https://arxiv.org/abs/2508.08701</link>
<guid>https://arxiv.org/abs/2508.08701</guid>
<content:encoded><![CDATA[
arXiv:2508.08701v2 Announce Type: replace 
Abstract: Deep learning models for visual recognition often exhibit systematic errors due to underrepresented semantic subpopulations. Although existing debugging frameworks can pinpoint these failures by identifying key failure attributes, repairing the model effectively remains difficult. Current solutions often rely on manually designed prompts to generate synthetic training images -- an approach prone to distribution shift and semantic errors. To overcome these challenges, we introduce a model repair module that builds on an interpretable failure attribution pipeline. Our approach uses a conditional text-to-image model to generate semantically faithful and targeted images for failure cases. To preserve the quality and relevance of the generated samples, we further employ a large vision-language model (LVLM) to filter the outputs, enforcing alignment with the original data distribution and maintaining semantic consistency. By retraining vision models with this rare-case-augmented synthetic dataset, we significantly reduce errors associated with rare cases. Our experiments demonstrate that this targeted repair strategy improves model robustness without introducing new bugs. Code is available at https://github.com/oxu2/SafeFix
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Unlabeled Data from Unknown Sources via Dual-Path Guidance for Deepfake Face Detection</title>
<link>https://arxiv.org/abs/2508.09022</link>
<guid>https://arxiv.org/abs/2508.09022</guid>
<content:encoded><![CDATA[
arXiv:2508.09022v3 Announce Type: replace 
Abstract: Existing deepfake detection methods heavily rely on static labeled datasets. However, with the proliferation of generative models, real-world scenarios are flooded with massive amounts of unlabeled fake face data from unknown sources. This presents a critical dilemma: detectors relying solely on existing data face generalization failure, while manual labeling for this new stream is infeasible due to the high realism of fakes. A more fundamental challenge is that, unlike typical unsupervised learning tasks where categories are clearly defined, real and fake faces share the same semantics, which leads to a decline in the performance of traditional unsupervised strategies. Therefore, there is an urgent need for a new paradigm designed specifically for this scenario to effectively utilize these unlabeled data. Accordingly, this paper proposes a dual-path guided network (DPGNet) to address two key challenges: (1) bridging the domain differences between faces generated by different generative models; and (2) utilizing unlabeled image samples. The method comprises two core modules: text-guided cross-domain alignment, which uses learnable cues to unify visual and textual embeddings into a domain-invariant feature space; and curriculum-driven pseudo-label generation, which dynamically utilizes unlabeled samples. Extensive experiments on multiple mainstream datasets show that DPGNet significantly outperforms existing techniques,, highlighting its effectiveness in addressing the challenges posed by the deepfakes using unlabeled data.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving detailed medial temporal lobe segmentation with upsampled isotropic training from implicit neural representation</title>
<link>https://arxiv.org/abs/2508.17171</link>
<guid>https://arxiv.org/abs/2508.17171</guid>
<content:encoded><![CDATA[
arXiv:2508.17171v2 Announce Type: replace 
Abstract: Imaging biomarkers in magnetic resonance imaging (MRI) are important tools for diagnosing, tracking and treating Alzheimer's disease (AD). Neurofibrillary tau pathology in AD is closely linked to neurodegeneration and generally follows a pattern of spread in the brain, with early stages involving subregions of the medial temporal lobe (MTL). Accurate segmentation of MTL subregions is needed to extract granular biomarkers of AD progression. MTL subregions are often imaged using T2-weighted (T2w) MRI scans that are highly anisotropic due to constraints of MRI physics and image acquisition, making it difficult to reliably model MTL subregions geometrically and extract morphological measures, such as thickness. In this study, we used an implicit neural representation method to combine isotropic T1-weighted (T1w) and anisotropic T2w MRI to upsample an atlas set of expert-annotated MTL subregions, establishing a multi-modality, high-resolution training set of isotropic data for automatic segmentation with the nnU-Net framework. In an independent test set, the morphological measures extracted using this isotropic model showed stronger effect sizes than models trained on anisotropic in distinguishing participants with mild cognitive impairment (MCI) and cognitively unimpaired individuals. In test-retest analysis, morphological measures extracted using the isotropic model had greater stability. This study demonstrates improved reliability of MRI-derived MTL subregion biomarkers without additional atlas annotation effort, which may more accurately quantify and track the relationship between AD pathology and brain atrophy for monitoring disease progression.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction</title>
<link>https://arxiv.org/abs/2508.19786</link>
<guid>https://arxiv.org/abs/2508.19786</guid>
<content:encoded><![CDATA[
arXiv:2508.19786v2 Announce Type: replace 
Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene reconstruction with fast rendering, is increasingly being applied to multi-view dynamic scene reconstruction. A common strategy involves learning a deformation field to model the temporal changes of a canonical set of 3D Gaussians. However, these deformation-based methods often produce blurred renderings and lose fine motion details in highly dynamic regions due to the inherent limitations of a single, unified model in representing diverse motion patterns. To address these challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian Splatting (MAPo), a novel framework for high-fidelity dynamic scene reconstruction. Its core is a dynamic score-based partitioning strategy that distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D Gaussians, we recursively partition them temporally and duplicate their deformation networks for each new temporal segment, enabling specialized modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs are treated as static to reduce computational costs. However, this temporal partitioning strategy for high-dynamic 3DGs can introduce visual discontinuities across frames at the partition boundaries. To address this, we introduce a cross-frame consistency loss, which not only ensures visual continuity but also further enhances rendering quality. Extensive experiments demonstrate that MAPo achieves superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.02966</link>
<guid>https://arxiv.org/abs/2509.02966</guid>
<content:encoded><![CDATA[
arXiv:2509.02966v2 Announce Type: replace 
Abstract: Accurate short-horizon trajectory prediction is crucial for safe and reliable autonomous driving. However, existing vision-language models (VLMs) often fail to accurately understand driving scenes and generate trustworthy trajectories. To address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM framework that predicts ego trajectories directly from consecutive front-view driving frames. KEPT integrates a temporal frequency-spatial fusion (TFSF) video encoder, which is trained via self-supervised learning with hard-negative mining, with a k-means & HNSW retrieval-augmented generation (RAG) pipeline. Retrieved prior knowledge is added into chain-of-thought (CoT) prompts with explicit planning constraints, while a triple-stage fine-tuning paradigm aligns the VLM backbone to enhance spatial perception and trajectory prediction capabilities. Evaluated on nuScenes dataset, KEPT achieves the best open-loop performance compared with baseline methods. Ablation studies on fine-tuning stages, Top-K value of RAG, different retrieval strategies, vision encoders, and VLM backbones are conducted to demonstrate the effectiveness of KEPT. These results indicate that KEPT offers a promising, data-efficient way toward trustworthy trajectory prediction in autonomous driving.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System</title>
<link>https://arxiv.org/abs/2509.02973</link>
<guid>https://arxiv.org/abs/2509.02973</guid>
<content:encoded><![CDATA[
arXiv:2509.02973v2 Announce Type: replace 
Abstract: Acquiring high-quality instance segmentation data is challenging due to the labor-intensive nature of the annotation process and significant class imbalances within datasets. Recent studies have utilized the integration of Copy-Paste and diffusion models to create more diverse datasets. However, these studies often lack deep collaboration between large language models (LLMs) and diffusion models, and underutilize the rich information within the existing training data. To address these limitations, we propose InstaDA, a novel, training-free Dual-Agent system designed to augment instance segmentation datasets. First, we introduce a Text-Agent (T-Agent) that enhances data diversity through collaboration between LLMs and diffusion models. This agent features a novel Prompt Rethink mechanism, which iteratively refines prompts based on the generated images. This process not only fosters collaboration but also increases image utilization and optimizes the prompts themselves. Additionally, we present an Image-Agent (I-Agent) aimed at enriching the overall data distribution. This agent augments the training set by generating new instances conditioned on the training images. To ensure practicality and efficiency, both agents operate as independent and automated workflows, enhancing usability. Experiments conducted on the LVIS 1.0 validation set indicate that InstaDA achieves significant improvements, with an increase of +4.0 in box average precision (AP) and +3.3 in mask AP compared to the baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common categories and mask AP gains of +0.2 on common categories and +0.5 on frequent categories.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition</title>
<link>https://arxiv.org/abs/2509.11916</link>
<guid>https://arxiv.org/abs/2509.11916</guid>
<content:encoded><![CDATA[
arXiv:2509.11916v2 Announce Type: replace 
Abstract: Facial emotion recognition (FER) models trained only on pixels often fail to generalize across datasets because facial appearance is an indirect and biased proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal distillation framework that transfers brain-informed priors into an image-only FER student via static Valence/Arousal (V/A) prototypes and a depression-inspired geometric prior (D-Geo). A teacher trained on EEG topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face pairing and no non-visual signals at deployment are required. The student (ResNet-18/50) is trained on FERPlus with conventional CE/KD and two lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with affective findings often reported in depression research (e.g., anhedonia-like contraction in high-valence regions). We evaluate both within-domain (FERPlus validation) and cross-dataset protocols (AffectNet-mini; optional CK+), reporting standard 8-way scores alongside present-only Macro-F1 and balanced accuracy to fairly handle label-set mismatch. Ablations attribute consistent gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability. The method is simple, deployable, and improves robustness without architectural complexity.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.12897</link>
<guid>https://arxiv.org/abs/2509.12897</guid>
<content:encoded><![CDATA[
arXiv:2509.12897v2 Announce Type: replace 
Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in images, yet their attention to these objects tends to be very brief. Motivated by the hypothesis that sustained focus on key objects can improve LVLMs' visual capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of CLVS is to incorporate a vision memory that smooths the attention distribution across layers. Specifically, we initialize this vision memory with position-unbiased visual attention in the first layer. In subsequent layers, the model's visual attention jointly considers the vision memory from previous layers, while the memory is updated iteratively, thereby maintaining smooth attention on key objects. Given that visual understanding primarily occurs in the early and middle layers of the model, we use uncertainty as an indicator of completed visual understanding and terminate the smoothing process accordingly. Experiments on four benchmarks across three LVLMs confirm the effectiveness and generalizability of our method. CLVS achieves state-of-the-art overall performance across a variety of visual understanding tasks and attains comparable results to the leading approaches on image captioning benchmarks.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain</title>
<link>https://arxiv.org/abs/2509.12959</link>
<guid>https://arxiv.org/abs/2509.12959</guid>
<content:encoded><![CDATA[
arXiv:2509.12959v2 Announce Type: replace 
Abstract: The integration of event cameras and spiking neural networks holds great promise for energy-efficient visual processing. However, the limited availability of event data and the sparse nature of DVS outputs pose challenges for effective training. Although some prior work has attempted to transfer semantic knowledge from RGB datasets to DVS, they often overlook the significant distribution gap between the two modalities. In this paper, we propose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing strategy that exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time-steps. To enable label mixing in cross-modal scenarios, we further introduce modality-aware auxiliary learning objectives. These objectives support the time-step mixup process and enhance the model's ability to discriminate effectively across different modalities. Our approach enables smoother knowledge transfer, alleviates modality shift during training, and achieves superior performance in spiking image classification tasks. Extensive experiments demonstrate the effectiveness of our method across multiple datasets. The code will be released after the double-blind review process.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment</title>
<link>https://arxiv.org/abs/2509.17818</link>
<guid>https://arxiv.org/abs/2509.17818</guid>
<content:encoded><![CDATA[
arXiv:2509.17818v2 Announce Type: replace 
Abstract: Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude "hard" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StrCGAN: A Generative Framework for Stellar Image Restoration</title>
<link>https://arxiv.org/abs/2509.19805</link>
<guid>https://arxiv.org/abs/2509.19805</guid>
<content:encoded><![CDATA[
arXiv:2509.19805v3 Announce Type: replace 
Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to enhance low-resolution astrophotography images. Our goal is to reconstruct high fidelity ground truth like representations of stellar objects, a task that is challenging due to the limited resolution and quality of small-telescope observations such as the MobilTelesco dataset. Traditional models such as CycleGAN provide a foundation for image to image translation but often distort the morphology of stars and produce barely resembling images. To overcome these limitations, we extend the CycleGAN framework with some key innovations: multi-spectral fusion to align optical and near infrared (NIR) domains, and astrophysical regularization modules to preserve stellar morphology. Ground truth references from multi-mission all sky surveys spanning optical to NIR guide the training process, ensuring that reconstructions remain consistent across spectral bands. Together, these components allow StrCGAN to generate reconstructions that are visually sharper outperforming standard GAN models in the task of astrophysical image enhancement.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning</title>
<link>https://arxiv.org/abs/2510.10068</link>
<guid>https://arxiv.org/abs/2510.10068</guid>
<content:encoded><![CDATA[
arXiv:2510.10068v2 Announce Type: replace 
Abstract: The computer vision domain has greatly benefited from an abundance of data across many modalities to improve on various visual tasks. Recently, there has been a lot of focus on self-supervised pre-training methods through Masked Autoencoders (MAE) \cite{he2022masked,bachmann2022multimae}, usually used as a first step before optimizing for a downstream task, such as classification or regression. This is very useful as it doesn't require any manually labeled data. In this work, we introduce Probabilistic Hyper-Graphs using Masked Autoencoders (PHG-MAE): a novel model that unifies the classical work on neural graphs \cite{leordeanu2021semi} with the modern approach of masked autoencoders under a common theoretical framework. Through random masking of entire modalities, not just patches, the model samples from the distribution of hyper-edges on each forward pass. Additionally, the model adapts the standard MAE algorithm by combining pre-training and fine-tuning into a single training loop. Moreover, our approach enables the creation of inference-time ensembles which, through aggregation, boost the final prediction performance and consistency. Lastly, we show that we can apply knowledge distillation on top of the ensembles with little loss in performance, even with models that have fewer than 1M parameters. While our work mostly focuses on outdoor UAV scenes that contain multiple world interpretations and modalities, the same steps can be followed in other similar domains, such as autonomous driving or indoor robotics. In order to streamline the process of integrating external pre-trained experts for computer vision multi-modal multi-task learning (MTL) scenarios, we developed a data-pipeline software. Using this tool, we have created and released a fully-automated extension of the Dronescapes dataset. All the technical details, code and reproduction steps are publicly released.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference</title>
<link>https://arxiv.org/abs/2510.11512</link>
<guid>https://arxiv.org/abs/2510.11512</guid>
<content:encoded><![CDATA[
arXiv:2510.11512v2 Announce Type: replace 
Abstract: Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing DINOv2 with Registers for Face Anti-Spoofing</title>
<link>https://arxiv.org/abs/2510.17201</link>
<guid>https://arxiv.org/abs/2510.17201</guid>
<content:encoded><![CDATA[
arXiv:2510.17201v2 Announce Type: replace 
Abstract: Face recognition systems are designed to be robust against variations in head pose, illumination, and image blur during capture. However, malicious actors can exploit these systems by presenting a face photo of a registered user, potentially bypassing the authentication process. Such spoofing attacks must be detected prior to face recognition. In this paper, we propose a DINOv2-based spoofing attack detection method to discern minute differences between live and spoofed face images. Specifically, we employ DINOv2 with registers to extract generalizable features and to suppress perturbations in the attention mechanism, which enables focused attention on essential and minute features. We demonstrate the effectiveness of the proposed method through experiments conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop: Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset. The project page is available at: https://gsisaoki.github.io/FAS-DINOv2-ICCVW/ .
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</title>
<link>https://arxiv.org/abs/2510.19654</link>
<guid>https://arxiv.org/abs/2510.19654</guid>
<content:encoded><![CDATA[
arXiv:2510.19654v2 Announce Type: replace 
Abstract: Despite remarkable progress in driving world models, their potential for autonomous systems remains largely untapped: the world models are mostly learned for world simulation and decoupled from trajectory planning. While recent efforts aim to unify world modeling and planning in a single framework, the synergistic facilitation mechanism of world modeling for planning still requires further exploration. In this work, we introduce a new driving paradigm named Policy World Model (PWM), which not only integrates world modeling and trajectory planning within a unified architecture, but is also able to benefit planning using the learned world knowledge through the proposed action-free future state forecasting scheme. Through collaborative state-action prediction, PWM can mimic the human-like anticipatory perception, yielding more reliable planning performance. To facilitate the efficiency of video forecasting, we further introduce a dynamically enhanced parallel token generation mechanism, equipped with a context-guided tokenizer and an adaptive dynamic focal loss. Despite utilizing only front camera input, our method matches or exceeds state-of-the-art approaches that rely on multi-view and multi-modal inputs. Code and model weights will be released at https://github.com/6550Zhao/Policy-World-Model.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Endoshare: A Publicly Available, Surgeons-Friendly Solution to De-Identify and Manage Surgical Videos</title>
<link>https://arxiv.org/abs/2510.20087</link>
<guid>https://arxiv.org/abs/2510.20087</guid>
<content:encoded><![CDATA[
arXiv:2510.20087v2 Announce Type: replace 
Abstract: Video-based assessment and surgical data science can advance surgical training, research, and quality improvement, yet adoption remains limited by heterogeneous recording formats and privacy concerns linked to video sharing. This work develops, evaluates, and publicly releases Endoshare, a surgeon-friendly application that merges, standardizes, and de-identifies endoscopic videos. Development followed an iterative, user-centered software life cycle. In the analysis phase, an internal survey of four clinicians and four computer scientists, based on 10 usability heuristics, identified early requirements and guided a cross-platform, privacy-by-design architecture. Prototype testing reported high usability for clinicians (4.68 +/- 0.40 out of 5) and for computer scientists (4.03 +/- 0.51 out of 5), with the lowest score (4.00 +/- 0.93 out of 5) relating to label clarity, prompting interface refinement to streamline case selection, video merging, automated out-of-body removal, and filename pseudonymization. In the testing phase, ten surgeons completed an external survey combining the same heuristics with Technology Acceptance Model constructs, reporting high perceived usefulness (5.07 +/- 1.75 out of 7), ease of use (5.15 +/- 1.71 out of 7), heuristic usability (4.38 +/- 0.48 out of 5), and strong recommendation likelihood (9.20 +/- 0.79 out of 10). A performance assessment across different hardware and configurations showed that processing time increased proportionally with video duration and was consistently lower in fast mode. Endoshare is a publicly available solution to manage surgical videos, with potential to support training, research, and quality improvement. Compliance certification and broader interoperability validation are needed to establish it as a reliable tool for surgical video management. The software is available at https://camma-public.github.io/Endoshare
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Target-aware Image Editing via Cycle-consistent Constraints</title>
<link>https://arxiv.org/abs/2510.20212</link>
<guid>https://arxiv.org/abs/2510.20212</guid>
<content:encoded><![CDATA[
arXiv:2510.20212v2 Announce Type: replace 
Abstract: Recent advances in pre-trained text-to-image flow models have enabled remarkable progress in text-based image editing. Mainstream approaches always adopt a corruption-then-restoration paradigm, where the source image is first corrupted into an ``intermediate state'' and then restored to the target image under the prompt guidance. However, current methods construct this intermediate state in a target-agnostic manner, i.e., they primarily focus on realizing source image reconstruction while neglecting the semantic gaps towards the specific editing target. This design inherently results in limited editability or inconsistency when the desired modifications substantially deviate from the source. In this paper, we argue that the intermediate state should be target-aware, i.e., selectively corrupting editing-relevant contents while preserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel inversion-free and flow-based editing framework that parameterizes corruption with learnable noises and optimizes them through a cycle-consistent process. By iteratively editing the source to the target and recovering back to the source with dual consistency constraints, FlowCycle learns to produce a target-aware intermediate state, enabling faithful modifications while preserving source consistency. Extensive ablations have demonstrated that FlowCycle achieves superior editing quality and consistency over state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning</title>
<link>https://arxiv.org/abs/2510.20519</link>
<guid>https://arxiv.org/abs/2510.20519</guid>
<content:encoded><![CDATA[
arXiv:2510.20519v2 Announce Type: replace 
Abstract: Inspired by recent advancements in LLM reasoning, the field of multimodal reasoning has seen remarkable progress, achieving significant performance gains on intricate tasks such as mathematical problem-solving. Despite this progress, current multimodal large reasoning models exhibit two key limitations. They tend to employ computationally expensive reasoning even for simple queries, leading to inefficiency. Furthermore, this focus on specialized reasoning often impairs their broader, more general understanding capabilities. In this paper, we propose Metis-HOME: a Hybrid Optimized Mixture-of-Experts framework designed to address this trade-off. Metis-HOME enables a ''Hybrid Thinking'' paradigm by structuring the original dense model into two distinct expert branches: a thinking branch tailored for complex, multi-step reasoning, and a non-thinking branch optimized for rapid, direct inference on tasks like general VQA and OCR. A lightweight, trainable router dynamically allocates queries to the most suitable expert. We instantiate Metis-HOME by adapting the Qwen2.5-VL-7B into an MoE architecture. Comprehensive evaluations reveal that our approach not only substantially enhances complex reasoning abilities but also improves the model's general capabilities, reversing the degradation trend observed in other reasoning-specialized models. Our work establishes a new paradigm for building powerful and versatile MLLMs, effectively resolving the prevalent reasoning-vs-generalization dilemma. Code and weights are available at https://github.com/MM-Thinking/Metis-HOME.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LayerComposer: Multi-Human Personalized Generation via Layered Canvas</title>
<link>https://arxiv.org/abs/2510.20820</link>
<guid>https://arxiv.org/abs/2510.20820</guid>
<content:encoded><![CDATA[
arXiv:2510.20820v3 Announce Type: replace 
Abstract: Despite their impressive visual fidelity, existing personalized image generators lack interactive control over spatial composition and scale poorly to multiple humans. To address these limitations, we present LayerComposer, an interactive and scalable framework for multi-human personalized generation. Inspired by professional image-editing software, LayerComposer provides intuitive reference-based human injection, allowing users to place and resize multiple subjects directly on a layered digital canvas to guide personalized generation. The core of our approach is the layered canvas, a novel representation where each subject is placed on a distinct layer, enabling interactive and occlusion-free composition. We further introduce a transparent latent pruning mechanism that improves scalability by decoupling computational cost from the number of subjects, and a layerwise cross-reference training strategy that mitigates copy-paste artifacts. Extensive experiments demonstrate that LayerComposer achieves superior spatial control, coherent composition, and identity preservation compared to state-of-the-art methods in multi-human personalized image generation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation</title>
<link>https://arxiv.org/abs/2510.24821</link>
<guid>https://arxiv.org/abs/2510.24821</guid>
<content:encoded><![CDATA[
arXiv:2510.24821v2 Announce Type: replace 
Abstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.27606</link>
<guid>https://arxiv.org/abs/2510.27606</guid>
<content:encoded><![CDATA[
arXiv:2510.27606v2 Announce Type: replace 
Abstract: Spatial understanding remains a weakness of Large Vision-Language Models (LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement learning with verifiable rewards (RLVR) pipelines depend on costly supervision, specialized tools, or constrained environments that limit scale. We introduce Spatial-SSRL, a self-supervised RL paradigm that derives verifiable signals directly from ordinary RGB or RGB-D images. Spatial-SSRL automatically formulates five pretext tasks that capture 2D and 3D spatial structure: shuffled patch reordering, flipped patch recognition, cropped patch inpainting, regional depth ordering, and relative 3D position prediction. These tasks provide ground-truth answers that are easy to verify and require no human or LVLM annotation. Training on our tasks substantially improves spatial reasoning while preserving general visual capabilities. On seven spatial understanding benchmarks in both image and video settings, Spatial-SSRL delivers average accuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our results show that simple, intrinsic supervision enables RLVR at scale and provides a practical route to stronger spatial intelligence in LVLMs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastGS: Training 3D Gaussian Splatting in 100 Seconds</title>
<link>https://arxiv.org/abs/2511.04283</link>
<guid>https://arxiv.org/abs/2511.04283</guid>
<content:encoded><![CDATA[
arXiv:2511.04283v2 Announce Type: replace 
Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to properly regulate the number of Gaussians during training, causing redundant computational time overhead. In this paper, we propose FastGS, a novel, simple, and general acceleration framework that fully considers the importance of each Gaussian based on multi-view consistency, efficiently solving the trade-off between training time and rendering quality. We innovatively design a densification and pruning strategy based on multi-view consistency, dispensing with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks & Temples, and Deep Blending datasets demonstrate that our method significantly outperforms the state-of-the-art methods in training speed, achieving a 3.32$\times$ training acceleration and comparable rendering quality compared with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration across various tasks, including dynamic scene reconstruction, surface reconstruction, sparse-view reconstruction, large-scale reconstruction, and simultaneous localization and mapping. The project page is available at https://fastgs.github.io/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CGCE: Classifier-Guided Concept Erasure in Generative Models</title>
<link>https://arxiv.org/abs/2511.05865</link>
<guid>https://arxiv.org/abs/2511.05865</guid>
<content:encoded><![CDATA[
arXiv:2511.05865v2 Announce Type: replace 
Abstract: Recent advancements in large-scale generative models have enabled the creation of high-quality images and videos, but have also raised significant safety concerns regarding the generation of unsafe content. To mitigate this, concept erasure methods have been developed to remove undesirable concepts from pre-trained models. However, existing methods remain vulnerable to adversarial attacks that can regenerate the erased content. Moreover, achieving robust erasure often degrades the model's generative quality for safe, unrelated concepts, creating a difficult trade-off between safety and performance. To address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE), an efficient plug-and-play framework that provides robust concept erasure for diverse generative models without altering their original weights. CGCE uses a lightweight classifier operating on text embeddings to first detect and then refine prompts containing undesired concepts. This approach is highly scalable, allowing for multi-concept erasure by aggregating guidance from several classifiers. By modifying only unsafe embeddings at inference time, our method prevents harmful content generation while preserving the model's original quality on benign prompts. Extensive experiments show that CGCE achieves state-of-the-art robustness against a wide range of red-teaming attacks. Our approach also maintains high generative utility, demonstrating a superior balance between safety and performance. We showcase the versatility of CGCE through its successful application to various modern T2I and T2V models, establishing it as a practical and effective solution for safe generative AI.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Semantic Instance Segmentation</title>
<link>https://arxiv.org/abs/2511.10003</link>
<guid>https://arxiv.org/abs/2511.10003</guid>
<content:encoded><![CDATA[
arXiv:2511.10003v2 Announce Type: replace 
Abstract: Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration</title>
<link>https://arxiv.org/abs/2511.14099</link>
<guid>https://arxiv.org/abs/2511.14099</guid>
<content:encoded><![CDATA[
arXiv:2511.14099v2 Announce Type: replace 
Abstract: All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation</title>
<link>https://arxiv.org/abs/2511.14259</link>
<guid>https://arxiv.org/abs/2511.14259</guid>
<content:encoded><![CDATA[
arXiv:2511.14259v2 Announce Type: replace 
Abstract: With the rapid advancement of generative models, powerful image editing methods now enable diverse and highly realistic image manipulations that far surpass traditional deepfake techniques, posing new challenges for manipulation detection. Existing image manipulation detection and localization (IMDL) benchmarks suffer from limited content diversity, narrow generative-model coverage, and insufficient interpretability, which hinders the generalization and explanation capabilities of current manipulation detection methods. To address these limitations, we introduce \textbf{ManipBench}, a large-scale benchmark for image manipulation detection and localization focusing on AI-edited images. ManipBench contains over 450K manipulated images produced by 25 state-of-the-art image editing models across 12 manipulation categories, among which 100K images are further annotated with bounding boxes, judgment cues, and textual explanations to support interpretable detection. Building upon ManipBench, we propose \textbf{ManipShield}, an all-in-one model based on a Multimodal Large Language Model (MLLM) that leverages contrastive LoRA fine-tuning and task-specific decoders to achieve unified image manipulation detection, localization, and explanation. Extensive experiments on ManipBench and several public datasets demonstrate that ManipShield achieves state-of-the-art performance and exhibits strong generality to unseen manipulation models. Both ManipBench and ManipShield will be released upon publication.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D-Guided Scalable Flow Matching for Generating Volumetric Tissue Spatial Transcriptomics from Serial Histology</title>
<link>https://arxiv.org/abs/2511.14613</link>
<guid>https://arxiv.org/abs/2511.14613</guid>
<content:encoded><![CDATA[
arXiv:2511.14613v2 Announce Type: replace 
Abstract: A scalable and robust 3D tissue transcriptomics profile can enable a holistic understanding of tissue organization and provide deeper insights into human biology and disease. Most predictive algorithms that infer ST directly from histology treat each section independently and ignore 3D structure, while existing 3D-aware approaches are not generative and do not scale well. We present Holographic Tissue Expression Inpainting and Analysis (HoloTea), a 3D-aware flow-matching framework that imputes spot-level gene expression from H&amp;E while explicitly using information from adjacent sections. Our key idea is to retrieve morphologically corresponding spots on neighboring slides in a shared feature space and fuse this cross section context into a lightweight ControlNet, allowing conditioning to follow anatomical continuity. To better capture the count nature of the data, we introduce a 3D-consistent prior for flow matching that combines a learned zero-inflated negative binomial (ZINB) prior with a spatial-empirical prior constructed from neighboring sections. A global attention block introduces 3D H&amp;E scaling linearly with the number of spots in the slide, enabling training and inference on large 3D ST datasets. Across three spatial transcriptomics datasets spanning different tissue types and resolutions, HoloTea consistently improves 3D expression accuracy and generalization compared to 2D and 3D baselines. We envision HoloTea advancing the creation of accurate 3D virtual tissues, ultimately accelerating biomarker discovery and deepening our understanding of disease.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-modal Generative AI: Multi-modal LLMs, Diffusions, and the Unification</title>
<link>https://arxiv.org/abs/2409.14993</link>
<guid>https://arxiv.org/abs/2409.14993</guid>
<content:encoded><![CDATA[
arXiv:2409.14993v3 Announce Type: replace-cross 
Abstract: Multi-modal generative AI (Artificial Intelligence) has attracted increasing attention from both academia and industry. Particularly, two dominant families of techniques have emerged: i) Multi-modal large language models (LLMs) demonstrate impressive ability for multi-modal understanding; and ii) Diffusion models exhibit remarkable multi-modal powers in terms of multi-modal generation. Therefore, this paper provides a comprehensive overview of multi-modal generative AI, including multi-modal LLMs, diffusions, and the unification for understanding and generation. To lay a solid foundation for unified models, we first provide a detailed review of both multi-modal LLMs and diffusion models respectively, including their probabilistic modeling procedure, multi-modal architecture design, and advanced applications to image/video LLMs as well as text-to-image/video generation. Furthermore, we explore the emerging efforts toward unified models for understanding and generation. To achieve the unification of understanding and generation, we investigate key designs including autoregressive-based and diffusion-based modeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then introduce several strategies for unified models, analyzing their potential advantages and disadvantages. In addition, we summarize the common datasets widely used for multi-modal generative AI pretraining. Last but not least, we present several challenging future research directions which may contribute to the ongoing advancement of multi-modal generative AI.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of Generative Learning Methods for Turbulence Surrogates</title>
<link>https://arxiv.org/abs/2411.16417</link>
<guid>https://arxiv.org/abs/2411.16417</guid>
<content:encoded><![CDATA[
arXiv:2411.16417v2 Announce Type: replace-cross 
Abstract: Numerical simulations of turbulent flows present significant challenges in fluid dynamics due to their complexity and high computational cost. High resolution techniques such as Direct Numerical Simulation (DNS) and Large Eddy Simulation (LES) are generally not computationally affordable, particularly for technologically relevant problems. Recent advances in machine learning, specifically in generative probabilistic models, offer promising alternatives as surrogates for turbulence. This paper investigates the application of three generative models - Variational Autoencoders (VAE), Deep Convolutional Generative Adversarial Networks (DCGAN), and Denoising Diffusion Probabilistic Models (DDPM) - in simulating a von K\'arm\'an vortex street around a fixed cylinder projected into 2D, as well as a real-world experimental dataset of the wake flow of a cylinder array. Training data was obtained by means of LES in the simulated case and Particle Image Velocimetry (PIV) in the experimental case. We evaluate each model's ability to capture the statistical properties and spatial structures of the turbulent flow. Our results demonstrate that DDPM and DCGAN effectively replicate all flow distributions, highlighting their potential as efficient and accurate tools for turbulence surrogacy. We find a strong argument for DCGAN, as although they are more difficult to train (due to problems such as mode collapse), they show the fastest inference and training time, require less data to train compared to VAE and DDPM, and provide the results most closely aligned with the input stream. In contrast, VAE train quickly (and can generate samples quickly) but do not produce adequate results, and DDPM, whilst effective, are significantly slower at both, inference and training time.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Medical Image Analysis through Geometric and Photometric transformations</title>
<link>https://arxiv.org/abs/2501.13643</link>
<guid>https://arxiv.org/abs/2501.13643</guid>
<content:encoded><![CDATA[
arXiv:2501.13643v2 Announce Type: replace-cross 
Abstract: Medical image analysis suffers from a lack of labeled data due to several challenges including patient privacy and lack of experts. Although some AI models only perform well with large amounts of data, we will move to data augmentation where there is a solution to improve the performance of our models and increase the dataset size through traditional or advanced techniques. In this paper, we evaluate the effectiveness of data augmentation techniques on two different medical image datasets. In the first step, we applied some transformation techniques to the skin cancer dataset containing benign and malignant classes. Then, we trained the convolutional neural network (CNN) on the dataset before and after augmentation, which significantly improved test accuracy from 90.74% to 96.88% and decreased test loss from 0.7921 to 0.1468 after augmentation. In the second step, we used the Mixup technique by mixing two random images and their corresponding masks using the retina and blood vessels dataset, then we trained the U-net model and obtained the Dice coefficient which increased from 0 before augmentation to 0.4163 after augmentation. The result shows the effect of using data augmentation to increase the dataset size on the classification and segmentation performance.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Cultural Differences in News Video Thumbnails via Computational Aesthetics</title>
<link>https://arxiv.org/abs/2505.21912</link>
<guid>https://arxiv.org/abs/2505.21912</guid>
<content:encoded><![CDATA[
arXiv:2505.21912v2 Announce Type: replace-cross 
Abstract: We propose a two-step approach for detecting differences in the style of images across sources of differing cultural affinity, where images are first clustered into finer visual themes based on content before their aesthetic features are compared. We test this approach on 2,400 YouTube video thumbnails taken equally from two U.S. and two Chinese YouTube channels, and relating equally to COVID-19 and the Ukraine conflict. Our results suggest that while Chinese thumbnails are less formal and more candid, U.S. channels tend to use more deliberate, proper photographs as thumbnails. In particular, U.S. thumbnails are less colorful, more saturated, darker, more finely detailed, less symmetric, sparser, less varied, and more up close and personal than Chinese thumbnails. We suggest that most of these differences reflect cultural preferences, and that our methods and observations can serve as a baseline against which suspected visual propaganda can be computed and compared.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Vision-Language Models for Evaluating World Models</title>
<link>https://arxiv.org/abs/2506.17967</link>
<guid>https://arxiv.org/abs/2506.17967</guid>
<content:encoded><![CDATA[
arXiv:2506.17967v2 Announce Type: replace-cross 
Abstract: World models - generative models that simulate environment dynamics conditioned on past observations and actions - are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency - capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce an evaluation protocol targeting two recognition tasks - action recognition and character recognition - each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a VLM-based evaluator for video world model rollouts adapted under data and compute constraints. In our extensive experiments totaling over 5,154 GPU-days, we explore full, partial, and parameter-efficient adaptation methods across various task formats, context lengths, sampling methods, and data compositions. The resulting unified evaluator achieves parity with task-specific checkpoints. Human studies across seven diverse environments confirm strong alignment with human judgments, establishing UNIVERSE as a lightweight, adaptable, and semantics-aware evaluator for video world models.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems</title>
<link>https://arxiv.org/abs/2508.00721</link>
<guid>https://arxiv.org/abs/2508.00721</guid>
<content:encoded><![CDATA[
arXiv:2508.00721v2 Announce Type: replace-cross 
Abstract: We present FMPlug, a novel plug-in framework that enhances foundation flow-matching (FM) priors for solving ill-posed inverse problems. Unlike traditional approaches that rely on domain-specific or untrained priors, FMPlug smartly leverages two simple but powerful insights: the similarity between observed and desired objects and the Gaussianity of generative flows. By introducing a time-adaptive warm-up strategy and sharp Gaussianity regularization, FMPlug unlocks the true potential of domain-agnostic foundation models. Our method beats state-of-the-art methods that use foundation FM priors by significant margins, on image super-resolution and Gaussian deblurring.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hestia: Voxel-Face-Aware Hierarchical Next-Best-View Acquisition for Efficient 3D Reconstruction</title>
<link>https://arxiv.org/abs/2508.01014</link>
<guid>https://arxiv.org/abs/2508.01014</guid>
<content:encoded><![CDATA[
arXiv:2508.01014v3 Announce Type: replace-cross 
Abstract: Advances in 3D reconstruction and novel view synthesis have enabled efficient and photorealistic rendering. However, images for reconstruction are still either largely manual or constrained by simple preplanned trajectories. To address this issue, recent works propose generalizable next-best-view planners that do not require online learning. Nevertheless, robustness and performance remain limited across various shapes. Hence, this study introduces Voxel-Face-Aware Hierarchical Next-Best-View Acquisition for Efficient 3D Reconstruction (Hestia), which addresses the shortcomings of the reinforcement learning-based generalizable approaches for five-degree-of-freedom viewpoint prediction. Hestia systematically improves the planners through four components: a more diverse dataset to promote robustness, a hierarchical structure to manage the high-dimensional continuous action search space, a close-greedy strategy to mitigate spurious correlations, and a face-aware design to avoid overlooking geometry. Experimental results show that Hestia achieves non-marginal improvements, with at least a 4% gain in coverage ratio, while reducing Chamfer Distance by 50% and maintaining real-time inference. In addition, Hestia outperforms prior methods by at least 12% in coverage ratio with a 5-image budget and remains robust to object placement variations. Finally, we demonstrate that Hestia, as a next-best-view planner, is feasible for the real-world application. Our project page is https://johnnylu305.github.io/hestia web.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRA-based methods on Unet for transfer learning in Subarachnoid Hematoma Segmentation</title>
<link>https://arxiv.org/abs/2508.01772</link>
<guid>https://arxiv.org/abs/2508.01772</guid>
<content:encoded><![CDATA[
arXiv:2508.01772v3 Announce Type: replace-cross 
Abstract: Aneurysmal subarachnoid hemorrhage (SAH) is a life-threatening neurological emergency with mortality rates exceeding 30%. Transfer learning from related hematoma types represents a potentially valuable but underexplored approach. Although Unet architectures remain the gold standard for medical image segmentation due to their effectiveness on limited datasets, Low-Rank Adaptation (LoRA) methods for parameter-efficient transfer learning have been rarely applied to convolutional neural networks in medical imaging contexts. We implemented a Unet architecture pre-trained on computed tomography scans from 124 traumatic brain injury patients across multiple institutions, then fine-tuned on 30 aneurysmal SAH patients from the University of Michigan Health System using 3-fold cross-validation. We developed a novel CP-LoRA method based on tensor CP-decomposition and introduced DoRA variants (DoRA-C, convDoRA, CP-DoRA) that decompose weight matrices into magnitude and directional components. We compared these approaches against existing LoRA methods (LoRA-C, convLoRA) and standard fine-tuning strategies across different modules on a multi-view Unet model. LoRA-based methods consistently outperformed standard Unet fine-tuning. Performance varied by hemorrhage volume, with all methods showing improved accuracy for larger volumes. CP-LoRA achieved comparable performance to existing methods while using significantly fewer parameters. Over-parameterization with higher ranks consistently yielded better performance than strictly low-rank adaptations. This study demonstrates that transfer learning between hematoma types is feasible and that LoRA-based methods significantly outperform conventional Unet fine-tuning for aneurysmal SAH segmentation.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable FPGA Framework for Real-Time Denoising in High-Throughput Imaging: A DRAM-Optimized Pipeline using High-Level Synthesis</title>
<link>https://arxiv.org/abs/2508.14917</link>
<guid>https://arxiv.org/abs/2508.14917</guid>
<content:encoded><![CDATA[
arXiv:2508.14917v2 Announce Type: replace-cross 
Abstract: High-throughput imaging workflows, such as Parallel Rapid Imaging with Spectroscopic Mapping (PRISM), generate data at rates that exceed conventional real-time processing capabilities. We present a scalable FPGA-based preprocessing pipeline for real-time denoising, implemented via High-Level Synthesis (HLS) and optimized for DRAM-backed buffering. Our architecture performs frame subtraction and averaging directly on streamed image data, minimizing latency through burst-mode AXI4 interfaces. The resulting kernel operates below the inter-frame interval, enabling inline denoising and reducing dataset size for downstream CPU/GPU analysis. Validated under PRISM-scale acquisition, this modular FPGA framework offers a practical solution for latency-sensitive imaging workflows in spectroscopy and microscopy.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting</title>
<link>https://arxiv.org/abs/2508.17811</link>
<guid>https://arxiv.org/abs/2508.17811</guid>
<content:encoded><![CDATA[
arXiv:2508.17811v2 Announce Type: replace-cross 
Abstract: Surface reconstruction has been widely studied in computer vision and graphics. However, existing surface reconstruction works struggle to recover accurate scene geometry when the input views are extremely sparse. To address this issue, we propose MeshSplat, a generalizable sparse-view surface reconstruction framework via Gaussian Splatting. Our key idea is to leverage 2DGS as a bridge, which connects novel view synthesis to learned geometric priors and then transfers these priors to achieve surface reconstruction. Specifically, we incorporate a feed-forward network to predict per-view pixel-aligned 2DGS, which enables the network to synthesize novel view images and thus eliminates the need for direct 3D ground-truth supervision. To improve the accuracy of 2DGS position and orientation prediction, we propose a Weighted Chamfer Distance Loss to regularize the depth maps, especially in overlapping areas of input views, and also a normal prediction network to align the orientation of 2DGS with normal vectors predicted by a monocular normal estimator. Extensive experiments validate the effectiveness of our proposed improvement, demonstrating that our method achieves state-of-the-art performance in generalizable sparse-view mesh reconstruction tasks. Project Page: https://hanzhichang.github.io/meshsplat_web
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioComposer: Leveraging Differentiable Geometry for Compositional Control of Anatomical Diffusion Models</title>
<link>https://arxiv.org/abs/2509.08015</link>
<guid>https://arxiv.org/abs/2509.08015</guid>
<content:encoded><![CDATA[
arXiv:2509.08015v2 Announce Type: replace-cross 
Abstract: Generative models of 3D cardiovascular anatomy can synthesize informative structures for clinical research and medical device evaluation, but face a trade-off between geometric controllability and realism. We propose CardioComposer: a programmable, inference-time framework for generating multi-class anatomical label maps based on interpretable ellipsoidal primitives. These primitives represent geometric attributes such as the size, shape, and position of discrete substructures. We specifically develop differentiable measurement functions based on voxel-wise geometric moments, enabling loss-based gradient guidance during diffusion model sampling. We demonstrate that these losses can constrain individual geometric attributes in a disentangled manner and provide compositional control over multiple substructures. Finally, we show that our method is compatible with a wide array of anatomical systems containing non-convex substructures, spanning cardiac, vascular, and skeletal organs.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions</title>
<link>https://arxiv.org/abs/2509.17177</link>
<guid>https://arxiv.org/abs/2509.17177</guid>
<content:encoded><![CDATA[
arXiv:2509.17177v3 Announce Type: replace-cross 
Abstract: We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High Resolution UDF Meshing via Iterative Networks</title>
<link>https://arxiv.org/abs/2509.17212</link>
<guid>https://arxiv.org/abs/2509.17212</guid>
<content:encoded><![CDATA[
arXiv:2509.17212v2 Announce Type: replace-cross 
Abstract: Unsigned Distance Fields (UDFs) are a natural implicit representation for open surfaces but, unlike Signed Distance Fields (SDFs), are challenging to triangulate into explicit meshes. This is especially true at high resolutions where neural UDFs exhibit higher noise levels, which makes it hard to capture fine details. Most current techniques perform within single voxels without reference to their neighborhood, resulting in missing surface and holes where the UDF is ambiguous or noisy. We show that this can be remedied by performing several passes and by reasoning on previously extracted surface elements to incorporate neighborhood information. Our key contribution is an iterative neural network that does this and progressively improves surface recovery within each voxel by spatially propagating information from increasingly distant neighbors. Unlike single-pass methods, our approach integrates newly detected surfaces, distance values, and gradients across multiple iterations, effectively correcting errors and stabilizing extraction in challenging regions. Experiments on diverse 3D models demonstrate that our method produces significantly more accurate and complete meshes than existing approaches, particularly for complex geometries, enabling UDF surface extraction at higher resolutions where traditional methods fail.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RadAgents: Multimodal Agentic Reasoning for Chest X-ray Interpretation with Radiologist-like Workflows</title>
<link>https://arxiv.org/abs/2509.20490</link>
<guid>https://arxiv.org/abs/2509.20490</guid>
<content:encoded><![CDATA[
arXiv:2509.20490v2 Announce Type: replace-cross 
Abstract: Agentic systems offer a potential path to solve complex clinical tasks through collaboration among specialized agents, augmented by tool use and external knowledge bases. Nevertheless, for chest X-ray (CXR) interpretation, prevailing methods remain limited: (i) reasoning is frequently neither clinically interpretable nor aligned with guidelines, reflecting mere aggregation of tool outputs; (ii) multimodal evidence is insufficiently fused, yielding text-only rationales that are not visually grounded; and (iii) systems rarely detect or resolve cross-tool inconsistencies and provide no principled verification mechanisms. To bridge the above gaps, we present RadAgents, a multi-agent framework that couples clinical priors with task-aware multimodal reasoning and encodes a radiologist-style workflow into a modular, auditable pipeline. In addition, we integrate grounding and multimodal retrieval-augmentation to verify and resolve context conflicts, resulting in outputs that are more reliable, transparent, and consistent with clinical practice.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v3 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OceanGym: A Benchmark Environment for Underwater Embodied Agents</title>
<link>https://arxiv.org/abs/2509.26536</link>
<guid>https://arxiv.org/abs/2509.26536</guid>
<content:encoded><![CDATA[
arXiv:2509.26536v2 Announce Type: replace-cross 
Abstract: We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimally Deep Networks - Adapting Model Depth to Datasets for Superior Efficiency</title>
<link>https://arxiv.org/abs/2510.10764</link>
<guid>https://arxiv.org/abs/2510.10764</guid>
<content:encoded><![CDATA[
arXiv:2510.10764v4 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) have provided brilliant performance across various tasks. However, this success often comes at the cost of unnecessarily large model sizes, high computational demands, and substantial memory footprints. Typically, powerful architectures are trained at full depths but not all datasets or tasks require such high model capacity. Training big and deep architectures on relatively low-complexity datasets frequently leads to wasted computation, unnecessary energy consumption, and excessive memory usage, which in turn makes deployment of models on resource-constrained devices impractical. To address this problem, we introduce the concept of Optimally Deep Networks (ODNs), which provides a balance between model depth and task complexity. Specifically, we propose a NAS like training strategy called progressive depth expansion, which begins by training neural networks at shallower depths and incrementally increases their depth as the earlier blocks converge, continuing this process until the target accuracy is reached. ODNs use only the optimal depth for the tasks at hand, removing redundant layers. This cuts down future training and inference costs, lowers the model memory footprint, enhances computational efficiency, and facilitates deployment on edge devices. Empirical results show that the optimal depths of ResNet-18 and ResNet-34 for MNIST and SVHN, achieve up to 98.64 % and 96.44 % reduction in memory footprint, while maintaining a competitive accuracy of 99.31 % and 96.08 %, respectively.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LightMem: Lightweight and Efficient Memory-Augmented Generation</title>
<link>https://arxiv.org/abs/2510.18866</link>
<guid>https://arxiv.org/abs/2510.18866</guid>
<content:encoded><![CDATA[
arXiv:2510.18866v2 Announce Type: replace-cross 
Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. On LongMemEval and LoCoMo, using GPT and Qwen backbones, LightMem consistently surpasses strong baselines, improving QA accuracy by up to 7.7% / 29.3%, reducing total token usage by up to 38x / 20.9x and API calls by up to 30x / 55.5x, while purely online test-time costs are even lower, achieving up to 106x / 117x token reduction and 159x / 310x fewer API calls. The code is available at https://github.com/zjunlp/LightMem.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GigaBrain-0: A World Model-Powered Vision-Language-Action Model</title>
<link>https://arxiv.org/abs/2510.19430</link>
<guid>https://arxiv.org/abs/2510.19430</guid>
<content:encoded><![CDATA[
arXiv:2510.19430v2 Announce Type: replace-cross 
Abstract: Training Vision-Language-Action (VLA) models for generalist robots typically requires large-scale real-world robot data, which is expensive and time-consuming to collect. The inefficiency of physical data collection severely limits the scalability, and generalization capacity of current VLA systems. To address this challenge, we introduce GigaBrain-0, a novel VLA foundation model empowered by world model-generated data (e.g., video generation, real2real transfer, human transfer, view transfer, sim2real transfer data). By leveraging world models to generate diverse data at scale, GigaBrain-0 significantly reduces reliance on real robot data while improving cross-task generalization. Our approach further improves policy robustness through RGBD input modeling and embodied Chain-of-Thought (CoT) supervision, enabling the model to reason about spatial geometry, object states, and long-horizon dependencies during task execution. This leads to substantial gains in real-world performance on dexterous, long-horizon, and mobile manipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achieves superior generalization across variations in appearances (e.g., textures, colors), object placements, and camera viewpoints. Additionally, we present GigaBrain-0-Small, an optimized lightweight variant designed to run efficiently on devices such as the NVIDIA Jetson AGX Orin.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.12937</link>
<guid>https://arxiv.org/abs/2511.12937</guid>
<content:encoded><![CDATA[
arXiv:2511.12937v2 Announce Type: replace-cross 
Abstract: Cross-platform strategy game automation remains a challenge due to diverse user interfaces and dynamic battlefield environments. Existing Vision--Language Models (VLMs) struggle with generalization across heterogeneous platforms and lack precision in interface understanding and action execution. We introduce Yanyun-3, a VLM-based agent that integrates Qwen2.5-VL for visual reasoning and UI-TARS for interface execution. We propose a novel data organization principle -- combination granularity -- to distinguish intra-sample fusion and inter-sample mixing of multimodal data (static images, multi-image sequences, and videos). The model is fine-tuned using QLoRA on a curated dataset across three strategy game platforms. The optimal strategy (M*V+S) achieves a 12.98x improvement in BLEU-4 score and a 63% reduction in inference time compared to full fusion. Yanyun-3 successfully executes core tasks (e.g., target selection, resource allocation) across platforms without platform-specific tuning. Our findings demonstrate that structured multimodal data organization significantly enhances VLM performance in embodied tasks. Yanyun-3 offers a generalizable framework for GUI automation, with broader implications for robotics and autonomous systems.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MHR: Momentum Human Rig</title>
<link>https://arxiv.org/abs/2511.15586</link>
<guid>https://arxiv.org/abs/2511.15586</guid>
<content:encoded><![CDATA[
arXiv:2511.15586v3 Announce Type: replace-cross 
Abstract: We present MHR, a parametric human body model that combines the decoupled skeleton/shape paradigm of ATLAS with a flexible, modern rig and pose corrective system inspired by the Momentum library. Our model enables expressive, anatomically plausible human animation, supporting non-linear pose correctives, and is designed for robust integration in AR/VR and graphics pipelines.
]]></content:encoded>
<pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.12547</link>
<guid>https://arxiv.org/abs/2511.12547</guid>
<content:encoded><![CDATA[
<div> Keywords: generative diffusion models, fine-grained augmentation, classifier guidance, temporal dynamics, confidence modulation<br /><br />Summary:<br /><br />Generative diffusion models have shown potential for data augmentation but face challenges in fine-grained visual categorization (FGVC) due to the need for capturing subtle, category-specific features. Standard methods like text-based Classifier-Free Guidance (CFG) often fail to provide the level of specificity required, sometimes generating misleading images that harm classifier performance. To overcome this, the authors propose Hierarchically Guided Fine-grained Augmentation (HiGFA), a novel approach that leverages the temporal sampling stages of diffusion models. HiGFA applies strong text and transformed contour guidance with fixed strength during early-to-mid sampling to set up the overall scene, style, and structure. In the final sampling stages, it shifts to specialized fine-grained classifier guidance while dynamically adjusting the strengths of all guidance signals based on their confidence predictions. This hierarchical and confidence-driven strategy balances the global formation of structure with detailed refinement, resulting in synthetic images that are both diverse and faithful to category-specific features. Experimental results on multiple FGVC datasets demonstrate HiGFA’s effectiveness, showing improved synthetic image quality and enhanced fine-grained classification performance. <div>
arXiv:2511.12547v2 Announce Type: replace 
Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation</title>
<link>https://arxiv.org/abs/2511.12919</link>
<guid>https://arxiv.org/abs/2511.12919</guid>
<content:encoded><![CDATA[
<div> Keywords: 6D pose estimation, one-reference view, autoregressive model, coordinate map tokenization, transformer decoder  

<br /><br />Summary:  
This article addresses the challenge of 6D pose estimation for novel objects without access to their full 3D models, a key task in robotics and augmented reality. It proposes CoordAR, an innovative autoregressive framework designed to estimate 6D object poses using only a single reference view, alleviating the need for comprehensive 3D data. CoordAR represents 3D-3D correspondences between the query and reference images as discrete tokens generated in a probabilistic, autoregressive manner, improving global consistency compared to traditional coordinate regression methods. The approach introduces a novel coordinate map tokenization scheme that discretizes the 3D space for probabilistic prediction, enhancing accuracy. Moreover, it employs a modality-decoupled encoding strategy that processes RGB appearance and coordinate information separately to better capture distinct features. CoordAR’s core architecture features an autoregressive transformer decoder conditioned on both position-aligned query features and the sequence of previously generated tokens, enabling effective correspondence regression. Experimental results demonstrate that CoordAR substantially outperforms existing state-of-the-art methods across multiple benchmarks, showing remarkable robustness to common challenges such as symmetry, occlusion, and real-world complexities. This work represents a significant advancement in one-reference view 6D pose estimation with improved consistency and uncertainty handling. <div>
arXiv:2511.12919v2 Announce Type: replace 
Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer</title>
<link>https://arxiv.org/abs/2511.14111</link>
<guid>https://arxiv.org/abs/2511.14111</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Transformers, Cascaded-ViT, Cascaded-Chunk Feed Forward Network, Energy Efficiency, Accuracy-Per-FLOP (APF)  <br /><br />Summary:<br /><br />Vision Transformers (ViTs) have shown strong performance in computer vision but are often limited by high computational, memory, and energy requirements, restricting their use in resource-constrained environments. This paper introduces Cascaded-ViT (CViT), a lightweight and compute-efficient vision transformer architecture that incorporates a novel feedforward design termed Cascaded-Chunk Feed Forward Network (CCFFN). CCFFN enhances parameter and FLOP efficiency by splitting input features, achieving this without compromising accuracy. Experimental results on ImageNet-1K demonstrate that the CViT-XL model attains a Top-1 accuracy of 75.5%, while reducing FLOPs by 15% and energy consumption by 3.3% compared to EfficientViT-M5. Across varied model sizes, CViT consistently maintains the lowest energy consumption, making it well-suited for deployment on battery-limited devices such as mobile phones and drones. Additionally, a newly proposed metric, Accuracy-Per-FLOP (APF), which assesses compute efficiency relative to accuracy, ranks CViT models among the top performers. Notably, CViT-L surpasses EfficientViT-M2 in accuracy by 2.2% while maintaining comparable APF scores, underscoring its balanced efficiency and performance. <div>
arXiv:2511.14111v2 Announce Type: replace 
Abstract: Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs</title>
<link>https://arxiv.org/abs/2511.14169</link>
<guid>https://arxiv.org/abs/2511.14169</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, token compression, object-level token merging, human vision system, computational efficiency<br /><br />Summary:<br /><br />1. Multimodal Large Language Models (MLLMs) effectively integrate text and image understanding by converting images into sequences of patch-level tokens aligned with their architectures. <br />2. However, patch-level tokenization causes a quadratic increase in the number of image tokens, which results in significant computational and memory demands. <br />3. This tokenization approach also misaligns with the human vision cognition system, leading to hallucinations and unnecessary computational redundancy. <br />4. To mitigate these issues, the authors propose an object-level token merging strategy termed Adaptive Token compression, which aligns more closely with human vision processes. <br />5. Experimental results across multiple benchmarks demonstrate that their method uses only about 10% of the tokens compared to vanilla models while maintaining nearly 96% of the original performance. <br />6. Furthermore, comprehensive comparisons with relevant works confirm that this approach effectively balances token compression and model accuracy. <br />7. The authors plan to release their code publicly to facilitate further research and application. <div>
arXiv:2511.14169v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstantViR: Real-Time Video Inverse Problem Solver with Distilled Diffusion Prior</title>
<link>https://arxiv.org/abs/2511.14208</link>
<guid>https://arxiv.org/abs/2511.14208</guid>
<content:encoded><![CDATA[
<div> Video inverse problems, diffusion models, video reconstruction, amortized inference, real-time processing  

<br /><br />Summary:  
1. The paper addresses video inverse problems important for applications like streaming, telepresence, and AR/VR, emphasizing the need for high perceptual quality alongside low latency.  
2. Existing diffusion-based methods provide strong reconstruction quality but either suffer from temporal artifacts due to image diffusion models with temporal regularizers or are too slow for real-time use when using native video diffusion models.  
3. The authors propose InstantViR, an amortized inference framework that uses a pre-trained video diffusion prior and distills a bidirectional video diffusion teacher model into a causal autoregressive student model, which restores degraded videos in a single forward pass.  
4. This distillation is prior-driven, needing only the teacher model and known degradation operators, and does not require paired clean/noisy video training data.  
5. To further improve speed, the paper introduces a LeanVAE backbone replacing the original VAE via a teacher-space regularized distillation, enabling fast latent-space processing.  
6. Experiments across tasks like streaming random inpainting, Gaussian deblurring, and super-resolution demonstrate that InstantViR matches or exceeds diffusion-based baselines’ quality while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100x speedups.  
7. The results establish that diffusion-based video reconstruction can be practical for real-time, interactive, and streaming use cases, integrating high-quality video restoration into modern vision applications. <div>
arXiv:2511.14208v2 Announce Type: replace 
Abstract: Video inverse problems are fundamental to streaming, telepresence, and AR/VR, where high perceptual quality must coexist with tight latency constraints. Diffusion-based priors currently deliver state-of-the-art reconstructions, but existing approaches either adapt image diffusion models with ad hoc temporal regularizers - leading to temporal artifacts - or rely on native video diffusion models whose iterative posterior sampling is far too slow for real-time use. We introduce InstantViR, an amortized inference framework for ultra-fast video reconstruction powered by a pre-trained video diffusion prior. We distill a powerful bidirectional video diffusion model (teacher) into a causal autoregressive student that maps a degraded video directly to its restored version in a single forward pass, inheriting the teacher's strong temporal modeling while completely removing iterative test-time optimization. The distillation is prior-driven: it only requires the teacher diffusion model and known degradation operators, and does not rely on externally paired clean/noisy video data. To further boost throughput, we replace the video-diffusion backbone VAE with a high-efficiency LeanVAE via an innovative teacher-space regularized distillation scheme, enabling low-latency latent-space processing. Across streaming random inpainting, Gaussian deblurring and super-resolution, InstantViR matches or surpasses the reconstruction quality of diffusion-based baselines while running at over 35 FPS on NVIDIA A100 GPUs, achieving up to 100 times speedups over iterative video diffusion solvers. These results show that diffusion-based video reconstruction is compatible with real-time, interactive, editable, streaming scenarios, turning high-quality video restoration into a practical component of modern vision systems.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2D Gaussians Spatial Transport for Point-supervised Density Regression</title>
<link>https://arxiv.org/abs/2511.14477</link>
<guid>https://arxiv.org/abs/2511.14477</guid>
<content:encoded><![CDATA[
<div> Gaussian Spatial Transport, Gaussian splatting, transport plan, pixel-annotation correspondence, computer vision tasks<br /><br />Summary:<br /><br />This paper introduces Gaussian Spatial Transport (GST), a novel framework that uses Gaussian splatting to map probability measures from image coordinate space to annotation maps. The authors propose a Gaussian splatting-based method to estimate pixel-annotation correspondence accurately. Leveraging this correspondence, they compute a transport plan grounded in Bayesian probability, offering a theoretically sound and practical approach. To integrate this transport plan into common neural network training workflows, the paper derives a specialized loss function that quantifies discrepancy after transport, allowing end-to-end optimization. The method's efficacy is validated through extensive experiments on representative computer vision tasks such as crowd counting and landmark detection, demonstrating improved results. Compared to traditional optimal transport methods, GST significantly improves training efficiency by eliminating expensive iterative computations of the transport plan during training. This enhancement makes GST more scalable and practical for real-world applications. The authors also provide open-source code to encourage further research and application of their framework. Overall, GST presents a novel, efficient, and effective approach for aligning image probabilities to annotation maps, with promising impact on tasks requiring precise spatial correspondence in computer vision. <div>
arXiv:2511.14477v2 Announce Type: replace 
Abstract: This paper introduces Gaussian Spatial Transport (GST), a novel framework that leverages Gaussian splatting to facilitate transport from the probability measure in the image coordinate space to the annotation map. We propose a Gaussian splatting-based method to estimate pixel-annotation correspondence, which is then used to compute a transport plan derived from Bayesian probability. To integrate the resulting transport plan into standard network optimization in typical computer vision tasks, we derive a loss function that measures discrepancy after transport. Extensive experiments on representative computer vision tasks, including crowd counting and landmark detection, validate the effectiveness of our approach. Compared to conventional optimal transport schemes, GST eliminates iterative transport plan computation during training, significantly improving efficiency. Code is available at https://github.com/infinite0522/GST.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fusing Biomechanical and Spatio-Temporal Features for Fall Prediction: Characterizing and Mitigating the Simulation-to-Reality Gap</title>
<link>https://arxiv.org/abs/2511.14620</link>
<guid>https://arxiv.org/abs/2511.14620</guid>
<content:encoded><![CDATA[
<div> Keywords: fall prediction, biomechanical data, spatio-temporal graph convolutional network, simulation-reality gap, personalization  

<br /><br />Summary:  
Falls are a major cause of injury and decreased independence among older adults, prompting the need for effective fall prediction systems. Vision-based fall prediction offers a non-invasive approach to anticipate falls moments before impact, but the limited availability of real-world fall data poses development challenges. This study introduces the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model integrating pose and biomechanical information via a cross-attention fusion mechanism, which outperforms the baseline ST-GCN by 5.32% and 2.91% F1-score on simulated stunt-actor and MUVIM datasets. The model’s spatio-temporal attention mechanism provides interpretability by highlighting critical joints and temporal phases involved in falls. Despite achieving an 89.0% F1-score with full supervision on simulated data, the model’s zero-shot generalization to unseen subjects falls sharply to 35.9%, indicating a significant simulation-to-reality gap likely caused by biases such as "intent-to-fall" cues. This gap widens for older adults with conditions like diabetes or frailty due to their unique movement patterns. To overcome these limitations, the study proposes personalization strategies and advocates for privacy-preserving data pipelines to facilitate real-world validation. Ultimately, bridging the gap between simulated and real-world data remains critical to developing reliable fall prediction systems for vulnerable elderly populations. <div>
arXiv:2511.14620v2 Announce Type: replace 
Abstract: Falls are a leading cause of injury and loss of independence among older adults. Vision-based fall prediction systems offer a non-invasive solution to anticipate falls seconds before impact, but their development is hindered by the scarcity of available fall data. Contributing to these efforts, this study proposes the Biomechanical Spatio-Temporal Graph Convolutional Network (BioST-GCN), a dual-stream model that combines both pose and biomechanical information using a cross-attention fusion mechanism. Our model outperforms the vanilla ST-GCN baseline by 5.32% and 2.91% F1-score on the simulated MCF-UA stunt-actor and MUVIM datasets, respectively. The spatio-temporal attention mechanisms in the ST-GCN stream also provide interpretability by identifying critical joints and temporal phases. However, a critical simulation-reality gap persists. While our model achieves an 89.0% F1-score with full supervision on simulated data, zero-shot generalization to unseen subjects drops to 35.9%. This performance decline is likely due to biases in simulated data, such as 'intent-to-fall' cues. For older adults, particularly those with diabetes or frailty, this gap is exacerbated by their unique kinematic profiles. To address this, we propose personalization strategies and advocate for privacy-preserving data pipelines to enable real-world validation. Our findings underscore the urgent need to bridge the gap between simulated and real-world data to develop effective fall prediction systems for vulnerable elderly populations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RN-SDEs: Limited-Angle CT Reconstruction with Residual Null-Space Diffusion Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2409.13930</link>
<guid>https://arxiv.org/abs/2409.13930</guid>
<content:encoded><![CDATA[
<div> Limited Angle Computed Tomography, Residual Null-Space Diffusion, Stochastic Differential Equations, Range-Null Space Decomposition, Image Reconstruction<br /><br />Summary:<br /><br />This paper addresses the challenge of Limited Angle Computed Tomography (LACT), where incomplete scanning angles cause distortions and artifacts in reconstructed images. To overcome this, the authors propose a novel approach called Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which incorporate mean-reverting stochastic differential equations to model the diffusion process more effectively. The RN-SDEs are designed to serve as a learned prior that enhances image reconstruction quality despite severe degradation. The study demonstrates the generalizability of RN-SDEs through application on two distinct LACT datasets, ChromSTEM and C4KC-KiTS, validating its wide applicability. A key innovation includes the use of Range-Null Space Decomposition (RNSD) based rectification, which enforces data consistency and improves reconstruction accuracy. Extensive experiments reveal that the proposed method outperforms existing state-of-the-art techniques across a broad range of LACT tasks. Furthermore, the paper provides a rigorous quantitative comparison highlighting the computational complexity and runtime efficiency of RN-SDEs, establishing their advantage not only in reconstruction quality but also in practical usability. Overall, this work offers a powerful and efficient solution for recovering high-quality images in challenging limited angle CT scenarios. <div>
arXiv:2409.13930v3 Announce Type: replace-cross 
Abstract: Computed tomography is a widely used imaging modality with applications ranging from medical imaging to material analysis. One major challenge arises from the lack of scanning information at certain angles, resulting in distortion or artifacts in the reconstructed images. This is referred to as the Limited Angle Computed Tomography (LACT) reconstruction problem. To address this problem, we propose the use of Residual Null-Space Diffusion Stochastic Differential Equations (RN-SDEs), which are a variant of diffusion models that characterize the diffusion process with mean-reverting (MR) stochastic differential equations. To demonstrate the generalizability of RN-SDEs, we conducted experiments with two different LACT datasets, ChromSTEM and C4KC-KiTS. Through extensive experiments, we demonstrate that by leveraging learned MR-SDEs as a prior and emphasizing data consistency using Range-Null Space Decomposition (RNSD) based rectification, we can recover high-quality images from severely degraded ones and achieve state-of-the-art performance in most LACT tasks. Additionally, we present a quantitative comparison of RN-SDE with other networks, in terms of computational complexity and runtime efficiency, highlighting the superior effectiveness of our proposed approach.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal AI for Body Fat Estimation: Computer Vision and Anthropometry with DEXA Benchmarks</title>
<link>https://arxiv.org/abs/2511.17576</link>
<guid>https://arxiv.org/abs/2511.17576</guid>
<content:encoded><![CDATA[
<div> Body Fat Percentage, AI Models, ResNet, Anthropometric Data, Low-Cost Estimation<br /><br />Summary:<br /><br />1. The study addresses the challenge of tracking body fat percentage, which is important for effective weight management, noting the limitations of expensive gold-standard methods like DEXA scans. <br />2. It proposes artificial intelligence (AI) models as accessible, low-cost alternatives utilizing frontal body images and basic anthropometric measurements. <br />3. Researchers compiled a unique dataset of 535 samples, combining 253 cases with anthropometric data (weight, height, neck, ankle, wrist) and 282 images scraped from Reddit posts where users self-reported their body fat percentages, some claiming DEXA-derived values. <br />4. Two model approaches were developed: (1) image-based models built on the ResNet architecture, and (2) regression models relying solely on anthropometric measurements. <br />5. The best-performing image-based model yielded a Root Mean Square Error (RMSE) of 4.44% and a coefficient of determination (R^2) of 0.807, indicating good predictive performance. <br />6. The study also outlines a multimodal fusion framework for combining image and measurement data, which could be employed when paired datasets are available in the future. <br />7. Overall, results indicate AI-assisted methods can provide practical, low-cost estimations of body fat, with potential applications for consumer health and fitness technologies. <div>
arXiv:2511.17576v1 Announce Type: new 
Abstract: Tracking body fat percentage is essential for effective weight management, yet gold-standard methods such as DEXA scans remain expensive and inaccessible for most people. This study evaluates the feasibility of artificial intelligence (AI) models as low-cost alternatives using frontal body images and basic anthropometric data. The dataset consists of 535 samples: 253 cases with recorded anthropometric measurements (weight, height, neck, ankle, and wrist) and 282 images obtained via web scraping from Reddit posts with self-reported body fat percentages, including some reported as DEXA-derived by the original posters. Because no public datasets exist for computer-vision-based body fat estimation, this dataset was compiled specifically for this study. Two approaches were developed: (1) ResNet-based image models and (2) regression models using anthropometric measurements. A multimodal fusion framework is also outlined for future expansion once paired datasets become available. The image-based model achieved a Root Mean Square Error (RMSE) of 4.44% and a Coefficient of Determination (R^2) of 0.807. These findings demonstrate that AI-assisted models can offer accessible and low-cost body fat estimates, supporting future consumer applications in health and fitness.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction-Driven Multimodal Representation Learning for Automated Media Understanding</title>
<link>https://arxiv.org/abs/2511.17596</link>
<guid>https://arxiv.org/abs/2511.17596</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Autoencoder, metadata extraction, broadcast media, LUMA dataset, reconstruction loss<br /><br />Summary:<br /><br />1. The article addresses the challenge in broadcast and media organizations of automating content indexing, tagging, and metadata generation using AI systems that are typically unimodal and thus limited in understanding complex cross-modal relationships.<br /><br />2. To overcome this, the authors propose a Multimodal Autoencoder (MMAE) designed to learn unified representations across text, audio, and visual modalities, enabling end-to-end automation of metadata extraction and semantic clustering.<br /><br />3. The MMAE is trained on the newly introduced LUMA dataset, which contains fully aligned multimodal triplets that reflect real-world media content, allowing for realistic training conditions.<br /><br />4. The model works by minimizing joint reconstruction losses across different modalities, which helps it discover modality-invariant semantic structures without the need for large paired or contrastive datasets.<br /><br />5. Experimental results show significant improvements in clustering and alignment metrics such as Silhouette score, Adjusted Rand Index (ARI), and Normalized Mutual Information (NMI) compared to linear baselines. These findings suggest that reconstruction-based multimodal embeddings can be foundational for scalable metadata generation and cross-modal retrieval in broadcast archives, enhancing automation, searchability, and efficiency in content management workflows. <div>
arXiv:2511.17596v1 Announce Type: new 
Abstract: Broadcast and media organizations increasingly rely on artificial intelligence to automate the labor-intensive processes of content indexing, tagging, and metadata generation. However, existing AI systems typically operate on a single modality-such as video, audio, or text-limiting their understanding of complex, cross-modal relationships in broadcast material. In this work, we propose a Multimodal Autoencoder (MMAE) that learns unified representations across text, audio, and visual data, enabling end-to-end automation of metadata extraction and semantic clustering. The model is trained on the recently introduced LUMA dataset, a fully aligned benchmark of multimodal triplets representative of real-world media content. By minimizing joint reconstruction losses across modalities, the MMAE discovers modality-invariant semantic structures without relying on large paired or contrastive datasets. We demonstrate significant improvements in clustering and alignment metrics (Silhouette, ARI, NMI) compared to linear baselines, indicating that reconstruction-based multimodal embeddings can serve as a foundation for scalable metadata generation and cross-modal retrieval in broadcast archives. These results highlight the potential of reconstruction-driven multimodal learning to enhance automation, searchability, and content management efficiency in modern broadcast workflows.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BCWildfire: A Long-term Multi-factor Dataset and Deep Learning Benchmark for Boreal Wildfire Risk Prediction</title>
<link>https://arxiv.org/abs/2511.17597</link>
<guid>https://arxiv.org/abs/2511.17597</guid>
<content:encoded><![CDATA[
<div> Wildfire, Dataset, Time-series Forecasting, Multimodal, British Columbia<br /><br />Summary:<br /><br />1. The paper addresses the challenge of wildfire risk prediction, which is complicated by the interplay of fuel conditions, meteorology, topography, and human activities.<br />2. It highlights the scarcity of comprehensive publicly available benchmark datasets that combine long-term temporal data, extensive spatial coverage, and multiple data modalities.<br />3. To fill this gap, the authors introduce a new dataset spanning 25 years at daily resolution, covering 240 million hectares in British Columbia and surrounding areas.<br />4. The dataset includes 38 covariates representing various wildfire drivers such as active fire detections, weather variables, fuel and terrain characteristics, and human-related factors.<br />5. Using this dataset, the study evaluates different time-series forecasting models including CNN-based, linear, Transformer-based, and Mamba-based architectures.<br />6. The authors also explore the impact of position embedding techniques and analyze the relative importance of different factors driving wildfire risk.<br />7. The dataset and code are publicly accessible at the provided GitHub repository, promoting reproducibility and further research in wildfire prediction. <div>
arXiv:2511.17597v1 Announce Type: new 
Abstract: Wildfire risk prediction remains a critical yet challenging task due to the complex interactions among fuel conditions, meteorology, topography, and human activity. Despite growing interest in data-driven approaches, publicly available benchmark datasets that support long-term temporal modeling, large-scale spatial coverage, and multimodal drivers remain scarce. To address this gap, we present a 25-year, daily-resolution wildfire dataset covering 240 million hectares across British Columbia and surrounding regions. The dataset includes 38 covariates, encompassing active fire detections, weather variables, fuel conditions, terrain features, and anthropogenic factors. Using this benchmark, we evaluate a diverse set of time-series forecasting models, including CNN-based, linear-based, Transformer-based, and Mamba-based architectures. We also investigate effectiveness of position embedding and the relative importance of different fire-driving factors. The dataset and the corresponding code can be found at https://github.com/SynUW/mmFire
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness of Structured Data Extraction from Perspectively Distorted Documents</title>
<link>https://arxiv.org/abs/2511.17607</link>
<guid>https://arxiv.org/abs/2511.17607</guid>
<content:encoded><![CDATA[
<div> Keywords: Optical Character Recognition, Multi-modal Large Language Models, Perspective Distortion, Gemini-1.5-pro, Structure-Recognition Accuracy<br /><br />Summary:<br /><br />1. The paper addresses Optical Character Recognition (OCR) for data extraction in documents, which is important for applications like digitizing medical records and recognizing road signs. <br /><br />2. Multi-modal Large Language Models (LLMs), specifically Gemini-1.5-pro, have shown strong performance in OCR tasks, but their accuracy can be affected by image distortions. <br /><br />3. While previous work focused on the impact of in-plane rotations on OCR accuracy, this study investigates the effects of perspective distortions commonly found in real-world document images, which have higher complexity and degrees of freedom.<br /><br />4. The authors model typical perspective distortions as isosceles-trapezoidal transformations, reducing the complexity from eight parameters to two key parameters: rotation angle and distortion ratio.<br /><br />5. Experiments using synthetically generated documents varying these parameters evaluate both character-recognition accuracy (traditional OCR accuracy) and structure-recognition accuracy (correctness of reading order). <br /><br />6. Results show that while character-recognition accuracy degrades moderately with distortion, structure-recognition accuracy suffers significantly.<br /><br />7. Importantly, a simple rotational correction can improve structure-recognition accuracy, highlighting a practical preprocessing step to enhance OCR performance with multi-modal LLMs on distorted documents.<br /><br />8. These findings provide useful insights for deploying LLM-based OCR in real-world scenarios with perspective distortions. <div>
arXiv:2511.17607v1 Announce Type: new 
Abstract: Optical Character Recognition (OCR) for data extraction from documents is essential to intelligent informatics, such as digitizing medical records and recognizing road signs. Multi-modal Large Language Models (LLMs) can solve this task and have shown remarkable performance. Recently, it has been noticed that the accuracy of data extraction by multi-modal LLMs can be affected when in-plane rotations are present in the documents. However, real-world document images are usually not only in-plane rotated but also perspectively distorted. This study investigates the impacts of such perturbations on the data extraction accuracy for the state-of-the-art model, Gemini-1.5-pro. Because perspective distortions have a high degree of freedom, designing experiments in the same manner as single-parametric rotations is difficult. We observed typical distortions of document images and showed that most of them approximately follow an isosceles-trapezoidal transformation, which allows us to evaluate distortions with a small number of parameters. We were able to reduce the number of independent parameters from eight to two, i.e. rotation angle and distortion ratio. Then, specific entities were extracted from synthetically generated sample documents with varying these parameters. As the performance of LLMs, we evaluated not only a character-recognition accuracy but also a structure-recognition accuracy. Whereas the former represents the classical indicators for optical character recognition, the latter is related to the correctness of reading order. In particular, the structure-recognition accuracy was found to be significantly degraded by document distortion. In addition, we found that this accuracy can be improved by a simple rotational correction. This insight will contribute to the practical use of multi-modal LLMs for OCR tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF</title>
<link>https://arxiv.org/abs/2511.17609</link>
<guid>https://arxiv.org/abs/2511.17609</guid>
<content:encoded><![CDATA[
<div> Keywords: Unscented Kalman Filter, multi-camera tracking, 3D ground truth, pose keypoints, homography projection

<br /><br />Summary:  
This paper presents a novel approach for accurate 3D ground truth estimation crucial for applications like autonomous navigation, surveillance, and robotics. The method uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint annotations from multiple calibrated cameras into precise 3D positions. By leveraging human-annotated 2D data, the proposed multi-camera single-object tracking algorithm projects 2D coordinates into robust 3D world coordinates using homography-based projection combined with UKF fusion. The approach processes multi-view input to estimate both object locations and shapes. It effectively handles common challenges such as occlusions, enabling reliable tracking across views. Evaluation on well-known datasets including CMC, Wildtrack, and Panoptic demonstrates its high accuracy in 3D localization, outperforming existing methods that provide only ground-plane positioning. Importantly, this method estimates the full 3D shape of objects, not just their location on the ground plane. Additionally, it offers a scalable and fully automatic solution suitable for multi-camera systems relying solely on 2D image annotations, removing the need for manual 3D labeling or specialized sensors. <div>
arXiv:2511.17609v1 Announce Type: new 
Abstract: Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unified Low-Light Traffic Image Enhancement via Multi-Stage Illumination Recovery and Adaptive Noise Suppression</title>
<link>https://arxiv.org/abs/2511.17612</link>
<guid>https://arxiv.org/abs/2511.17612</guid>
<content:encoded><![CDATA[
<div> low-light enhancement, traffic images, unsupervised learning, image decomposition, deep learning<br /><br />Summary: Enhancing low-light traffic images is essential for improving perception in autonomous driving, intelligent transportation, and urban surveillance. Due to challenges like low illumination, noise, motion blur, and glare from headlights and street lamps, nighttime traffic images often have poor visibility, which hinders object detection and scene understanding. The authors propose a fully unsupervised multi-stage deep learning framework specifically designed for low-light traffic image enhancement. The framework works by decomposing input images into illumination and reflectance components and progressively refining them through three specialized modules: Illumination Adaptation to correct brightness globally and locally; Reflectance Restoration employing spatial-channel attention to suppress noise and recover structural details; and Over-Exposure Compensation to reconstruct saturated areas and balance scene luminance. The network is trained without paired ground-truth images, relying on self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses. Experimental results on general and traffic-specific datasets show the method outperforms state-of-the-art techniques quantitatively (via PSNR, SSIM, LPIPS, NIQE) and qualitatively. The framework successfully enhances visibility and preserves image structure, thus improving the reliability of downstream perception tasks in real-world low-light traffic environments. <div>
arXiv:2511.17612v1 Announce Type: new 
Abstract: Enhancing low-light traffic images is crucial for reliable perception in autonomous driving, intelligent transportation, and urban surveillance systems. Nighttime and dimly lit traffic scenes often suffer from poor visibility due to low illumination, noise, motion blur, non-uniform lighting, and glare from vehicle headlights or street lamps, which hinder tasks such as object detection and scene understanding. To address these challenges, we propose a fully unsupervised multi-stage deep learning framework for low-light traffic image enhancement. The model decomposes images into illumination and reflectance components, progressively refined by three specialized modules: (1) Illumination Adaptation, for global and local brightness correction; (2) Reflectance Restoration, for noise suppression and structural detail recovery using spatial-channel attention; and (3) Over-Exposure Compensation, for reconstructing saturated regions and balancing scene luminance. The network is trained using self-supervised reconstruction, reflectance smoothness, perceptual consistency, and domain-aware regularization losses, eliminating the need for paired ground-truth images. Experiments on general and traffic-specific datasets demonstrate superior performance over state-of-the-art methods in both quantitative metrics (PSNR, SSIM, LPIPS, NIQE) and qualitative visual quality. Our approach enhances visibility, preserves structure, and improves downstream perception reliability in real-world low-light traffic scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSMix: Hard and Soft Mixing Data Augmentation for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.17614</link>
<guid>https://arxiv.org/abs/2511.17614</guid>
<content:encoded><![CDATA[
<div> Keywords: medical image segmentation, data augmentation, HSMix, superpixels, brightness mixing<br /><br />Summary:<br /><br />1. Medical image segmentation is often hindered by data scarcity due to the high cost of annotation and the rarity of some diseases, which leads to overfitting.<br />2. Traditional strategies like self-supervised and semi-supervised learning help mitigate this but involve complexities such as hand-crafted pretexts or precise pseudo-labeling.<br />3. Data augmentation offers a simpler alternative to address data scarcity and has improved performance in image recognition; however, local image editing augmentation techniques for segmentation remain underexplored.<br />4. The authors propose HSMix, a novel data augmentation method involving hard and soft mixing of medical images using homogeneous regions (superpixels) from two source images, combined with brightness adjustments based on pixel-wise saliency coefficients.<br />5. Corresponding ground-truth segmentation masks undergo the same mixing to ensure label consistency.<br />6. HSMix leverages contour and saliency information to preserve local semantic integrity while enhancing augmentation diversity.<br />7. It is a plug-and-play, model-agnostic method applicable across various medical imaging modalities.<br />8. Extensive experiments show its effectiveness across diverse medical segmentation tasks.<br />9. The source code is publicly available at the provided GitHub repository for reproducibility and further research. <div>
arXiv:2511.17614v1 Announce Type: new 
Abstract: Due to the high cost of annotation or the rarity of some diseases, medical image segmentation is often limited by data scarcity and the resulting overfitting problem. Self-supervised learning and semi-supervised learning can mitigate the data scarcity challenge to some extent. However, both of these paradigms are complex and require either hand-crafted pretexts or well-defined pseudo-labels. In contrast, data augmentation represents a relatively simple and straightforward approach to addressing data scarcity issues. It has led to significant improvements in image recognition tasks. However, the effectiveness of local image editing augmentation techniques in the context of segmentation has been less explored. We propose HSMix, a novel approach to local image editing data augmentation involving hard and soft mixing for medical semantic segmentation. In our approach, a hard-augmented image is created by combining homogeneous regions (superpixels) from two source images. A soft mixing method further adjusts the brightness of these composed regions with brightness mixing based on locally aggregated pixel-wise saliency coefficients. The ground-truth segmentation masks of the two source images undergo the same mixing operations to generate the associated masks for the augmented images. Our method fully exploits both the prior contour and saliency information, thus preserving local semantic information in the augmented images while enriching the augmentation space with more diversity. Our method is a plug-and-play solution that is model agnostic and applicable to a range of medical imaging modalities. Extensive experimental evidence has demonstrated its effectiveness in a variety of medical segmentation tasks. The source code is available in https://github.com/DanielaPlusPlus/HSMix.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Multi-Concept Adaptive Blending for High-Fidelity Text-to-Image Synthesis</title>
<link>https://arxiv.org/abs/2511.17615</link>
<guid>https://arxiv.org/abs/2511.17615</guid>
<content:encoded><![CDATA[
<div> Keywords: Text-to-Image, Multi-Concept Personalization, Adaptive Blending, Appearance Attention, Concept Leakage<br /><br />Summary:<br /><br />This paper addresses the challenge of integrating multiple personalized concepts into a single image using text-to-image (T2I) synthesis, a task that existing methods struggle with especially in complex multi-object scenes. The authors identify that current approaches often cause unintended alterations both in personalized and non-personalized regions, resulting in the loss of prompt structure and semantic inconsistencies. To overcome these issues, they propose PnP-MIX, a plug-and-play multi-concept adaptive blending method that requires no additional tuning. PnP-MIX leverages guided appearance attention to faithfully represent each personalized concept’s appearance. Additionally, the approach includes a mask-guided noise mixing strategy designed to protect non-personalized regions (e.g., background or unrelated objects) while accurately integrating personalized objects. To further enhance compositional fidelity and reduce concept leakage—where features of personalized concepts improperly spread to other regions—the authors introduce background dilution++, a technique that improves localization of features within intended areas. Extensive experiments demonstrate that PnP-MIX consistently outperforms existing methods in both single- and multi-concept personalization settings, proving its robustness and superior performance without needing model retraining or tuning. <div>
arXiv:2511.17615v1 Announce Type: new 
Abstract: Integrating multiple personalized concepts into a single image has recently become a significant area of focus within Text-to-Image (T2I) generation. However, existing methods often underperform on complex multi-object scenes due to unintended alterations in both personalized and non-personalized regions. This not only fails to preserve the intended prompt structure but also disrupts interactions among regions, leading to semantic inconsistencies. To address this limitation, we introduce plug-and-play multi-concept adaptive blending for high-fidelity text-to-image synthesis (PnP-MIX), an innovative, tuning-free approach designed to seamlessly embed multiple personalized concepts into a single generated image. Our method leverages guided appearance attention to faithfully reflect the intended appearance of each personalized concept. To further enhance compositional fidelity, we present a mask-guided noise mixing strategy that preserves the integrity of non-personalized regions such as the background or unrelated objects while enabling the precise integration of personalized objects. Finally, to mitigate concept leakage, i.e., the inadvertent leakage of personalized concept features into other regions, we propose background dilution++, a novel strategy that effectively reduces such leakage and promotes accurate localization of features within personalized regions. Extensive experimental results demonstrate that PnP-MIX consistently surpasses existing methodologies in both single- and multi-concept personalization scenarios, underscoring its robustness and superior performance without additional model tuning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach</title>
<link>https://arxiv.org/abs/2511.17618</link>
<guid>https://arxiv.org/abs/2511.17618</guid>
<content:encoded><![CDATA[
<div> Keywords: Video Question Answering, Foundational Question Generation, Spatio-temporal Dynamics, Embedding Integration, Visual-Question Alignment

<br /><br />Summary:  
Conventional Video Question Answering (VQA) methods focus primarily on learning from existing question-answer pairs that are mostly event-centric, which limits the model's ability to understand the comprehensive context of a video scene. Due to the scarcity of annotations related to fundamental information such as object categories, spatial arrangements, and descriptive visual attributes, these models struggle to form a holistic understanding of the environment, thereby restricting their reasoning and generalization capabilities. To address this, the paper introduces FIQ (Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach), a novel framework that generates foundational Q&amp;A pairs directly from descriptive information extracted from videos. This approach enriches training data with core scene-level attributes, enabling models to develop deeper foundational comprehension and improve reasoning across tasks. Additionally, the authors propose a VQ-CAlign module designed to align question embeddings with corresponding visual features, maintaining essential contextual cues and enhancing the model’s adaptability for downstream VQA tasks. Experimental evaluations on the SUTD-TrafficQA dataset demonstrate that FIQ outperforms current baseline methods, achieving state-of-the-art performance in video question answering by providing improved generalizability and reasoning capabilities. <div>
arXiv:2511.17618v1 Announce Type: new 
Abstract: Conventional VQA approaches primarily rely on question-answer (Q&amp;A) pairs to learn the spatio-temporal dynamics of video content. However, most existing annotations are event-centric, which restricts the model's ability to capture the comprehensive context of a scene. The lack of fundamental information such as object categories, spatial configurations, and descriptive visual attributes prevents the model from forming a complete understanding of the environment, ultimately limiting its generalization and reasoning capability. In this paper, we introduce Foundational Question Generation for Video Question Answering via an Embedding-Integrated Approach (FIQ), a framework designed to enhance the reasoning capability of VQA models by improving their foundational comprehension of video content. FIQ generates Q&amp;A pairs from descriptive information extracted directly from videos, thereby enriching the dataset with core scene-level attributes. These generated pairs help the model develop a more holistic understanding of the video, leading to improved generalizability and reasoning performance. In addition, we propose a VQ-CAlign module that aligns task-specific question embeddings with corresponding visual features, preserving essential contextual cues and enhancing adaptability to downstream tasks. Experimental results on the SUTD-TrafficQA dataset demonstrate that FIQ achieves state-of-the-art performance, surpassing existing baseline approaches.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Encoding and Annotating of 3D Bounding Box: Corner-Aware 3D Object Detection from Point Clouds</title>
<link>https://arxiv.org/abs/2511.17619</link>
<guid>https://arxiv.org/abs/2511.17619</guid>
<content:encoded><![CDATA[
<div> Keywords: LiDAR, 3D object detection, corner-aligned regression, bird’s-eye-view, weak supervision  

<br /><br />Summary:  
This paper addresses a key challenge in LiDAR-based 3D object detection, where traditional center-aligned regression is unstable because object centers often lie in sparse or empty regions on the bird’s-eye-view (BEV). This instability arises due to the front-surface bias of LiDAR point clouds, which results in noisy and inaccurate bounding box predictions. To solve this, the authors propose a corner-aligned regression method that shifts the prediction target from the unstable centers to geometrically informative corners, which are located in denser, more observable regions of the BEV. The approach leverages geometric constraints linking corners and 2D image boxes, enabling partial recovery of 3D bounding box parameters from corner annotations. This facilitates a weakly supervised learning paradigm that does not require complete 3D labels. They design a simple, corner-aware detection head that can be integrated into existing detectors. Experiments on the KITTI dataset demonstrate that this method improves performance by 3.5% AP compared to center-based baselines and achieves 83% of the accuracy of fully supervised models using only BEV corner clicks, highlighting the effectiveness and practical benefit of the corner-aligned regression strategy. <div>
arXiv:2511.17619v1 Announce Type: new 
Abstract: Center-aligned regression remains dominant in LiDAR-based 3D object detection, yet it suffers from fundamental instability: object centers often fall in sparse or empty regions of the bird's-eye-view (BEV) due to the front-surface-biased nature of LiDAR point clouds, leading to noisy and inaccurate bounding box predictions. To circumvent this limitation, we revisit bounding box representation and propose corner-aligned regression, which shifts the prediction target from unstable centers to geometrically informative corners that reside in dense, observable regions. Leveraging the inherent geometric constraints among corners and image 2D boxes, partial parameters of 3D bounding boxes can be recovered from corner annotations, enabling a weakly supervised paradigm without requiring complete 3D labels. We design a simple yet effective corner-aware detection head that can be plugged into existing detectors. Experiments on KITTI show our method improves performance by 3.5% AP over center-based baseline, and achieves 83% of fully supervised accuracy using only BEV corner clicks, demonstrating the effectiveness of our corner-aware regression strategy.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BD-Net: Has Depth-Wise Convolution Ever Been Applied in Binary Neural Networks?</title>
<link>https://arxiv.org/abs/2511.17633</link>
<guid>https://arxiv.org/abs/2511.17633</guid>
<content:encoded><![CDATA[
<div> Keywords: Binary Neural Networks, low-bit precision, depth-wise convolutions, model compression, MobileNet V1<br /><br />Summary:<br /><br />This paper addresses the challenges of extreme quantization in Binary Neural Networks (BNNs), particularly focusing on lightweight architectures with depth-wise convolutions, which traditionally suffer from limited representational capacity and unstable training. To overcome these issues, the authors introduce a novel 1.58-bit convolution technique that improves expressiveness beyond standard binary constraints. Additionally, they propose a pre-Batch Normalization (pre-BN) residual connection designed to stabilize optimization processes by enhancing the Hessian condition number. These two innovations collectively enable, for the first time, successful binarization of depth-wise convolutions in BNNs. The method demonstrates remarkable efficiency, achieving only 33 million operations (OPs) on the ImageNet dataset when using MobileNet V1 as the base architecture. This establishes a new state-of-the-art performance within BNNs, surpassing prior approaches with comparable computational budgets. Beyond ImageNet, the approach consistently delivers superior accuracy on multiple benchmarks, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements reaching up to 9.3 percentage points. Overall, the work significantly advances low-bit precision neural network design by balancing efficiency, accuracy, and training stability. <div>
arXiv:2511.17633v1 Announce Type: new 
Abstract: Recent advances in model compression have highlighted the potential of low-bit precision techniques, with Binary Neural Networks (BNNs) attracting attention for their extreme efficiency. However, extreme quantization in BNNs limits representational capacity and destabilizes training, posing significant challenges for lightweight architectures with depth-wise convolutions. To address this, we propose a 1.58-bit convolution to enhance expressiveness and a pre-BN residual connection to stabilize optimization by improving the Hessian condition number. These innovations enable, to the best of our knowledge, the first successful binarization of depth-wise convolutions in BNNs. Our method achieves 33M OPs on ImageNet with MobileNet V1, establishing a new state-of-the-art in BNNs by outperforming prior methods with comparable OPs. Moreover, it consistently outperforms existing methods across various datasets, including CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet, and Oxford Flowers 102, with accuracy improvements of up to 9.3 percentage points.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Score Pre-computation for Diffusion Models via Cross-Matrix Krylov Projection</title>
<link>https://arxiv.org/abs/2511.17634</link>
<guid>https://arxiv.org/abs/2511.17634</guid>
<content:encoded><![CDATA[
<div> Keywords: Score-based diffusion, Fokker-Planck, Krylov projection, Sparse solvers, Image generation<br /><br />Summary: This paper introduces a novel framework designed to accelerate score-based diffusion models by reformulating the stable diffusion process within the Fokker-Planck equation framework. This approach transforms image generation into solving large linear systems, which typically demands significant computational resources when processing many images. To address this, the authors propose a cross-matrix Krylov projection method that leverages mathematical similarities among matrices by constructing a shared subspace from "seed" matrices to efficiently solve for subsequent "target" matrices. Experimental results demonstrate that this technique achieves a substantial runtime reduction ranging from 15.8% to 43.7% compared to conventional sparse linear solvers. Furthermore, when benchmarked against DDPM baselines in denoising tasks, the method attains speedups of up to 115×. Importantly, the framework maintains high-quality image generation under fixed computational budgets, unlike DDPM methods which fail to produce recognizable images under the same constraints. Overall, this work presents a practical and efficient solution for high-quality image synthesis in environments with limited computational resources. <div>
arXiv:2511.17634v1 Announce Type: new 
Abstract: This paper presents a novel framework to accelerate score-based diffusion models. It first converts the standard stable diffusion model into the Fokker-Planck formulation which results in solving large linear systems for each image. For training involving many images, it can lead to a high computational cost. The core innovation is a cross-matrix Krylov projection method that exploits mathematical similarities between matrices, using a shared subspace built from ``seed" matrices to rapidly solve for subsequent ``target" matrices. Our experiments show that this technique achieves a 15.8\% to 43.7\% time reduction over standard sparse solvers. Additionally, we compare our method against DDPM baselines in denoising tasks, showing a speedup of up to 115$\times$. Furthermore, under a fixed computational budget, our model is able to produce high-quality images while DDPM fails to generate recognizable content, illustrating our approach is a practical method for efficient generation in resource-limited settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upstream Probabilistic Meta-Imputation for Multimodal Pediatric Pancreatitis Classification</title>
<link>https://arxiv.org/abs/2511.17635</link>
<guid>https://arxiv.org/abs/2511.17635</guid>
<content:encoded><![CDATA[
<div> Pediatric pancreatitis, machine learning, meta-imputation, Gaussian mixture models, MRI radiomics<br /><br />Summary:<br /><br />1. Pediatric pancreatitis, encompassing both acute and chronic forms, is a severe inflammatory disease that is difficult to diagnose clinically. <br />2. Diagnosing pediatric pancreatitis using machine learning techniques is challenging due to limited data samples and complex multimodal MRI imaging. <br />3. The study proposes Upstream Probabilistic Meta-Imputation (UPMI), a lightweight data augmentation method applied in a low-dimensional meta-feature space, rather than directly on image data. <br />4. Logistic regression models specific to MRI modalities (T1-weighted and T2-weighted images) generate probabilities that form a 7-dimensional meta-feature vector representing patient data. <br />5. Class-conditional Gaussian mixture models are estimated within each fold of cross-validation to generate synthetic meta-features, which supplement real meta-features for training a Random Forest meta-classifier. <br />6. The method was tested on 67 pediatric patients with paired T1W/T2W MRI scans and demonstrated a mean AUC of 0.908 ± 0.072, outperforming the baseline model trained on real data alone (mean AUC 0.864 ± 0.061), thus providing around 5% relative improvement in diagnostic accuracy. <div>
arXiv:2511.17635v1 Announce Type: new 
Abstract: Pediatric pancreatitis is a progressive and debilitating inflammatory condition, including acute pancreatitis and chronic pancreatitis, that presents significant clinical diagnostic challenges. Machine learning-based methods also face diagnostic challenges due to limited sample availability and multimodal imaging complexity. To address these challenges, this paper introduces Upstream Probabilistic Meta-Imputation (UPMI), a light-weight augmentation strategy that operates upstream of a meta-learner in a low-dimensional meta-feature space rather than in image space. Modality-specific logistic regressions (T1W and T2W MRI radiomics) produce probability outputs that are transformed into a 7-dimensional meta-feature vector. Class-conditional Gaussian mixture models (GMMs) are then fit within each cross-validation fold to sample synthetic meta-features that, combined with real meta-features, train a Random Forest (RF) meta-classifier. On 67 pediatric subjects with paired T1W/T2W MRIs, UPMI achieves a mean AUC of 0.908 $\pm$ 0.072, a $\sim$5% relative gain over a real-only baseline (AUC 0.864 $\pm$ 0.061).
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TSRE: Channel-Aware Typical Set Refinement for Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2511.17636</link>
<guid>https://arxiv.org/abs/2511.17636</guid>
<content:encoded><![CDATA[
<div> Keywords: Out-of-Distribution detection, activation rectification, typical set refinement, skewness-based refinement, energy score  

<br /><br />Summary: Out-of-Distribution (OOD) detection is essential for deploying machine learning models reliably in open-world environments where inputs can be unexpected or anomalous. Activation-based methods improve OOD detection by suppressing anomalous activations and increasing the separation between in-distribution (ID) and OOD samples. However, existing activation rectification approaches tend to ignore individual channel characteristics and distributional skewness, leading to inaccurate estimation of the typical set and improper inclusion of anomalous activations. To overcome this, the paper proposes a typical set refinement technique that incorporates both discriminability and activity to create a channel-aware typical set. Additionally, a skewness-based refinement is introduced to correct distributional bias in the typical set estimation. The refined activations are then used to compute an energy score for OOD detection. Experiments conducted on ImageNet-1K and CIFAR-100 datasets show that the proposed method achieves state-of-the-art performance. Moreover, it generalizes well across different backbone architectures and OOD scoring functions, indicating robustness and broad applicability of the approach. <div>
arXiv:2511.17636v1 Announce Type: new 
Abstract: Out-of-Distribution (OOD) detection is a critical capability for ensuring the safe deployment of machine learning models in open-world environments, where unexpected or anomalous inputs can compromise model reliability and performance. Activation-based methods play a fundamental role in OOD detection by mitigating anomalous activations and enhancing the separation between in-distribution (ID) and OOD data. However, existing methods apply activation rectification while often overlooking channel's intrinsic characteristics and distributional skewness, which results in inaccurate typical set estimation. This discrepancy can lead to the improper inclusion of anomalous activations across channels. To address this limitation, we propose a typical set refinement method based on discriminability and activity, which rectifies activations into a channel-aware typical set. Furthermore, we introduce a skewness-based refinement to mitigate distributional bias in typical set estimation. Finally, we leverage the rectified activations to compute the energy score for OOD detection. Experiments on the ImageNet-1K and CIFAR-100 benchmarks demonstrate that our method achieves state-of-the-art performance and generalizes effectively across backbones and score functions.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SWITCH: Benchmarking Modeling and Handling of Tangible Interfaces in Long-horizon Embodied Scenarios</title>
<link>https://arxiv.org/abs/2511.17649</link>
<guid>https://arxiv.org/abs/2511.17649</guid>
<content:encoded><![CDATA[
<div> Autonomous intelligence, Tangible Control Interfaces, Embodied Benchmark, Multi-modal Reasoning, SWITCH Benchmark<br /><br />Summary:<br /><br />The paper introduces SWITCH (Semantic World Interface Tasks for Control and Handling), a new embodied, task-driven benchmark designed to evaluate autonomous agents' abilities to interact effectively with real-world tangible control interfaces (TCIs) like light switches, appliance panels, and embedded GUIs. These TCIs require commonsense reasoning, physics understanding, causal prediction, and outcome verification due to delayed or indirect effects. Current benchmarks inadequately test crucial aspects like grounding, partial observability from video input, and post-hoc verification in practical, situated environments — gaps SWITCH aims to address. SWITCH-Basic, the first iteration, assesses five complementary capabilities: task-aware visual question answering (VQA), semantic UI grounding, action generation, state-transition prediction, and result verification, all from egocentric RGB video input across a diverse set of 98 real devices spanning 351 tasks. Evaluation of commercial and open large multimodal models (LMMMs) shows inconsistent performance, often with a tendency to rely on textual cues rather than fully leveraging visual or video evidence, highlighting current models' limitations in these contexts. The benchmark is released openly along with data, code, and held-out splits to support reproducible evaluation and facilitate community-driven improvements and future iterations, aiming to foster advancements in embodied intelligence and autonomous control. Resources are available at the provided GitHub link. <div>
arXiv:2511.17649v1 Announce Type: new 
Abstract: Autonomous intelligence requires not only perception and reasoning, but critically, effective interaction with the existing world and its infrastructure. Everyday environments are rich in tangible control interfaces (TCIs), e.g., light switches, appliance panels, and embedded GUIs, that demand commonsense and physics reasoning, but also causal prediction and outcome verification in time and space (e.g., delayed heating, remote lights). Moreover, failures here have potential safety implications, yet current benchmarks rarely test grounding, partial observability (video), or post-hoc verification in situated settings. We introduce SWITCH (Semantic World Interface Tasks for Control and Handling), an embodied, task-driven benchmark created through iterative releases to probe these gaps. Its first iteration, SWITCH-Basic, evaluates five complementary abilities:task-aware VQA, semantic UI grounding, action generation, state-transition prediction, and result verification, under egocentric RGB video input and device diversity. Across 351 tasks spanning 98 real devices and appliances, commercial and open LMMMs exhibit inconsistent performance even on single-step interactions, often over-relying on textual cues and under-using visual or video evidence (and high aggregate scores can mask such failures). SWITCH provides data, code, and held-out splits to enable reproducible evaluation and community contributions toward more challenging future iterations of the benchmark and the creation of training datasets. Benchmark resources are available at: https://github.com/BAAI-Agents/SWITCH.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Deep Learning for Brain Tumor Classification: Comprehensive Benchmarking with Dual Interpretability and Lightweight Deployment</title>
<link>https://arxiv.org/abs/2511.17655</link>
<guid>https://arxiv.org/abs/2511.17655</guid>
<content:encoded><![CDATA[
<div> Keywords: brain tumor classification, deep learning, MRI, compact CNN, explainability<br /><br />Summary:<br /><br />This study presents a comprehensive deep learning system for automated brain tumor classification using MRI images, benchmarking six architectures including five ImageNet-pretrained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom compact CNN with 1.31 million parameters. The system standardizes preprocessing, training protocols, and evaluation metrics, employing AdamW optimizer, CosineAnnealingLR scheduler, and early stopping with patience = 7, ensuring consistent performance comparison across models. Interpretability is enhanced using Grad-CAM and GradientShap explanations to identify anatomically relevant attention areas, addressing black-box concerns. The compact CNN achieves a remarkable 96.49% accuracy, being 100 times smaller than Inception-ResNet V2 and enabling real-time inference (~375 ms) on edge devices, making it feasible for deployment in low-resource settings. Evaluation extends beyond accuracy, incorporating intersection over union, Hausdorff distance, precision-recall curves, and confusion matrices, providing a robust assessment framework. Inception-ResNet V2 attains state-of-the-art results with 99.53% accuracy and above 99.50% in precision, recall, and F1-score, surpassing recent benchmarks. This end-to-end solution balances accuracy, interpretability, and deployability, facilitating trustworthy AI applications in both advanced and under-resourced healthcare systems, with the potential for clinical screening and triage use. <div>
arXiv:2511.17655v1 Announce Type: new 
Abstract: Our study provides a full deep learning system for automated classification of brain tumors from MRI images, includes six benchmarked architectures (five ImageNet-pre-trained models (VGG-16, Inception V3, ResNet-50, Inception-ResNet V2, Xception) and a custom built, compact CNN (1.31M params)). The study moves the needle forward in a number of ways, including (1) full standardization of assessment with respect to preprocessing, training sets/protocols (optimizing networks with the AdamW optimizer, CosineAnnealingLR, patiene for early stopping = 7), and metrics to assess performance were identical along all models; (2) a high level of confidence in the localizations based on prior studies as both Grad-CAM and GradientShap explanation were used to establish anatomically important and meaningful attention regions and address the black-box issue; (3) a compact 1.31 million parameter CNN was developed that achieved 96.49% testing accuracy and was 100 times smaller than Inception-ResNet V2 while permitting real-time inference (375ms) on edge devices; (4) full evaluation beyond accuracy reporting based on measures of intersection over union, Hausdorff distance, and precision-recall curves, and confusion matrices across all splits. Inception-ResNet V2 reached state-of-the-art performance, achieving a 99.53% accuracy on testing and obtaining a precision, recall, and F1-score of at least 99.50% dominant performance based on metrics of recent studies. We demonstrated a lightweight model that is suitable to deploy on devices that do not have multi-GPU infrastructure in under-resourced settings. This end-to-end solution considers accuracy, interpretability, and deployability of trustworthy AI to create the framework necessary for performance assessment and deployment within advance and low-resource healthcare systems to an extent that enabled participation at the clinical screening and triage level.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedPEFT-CL: Dual-Phase Parameter-Efficient Continual Learning with Medical Semantic Adapter and Bidirectional Memory Consolidation</title>
<link>https://arxiv.org/abs/2511.17668</link>
<guid>https://arxiv.org/abs/2511.17668</guid>
<content:encoded><![CDATA[
<div> Medical vision-language segmentation, continual learning, parameter-efficient fine-tuning, catastrophic forgetting, Fisher-memory coordination  

<br /><br />Summary:  
This article addresses the problem of catastrophic forgetting in medical vision-language segmentation models when adapting to new anatomical structures, which currently necessitates full retraining and hinders clinical use. It highlights the lack of targeted continual learning methods designed specifically for medical vision-language tasks. To overcome this, the authors propose MedPEFT-CL, a parameter-efficient continual learning framework built on a dual-phase CLIPSeg architecture. The first phase, adaptive learning, uses semantic similarity-based adapter allocation and prompt similarity analysis to efficiently fine-tune parameters for new medical tasks. The second phase, knowledge consolidation, utilizes bi-directional Fisher-memory coordination to prevent forgetting of previously learned tasks. This approach creates a reinforcing cycle where consolidation guides replay priorities and the introduction of new tasks generates challenging samples that improve retention strategies. Key contributions include (1) a semantic-driven adapter allocation mechanism enabling efficient learning of new tasks, (2) a bi-modal LoRA adaptation that reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination to mitigate catastrophic forgetting. Extensive experiments on diverse medical datasets demonstrate that MedPEFT-CL effectively preserves past knowledge, mitigates forgetting, and achieves strong performance retention with minimal parameter overhead, making it suitable for continual learning scenarios in medical vision-language applications. <div>
arXiv:2511.17668v1 Announce Type: new 
Abstract: Medical vision-language segmentation models suffer from catastrophic forgetting when adapting to new anatomical structures, requiring complete retraining that limits their clinical deployment. Although continual learning approaches have been studied for various applications, targeted research on continual learning approaches specifically designed for medical vision-language tasks remains underexplored. We propose MedPEFT-CL, a parameter-efficient continual learning framework that addresses both efficient learning of new tasks and preservation of previous knowledge through a dual-phase architecture based on CLIPSeg. Our dual-phase architecture features an adaptive learning phase that employs semantic similarity-based adapter allocation and parameter-efficient fine-tuning for medical tasks through prompt similarity analysis, and a knowledge consolidation phase employing bi-directional Fisher-memory coordination. This creates a reinforcing cycle: consolidation directs replay priorities while new tasks provide challenging samples that improve retention strategies. Our key contributions are: (1) a semantic-driven adapter allocation mechanism that enables efficient learning of new medical tasks, (2) a bi-modal LoRA adaptation that significantly reduces trainable parameters while maintaining cross-modal learning, and (3) bidirectional Fisher-memory coordination that prevents catastrophic forgetting from previous medical tasks. Extensive experiments across diverse medical datasets demonstrate superior forgetting mitigation and performance retention with minimal parameter overhead, making the framework effective for continual learning in medical vision-language scenarios.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Person Recognition in Aerial Surveillance: A Decade Survey</title>
<link>https://arxiv.org/abs/2511.17674</link>
<guid>https://arxiv.org/abs/2511.17674</guid>
<content:encoded><![CDATA[
<div> Keywords: aerial surveillance, human-centric tasks, drones, computer vision, machine learning<br /><br />Summary:<br /><br />This paper presents a comprehensive review of over 150 research papers from the past decade focusing on human-centric aerial surveillance tasks. It highlights the emerging use of airborne platforms such as drones and UAVs, which provide advantages including large-scale coverage, mobility, ease of deployment, and covert observation capabilities. The core focus is on detecting, identifying, and re-identifying humans from aerial perspectives, a domain distinct from traditional ground-based surveillance. The paper identifies and discusses unique challenges associated with aerial human-centric tasks, such as viewpoint variations, occlusions, and resolution constraints. It compiles and analyzes publicly available aerial datasets relevant to these tasks, serving as a valuable resource for researchers. Moreover, it delves into contemporary computer vision and machine learning approaches, evaluating how current methods address aerial-specific challenges and suggesting techniques for their improvement. Finally, the review concludes by outlining existing research gaps and open questions, offering guidance for future studies to advance the field of aerial human surveillance effectively. <div>
arXiv:2511.17674v1 Announce Type: new 
Abstract: The rapid emergence of airborne platforms and imaging sensors is enabling new forms of aerial surveillance due to their unprecedented advantages in scale, mobility, deployment, and covert observation capabilities. This paper provides a comprehensive overview of 150+ papers over the last 10 years of human-centric aerial surveillance tasks from a computer vision and machine learning perspective. It aims to provide readers with an in-depth systematic review and technical analysis of the current state of aerial surveillance tasks using drones, UAVs, and other airborne platforms. The object of interest is humans, where human subjects are to be detected, identified, and re-identified. More specifically, for each of these tasks, we first identify unique challenges in performing these tasks in an aerial setting compared to the popular ground-based setting and subsequently compile and analyze aerial datasets publicly available for each task. Most importantly, we delve deep into the approaches in the aerial surveillance literature with a focus on investigating how they presently address aerial challenges and techniques for improvement. We conclude the paper by discussing the gaps and open research questions to inform future research avenues.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models</title>
<link>https://arxiv.org/abs/2511.17681</link>
<guid>https://arxiv.org/abs/2511.17681</guid>
<content:encoded><![CDATA[
<div> Keywords: Referring Multi-Object Tracking, multi-modal large language models, motion modality, Vision-Motion-Reference Alignment, Motion-Guided Prediction Head<br /><br />Summary:<br /><br />Referring Multi-Object Tracking (RMOT) builds upon traditional multi-object tracking by incorporating natural language references for multi-modal fusion tracking, but current benchmarks mainly focus on static features like appearance and initial positions, neglecting dynamic object motion changes such as velocity and direction shifts. This static regulation results in temporal inconsistencies between static language references and dynamic visual data, limiting tracking performance. To overcome these challenges, the authors propose the Vision-Motion-Reference aligned RMOT framework (VMRMOT), which integrates a motion modality extracted from object dynamics, enhancing alignment across vision, motion, and language through multi-modal large language models (MLLMs). They introduce motion-aware descriptions based on dynamic behaviors and leverage MLLMs’ temporal reasoning capabilities to extract motion features as the motion modality. The Vision-Motion-Reference Alignment (VMRA) module is designed to hierarchically align visual queries with motion and language references, improving cross-modal consistency, while the Motion-Guided Prediction Head (MGPH) utilizes the motion modality to boost prediction performance. This approach marks the first use of MLLMs for vision-reference alignment in RMOT. Extensive experiments on several RMOT benchmarks show that VMRMOT surpasses existing state-of-the-art methods, demonstrating the effectiveness of incorporating motion information and multimodal alignment in tracking tasks. <div>
arXiv:2511.17681v1 Announce Type: new 
Abstract: Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Counting Mechanisms in Large Language and Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.17699</link>
<guid>https://arxiv.org/abs/2511.17699</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, counting tasks, mechanistic interpretability, numerical representation, vision-language models<br /><br />Summary:<br /><br />This paper investigates how large language models (LLMs) and large vision-language models (LVLMs) represent and process numerical information during counting tasks. Controlled experiments using repeated textual and visual items facilitate analysis of model behavior by means of causal mediation and activation patching. The authors introduce CountScope, a specialized tool for mechanistic interpretability focused on numerical content. Results demonstrate that individual tokens or visual features embed latent positional count information which can be extracted and generalized across different contexts. Layerwise analysis reveals a structured emergence of numerical representations: lower layers mainly encode smaller counts while higher layers represent larger numbers. The study identifies an internal counter mechanism that updates incrementally with each item, localized primarily in the final token or visual region, and transferable between contexts. In LVLMs, numerical information is found not only in textual embeddings but also within visual embeddings, with counts shifting between background and foreground regions depending on spatial arrangement. Models also exploit structural cues such as textual separators, which act as shortcuts for tracking counts and affect numerical prediction accuracy. Overall, counting is shown to be a structured, progressive, and layerwise computational process in both LLMs and LVLMs, influenced by the architectural properties of the vision encoder. <div>
arXiv:2511.17699v1 Announce Type: new 
Abstract: This paper examines how large language models (LLMs) and large vision-language models (LVLMs) represent and compute numerical information in counting tasks. We use controlled experiments with repeated textual and visual items and analyze model behavior through causal mediation and activation patching. To this end, we design a specialized tool, CountScope, for mechanistic interpretability of numerical content. Results show that individual tokens or visual features encode latent positional count information that can be extracted and transferred across contexts. Layerwise analyses reveal a progressive emergence of numerical representations, with lower layers encoding small counts and higher layers representing larger ones. We identify an internal counter mechanism that updates with each item, stored mainly in the final token or region and transferable between contexts. In LVLMs, numerical information also appears in visual embeddings, shifting between background and foreground regions depending on spatial composition. Models rely on structural cues such as separators in text, which act as shortcuts for tracking item counts and influence the accuracy of numerical predictions. Overall, counting emerges as a structured, layerwise process in LLMs and follows the same general pattern in LVLMs, shaped by the properties of the vision encoder.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions</title>
<link>https://arxiv.org/abs/2511.17722</link>
<guid>https://arxiv.org/abs/2511.17722</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision Language Models, counting performance, attention allocation, synthetic benchmark, attention interventions  

<br /><br />Summary:  
The article addresses how Vision Language Models (VLMs) demonstrate inherent biases when responding to queries about visual properties of images, particularly in tasks requiring specific focus such as counting. The authors create a synthetic benchmark dataset alongside an evaluation framework designed to systematically assess how changes in image and prompt properties affect counting performance. They explore variations in input parameters including the number of objects, object color, background color, texture, and prompt specificity to understand fluctuations in attention allocation within open-source VLMs. The study further introduces attention-based interventions aimed at modulating the model’s focus on visual tokens across different layers. Evaluations of these interventions reveal that although counting remains a difficult task for VLMs—especially under conditions of high visual or linguistic complexity—certain attention modulation techniques can achieve modest improvements. Overall, the research contributes a comprehensive methodology to investigate VLM behavior in counting tasks, emphasizing the interplay between model attention mechanisms and input characteristics, and highlights potential paths toward enhancing counting accuracy in future VLM developments. <div>
arXiv:2511.17722v1 Announce Type: new 
Abstract: Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AngioDG: Interpretable Channel-informed Feature-modulated Single-source Domain Generalization for Coronary Vessel Segmentation in X-ray Angiography</title>
<link>https://arxiv.org/abs/2511.17724</link>
<guid>https://arxiv.org/abs/2511.17724</guid>
<content:encoded><![CDATA[
<div> Keywords: Cardiovascular diseases, X-ray Coronary Angiography, vessel segmentation, domain generalization, channel regularization

<br /><br />Summary: Cardiovascular diseases remain the top cause of mortality worldwide, with X-ray Coronary Angiography (XCA) serving as the clinical gold standard for real-time cardiac interventions. Accurate segmentation of coronary vessels from XCA images is crucial for quantitative assessments such as stenosis severity measurement, which supports clinical decision-making. However, domain shifts caused by diverse imaging protocols and patient demographics complicate the development of generalized vessel segmentation models. This problem is intensified by the scarcity of annotated datasets, necessitating the use of Single-source Domain Generalization (SDG) techniques. Existing SDG methods predominantly rely on augmentation, which may not effectively prevent overfitting to artificial or synthetic domains. To address these challenges, the authors propose "AngioDG," a novel approach incorporating a channel regularization strategy that enhances domain generalization. AngioDG interprets contributions of early feature channels to task-specific metrics and dynamically reweights these channels, amplifying domain-invariant features while suppressing domain-specific ones. Evaluation on six distinct XCA datasets demonstrates that AngioDG achieves superior out-of-distribution segmentation performance compared to existing methods, without compromising in-domain test accuracy, thus proving its effectiveness and robustness for practical applications in coronary vessel segmentation. <div>
arXiv:2511.17724v1 Announce Type: new 
Abstract: Cardiovascular diseases are the leading cause of death globally, with X-ray Coronary Angiography (XCA) as the gold standard during real-time cardiac interventions. Segmentation of coronary vessels from XCA can facilitate downstream quantitative assessments, such as measurement of the stenosis severity and enhancing clinical decision-making. However, developing generalizable vessel segmentation models for XCA is challenging due to variations in imaging protocols and patient demographics that cause domain shifts. These limitations are exacerbated by the lack of annotated datasets, making Single-source Domain Generalization (SDG) a necessary solution for achieving generalization. Existing SDG methods are largely augmentation-based, which may not guarantee the mitigation of overfitting to augmented or synthetic domains. We propose a novel approach, ``AngioDG", to bridge this gap by channel regularization strategy to promote generalization. Our method identifies the contributions of early feature channels to task-specific metrics for DG, facilitating interpretability, and then reweights channels to calibrate and amplify domain-invariant features while attenuating domain-specific ones. We evaluate AngioDG on 6 x-ray angiography datasets for coronary vessels segmentation, achieving the best out-of-distribution performance among the compared methods, while maintaining consistent in-domain test performance.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Potential and Limitations of Vision-Language Models for Human Motion Understanding: A Case Study in Data-Driven Stroke Rehabilitation</title>
<link>https://arxiv.org/abs/2511.17727</link>
<guid>https://arxiv.org/abs/2511.17727</guid>
<content:encoded><![CDATA[
<div> Stroke rehabilitation, vision-language models, motion identification, dose quantification, impairment prediction<br /><br />Summary:<br /><br />1. The study explores the application of vision-language models (VLMs) in stroke rehabilitation, focusing on two main challenges: automatic quantification of rehabilitation dose and impairment from videos.<br /><br />2. The problems are framed as motion-identification tasks that can be addressed by VLMs without requiring task-specific training or finetuning.<br /><br />3. The evaluation was conducted on a cohort consisting of 29 healthy controls and 51 stroke survivors to assess the effectiveness of current VLMs.<br /><br />4. Results indicate that existing VLMs do not have sufficient fine-grained motion understanding to precisely quantify rehabilitation dose or reliably predict impairment scores.<br /><br />5. Despite limitations, optimized prompting and post-processing enable VLMs to classify high-level activities from a few video frames, detect motion and grasp with moderate accuracy, and estimate dose counts within 25% of the ground truth for mildly impaired and healthy individuals.<br /><br />6. The findings emphasize both the current shortcomings and the future potential of using VLMs for data-driven stroke rehabilitation and broader clinical video analysis applications. <div>
arXiv:2511.17727v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have demonstrated remarkable performance across a wide range of computer-vision tasks, sparking interest in their potential for digital health applications. Here, we apply VLMs to two fundamental challenges in data-driven stroke rehabilitation: automatic quantification of rehabilitation dose and impairment from videos. We formulate these problems as motion-identification tasks, which can be addressed using VLMs. We evaluate our proposed framework on a cohort of 29 healthy controls and 51 stroke survivors. Our results show that current VLMs lack the fine-grained motion understanding required for precise quantification: dose estimates are comparable to a baseline that excludes visual information, and impairment scores cannot be reliably predicted. Nevertheless, several findings suggest future promise. With optimized prompting and post-processing, VLMs can classify high-level activities from a few frames, detect motion and grasp with moderate accuracy, and approximate dose counts within 25% of ground truth for mildly impaired and healthy participants, all without task-specific training or finetuning. These results highlight both the current limitations and emerging opportunities of VLMs for data-driven stroke rehabilitation and broader clinical video analysis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisReason: A Large-Scale Dataset for Visual Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2511.17731</link>
<guid>https://arxiv.org/abs/2511.17731</guid>
<content:encoded><![CDATA[
<div> Chain-of-Thought, multimodal reasoning, VisReason, visual datasets, large-scale annotation  

<br /><br />Summary:  
This paper addresses the gap in Chain-of-Thought (CoT) prompting for multimodal large language models (MLLMs), highlighting the lack of extensive datasets that enable rich, spatially grounded visual reasoning. It introduces VisReason, a large-scale dataset with 489,000 annotated examples across four diverse domains, featuring multi-round, human-like rationales that support interpretable and compositional visual reasoning processes. Additionally, the authors present VisReason-Pro, a refined subset of 165,000 examples created using an expert-level GPT annotator, which includes detailed reasoning traces and 3D spatial grounding facilitated by depth-aware annotations. The datasets are used to fine-tune the Qwen2.5-VL model, resulting in significant gains in stepwise visual reasoning accuracy, interpretability, and generalization across benchmarks. Experimental results demonstrate that training on VisReason improves the systematic and generalizable reasoning capabilities of MLLMs. The paper positions VisReason as a foundational resource to advance human-like visual reasoning and stimulate progress towards next-generation multimodal intelligence systems. <div>
arXiv:2511.17731v1 Announce Type: new 
Abstract: Chain-of-Thought (CoT) prompting has proven remarkably effective for eliciting complex reasoning in large language models (LLMs). Yet, its potential in multimodal large language models (MLLMs) remains largely untapped, hindered by the absence of large-scale datasets that capture the rich, spatially grounded reasoning intrinsic to visual understanding. Existing visual-CoT resources are typically small, domain-specific, or lack the human-like stepwise structure necessary for compositional visual reasoning. In this paper, we introduce VisReason, a large-scale dataset designed to advance visual Chain-of-Thought reasoning. VisReason comprises 489K annotated examples spanning four diverse domains, each featuring multi-round, human-like rationales that guide MLLMs through interpretable visual reasoning steps. Building upon this, we curate VisReason-Pro, a 165K subset produced with a stronger expert-level GPT annotator, enriched with detailed reasoning traces and 3D spatial grounding via depth-informed annotations. Fine-tuning the state-of-the-art Qwen2.5-VL model on VisReason and VisReason-Pro yields substantial improvements in step-by-step visual reasoning accuracy, interpretability, and cross-benchmark generalization. These results demonstrate that VisReason equips MLLMs with more systematic and generalizable reasoning capabilities. We envision VisReason as a cornerstone for cultivating human-like visual reasoning, paving the way toward the next generation of multimodal intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Open-Ended Visual Scientific Discovery with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2511.17735</link>
<guid>https://arxiv.org/abs/2511.17735</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoders, foundation models, feature discovery, ecological imagery, scientific archives<br /><br />Summary: This paper addresses the challenge of discovering unknown patterns in vast scientific archives by leveraging foundation models trained on large-scale datasets. Unlike typical methods that focus on extracting features for pre-specified targets and are limited to confirming known patterns, the authors explore the potential of sparse autoencoders (SAEs) to enable open-ended feature discovery. They conduct controlled rediscovery studies to evaluate the alignment of SAE-learned features with semantic concepts on standard segmentation benchmarks, comparing their performance with other label-free methods using concept-alignment metrics. A key scientific case study applies this approach to ecological imagery, demonstrating the ability of SAEs to identify fine-grained anatomical structures without relying on segmentation or part labels, thus validating the method against ground-truth data. Despite focusing primarily on vision tasks within ecology, the methodology is domain-agnostic and applicable to other scientific model domains such as protein structures, genomics, and climate data. The results suggest that sparse decomposition through SAEs provides a practical and scalable tool for probing the internal representations of foundation models, facilitating a transition from merely confirming known information to enabling genuine scientific discovery across disciplines. <div>
arXiv:2511.17735v1 Announce Type: new 
Abstract: Scientific archives now contain hundreds of petabytes of data across genomics, ecology, climate, and molecular biology that could reveal undiscovered patterns if systematically analyzed at scale. Large-scale, weakly-supervised datasets in language and vision have driven the development of foundation models whose internal representations encode structure (patterns, co-occurrences and statistical regularities) beyond their training objectives. Most existing methods extract structure only for pre-specified targets; they excel at confirmation but do not support open-ended discovery of unknown patterns. We ask whether sparse autoencoders (SAEs) can enable open-ended feature discovery from foundation model representations. We evaluate this question in controlled rediscovery studies, where the learned SAE features are tested for alignment with semantic concepts on a standard segmentation benchmark and compared against strong label-free alternatives on concept-alignment metrics. Applied to ecological imagery, the same procedure surfaces fine-grained anatomical structure without access to segmentation or part labels, providing a scientific case study with ground-truth validation. While our experiments focus on vision with an ecology case study, the method is domain-agnostic and applicable to models in other sciences (e.g., proteins, genomics, weather). Our results indicate that sparse decomposition provides a practical instrument for exploring what scientific foundation models have learned, an important prerequisite for moving from confirmation to genuine discovery.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AEGIS: Preserving privacy of 3D Facial Avatars with Adversarial Perturbations</title>
<link>https://arxiv.org/abs/2511.17747</link>
<guid>https://arxiv.org/abs/2511.17747</guid>
<content:encoded><![CDATA[
<div> Keywords: 3D Gaussian Splatting, identity masking, adversarial perturbations, privacy preservation, face verification<br /><br />Summary:<br /><br />The article introduces AEGIS, a novel privacy-preserving framework designed specifically for 3D Gaussian Splatting-based photorealistic facial avatars. With the surge of 3D avatars in biometric authentication systems, concerns about online identity theft have intensified, particularly due to the lack of robust methods for identity masking beyond 2D images. AEGIS uniquely addresses the challenge by applying adversarial perturbations directly to the Gaussian color coefficients of the avatar, guided by a pre-trained face verification network. This approach ensures consistent identity concealment across multiple viewpoints without needing to retrain the avatar’s geometry or structure. The framework successfully achieves full de-identification, demonstrated by the reduction of face retrieval and verification accuracy to 0%. Despite these modifications, AEGIS maintains high perceptual quality, as measured by SSIM and PSNR scores (0.9555 and 35.52 dB respectively). Importantly, it preserves essential facial attributes like age, race, gender, and emotion, balancing privacy protection with minimal visual distortion. This work fills a critical gap in 3D avatar privacy and provides a practical solution for safeguarding identity in dynamic, multi-view biometric contexts. <div>
arXiv:2511.17747v1 Announce Type: new 
Abstract: The growing adoption of photorealistic 3D facial avatars, particularly those utilizing efficient 3D Gaussian Splatting representations, introduces new risks of online identity theft, especially in systems that rely on biometric authentication. While effective adversarial masking methods have been developed for 2D images, a significant gap remains in achieving robust, viewpoint-consistent identity protection for dynamic 3D avatars. To address this, we present AEGIS, the first privacy-preserving identity masking framework for 3D Gaussian Avatars that maintains the subject's perceived characteristics. Our method aims to conceal identity-related facial features while preserving the avatar's perceptual realism and functional integrity. AEGIS applies adversarial perturbations to the Gaussian color coefficients, guided by a pre-trained face verification network, ensuring consistent protection across multiple viewpoints without retraining or modifying the avatar's geometry. AEGIS achieves complete de-identification, reducing face retrieval and verification accuracy to 0%, while maintaining high perceptual quality (SSIM = 0.9555, PSNR = 35.52 dB). It also preserves key facial attributes such as age, race, gender, and emotion, demonstrating strong privacy protection with minimal visual distortion.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPIDER: Spatial Image CorresponDence Estimator for Robust Calibration</title>
<link>https://arxiv.org/abs/2511.17750</link>
<guid>https://arxiv.org/abs/2511.17750</guid>
<content:encoded><![CDATA[
<div> Keywords: image correspondence, feature matching, 3D geometry, vision foundation models, SPIDER<br /><br />Summary:<br /><br />Reliable image correspondences are essential for vision-based spatial perception, enabling tasks such as 3D structure reconstruction and camera pose estimation. However, matching features across diverse domains like aerial, indoor, and outdoor scenes remains difficult due to significant variations in appearance, scale, and viewpoint. Traditionally, feature matching has focused on 2D-to-2D correspondences, but recent advances in 3D foundation models have introduced spatially coherent matching based on two-view geometry. Despite their strengths, these 3D models tend to focus on dominant planar regions such as walls or ground surfaces, often missing finer geometric details, especially when the viewpoint changes drastically. To understand these limitations, the authors conducted linear probe experiments evaluating various vision foundation models' performance in image matching. Based on these findings, they propose SPIDER, a universal image matching framework that uses a shared backbone for feature extraction combined with two specialized network heads designed to estimate both 2D and 3D correspondences ranging from coarse to fine scales. Finally, the authors present a new evaluation benchmark targeting unconstrained scenarios with large baselines to better assess image matching performance. SPIDER significantly outperforms state-of-the-art methods, showcasing its versatility and strength as a universal image matching approach. <div>
arXiv:2511.17750v1 Announce Type: new 
Abstract: Reliable image correspondences form the foundation of vision-based spatial perception, enabling recovery of 3D structure and camera poses. However, unconstrained feature matching across domains such as aerial, indoor, and outdoor scenes remains challenging due to large variations in appearance, scale and viewpoint. Feature matching has been conventionally formulated as a 2D-to-2D problem; however, recent 3D foundation models provides spatial feature matching properties based on two-view geometry. While powerful, we observe that these spatially coherent matches often concentrate on dominant planar regions, e.g., walls or ground surfaces, while being less sensitive to fine-grained geometric details, particularly under large viewpoint changes. To better understand these trade-offs, we first perform linear probe experiments to evaluate the performance of various vision foundation models for image matching. Building on these insights, we introduce SPIDER, a universal feature matching framework that integrates a shared feature extraction backbone with two specialized network heads for estimating both 2D-based and 3D-based correspondences from coarse to fine. Finally, we introduce an image-matching evaluation benchmark that focuses on unconstrained scenarios with large baselines. SPIDER significantly outperforms SoTA methods, demonstrating its strong ability as a universal image-matching method.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CORA: Consistency-Guided Semi-Supervised Framework for Reasoning Segmentation</title>
<link>https://arxiv.org/abs/2511.17755</link>
<guid>https://arxiv.org/abs/2511.17755</guid>
<content:encoded><![CDATA[
<div> Reasoning segmentation, semi-supervised learning, multimodal LLM, pseudo-label filtering, contrastive alignment<br /><br />Summary:  
Reasoning segmentation aims to generate pixel-accurate masks for targets described by complex and often implicit instructions, necessitating context-dependent reasoning across scenes. Despite recent advances from multimodal language models in instruction-following segmentation, these models struggle with generalization due to limited and costly labeled data capturing rich linguistic context. To address this, the authors propose CORA, a semi-supervised framework that leverages both limited labeled images and a large set of unlabeled data for training. CORA introduces three core innovations: conditional visual instructions that encode spatial and contextual object relationships, a noisy pseudo-label filtering technique based on consistency of outputs from Multimodal Large Language Models (LLMs) across semantically equivalent queries, and a token-level contrastive alignment mechanism to improve feature consistency between labeled and pseudo-labeled samples. Collectively, these components enable CORA to perform robust reasoning segmentation under minimal supervision. Evaluation on benchmark datasets shows strong results, with CORA outperforming existing baselines by 2.3% on Cityscapes using only 100 labeled images, and improving performance by 2.4% on PanNuke with just 180 labeled images. This demonstrates CORA’s effectiveness in scenarios with constrained annotation availability across diverse domains such as urban scenes and histopathology. <div>
arXiv:2511.17755v1 Announce Type: new 
Abstract: Reasoning segmentation seeks pixel-accurate masks for targets referenced by complex, often implicit instructions, requiring context-dependent reasoning over the scene. Recent multimodal language models have advanced instruction following segmentation, yet generalization remains limited. The key bottleneck is the high cost of curating diverse, high-quality pixel annotations paired with rich linguistic supervision leading to brittle performance under distribution shift. Therefore, we present CORA, a semi-supervised reasoning segmentation framework that jointly learns from limited labeled data and a large corpus of unlabeled images. CORA introduces three main components: 1) conditional visual instructions that encode spatial and contextual relationships between objects; 2) a noisy pseudo-label filter based on the consistency of Multimodal LLM's outputs across semantically equivalent queries; and 3) a token-level contrastive alignment between labeled and pseudo-labeled samples to enhance feature consistency. These components enable CORA to perform robust reasoning segmentation with minimal supervision, outperforming existing baselines under constrained annotation settings. CORA achieves state-of-the-art results, requiring as few as 100 labeled images on Cityscapes, a benchmark dataset for urban scene understanding, surpassing the baseline by $+2.3\%$. Similarly, CORA improves performance by $+2.4\%$ with only 180 labeled images on PanNuke, a histopathology dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Dirichlet Transformer VAE for Hyperspectral Unmixing with Bundled Endmembers</title>
<link>https://arxiv.org/abs/2511.17757</link>
<guid>https://arxiv.org/abs/2511.17757</guid>
<content:encoded><![CDATA[
<div> Hyperspectral Unmixing, Transformer, Variational Autoencoder, Dirichlet Prior, Endmember Variability<br /><br />Summary:  
The paper introduces the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T), a novel model for hyperspectral unmixing aimed at separating mixed spectral signatures into pure material components.  The approach harnesses the global context modeling strength of transformer architectures combined with a Dirichlet prior in the latent space, which enforces physically meaningful constraints like sum-to-one and non-negativity for the abundance estimates. Unlike traditional methods relying on fixed ground truth spectra, LDVAE-T treats materials as bundled endmembers, allowing the model to capture intrinsic spectral variability. Specifically, the decoder outputs a mean spectrum and segmentwise covariance for each endmember in each patch, modeling correlated spectral variability. The transformer encoder then produces Dirichlet-distributed abundances to mix these learned bundles, preserving both interpretability and flexibility. Experimentation on three benchmark datasets—Samson, Jasper Ridge, and HYDICE Urban—demonstrates that LDVAE-T consistently outperforms current state-of-the-art models in abundance estimation and endmember extraction. Performance improvements are quantitatively measured using root mean squared error and spectral angle distance, showing the method’s enhanced accuracy and robustness in hyperspectral image analysis. <div>
arXiv:2511.17757v1 Announce Type: new 
Abstract: Hyperspectral images capture rich spectral information that enables per-pixel material identification; however, spectral mixing often obscures pure material signatures. To address this challenge, we propose the Latent Dirichlet Transformer Variational Autoencoder (LDVAE-T) for hyperspectral unmixing. Our model combines the global context modeling capabilities of transformer architectures with physically meaningful constraints imposed by a Dirichlet prior in the latent space. This prior naturally enforces the sum-to-one and non-negativity conditions essential for abundance estimation, thereby improving the quality of predicted mixing ratios. A key contribution of LDVAE-T is its treatment of materials as bundled endmembers, rather than relying on fixed ground truth spectra. In the proposed method our decoder predicts, for each endmember and each patch, a mean spectrum together with a structured (segmentwise) covariance that captures correlated spectral variability. Reconstructions are formed by mixing these learned bundles with Dirichlet-distributed abundances garnered from a transformer encoder, allowing the model to represent intrinsic material variability while preserving physical interpretability. We evaluate our approach on three benchmark datasets, Samson, Jasper Ridge, and HYDICE Urban and show that LDVAE-T consistently outperforms state-of-the-art models in abundance estimation and endmember extraction, as measured by root mean squared error and spectral angle distance, respectively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deepfake Geography: Detecting AI-Generated Satellite Images</title>
<link>https://arxiv.org/abs/2511.17766</link>
<guid>https://arxiv.org/abs/2511.17766</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, satellite imagery, Vision Transformers, deepfake detection, model interpretability  

<br /><br />Summary:  
The rapid progress of generative models such as StyleGAN2 and Stable Diffusion threatens the authenticity of satellite imagery, which is crucial for scientific and security applications. Unlike facial deepfake detection, satellite imagery presents unique challenges including terrain inconsistencies and structural artifacts that require specialized approaches. This study compares Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for identifying AI-generated satellite images. A large, curated dataset of over 130,000 RGB images from DM-AER and FSI datasets was used for evaluation. Results show that ViTs outperform CNNs significantly, achieving 95.11% accuracy versus 87.02%, due to their superior capability to capture long-range dependencies and global semantic information. To enhance transparency, interpretability techniques like Grad-CAM (for CNNs) and Chefer's attention attribution (for ViTs) were applied, uncovering differing model behaviors and supporting the trustworthiness of decisions. The study finds ViTs excel at detecting structural inconsistencies and repetitive texture patterns typical of synthetic satellite images. Future research will expand these findings to multispectral and SAR data, and incorporate frequency-domain analysis to improve detection robustness and protect the integrity of satellite imagery in critical use cases. <div>
arXiv:2511.17766v1 Announce Type: new 
Abstract: The rapid advancement of generative models such as StyleGAN2 and Stable Diffusion poses a growing threat to the authenticity of satellite imagery, which is increasingly vital for reliable analysis and decision-making across scientific and security domains. While deepfake detection has been extensively studied in facial contexts, satellite imagery presents distinct challenges, including terrain-level inconsistencies and structural artifacts. In this study, we conduct a comprehensive comparison between Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for detecting AI-generated satellite images. Using a curated dataset of over 130,000 labeled RGB images from the DM-AER and FSI datasets, we show that ViTs significantly outperform CNNs in both accuracy (95.11 percent vs. 87.02 percent) and overall robustness, owing to their ability to model long-range dependencies and global semantic structures. We further enhance model transparency using architecture-specific interpretability methods, including Grad-CAM for CNNs and Chefer's attention attribution for ViTs, revealing distinct detection behaviors and validating model trustworthiness. Our results highlight the ViT's superior performance in detecting structural inconsistencies and repetitive textural patterns characteristic of synthetic imagery. Future work will extend this research to multispectral and SAR modalities and integrate frequency-domain analysis to further strengthen detection capabilities and safeguard satellite imagery integrity in high-stakes applications.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?</title>
<link>https://arxiv.org/abs/2511.17792</link>
<guid>https://arxiv.org/abs/2511.17792</guid>
<content:encoded><![CDATA[
<div> Keywords: world models, robot path planning, Target-Bench, semantic targets, trajectory accuracy<br /><br />Summary: The paper introduces Target-Bench, the first benchmark designed specifically to evaluate world models on mapless robot path planning toward semantic targets in real-world environments. Target-Bench includes 450 video sequences collected by robots across 45 semantic categories, accompanied by SLAM-based ground truth trajectories to support quantitative analysis. The authors develop an evaluation pipeline that recovers camera motion from generated videos and assesses planning performance using five complementary metrics focused on target-reaching capability, trajectory accuracy, and directional consistency. State-of-the-art world models such as Sora 2, Veo 3.1, and the Wan series are evaluated, with the best off-the-shelf model (Wan2.2-Flash) achieving an overall score of only 0.299, highlighting significant current limitations for robotic planning tasks. They further demonstrate that fine-tuning an open-source 5-billion-parameter world model on only 325 scenarios from Target-Bench improves the overall score to 0.345, marking over a 400% increase relative to the base model (0.066) and a 15% improvement over the best off-the-shelf model. The authors commit to releasing the code and dataset to foster further research and improvements in world model-based robot path planning. <div>
arXiv:2511.17792v1 Announce Type: new 
Abstract: While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attention Guided Alignment in Efficient Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.17793</link>
<guid>https://arxiv.org/abs/2511.17793</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, attention patterns, object hallucination, cross-attention layers, Segment Anything Model<br /><br />Summary: This paper investigates the challenges faced by Large Vision-Language Models (VLMs), focusing on the multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs). The authors identify that concatenation-based architectures often struggle to differentiate semantically matching image-text pairs from non-matching ones, which contributes significantly to object hallucination in these models. To overcome this, they propose a novel framework called Attention-Guided Efficient Vision-Language Models (AGE-VLM). AGE-VLM introduces interleaved cross-attention layers designed to embed vision capabilities directly into small pretrained language models, thereby improving visual grounding. The approach leverages spatial knowledge distilled from the Segment Anything Model (SAM) to guide the attention mechanism, ensuring the model "looks" at the correct image regions. This targeted attention reduces hallucination substantially. The framework is evaluated on various vision-centric benchmarks, where it demonstrates performance that is either superior or comparable to existing efficient VLM methods. The study offers key insights into attention behavior in vision-language fusion, paving the way for future research aimed at enhancing visual and linguistic understanding in multimodal AI systems. <div>
arXiv:2511.17793v1 Announce Type: new 
Abstract: Large Vision-Language Models (VLMs) rely on effective multimodal alignment between pre-trained vision encoders and Large Language Models (LLMs) to integrate visual and textual information. This paper presents a comprehensive analysis of attention patterns in efficient VLMs, revealing that concatenation-based architectures frequently fail to distinguish between semantically matching and non-matching image-text pairs. This is a key factor for object hallucination in these models. To address this, we introduce Attention-Guided Efficient Vision-Language Models (AGE-VLM), a novel framework that enhances visual grounding through interleaved cross-attention layers to instill vision capabilities in pretrained small language models. This enforces in VLM the ability "look" at the correct image regions by leveraging spatial knowledge distilled from the Segment Anything Model (SAM), significantly reducing hallucination. We validate our approach across different vision-centric benchmarks where our method is better or comparable to prior work on efficient VLMs. Our findings provide valuable insights for future research aimed at achieving enhanced visual and linguistic understanding in VLMs.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pillar-0: A New Frontier for Radiology Foundation Models</title>
<link>https://arxiv.org/abs/2511.17803</link>
<guid>https://arxiv.org/abs/2511.17803</guid>
<content:encoded><![CDATA[
<div> Radiology, Foundation Model, CT, MRI, AUROC  

<br /><br />Summary:  
This paper introduces Pillar-0, a radiology foundation model pretrained on a large dataset comprising 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a major academic center. The study also presents RATE, a scalable framework that leverages large language models (LLMs) to extract structured labels for 366 radiologic findings with near-perfect accuracy. Pillar-0 processes volumetric CT and MRI data in a way that preserves high-fidelity 3D and grayscale contrast information, addressing limitations of prior models that treated scans as low-quality 2D slices. Testing on extensive internal datasets shows Pillar-0 achieves mean AUROCs between 82.9 and 90.1 across different body regions, outperforming leading existing models such as MedGemma, MedImageInsight, Lingshu, and Merlin by 7.8 to 15.8 AUROC points and leading in 87.2% of tasks. External validation on the Stanford Abdominal CT dataset confirms its superiority over top baselines. Beyond classification, Pillar-0 extends to lung cancer risk prediction, surpassing the previous state-of-the-art Sybil by 3.0 C-index points on NLST and generalizing well to other cohorts. Additionally, it demonstrates sample efficiency in brain hemorrhage detection, achieving over 95 AUROC using significantly less data. Together, Pillar-0 and RATE offer an open, clinically rigorous foundation for high-performance radiology AI, enabling advanced applications previously limited by computational and data constraints. <div>
arXiv:2511.17803v1 Announce Type: new 
Abstract: Radiology plays an integral role in modern medicine, yet rising imaging volumes have far outpaced workforce growth. Foundation models offer a path toward assisting with the full spectrum of radiology tasks, but existing medical models remain limited: they process volumetric CT and MRI as low-fidelity 2D slices, discard critical grayscale contrast information, and lack evaluation frameworks that reflect real clinical practice. We introduce Pillar-0, a radiology foundation model pretrained on 42,990 abdomen-pelvis CTs, 86,411 chest CTs, 14,348 head CTs, and 11,543 breast MRIs from a large academic center, together with RATE, a scalable framework that extracts structured labels for 366 radiologic findings with near-perfect accuracy using LLMs. Across internal test sets of 14,230 abdomen-pelvis CTs, 10,646 chest CTs, 4,906 head CTs, and 1,585 breast MRIs, Pillar-0 establishes a new performance frontier, achieving mean AUROCs of 86.4, 88.0, 90.1, and 82.9, outperforming MedGemma (Google), MedImageInsight (Microsoft), Lingshu (Alibaba), and Merlin (Stanford) by 7.8-15.8 AUROC points and ranking best in 87.2\% (319/366) tasks. Pillar-0 similarly outperforms all baselines in an external validation on the Stanford Abdominal CT dataset, including Merlin (82.2 vs 80.6 AUROC). Pillar-0 extends to tasks beyond its pretraining, such as long-horizon lung cancer risk prediction, where it improves upon the state-of-the-art Sybil by 3.0 C-index points on NLST, and generalizes with gains of 5.9 (MGH) and 1.9 (CGMH). In brain hemorrhage detection, Pillar-0 obtained a >95 AUROC when using only 1/20th of the data of the next most sample efficient baseline. Pillar-0 and RATE together provide an open, clinically rigorous foundation for building high-performance radiology systems, enabling applications that were previously infeasible due to computational, data, and evaluation constraints.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Stitch in Time: Learning Procedural Workflow via Self-Supervised Plackett-Luce Ranking</title>
<link>https://arxiv.org/abs/2511.17805</link>
<guid>https://arxiv.org/abs/2511.17805</guid>
<content:encoded><![CDATA[
<div> Keywords: procedural activities, self-supervised learning, Plackett-Luce model, video representation, temporal order<br /><br />Summary:<br />1. The paper addresses the challenge that current self-supervised learning (SSL) methods fail to capture the procedural nature inherent in activities such as cooking and surgical operations, which follow a specific temporal order.<br />2. It demonstrates with a motivating experiment that models pretrained on forward and time-reversed video sequences produce nearly identical features, indicating a lack of awareness of workflow progression.<br />3. To overcome this, the authors propose PL-Stitch, a novel SSL framework that leverages the temporal order of video frames as an explicit supervisory signal.<br />4. PL-Stitch incorporates two probabilistic objectives based on the Plackett-Luce (PL) model: a primary objective which trains the model to sort frames chronologically, and a secondary spatio-temporal jigsaw loss that captures detailed cross-frame object relationships.<br />5. Extensive evaluation on five benchmarks related to surgical phase recognition and cooking action segmentation shows that PL-Stitch significantly outperforms existing methods, yielding improvements such as +11.4 percentage points in k-NN accuracy on Cholec80 and +5.7 points in linear probing accuracy on Breakfast, thus proving its effectiveness in procedural video representation learning. <div>
arXiv:2511.17805v1 Announce Type: new 
Abstract: Procedural activities, ranging from routine cooking to complex surgical operations, are highly structured as a set of actions conducted in a specific temporal order. Despite their success on static images and short clips, current self-supervised learning methods often overlook the procedural nature that underpins such activities. We expose the lack of procedural awareness in current SSL methods with a motivating experiment: models pretrained on forward and time-reversed sequences produce highly similar features, confirming that their representations are blind to the underlying procedural order. To address this shortcoming, we propose PL-Stitch, a self-supervised framework that harnesses the inherent temporal order of video frames as a powerful supervisory signal. Our approach integrates two novel probabilistic objectives based on the Plackett-Luce (PL) model. The primary PL objective trains the model to sort sampled frames chronologically, compelling it to learn the global workflow progression. The secondary objective, a spatio-temporal jigsaw loss, complements the learning by capturing fine-grained, cross-frame object correlations. Our approach consistently achieves superior performance across five surgical and cooking benchmarks. Specifically, PL-Stitch yields significant gains in surgical phase recognition (e.g., +11.4 pp k-NN accuracy on Cholec80) and cooking action segmentation (e.g., +5.7 pp linear probing accuracy on Breakfast), demonstrating its effectiveness for procedural video representation learning.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REXO: Indoor Multi-View Radar Object Detection via 3D Bounding Box Diffusion</title>
<link>https://arxiv.org/abs/2511.17806</link>
<guid>https://arxiv.org/abs/2511.17806</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view radar, 3D bounding box diffusion, indoor perception, cross-view feature association, object detection  

<br /><br />Summary:  
1. The paper addresses multi-view indoor radar perception, emphasizing its advantages in cost-effectiveness and privacy preservation compared to other sensing modalities.  
2. Existing methods often depend on implicit cross-view radar feature associations, such as proposal pairing or query-to-feature cross-attention, which can cause ambiguous feature matching and reduce detection performance in complex indoor environments.  
3. To overcome these issues, the authors propose REXO, a novel approach that extends the 2D bounding box diffusion process from DiffusionDet into 3D radar space, enabling explicit cross-view radar feature association.  
4. REXO uses noisy 3D bounding boxes to guide a cross-view radar-conditioned denoising process, improving the clarity and accuracy of feature matching across views.  
5. By incorporating prior knowledge that the detected person is in contact with the ground, REXO reduces diffusion parameters, enhancing computational efficiency without compromising performance.  
6. The method is validated on two publicly available indoor radar datasets, HIBER and MMVR, where it outperforms state-of-the-art approaches by significant margins (+4.22 AP on HIBER and +11.02 AP on MMVR), demonstrating its effectiveness for multi-view indoor radar object detection tasks. <div>
arXiv:2511.17806v1 Announce Type: new 
Abstract: Multi-view indoor radar perception has drawn attention due to its cost-effectiveness and low privacy risks. Existing methods often rely on {implicit} cross-view radar feature association, such as proposal pairing in RFMask or query-to-feature cross-attention in RETR, which can lead to ambiguous feature matches and degraded detection in complex indoor scenes. To address these limitations, we propose \textbf{REXO} (multi-view Radar object dEtection with 3D bounding boX diffusiOn), which lifts the 2D bounding box (BBox) diffusion process of DiffusionDet into the 3D radar space. REXO utilizes these noisy 3D BBoxes to guide an {explicit} cross-view radar feature association, enhancing the cross-view radar-conditioned denoising process. By accounting for prior knowledge that the person is in contact with the ground, REXO reduces the number of diffusion parameters by determining them from this prior. Evaluated on two open indoor radar datasets, our approach surpasses state-of-the-art methods by a margin of +4.22 AP on the HIBER dataset and +11.02 AP on the MMVR dataset.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-Weighted Non-IID Sampling for Flow Matching Models</title>
<link>https://arxiv.org/abs/2511.17812</link>
<guid>https://arxiv.org/abs/2511.17812</guid>
<content:encoded><![CDATA[
<div> Keywords: flow-matching models, importance-weighted sampling, non-IID sampling, score-based regularization, residual velocity field<br /><br />Summary:  
The paper addresses the challenge of estimating expectations of functions from flow-matching model outputs under limited sampling budgets. Traditional independent sampling methods result in high-variance estimates, particularly when rare but influential outcomes dominate. The authors propose an innovative importance-weighted non-IID sampling framework that draws multiple correlated samples to better explore diverse, prominent areas of the flow's distribution while preserving unbiasedness through estimated importance weights. A key contribution is the introduction of a score-based regularization mechanism that leverages the gradient of the log probability (score function) to promote sample diversity specifically within high-density regions, preventing samples from drifting off the data manifold. Additionally, the work pioneers an approach for computing importance weights of non-IID samples by learning a residual velocity field, ensuring the generated samples' marginal distribution matches the target distribution. Empirical results demonstrate that this method produces both diverse and high-quality samples as well as precise estimates of importance weights and expectations. Overall, the approach significantly improves the reliable characterization and estimation of outputs from flow-matching models. The authors also commit to releasing their code publicly on GitHub, enabling further research and verification. <div>
arXiv:2511.17812v1 Announce Type: new 
Abstract: Flow-matching models effectively represent complex distributions, yet estimating expectations of functions of their outputs remains challenging under limited sampling budgets. Independent sampling often yields high-variance estimates, especially when rare but with high-impact outcomes dominate the expectation. We propose an importance-weighted non-IID sampling framework that jointly draws multiple samples to cover diverse, salient regions of a flow's distribution while maintaining unbiased estimation via estimated importance weights. To balance diversity and quality, we introduce a score-based regularization for the diversity mechanism, which uses the score function, i.e., the gradient of the log probability, to ensure samples are pushed apart within high-density regions of the data manifold, mitigating off-manifold drift. We further develop the first approach for importance weighting of non-IID flow samples by learning a residual velocity field that reproduces the marginal distribution of the non-IID samples. Empirically, our method produces diverse, high-quality samples and accurate estimates of both importance weights and expectations, advancing the reliable characterization of flow-matching model outputs. Our code will be publicly available on GitHub.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QAL: A Loss for Recall Precision Balance in 3D Reconstruction</title>
<link>https://arxiv.org/abs/2511.17824</link>
<guid>https://arxiv.org/abs/2511.17824</guid>
<content:encoded><![CDATA[
<div> Volumetric learning, 3D vision, Quality-Aware Loss, Chamfer Distance, robotic manipulation<br /><br />Summary:<br /><br />This paper addresses the limitations of existing training objectives for volumetric learning in 3D vision tasks such as completion, reconstruction, and mesh generation, which predominantly rely on Chamfer Distance (CD) or Earth Mover’s Distance (EMD). These traditional metrics do not effectively balance recall and precision, often missing thin structures and under-represented regions. To overcome this, the authors propose a novel Quality-Aware Loss (QAL), which serves as a drop-in replacement for CD/EMD. QAL uniquely combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into adjustable components. Experimental results demonstrate that QAL consistently improves coverage, achieving average gains of +4.3 points over CD and +2.8 points over other leading alternatives. These improvements, while modest in percentage, significantly enhance the recovery of fine details that previous methods overlook. Extensive ablation studies confirm the stable performance of QAL across various hyperparameters and output resolutions. Further, retraining experiments on datasets such as PCN and ShapeNet validate its generalization across architectures and data. Importantly, completions trained with QAL yield higher grasp scores in the GraspNet evaluation, indicating direct benefits for robotic manipulation reliability. Overall, QAL presents a principled, interpretable, and practical training objective for enhanced 3D vision and safety-critical robotics applications. <div>
arXiv:2511.17824v1 Announce Type: new 
Abstract: Volumetric learning underpins many 3D vision tasks such as completion, reconstruction, and mesh generation, yet training objectives still rely on Chamfer Distance (CD) or Earth Mover's Distance (EMD), which fail to balance recall and precision. We propose Quality-Aware Loss (QAL), a drop-in replacement for CD/EMD that combines a coverage-weighted nearest-neighbor term with an uncovered-ground-truth attraction term, explicitly decoupling recall and precision into tunable components.
  Across diverse pipelines, QAL achieves consistent coverage gains, improving by an average of +4.3 pts over CD and +2.8 pts over the best alternatives. Though modest in percentage, these improvements reliably recover thin structures and under-represented regions that CD/EMD overlook. Extensive ablations confirm stable performance across hyperparameters and across output resolutions, while full retraining on PCN and ShapeNet demonstrates generalization across datasets and backbones. Moreover, QAL-trained completions yield higher grasp scores under GraspNet evaluation, showing that improved coverage translates directly into more reliable robotic manipulation.
  QAL thus offers a principled, interpretable, and practical objective for robust 3D vision and safety-critical robotics pipelines
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward explainable AI approaches for breast imaging: adapting foundation models to diverse populations</title>
<link>https://arxiv.org/abs/2511.17828</link>
<guid>https://arxiv.org/abs/2511.17828</guid>
<content:encoded><![CDATA[
<div> Keywords: Foundation models, breast imaging, BiomedCLIP, BI-RADS classification, multi-modality training<br /><br />Summary:<br /><br />This study explores the application of foundation models, specifically BiomedCLIP, in the domain of breast imaging, which has been less investigated compared to other medical imaging tasks. The researchers adapted BiomedCLIP to automate BI-RADS breast density classification by utilizing multi-modality mammographic data including synthesized 2D images, digital mammography (DM), and digital breast tomosynthesis (DBT). A comprehensive dataset of 96,995 mammographic images was employed, comparing single-modality training (synthesized 2D only) against multi-modality training approaches. Both approaches achieved comparable accuracy (0.73 for single-modality and 0.74 for multi-modality), but the multi-modality model demonstrated greater versatility across imaging types and maintained consistently high AUC values above 0.84 across different BI-RADS categories. Additionally, external validation on the RSNA and EMBED datasets confirmed strong generalization capabilities with AUC scores ranging from 0.80 to 0.93. GradCAM visualizations illustrated clinically relevant and consistent model attention, enhancing interpretability and robustness. Overall, this research highlights the promise of foundation models like BiomedCLIP for breast imaging tasks, suggesting potential future applications in diagnostics and broader clinical utility. <div>
arXiv:2511.17828v1 Announce Type: new 
Abstract: Foundation models hold promise for specialized medical imaging tasks, though their effectiveness in breast imaging remains underexplored. This study leverages BiomedCLIP as a foundation model to address challenges in model generalization. BiomedCLIP was adapted for automated BI-RADS breast density classification using multi-modality mammographic data (synthesized 2D images, digital mammography, and digital breast tomosynthesis). Using 96,995 images, we compared single-modality (s2D only) and multi-modality training approaches, addressing class imbalance through weighted contrastive learning. Both approaches achieved similar accuracy (multi-modality: 0.74, single-modality: 0.73), with the multi-modality model offering broader applicability across different imaging modalities and higher AUC values consistently above 0.84 across BI-RADS categories. External validation on the RSNA and EMBED datasets showed strong generalization capabilities (AUC range: 0.80-0.93). GradCAM visualizations confirmed consistent and clinically relevant attention patterns, highlighting the models interpretability and robustness. This research underscores the potential of foundation models for breast imaging applications, paving the way for future extensions for diagnostic tasks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Show Me: Unifying Instructional Image and Video Generation with Diffusion Models</title>
<link>https://arxiv.org/abs/2511.17839</link>
<guid>https://arxiv.org/abs/2511.17839</guid>
<content:encoded><![CDATA[
<div> Keywords: video diffusion models, image manipulation, video prediction, spatial-temporal consistency, instruction-guided generation<br /><br />Summary:<br /><br />The paper addresses the challenge of generating visual instructions in context by unifying two traditionally separate tasks: text-guided image manipulation and video prediction. Prior methods either focus on static spatial changes or temporal action sequences but rarely integrate both, leading to shortcomings such as ignoring the progression of actions or the final goals. To overcome this, the authors propose ShowMe, a framework that selectively activates spatial and temporal components within video diffusion models to perform both tasks seamlessly. They introduce structure and motion consistency rewards to ensure structural fidelity and temporal coherence in generated outputs. The framework leverages the spatial knowledge gained from video pretraining to enhance the realism and contextual consistency of non-rigid image edits. Concurrently, the instruction-guided manipulation capability strengthens the model's goal-oriented reasoning for video prediction. Experimental results across multiple benchmarks demonstrate that ShowMe outperforms specialized expert models in both instructional image and video generation tasks. This highlights the effectiveness of video diffusion models as unified transformers capable of modeling action-object states over space and time, enabling more interactive and context-aware visual instruction generation. <div>
arXiv:2511.17839v1 Announce Type: new 
Abstract: Generating visual instructions in a given context is essential for developing interactive world simulators. While prior works address this problem through either text-guided image manipulation or video prediction, these tasks are typically treated in isolation. This separation reveals a fundamental issue: image manipulation methods overlook how actions unfold over time, while video prediction models often ignore the intended outcomes. To this end, we propose ShowMe, a unified framework that enables both tasks by selectively activating the spatial and temporal components of video diffusion models. In addition, we introduce structure and motion consistency rewards to improve structural fidelity and temporal coherence. Notably, this unification brings dual benefits: the spatial knowledge gained through video pretraining enhances contextual consistency and realism in non-rigid image edits, while the instruction-guided manipulation stage equips the model with stronger goal-oriented reasoning for video prediction. Experiments on diverse benchmarks demonstrate that our method outperforms expert models in both instructional image and video generation, highlighting the strength of video diffusion models as a unified action-object state transformer.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception</title>
<link>https://arxiv.org/abs/2511.17843</link>
<guid>https://arxiv.org/abs/2511.17843</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent cooperative perception, bandwidth efficiency, semantic feature encoding, redundancy elimination, communication policy  

<br /><br />Summary:  
This paper addresses the challenge of limited communication bandwidth in multi-agent cooperative perception (CP) systems, such as those used in autonomous driving, where occlusion and limited sensing range hinder single-agent performance. The authors critique existing methods that focus on compression or heuristic message selection without considering the semantic relevance or redundancy across agents’ sensory data. To maximize the utility of every transmitted bit, they propose a joint semantic feature encoding and transmission framework named JigsawComm. JigsawComm is end-to-end trainable and uses a regularized encoder to extract semantically relevant, sparse features from each agent. It incorporates a lightweight Feature Utility Estimator that predicts each agent's feature contribution to the final perception task, producing meta utility maps exchanged among agents. These maps enable the computation of an optimal transmission policy that selects features with the highest utility scores per location, effectively eliminating redundant transmissions. This policy scales with constant communication cost \(\mathcal{O}(1)\) regardless of the number of agents. Evaluations on the OPV2V and DAIR-V2X benchmarks demonstrate that JigsawComm can reduce data volume by more than 500 times compared to prior methods while maintaining or improving perception accuracy, highlighting its communication efficiency and semantic awareness in cooperative perception tasks. <div>
arXiv:2511.17843v1 Announce Type: new 
Abstract: Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Data-Efficient Adaptation for Controllable Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2511.17844</link>
<guid>https://arxiv.org/abs/2511.17844</guid>
<content:encoded><![CDATA[
<div> Fine-tuning, text-to-video diffusion models, generative controls, synthetic data, data efficiency<br /><br />Summary:<br /><br />This paper addresses the challenge of fine-tuning large-scale text-to-video diffusion models to incorporate new generative controls, such as physical camera parameters like shutter speed and aperture. Traditional methods require vast, high-quality datasets which are difficult and expensive to obtain. The authors propose a novel, data-efficient fine-tuning strategy that leverages sparse, low-quality synthetic data instead of photorealistic real-world data. Their experiments reveal that models fine-tuned on such simple synthetic datasets not only learn the desired controls effectively but also outperform models fine-tuned on high-fidelity "real" data. To explain this unexpected outcome, the paper provides a comprehensive framework that both intuitively and quantitatively justifies why training on synthetic data leads to superior generative control performance. Overall, the work highlights a promising direction for improving text-to-video model customization while significantly lowering the data requirements and costs associated with fine-tuning these complex generative systems. <div>
arXiv:2511.17844v1 Announce Type: new 
Abstract: Fine-tuning large-scale text-to-video diffusion models to add new generative controls, such as those over physical camera parameters (e.g., shutter speed or aperture), typically requires vast, high-fidelity datasets that are difficult to acquire. In this work, we propose a data-efficient fine-tuning strategy that learns these controls from sparse, low-quality synthetic data. We show that not only does fine-tuning on such simple data enable the desired controls, it actually yields superior results to models fine-tuned on photorealistic "real" data. Beyond demonstrating these results, we provide a framework that justifies this phenomenon both intuitively and quantitatively.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use</title>
<link>https://arxiv.org/abs/2511.17881</link>
<guid>https://arxiv.org/abs/2511.17881</guid>
<content:encoded><![CDATA[
<div> Document Visual Question Answering, spatial graph reasoning, memory-augmented inference, multi-modal framework, interpretable reasoning<br /><br />Summary: Document Visual Question Answering (DocVQA) tasks require models to understand and integrate textual semantics, spatial layouts, and visual features, yet current approaches face challenges in explicit spatial reasoning, handling high-resolution documents efficiently, performing multi-hop reasoning, and maintaining interpretability. The proposed MGA-VQA framework addresses these limitations by combining token-level encoding with spatial graph reasoning, allowing the model to capture intricate spatial relationships within documents. It further incorporates a memory-augmented inference mechanism to improve multi-step reasoning capabilities, enhancing the model’s ability to process complex queries. To manage computational efficiency, MGA-VQA employs question-guided compression techniques that reduce processing overhead without sacrificing critical information. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways, enabling transparent reasoning processes, along with structured memory access which makes intermediate reasoning steps easier to understand. Comprehensive evaluations on six benchmark datasets—FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO—demonstrate that MGA-VQA achieves superior accuracy and efficiency, consistently improving both answer prediction and spatial localization tasks. The results highlight the framework’s strength in balancing performance with interpretability and computational efficiency in DocVQA scenarios. <div>
arXiv:2511.17881v1 Announce Type: new 
Abstract: Document Visual Question Answering (DocVQA) requires models to jointly understand textual semantics, spatial layout, and visual features. Current methods struggle with explicit spatial relationship modeling, inefficiency with high-resolution documents, multi-hop reasoning, and limited interpretability. We propose MGA-VQA, a multi-modal framework that integrates token-level encoding, spatial graph reasoning, memory-augmented inference, and question-guided compression. Unlike prior black-box models, MGA-VQA introduces interpretable graph-based decision pathways and structured memory access for enhanced reasoning transparency. Evaluation across six benchmarks (FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO) demonstrates superior accuracy and efficiency, with consistent improvements in both answer prediction and spatial localization.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArticFlow: Generative Simulation of Articulated Mechanisms</title>
<link>https://arxiv.org/abs/2511.17883</link>
<guid>https://arxiv.org/abs/2511.17883</guid>
<content:encoded><![CDATA[
<div> Articulated 3D generation, flow matching, action-conditioned kinematics, latent flow, point cloud synthesis<br /><br />Summary:<br /><br />1. The paper addresses the challenge of generating articulated 3D shapes, which involve complex, action-dependent deformations and suffer from limited datasets compared to static 3D shape generation.<br />2. The authors propose ArticFlow, a novel two-stage flow matching framework that learns a controllable velocity field transforming noise to target point sets with explicit action control.<br />3. ArticFlow consists of two coupled components: a latent flow that maps noise into a shape-prior code and a point flow that moves points conditioned on both the action and the shape prior, enabling flexible generation across various articulated categories.<br />4. Experiments on the MuJoCo Menagerie dataset demonstrate that ArticFlow functions effectively as both a generative model and a neural simulator, capable of predicting action-conditioned kinematics and synthesizing novel morphologies by interpolating in the latent space.<br />5. Compared to specialized object simulators and action-conditioned static point-cloud generators, ArticFlow shows superior kinematic accuracy and improved shape quality, highlighting action-conditioned flow matching as a promising approach for controllable, high-quality articulated mechanism generation. <div>
arXiv:2511.17883v1 Announce Type: new 
Abstract: Recent advances in generative models have produced strong results for static 3D shapes, whereas articulated 3D generation remains challenging due to action-dependent deformations and limited datasets. We introduce ArticFlow, a two-stage flow matching framework that learns a controllable velocity field from noise to target point sets under explicit action control. ArticFlow couples (i) a latent flow that transports noise to a shape-prior code and (ii) a point flow that transports points conditioned on the action and the shape prior, enabling a single model to represent diverse articulated categories and generalize across actions. On MuJoCo Menagerie, ArticFlow functions both as a generative model and as a neural simulator: it predicts action-conditioned kinematics from a compact prior and synthesizes novel morphologies via latent interpolation. Compared with object-specific simulators and an action-conditioned variant of static point-cloud generators, ArticFlow achieves higher kinematic accuracy and better shape quality. Results show that action-conditioned flow matching is a practical route to controllable and high-quality articulated mechanism generation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning</title>
<link>https://arxiv.org/abs/2511.17885</link>
<guid>https://arxiv.org/abs/2511.17885</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal large language models, visual token pruning, mixture-of-experts, FastMMoE, inference acceleration<br /><br />Summary:<br /><br />1. Multimodal large language models (MLLMs) face challenges with high-resolution visual inputs, which produce lengthy sequences of visual tokens, causing increased computational and memory demands as well as longer inference latency. <br /><br />2. Reducing redundant visual tokens is essential to alleviate these burdens and enable MLLM deployment in environments with limited resources or strict latency requirements.<br /><br />3. Existing visual token pruning methods primarily depend on attention-based redundancy detection and are designed with dense architectures in mind, limiting their effectiveness on mixture-of-experts (MoE) models.<br /><br />4. The proposed Fast Multimodal Mixture-of-Experts (FastMMoE) is a training-free acceleration framework tailored for MoE-based MLLMs, which strategically reduces expert activation and prunes visual tokens based on similarities in their routing probability distributions.<br /><br />5. Experimental results on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 show that FastMMoE can reduce FLOPs by up to 55.0% while preserving approximately 95.5% of the original model performance, outperforming state-of-the-art dense-model pruning methods like FastV and SparseVLM across various retention rates. <div>
arXiv:2511.17885v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA</title>
<link>https://arxiv.org/abs/2511.17886</link>
<guid>https://arxiv.org/abs/2511.17886</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-language models, Knowledge distillation, CLIP-style models, Multimodal tasks, Performance scaling<br /><br />Summary:<br /><br />1. Vision-language models (VLMs) have demonstrated significant success in numerous multimodal applications but require extensive computational resources, limiting their ease of deployment.  
2. Knowledge distillation (KD) is a proven technique for creating lightweight models that maintain strong performance, validated in both natural language processing and computer vision.  
3. Despite its effectiveness in those fields, KD has seen limited application in VLMs, especially for CLIP-style models, and mostly in small teacher models and narrowly scoped tasks such as classification or retrieval.  
4. This work conducts the first comprehensive study of KD applied across a spectrum of CLIP-style teacher models ranging from standard baselines to large state-of-the-art architectures.  
5. Contrary to established trends in NLP and vision domains, the study reveals that stronger teacher models do not consistently produce better student models in VLM distillation. In fact, many current distillation methods do not scale well, resulting in decreased performance on complex downstream multimodal tasks like visual question answering.  
6. These findings challenge existing assumptions in knowledge distillation for multimodal models and highlight the need for new approaches designed to enhance parameter efficiency without compromising task performance. <div>
arXiv:2511.17886v1 Announce Type: new 
Abstract: Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINDiff: Mask-Integrated Negative Attention for Controlling Overfitting in Text-to-Image Personalization</title>
<link>https://arxiv.org/abs/2511.17888</link>
<guid>https://arxiv.org/abs/2511.17888</guid>
<content:encoded><![CDATA[
<div> Keywords: text-to-image, overfitting, negative attention, DreamBooth, inference-time control<br /><br />Summary:  
This paper addresses the overfitting issue present in large-scale text-to-image personalization, particularly when training on a small number of images. Traditional solutions like DreamBooth use a class-specific prior-preservation loss to combat overfitting, which increases training computational cost and limits user control during inference. To overcome these problems, the authors propose Mask-Integrated Negative Attention Diffusion (MINDiff), a novel approach that introduces negative attention to suppress the subject’s influence in irrelevant masked regions by modifying the cross-attention mechanism during inference. This enables more precise semantic control and improves alignment between the text prompt and generated image by reducing subject dominance outside relevant areas. Users can adjust a scale parameter λ at inference time to balance the trade-off between subject fidelity and text alignment, providing enhanced flexibility. Importantly, MINDiff operates entirely during inference without changing the model architecture or requiring re-training, allowing easy application to existing DreamBooth models. Experimental results demonstrate that MINDiff more effectively mitigates overfitting compared to the prior-preservation loss method. The authors also provide their implementation publicly, facilitating adoption and further exploration of their technique. <div>
arXiv:2511.17888v1 Announce Type: new 
Abstract: In the personalization process of large-scale text-to-image models, overfitting often occurs when learning specific subject from a limited number of images. Existing methods, such as DreamBooth, mitigate this issue through a class-specific prior-preservation loss, which requires increased computational cost during training and limits user control during inference time. To address these limitations, we propose Mask-Integrated Negative Attention Diffusion (MINDiff). MINDiff introduces a novel concept, negative attention, which suppresses the subject's influence in masked irrelevant regions. We achieve this by modifying the cross-attention mechanism during inference. This enables semantic control and improves text alignment by reducing subject dominance in irrelevant regions. Additionally, during the inference time, users can adjust a scale parameter lambda to balance subject fidelity and text alignment. Our qualitative and quantitative experiments on DreamBooth models demonstrate that MINDiff mitigates overfitting more effectively than class-specific prior-preservation loss. As our method operates entirely at inference time and does not alter the model architecture, it can be directly applied to existing DreamBooth models without re-training. Our code is available at https://github.com/seuleepy/MINDiff.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoupled Audio-Visual Dataset Distillation</title>
<link>https://arxiv.org/abs/2511.17890</link>
<guid>https://arxiv.org/abs/2511.17890</guid>
<content:encoded><![CDATA[
<div> Audio-Visual Dataset Distillation, Cross-Modal Alignment, Pretraining, Decoupled Representations, Dataset Compression<br /><br />Summary:<br /><br />1. The paper addresses Audio-Visual Dataset Distillation, which aims to compress large-scale audio-visual datasets into smaller subsets while maintaining the original performance. <br />2. Existing Distribution Matching (DM) methods fail to effectively capture intrinsic cross-modal alignment, leading to challenges in the distillation process.<br />3. Previous attempts to introduce cross-modal matching suffer from two main issues: inconsistent modality mapping spaces caused by independently and randomly initialized encoders, and deterioration of modality-specific information due to direct modality interactions.<br />4. To overcome these challenges, the authors propose DAVDD, a pretraining-based decoupled distillation framework that uses a diverse pretrained feature bank for stable modality features and a lightweight decoupler bank to disentangle common and private representations.<br />5. DAVDD introduces Common Intermodal Matching combined with a Sample-Distribution Joint Alignment strategy to align shared representations both at the sample and global distribution levels, while completely isolating private representations to preserve modality-specific cues.<br />6. Extensive experiments on multiple benchmarks demonstrate that DAVDD achieves state-of-the-art results under various IPC (Images Per Class) settings, proving the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation.<br />7. The authors plan to release the code to facilitate further research in this area. <div>
arXiv:2511.17890v1 Announce Type: new 
Abstract: Audio-Visual Dataset Distillation aims to compress large-scale datasets into compact subsets while preserving the performance of the original data. However, conventional Distribution Matching (DM) methods struggle to capture intrinsic cross-modal alignment. Subsequent studies have attempted to introduce cross-modal matching, but two major challenges remain: (i) independently and randomly initialized encoders lead to inconsistent modality mapping spaces, increasing training difficulty; and (ii) direct interactions between modalities tend to damage modality-specific (private) information, thereby degrading the quality of the distilled data. To address these challenges, we propose DAVDD, a pretraining-based decoupled audio-visual distillation framework. DAVDD leverages a diverse pretrained bank to obtain stable modality features and uses a lightweight decoupler bank to disentangle them into common and private representations. To effectively preserve cross-modal structure, we further introduce Common Intermodal Matching together with a Sample-Distribution Joint Alignment strategy, ensuring that shared representations are aligned both at the sample level and the global distribution level. Meanwhile, private representations are entirely isolated from cross-modal interaction, safeguarding modality-specific cues throughout distillation. Extensive experiments across multiple benchmarks show that DAVDD achieves state-of-the-art results under all IPC settings, demonstrating the effectiveness of decoupled representation learning for high-quality audio-visual dataset distillation. Code will be released.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CUS-GS: A Compact Unified Structured Gaussian Splatting Framework for Multimodal Scene Representation</title>
<link>https://arxiv.org/abs/2511.17904</link>
<guid>https://arxiv.org/abs/2511.17904</guid>
<content:encoded><![CDATA[
<div> Gaussian Splatting, 3D scene representation, multimodal semantics, voxelized anchor structure, feature-aware significance evaluation<br /><br />Summary:<br /><br />This paper introduces CUS-GS, a Compact Unified Structured Gaussian Splatting representation aimed at bridging the gap between semantics-oriented and structure-oriented 3D scene representations. The authors design a voxelized anchor structure that forms a spatial scaffold to capture explicit 3D geometry while simultaneously extracting multimodal semantic features from foundation models including CLIP, DINOv2, and SEEM. A key contribution is the multimodal latent feature allocation mechanism that unifies appearance, geometry, and semantic information across heterogeneous feature spaces, enabling a consistent multimodal representation. Additionally, the method incorporates a feature-aware significance evaluation strategy to dynamically guide the growth and pruning of anchors, effectively eliminating redundant or invalid anchors while preserving semantic integrity. Extensive experimental validation demonstrates that CUS-GS attains competitive performance against state-of-the-art approaches but with significantly fewer parameters—6 million compared to around 35 million for comparable models—indicating superior model efficiency. This highlights an excellent balance between accuracy and compactness in 3D scene representation, making CUS-GS a compelling framework for effective and efficient multimodal 3D modeling. <div>
arXiv:2511.17904v1 Announce Type: new 
Abstract: Recent advances in Gaussian Splatting based 3D scene representation have shown two major trends: semantics-oriented approaches that focus on high-level understanding but lack explicit 3D geometry modeling, and structure-oriented approaches that capture spatial structures yet provide limited semantic abstraction. To bridge this gap, we present CUS-GS, a compact unified structured Gaussian Splatting representation, which connects multimodal semantic features with structured 3D geometry. Specifically, we design a voxelized anchor structure that constructs a spatial scaffold, while extracting multimodal semantic features from a set of foundation models (e.g., CLIP, DINOv2, SEEM). Moreover, we introduce a multimodal latent feature allocation mechanism to unify appearance, geometry, and semantics across heterogeneous feature spaces, ensuring a consistent representation across multiple foundation models. Finally, we propose a feature-aware significance evaluation strategy to dynamically guide anchor growing and pruning, effectively removing redundant or invalid anchors while maintaining semantic integrity. Extensive experiments show that CUS-GS achieves competitive performance compared to state-of-the-art methods using as few as 6M parameters - an order of magnitude smaller than the closest rival at 35M - highlighting the excellent trade off between performance and model efficiency of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation</title>
<link>https://arxiv.org/abs/2511.17914</link>
<guid>https://arxiv.org/abs/2511.17914</guid>
<content:encoded><![CDATA[
<div> Dataset Distillation, Long-tailed Distribution, Soft Labels, Adaptive Soft-label Alignment, Imbalance-aware Generalization Bound<br /><br />Summary: Dataset distillation is a technique that compresses large datasets into smaller, highly informative synthetic datasets to reduce storage and training costs. However, existing methods primarily focus on balanced datasets and face challenges when applied to real-world long-tailed distributions where some classes have significantly fewer samples. This work highlights the importance of soft labels in addressing these challenges in long-tailed dataset distillation and analyzes the causes of performance degradation. The authors derive an imbalance-aware generalization bound for models trained on distilled datasets, providing theoretical insight into the issue. They identify two main sources of bias in soft labels—one arising from the distillation model itself and the other from the synthetic distilled images—by systematically perturbing data imbalance levels. To mitigate these biases, the paper proposes ADSA, an Adaptive Soft-label Alignment module designed to calibrate and correct the entangled biases. ADSA is lightweight and easily integrated into existing distillation frameworks. Experiments on the ImageNet-1k-LT dataset demonstrate that ADSA significantly improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4% with extreme data compression settings. The method proves robust across various label budgets and distillation methods, offering a generalizable solution for long-tailed dataset distillation. The code is publicly available for reproducibility and further research. <div>
arXiv:2511.17914v1 Announce Type: new 
Abstract: Dataset distillation compresses large-scale datasets into compact, highly informative synthetic data, significantly reducing storage and training costs. However, existing research primarily focuses on balanced datasets and struggles to perform under real-world long-tailed distributions. In this work, we emphasize the critical role of soft labels in long-tailed dataset distillation and uncover the underlying mechanisms contributing to performance degradation. Specifically, we derive an imbalance-aware generalization bound for model trained on distilled dataset. We then identify two primary sources of soft-label bias, which originate from the distillation model and the distilled images, through systematic perturbation of the data imbalance levels. To address this, we propose ADSA, an Adaptive Soft-label Alignment module that calibrates the entangled biases. This lightweight module integrates seamlessly into existing distillation pipelines and consistently improves performance. On ImageNet-1k-LT with EDC and IPC=50, ADSA improves tail-class accuracy by up to 11.8% and raises overall accuracy to 41.4%. Extensive experiments demonstrate that ADSA provides a robust and generalizable solution under limited label budgets and across a range of distillation techniques. Code is available at: https://github.com/j-cyoung/ADSA_DD.git.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frequency-Adaptive Sharpness Regularization for Improving 3D Gaussian Splatting Generalization</title>
<link>https://arxiv.org/abs/2511.17918</link>
<guid>https://arxiv.org/abs/2511.17918</guid>
<content:encoded><![CDATA[
<div> 3D Gaussian Splatting, novel view synthesis, few-shot generalization, Frequency-Adaptive Sharpness Regularization, loss landscape sharpness<br /><br />Summary:<br /><br />This paper addresses the problem of 3D Gaussian Splatting (3DGS) overfitting to sparse observations in few-shot novel viewpoint scenarios, which limits its generalization capabilities. Viewing novel view synthesis as a generalization challenge, the authors propose Frequency-Adaptive Sharpness Regularization (FASR) to improve 3DGS training by guiding it toward solutions that generalize better to unseen viewpoints. While Sharpness-Aware Minimization (SAM) is known to improve generalization in classification by smoothing the loss landscape, directly applying it to 3DGS is ineffective because it over-regularizes, leading to loss of high-frequency details. To overcome this, FASR adapts regularization strength and neighborhood radius based on local image frequency, balancing sharpness reduction and detail preservation. This frequency-adaptive approach prevents artifacts such as floaters in novel views and retains fine details that standard SAM tends to oversmooth. Extensive experiments on diverse datasets and configurations show that FASR consistently enhances the performance of multiple 3DGS baseline methods. The authors also commit to releasing their code publicly to support reproducibility and further research. <div>
arXiv:2511.17918v1 Announce Type: new 
Abstract: Despite 3D Gaussian Splatting (3DGS) excelling in most configurations, it lacks generalization across novel viewpoints in a few-shot scenario because it overfits to the sparse observations. We revisit 3DGS optimization from a machine learning perspective, framing novel view synthesis as a generalization problem to unseen viewpoints-an underexplored direction. We propose Frequency-Adaptive Sharpness Regularization (FASR), which reformulates the 3DGS training objective, thereby guiding 3DGS to converge toward a better generalization solution. Although Sharpness-Aware Minimization (SAM) similarly reduces the sharpness of the loss landscape to improve generalization of classification models, directly employing it to 3DGS is suboptimal due to the discrepancy between the tasks. Specifically, it hinders reconstructing high-frequency details due to excessive regularization, while reducing its strength leads to under-penalizing sharpness. To address this, we reflect the local frequency of images to set the regularization weight and the neighborhood radius when estimating the local sharpness. It prevents floater artifacts in novel viewpoints and reconstructs fine details that SAM tends to oversmooth. Across datasets with various configurations, our method consistently improves a wide range of baselines. Code will be available at https://bbangsik13.github.io/FASR.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PA-FAS: Towards Interpretable and Generalizable Multimodal Face Anti-Spoofing via Path-Augmented Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.17927</link>
<guid>https://arxiv.org/abs/2511.17927</guid>
<content:encoded><![CDATA[
arXiv:2511.17927v1 Announce Type: new 
Abstract: Face anti-spoofing (FAS) has recently advanced in multimodal fusion, cross-domain generalization, and interpretability. With large language models and reinforcement learning (RL), strategy-based training offers new opportunities to jointly model these aspects. However, multimodal reasoning is more complex than unimodal reasoning, requiring accurate feature representation and cross-modal verification while facing scarce, high-quality annotations, which makes direct application of RL sub-optimal. We identify two key limitations of supervised fine-tuning plus RL (SFT+RL) for multimodal FAS: (1) limited multimodal reasoning paths restrict the use of complementary modalities and shrink the exploration space after SFT, weakening the effect of RL; and (2) mismatched single-task supervision versus diverse reasoning paths causes reasoning confusion, where models may exploit shortcuts by mapping images directly to answers and ignoring the intended reasoning. To address this, we propose PA-FAS, which enhances reasoning paths by constructing high-quality extended reasoning sequences from limited annotations, enriching paths and relaxing exploration constraints. We further introduce an answer-shuffling mechanism during SFT to force comprehensive multimodal analysis instead of using superficial cues, thereby encouraging deeper reasoning and mitigating shortcut learning. PA-FAS significantly improves multimodal reasoning accuracy and cross-domain generalization, and better unifies multimodal fusion, generalization, and interpretability for trustworthy FAS.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaTAD: When State-Space Models Meet Long-Range Temporal Action Detection</title>
<link>https://arxiv.org/abs/2511.17929</link>
<guid>https://arxiv.org/abs/2511.17929</guid>
<content:encoded><![CDATA[
arXiv:2511.17929v1 Announce Type: new 
Abstract: Temporal Action Detection (TAD) aims to identify and localize actions by determining their starting and ending frames within untrimmed videos. Recent Structured State-Space Models such as Mamba have demonstrated potential in TAD due to their long-range modeling capability and linear computational complexity. On the other hand, structured state-space models often face two key challenges in TAD, namely, decay of temporal context due to recursive processing and self-element conflict during global visual context modeling, which become more severe while handling long-span action instances. Additionally, traditional methods for TAD struggle with detecting long-span action instances due to a lack of global awareness and inefficient detection heads. This paper presents MambaTAD, a new state-space TAD model that introduces long-range modeling and global feature detection capabilities for accurate temporal action detection. MambaTAD comprises two novel designs that complement each other with superior TAD performance. First, it introduces a Diagonal-Masked Bidirectional State-Space (DMBSS) module which effectively facilitates global feature fusion and temporal action detection. Second, it introduces a global feature fusion head that refines the detection progressively with multi-granularity features and global awareness. In addition, MambaTAD tackles TAD in an end-to-end one-stage manner using a new state-space temporal adapter(SSTA) which reduces network parameters and computation cost with linear complexity. Extensive experiments show that MambaTAD achieves superior TAD performance consistently across multiple public benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniRSCD: A Unified Novel Architectural Paradigm for Remote Sensing Change Detection</title>
<link>https://arxiv.org/abs/2511.17930</link>
<guid>https://arxiv.org/abs/2511.17930</guid>
<content:encoded><![CDATA[
arXiv:2511.17930v1 Announce Type: new 
Abstract: In recent years, remote sensing change detection has garnered significant attention due to its critical role in resource monitoring and disaster assessment. Change detection tasks exist with different output granularities such as BCD, SCD, and BDA. However, existing methods require substantial expert knowledge to design specialized decoders that compensate for information loss during encoding across different tasks. This not only introduces uncertainty into the process of selecting optimal models for abrupt change scenarios (such as disaster outbreaks) but also limits the universality of these architectures. To address these challenges, this paper proposes a unified, general change detection framework named UniRSCD. Building upon a state space model backbone, we introduce a frequency change prompt generator as a unified encoder. The encoder dynamically scans bitemporal global context information while integrating high-frequency details with low-frequency holistic information, thereby eliminating the need for specialized decoders for feature compensation. Subsequently, the unified decoder and prediction head establish a shared representation space through hierarchical feature interaction and task-adaptive output mapping. This integrating various tasks such as binary change detection and semantic change detection into a unified architecture, thereby accommodating the differing output granularity requirements of distinct change detection tasks. Experimental results demonstrate that the proposed architecture can adapt to multiple change detection tasks and achieves leading performance on five datasets, including the binary change dataset LEVIR-CD, the semantic change dataset SECOND, and the building damage assessment dataset xBD.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion</title>
<link>https://arxiv.org/abs/2511.17932</link>
<guid>https://arxiv.org/abs/2511.17932</guid>
<content:encoded><![CDATA[
arXiv:2511.17932v1 Announce Type: new 
Abstract: Given just a few glimpses of a scene, can you imagine the movie playing out as the camera glides through it? That's the lens we take on \emph{sparse-input novel view synthesis}, not only as filling spatial gaps between widely spaced views, but also as \emph{completing a natural video} unfolding through space.
  We recast the task as \emph{test-time natural video completion}, using powerful priors from \emph{pretrained video diffusion models} to hallucinate plausible in-between views. Our \emph{zero-shot, generation-guided} framework produces pseudo views at novel camera poses, modulated by an \emph{uncertainty-aware mechanism} for spatial coherence. These synthesized frames densify supervision for \emph{3D Gaussian Splatting} (3D-GS) for scene reconstruction, especially in under-observed regions. An iterative feedback loop lets 3D geometry and 2D view synthesis inform each other, improving both the scene reconstruction and the generated views.
  The result is coherent, high-fidelity renderings from sparse inputs \emph{without any scene-specific training or fine-tuning}. On LLFF, DTU, DL3DV, and MipNeRF-360, our method significantly outperforms strong 3D-GS baselines under extreme sparsity.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction</title>
<link>https://arxiv.org/abs/2511.17941</link>
<guid>https://arxiv.org/abs/2511.17941</guid>
<content:encoded><![CDATA[
arXiv:2511.17941v1 Announce Type: new 
Abstract: V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SciEducator: Scientific Video Understanding and Educating via Deming-Cycle Multi-Agent System</title>
<link>https://arxiv.org/abs/2511.17943</link>
<guid>https://arxiv.org/abs/2511.17943</guid>
<content:encoded><![CDATA[
arXiv:2511.17943v1 Announce Type: new 
Abstract: Recent advancements in multimodal large language models (MLLMs) and video agent systems have significantly improved general video understanding. However, when applied to scientific video understanding and educating, a domain that demands external professional knowledge integration and rigorous step-wise reasoning, existing approaches often struggle. To bridge this gap, we propose SciEducator, the first iterative self-evolving multi-agent system for scientific video comprehension and education. Rooted in the classical Deming Cycle from management science, our design reformulates its Plan-Do-Study-Act philosophy into a self-evolving reasoning and feedback mechanism, which facilitates the interpretation of intricate scientific activities in videos. Moreover, SciEducator can produce multimodal educational content tailored to specific scientific processes, including textual instructions, visual guides, audio narrations, and interactive references. To support evaluation, we construct SciVBench, a benchmark consisting of 500 expert-verified and literature-grounded science QA pairs across five categories, covering physical, chemical, and everyday phenomena. Extensive experiments demonstrate that SciEducator substantially outperforms leading closed-source MLLMs (e.g., Gemini, GPT-4o) and state-of-the-art video agents on the benchmark, establishing a new paradigm for the community.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Temporal Sampling for Efficient MLLM Video Understanding</title>
<link>https://arxiv.org/abs/2511.17945</link>
<guid>https://arxiv.org/abs/2511.17945</guid>
<content:encoded><![CDATA[
arXiv:2511.17945v1 Announce Type: new 
Abstract: Processing long videos with multimodal large language models (MLLMs) poses a significant computational challenge, as the model's self-attention mechanism scales quadratically with the number of video tokens, resulting in high computational demand and slow inference speed. Current solutions, such as rule-based sub-sampling, learned frame selector, or memory-based summarization, often introduce their own trade-offs: they compromise accuracy, necessitate additional training, or decrease inference speed. In this paper, we propose Test-Time Temporal Sampling (T3S), a training-free, plug-and-play inference wrapper that enables MLLMs to process long videos both efficiently and effectively. T3S exploits spatiotemporal redundancy by generating multiple short and diverse subsequences of video tokens at inference time, packing them within a single forward pass, and aggregating their predictions. This multi-subsequence formulation broadens visual coverage while reducing the computational cost of self-attention from $O(L^2)$ to $O(\sum_{i=1}^m \alpha_i^2L^2)$, where $\sum_{i=1}^m \alpha_i^2 < 1$. Extensive experiments on long video understanding benchmarks demonstrate that T3S improves accuracy by up to 3.1% and reduces first token delay by $2.04\times$, all with minimal integration effort. Our approach operates entirely at inference time, requires no model modifications or fine-tuning, and is compatible with a wide range of pretrained MLLMs. T3S turns video redundancy into a computational advantage, offering a scalable solution for long-video understanding. The code is available at https://github.com/kaibinwang3/T3S.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-speaker Attention Alignment for Multimodal Social Interaction</title>
<link>https://arxiv.org/abs/2511.17952</link>
<guid>https://arxiv.org/abs/2511.17952</guid>
<content:encoded><![CDATA[
arXiv:2511.17952v1 Announce Type: new 
Abstract: Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.17958</link>
<guid>https://arxiv.org/abs/2511.17958</guid>
<content:encoded><![CDATA[
arXiv:2511.17958v1 Announce Type: new 
Abstract: Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VITAL: Vision-Encoder-centered Pre-training for LMMs in Visual Quality Assessment</title>
<link>https://arxiv.org/abs/2511.17962</link>
<guid>https://arxiv.org/abs/2511.17962</guid>
<content:encoded><![CDATA[
arXiv:2511.17962v1 Announce Type: new 
Abstract: Developing a robust visual quality assessment (VQualA) large multi-modal model (LMM) requires achieving versatility, powerfulness, and transferability.
  However, existing VQualA LMMs typically focus on a single task and rely on full-parameter fine-tuning, which makes them prone to overfitting on specific modalities or task types, thereby limiting their generalization capacity and transferability. To address this, we propose a vision-encoder-centered generative pre-training pipeline and develop the VITAL-Series LMMs. (1) We adopt a machine-executed annotation-scrutiny paradigm, constructing over 4.5M vision-language (VL) pairs-the largest VQualA training dataset to date. (2) We employ a multi-task training workflow that simultaneously enhances the model's quantitative scoring precision and strengthens its capability for quality interpretation across both image and video modalities. (3) Building upon the vision encoder, we realize an efficient model zoo extension: the model zoo exhibits strong zero-shot performance, and each paired decoder requires only a swift warm-up using less than 1/1000 of the pre-training data to achieve performance comparable to the fully trained counterpart. Overall, our work lays a cornerstone for advancing toward the foundation LMM for VQualA.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-ReID: Multi-granularity Information Interaction for Video-Based Visible-Infrared Person Re-Identification</title>
<link>https://arxiv.org/abs/2511.17964</link>
<guid>https://arxiv.org/abs/2511.17964</guid>
<content:encoded><![CDATA[
arXiv:2511.17964v1 Announce Type: new 
Abstract: Large-scale vision-language models (e.g., CLIP) have recently achieved remarkable performance in retrieval tasks, yet their potential for Video-based Visible-Infrared Person Re-Identification (VVI-ReID) remains largely unexplored. The primary challenges are narrowing the modality gap and leveraging spatiotemporal information in video sequences. To address the above issues, in this paper, we propose a novel cross-modality feature learning framework named X-ReID for VVI-ReID. Specifically, we first propose a Cross-modality Prototype Collaboration (CPC) to align and integrate features from different modalities, guiding the network to reduce the modality discrepancy. Then, a Multi-granularity Information Interaction (MII) is designed, incorporating short-term interactions from adjacent frames, long-term cross-frame information fusion, and cross-modality feature alignment to enhance temporal modeling and further reduce modality gaps. Finally, by integrating multi-granularity information, a robust sequence-level representation is achieved. Extensive experiments on two large-scale VVI-ReID benchmarks (i.e., HITSZ-VCM and BUPTCampus) demonstrate the superiority of our method over state-of-the-art methods. The source code is released at https://github.com/AsuradaYuci/X-ReID.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Signal: Selective Interaction and Global-local Alignment for Multi-Modal Object Re-Identification</title>
<link>https://arxiv.org/abs/2511.17965</link>
<guid>https://arxiv.org/abs/2511.17965</guid>
<content:encoded><![CDATA[
arXiv:2511.17965v1 Announce Type: new 
Abstract: Multi-modal object Re-IDentification (ReID) is devoted to retrieving specific objects through the exploitation of complementary multi-modal image information. Existing methods mainly concentrate on the fusion of multi-modal features, yet neglecting the background interference. Besides, current multi-modal fusion methods often focus on aligning modality pairs but suffer from multi-modal consistency alignment. To address these issues, we propose a novel selective interaction and global-local alignment framework called Signal for multi-modal object ReID. Specifically, we first propose a Selective Interaction Module (SIM) to select important patch tokens with intra-modal and inter-modal information. These important patch tokens engage in the interaction with class tokens, thereby yielding more discriminative features. Then, we propose a Global Alignment Module (GAM) to simultaneously align multi-modal features by minimizing the volume of 3D polyhedra in the gramian space. Meanwhile, we propose a Local Alignment Module (LAM) to align local features in a shift-aware manner. With these modules, our proposed framework could extract more discriminative features for object ReID. Extensive experiments on three multi-modal object ReID benchmarks (i.e., RGBNT201, RGBNT100, MSVR310) validate the effectiveness of our method. The source code is available at https://github.com/010129/Signal.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADTrack: Learning Contextual Aggregation with Deformable Alignment for Robust RGBT Tracking</title>
<link>https://arxiv.org/abs/2511.17967</link>
<guid>https://arxiv.org/abs/2511.17967</guid>
<content:encoded><![CDATA[
arXiv:2511.17967v1 Announce Type: new 
Abstract: RGB-Thermal (RGBT) tracking aims to exploit visible and thermal infrared modalities for robust all-weather object tracking. However, existing RGBT trackers struggle to resolve modality discrepancies, which poses great challenges for robust feature representation. This limitation hinders effective cross-modal information propagation and fusion, which significantly reduces the tracking accuracy. To address this limitation, we propose a novel Contextual Aggregation with Deformable Alignment framework called CADTrack for RGBT Tracking. To be specific, we first deploy the Mamba-based Feature Interaction (MFI) that establishes efficient feature interaction via state space models. This interaction module can operate with linear complexity, reducing computational cost and improving feature discrimination. Then, we propose the Contextual Aggregation Module (CAM) that dynamically activates backbone layers through sparse gating based on the Mixture-of-Experts (MoE). This module can encode complementary contextual information from cross-layer features. Finally, we propose the Deformable Alignment Module (DAM) to integrate deformable sampling and temporal propagation, mitigating spatial misalignment and localization drift. With the above components, our CADTrack achieves robust and accurate tracking in complex scenarios. Extensive experiments on five RGBT tracking benchmarks verify the effectiveness of our proposed method. The source code is released at https://github.com/IdolLab/CADTrack.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Pseudo-replay for Exemplar-free Class-incremental Learning</title>
<link>https://arxiv.org/abs/2511.17973</link>
<guid>https://arxiv.org/abs/2511.17973</guid>
<content:encoded><![CDATA[
arXiv:2511.17973v1 Announce Type: new 
Abstract: Exemplar-free class-incremental learning (EFCIL) aims to retain old knowledge acquired in the previous task while learning new classes, without storing the previous images due to storage constraints or privacy concerns. In EFCIL, the plasticity-stability dilemma, learning new tasks versus catastrophic forgetting, is a significant challenge, primarily due to the unavailability of images from earlier tasks. In this paper, we introduce adversarial pseudo-replay (APR), a method that perturbs the images of the new task with adversarial attack, to synthesize the pseudo-replay images online without storing any replay samples. During the new task training, the adversarial attack is conducted on the new task images with augmented old class mean prototypes as targets, and the resulting images are used for knowledge distillation to prevent semantic drift. Moreover, we calibrate the covariance matrices to compensate for the semantic drift after each task, by learning a transfer matrix on the pseudo-replay samples. Our method reconciles stability and plasticity, achieving state-of-the-art on challenging cold-start settings of the standard EFCIL benchmarks.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FeRA: Frequency-Energy Constrained Routing for Effective Diffusion Adaptation Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17979</link>
<guid>https://arxiv.org/abs/2511.17979</guid>
<content:encoded><![CDATA[
arXiv:2511.17979v1 Announce Type: new 
Abstract: Diffusion models have achieved remarkable success in generative modeling, yet how to effectively adapt large pretrained models to new tasks remains challenging. We revisit the reconstruction behavior of diffusion models during denoising to unveil the underlying frequency energy mechanism governing this process. Building upon this observation, we propose FeRA, a frequency driven fine tuning framework that aligns parameter updates with the intrinsic frequency energy progression of diffusion. FeRA establishes a comprehensive frequency energy framework for effective diffusion adaptation fine tuning, comprising three synergistic components: (i) a compact frequency energy indicator that characterizes the latent bandwise energy distribution, (ii) a soft frequency router that adaptively fuses multiple frequency specific adapter experts, and (iii) a frequency energy consistency regularization that stabilizes diffusion optimization and ensures coherent adaptation across bands. Routing operates in both training and inference, with inference time routing dynamically determined by the latent frequency energy. It integrates seamlessly with adapter based tuning schemes and generalizes well across diffusion backbones and resolutions. By aligning adaptation with the frequency energy mechanism, FeRA provides a simple, stable, and compatible paradigm for effective and robust diffusion model adaptation.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plan-X: Instruct Video Generation via Semantic Planning</title>
<link>https://arxiv.org/abs/2511.17986</link>
<guid>https://arxiv.org/abs/2511.17986</guid>
<content:encoded><![CDATA[
arXiv:2511.17986v1 Announce Type: new 
Abstract: Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured "semantic sketches" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.17988</link>
<guid>https://arxiv.org/abs/2511.17988</guid>
<content:encoded><![CDATA[
arXiv:2511.17988v1 Announce Type: new 
Abstract: Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SD-PSFNet: Sequential and Dynamic Point Spread Function Network for Image Deraining</title>
<link>https://arxiv.org/abs/2511.17993</link>
<guid>https://arxiv.org/abs/2511.17993</guid>
<content:encoded><![CDATA[
arXiv:2511.17993v1 Announce Type: new 
Abstract: Image deraining is crucial for vision applications but is challenged by the complex multi-scale physics of rain and its coupling with scenes. To address this challenge, a novel approach inspired by multi-stage image restoration is proposed, incorporating Point Spread Function (PSF) mechanisms to reveal the image degradation process while combining dynamic physical modeling with sequential feature fusion transfer, named SD-PSFNet. Specifically, SD-PSFNet employs a sequential restoration architecture with three cascaded stages, allowing multiple dynamic evaluations and refinements of the degradation process estimation. The network utilizes components with learned PSF mechanisms to dynamically simulate rain streak optics, enabling effective rain-background separation while progressively enhancing outputs through novel PSF components at each stage. Additionally, SD-PSFNet incorporates adaptive gated fusion for optimal cross-stage feature integration, enabling sequential refinement from coarse rain removal to fine detail restoration. Our model achieves state-of-the-art PSNR/SSIM metrics on Rain100H (33.12dB/0.9371), RealRain-1k-L (42.28dB/0.9872), and RealRain-1k-H (41.08dB/0.9838). In summary, SD-PSFNet demonstrates excellent capability in complex scenes and dense rainfall conditions, providing a new physics-aware approach to image deraining.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAISECity: A Multimodal Agent Framework for Reality-Aligned 3D World Generation at City-Scale</title>
<link>https://arxiv.org/abs/2511.18005</link>
<guid>https://arxiv.org/abs/2511.18005</guid>
<content:encoded><![CDATA[
arXiv:2511.18005v1 Announce Type: new 
Abstract: City-scale 3D generation is of great importance for the development of embodied intelligence and world models. Existing methods, however, face significant challenges regarding quality, fidelity, and scalability in 3D world generation. Thus, we propose RAISECity, a \textbf{R}eality-\textbf{A}ligned \textbf{I}ntelligent \textbf{S}ynthesis \textbf{E}ngine that creates detailed, \textbf{C}ity-scale 3D worlds. We introduce an agentic framework that leverages diverse multimodal foundation tools to acquire real-world knowledge, maintain robust intermediate representations, and construct complex 3D scenes. This agentic design, featuring dynamic data processing, iterative self-reflection and refinement, and the invocation of advanced multimodal tools, minimizes cumulative errors and enhances overall performance. Extensive quantitative experiments and qualitative analyses validate the superior performance of RAISECity in real-world alignment, shape precision, texture fidelity, and aesthetics level, achieving over a 90% win-rate against existing baselines for overall perceptual quality. This combination of 3D quality, reality alignment, scalability, and seamless compatibility with computer graphics pipelines makes RAISECity a promising foundation for applications in immersive media, embodied intelligence, and world models.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is Complete Labeling Necessary? Understanding Active Learning in Longitudinal Medical Imaging</title>
<link>https://arxiv.org/abs/2511.18007</link>
<guid>https://arxiv.org/abs/2511.18007</guid>
<content:encoded><![CDATA[
arXiv:2511.18007v1 Announce Type: new 
Abstract: Detecting changes in longitudinal medical imaging using deep learning requires a substantial amount of accurately labeled data. However, labeling these images is notably more costly and time-consuming than labeling other image types, as it requires labeling across various time points, where new lesions can be minor, and subtle changes are easily missed. Deep Active Learning (DAL) has shown promise in minimizing labeling costs by selectively querying the most informative samples, but existing studies have primarily focused on static tasks like classification and segmentation. Consequently, the conventional DAL approach cannot be directly applied to change detection tasks, which involve identifying subtle differences across multiple images. In this study, we propose a novel DAL framework, named Longitudinal Medical Imaging Active Learning (LMI-AL), tailored specifically for longitudinal medical imaging. By pairing and differencing all 2D slices from baseline and follow-up 3D images, LMI-AL iteratively selects the most informative pairs for labeling using DAL, training a deep learning model with minimal manual annotation. Experimental results demonstrate that, with less than 8% of the data labeled, LMI-AL can achieve performance comparable to models trained on fully labeled datasets. We also provide a detailed analysis of the method's performance, as guidance for future research. The code is publicly available at https://github.com/HelenMa9998/Longitudinal_AL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoadBench: Benchmarking MLLMs on Fine-Grained Spatial Understanding and Reasoning under Urban Road Scenarios</title>
<link>https://arxiv.org/abs/2511.18011</link>
<guid>https://arxiv.org/abs/2511.18011</guid>
<content:encoded><![CDATA[
arXiv:2511.18011v1 Announce Type: new 
Abstract: Multimodal large language models (MLLMs) have demonstrated powerful capabilities in general spatial understanding and reasoning. However, their fine-grained spatial understanding and reasoning capabilities in complex urban scenarios have not received significant attention in the fields of both research and industry. To fill this gap, we focus primarily on road markings as a typical example of fine-grained spatial elements under urban scenarios, given the essential role of the integrated road traffic network they form within cities. Around road markings and urban traffic systems, we propose RoadBench, a systematic benchmark that comprehensively evaluates MLLMs' fine-grained spatial understanding and reasoning capabilities using BEV and FPV image inputs. This benchmark comprises six tasks consisting of 9,121 strictly manually verified test cases. These tasks form a systematic evaluation framework that bridges understanding at local spatial scopes to global reasoning. They not only test MLLMs' capabilities in recognition, joint understanding, and reasoning but also assess their ability to integrate image information with domain knowledge. After evaluating 14 mainstream MLLMs, we confirm that RoadBench is a challenging benchmark for MLLMs while revealing significant shortcomings in existing MLLMs' fine-grained spatial understanding and reasoning capabilities within urban scenarios. In certain tasks, their performance even falls short of simple rule-based or random selection baselines. These findings, along with RoadBench itself, will contribute to the comprehensive advancement of spatial understanding capabilities for MLLMs. The benchmark code, example datasets, and raw evaluation results are available in the supplementary material.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State and Scene Enhanced Prototypes for Weakly Supervised Open-Vocabulary Object Detection</title>
<link>https://arxiv.org/abs/2511.18012</link>
<guid>https://arxiv.org/abs/2511.18012</guid>
<content:encoded><![CDATA[
arXiv:2511.18012v1 Announce Type: new 
Abstract: Open-Vocabulary Object Detection (OVOD) aims to generalize object recognition to novel categories, while Weakly Supervised OVOD (WS-OVOD) extends this by combining box-level annotations with image-level labels. Despite recent progress, two critical challenges persist in this setting. First, existing semantic prototypes, even when enriched by LLMs, are static and limited, failing to capture the rich intra-class visual variations induced by different object states (e.g., a cat's pose). Second, the standard pseudo-box generation introduces a semantic mismatch between visual region proposals (which contain context) and object-centric text embeddings. To tackle these issues, we introduce two complementary prototype enhancement strategies. To capture intra-class variations in appearance and state, we propose the State-Enhanced Semantic Prototypes (SESP), which generates state-aware textual descriptions (e.g., "a sleeping cat") to capture diverse object appearances, yielding more discriminative prototypes. Building on this, we further introduce Scene-Augmented Pseudo Prototypes (SAPP) to address the semantic mismatch. SAPP incorporates contextual semantics (e.g., "cat lying on sofa") and utilizes a soft alignment mechanism to promote contextually consistent visual-textual representations. By integrating SESP and SAPP, our method effectively enhances both the richness of semantic prototypes and the visual-textual alignment, achieving notable improvements.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Retinal Ganglion Cells with Neural Differential Equations</title>
<link>https://arxiv.org/abs/2511.18014</link>
<guid>https://arxiv.org/abs/2511.18014</guid>
<content:encoded><![CDATA[
arXiv:2511.18014v1 Announce Type: new 
Abstract: This work explores Liquid Time-Constant Networks (LTCs) and Closed-form Continuous-time Networks (CfCs) for modeling retinal ganglion cell activity in tiger salamanders across three datasets. Compared to a convolutional baseline and an LSTM, both architectures achieved lower MAE, faster convergence, smaller model sizes, and favorable query times, though with slightly lower Pearson correlation. Their efficiency and adaptability make them well suited for scenarios with limited data and frequent retraining, such as edge deployments in vision prosthetics.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MambaX: Image Super-Resolution with State Predictive Control</title>
<link>https://arxiv.org/abs/2511.18028</link>
<guid>https://arxiv.org/abs/2511.18028</guid>
<content:encoded><![CDATA[
arXiv:2511.18028v1 Announce Type: new 
Abstract: Image super-resolution (SR) is a critical technology for overcoming the inherent hardware limitations of sensors. However, existing approaches mainly focus on directly enhancing the final resolution, often neglecting effective control over error propagation and accumulation during intermediate stages. Recently, Mamba has emerged as a promising approach that can represent the entire reconstruction process as a state sequence with multiple nodes, allowing for intermediate intervention. Nonetheless, its fixed linear mapper is limited by a narrow receptive field and restricted flexibility, which hampers its effectiveness in fine-grained images. To address this, we created a nonlinear state predictive control model \textbf{MambaX} that maps consecutive spectral bands into a latent state space and generalizes the SR task by dynamically learning the nonlinear state parameters of control equations. Compared to existing sequence models, MambaX 1) employs dynamic state predictive control learning to approximate the nonlinear differential coefficients of state-space models; 2) introduces a novel state cross-control paradigm for multimodal SR fusion; and 3) utilizes progressive transitional learning to mitigate heterogeneity caused by domain and modality shifts. Our evaluation demonstrates the superior performance of the dynamic spectrum-state representation model in both single-image SR and multimodal fusion-based SR tasks, highlighting its substantial potential to advance spectrally generalized modeling across arbitrary dimensions and modalities.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Event Frame Sensors: Modeling, Calibration, and Simulation</title>
<link>https://arxiv.org/abs/2511.18037</link>
<guid>https://arxiv.org/abs/2511.18037</guid>
<content:encoded><![CDATA[
arXiv:2511.18037v1 Announce Type: new 
Abstract: Event frame hybrid sensors integrate an Active Pixel Sensor (APS) and an Event Vision Sensor (EVS) within a single chip, combining the high dynamic range and low latency of the EVS with the rich spatial intensity information from the APS. While this tight integration offers compact, temporally precise imaging, the complex circuit architecture introduces non-trivial noise patterns that remain poorly understood and unmodeled. In this work, we present the first unified, statistics-based imaging noise model that jointly describes the noise behavior of APS and EVS pixels. Our formulation explicitly incorporates photon shot noise, dark current noise, fixed-pattern noise, and quantization noise, and links EVS noise to illumination level and dark current. Based on this formulation, we further develop a calibration pipeline to estimate noise parameters from real data and offer a detailed analysis of both APS and EVS noise behaviors. Finally, we propose HESIM, a statistically grounded simulator that generates RAW frames and events under realistic, jointly calibrated noise statistics. Experiments on two hybrid sensors validate our model across multiple imaging tasks (e.g., video frame interpolation and deblurring), demonstrating strong transfer from simulation to real data.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UltraFlux: Data-Model Co-Design for High-quality Native 4K Text-to-Image Generation across Diverse Aspect Ratios</title>
<link>https://arxiv.org/abs/2511.18050</link>
<guid>https://arxiv.org/abs/2511.18050</guid>
<content:encoded><![CDATA[
arXiv:2511.18050v1 Announce Type: new 
Abstract: Diffusion transformers have recently delivered strong text-to-image generation around 1K resolution, but we show that extending them to native 4K across diverse aspect ratios exposes a tightly coupled failure mode spanning positional encoding, VAE compression, and optimization. Tackling any of these factors in isolation leaves substantial quality on the table. We therefore take a data-model co-design view and introduce UltraFlux, a Flux-based DiT trained natively at 4K on MultiAspect-4K-1M, a 1M-image 4K corpus with controlled multi-AR coverage, bilingual captions, and rich VLM/IQA metadata for resolution- and AR-aware sampling. On the model side, UltraFlux couples (i) Resonance 2D RoPE with YaRN for training-window-, frequency-, and AR-aware positional encoding at 4K; (ii) a simple, non-adversarial VAE post-training scheme that improves 4K reconstruction fidelity; (iii) an SNR-Aware Huber Wavelet objective that rebalances gradients across timesteps and frequency bands; and (iv) a Stage-wise Aesthetic Curriculum Learning strategy that concentrates high-aesthetic supervision on high-noise steps governed by the model prior. Together, these components yield a stable, detail-preserving 4K DiT that generalizes across wide, square, and tall ARs. On the Aesthetic-Eval at 4096 benchmark and multi-AR 4K settings, UltraFlux consistently outperforms strong open-source baselines across fidelity, aesthetic, and alignment metrics, and-with a LLM prompt refiner-matches or surpasses the proprietary Seedream 4.0.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IE-Critic-R1: Advancing the Explanatory Measurement of Text-Driven Image Editing for Human Perception Alignment</title>
<link>https://arxiv.org/abs/2511.18055</link>
<guid>https://arxiv.org/abs/2511.18055</guid>
<content:encoded><![CDATA[
arXiv:2511.18055v1 Announce Type: new 
Abstract: Recent advances in text-driven image editing have been significant, yet the task of accurately evaluating these edited images continues to pose a considerable challenge. Different from the assessment of text-driven image generation, text-driven image editing is characterized by simultaneously conditioning on both text and a source image. The edited images often retain an intrinsic connection to the original image, which dynamically change with the semantics of the text. However, previous methods tend to solely focus on text-image alignment or have not well aligned with human perception. In this work, we introduce the Text-driven Image Editing Benchmark suite (IE-Bench) to enhance the assessment of text-driven edited images. IE-Bench includes a database contains diverse source images, various editing prompts and the corresponding edited results from different editing methods, and nearly 4,000 samples with corresponding Mean Opinion Scores (MOS) provided by 15 human subjects. Furthermore, we introduce IE-Critic-R1, which, benefiting from Reinforcement Learning from Verifiable Rewards (RLVR), provides more comprehensive and explainable quality assessment for text-driven image editing that aligns with human perception. Extensive experiments demonstrate IE-Critic-R1's superior subjective-alignments on the text-driven image editing task compared with previous metrics. Related data and codes are available to the public.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Semi-Supervised Active Learning for Remote Sensing</title>
<link>https://arxiv.org/abs/2511.18058</link>
<guid>https://arxiv.org/abs/2511.18058</guid>
<content:encoded><![CDATA[
arXiv:2511.18058v1 Announce Type: new 
Abstract: The performance of deep learning models in remote sensing (RS) strongly depends on the availability of high-quality labeled data. However, collecting large-scale annotations is costly and time-consuming, while vast amounts of unlabeled imagery remain underutilized. To address this challenge, we propose a Hierarchical Semi-Supervised Active Learning (HSSAL) framework that integrates semi-supervised learning (SSL) and a novel hierarchical active learning (HAL) in a closed iterative loop. In each iteration, SSL refines the model using both labeled data through supervised learning and unlabeled data via weak-to-strong self-training, improving feature representation and uncertainty estimation. Guided by the refined representations and uncertainty cues of unlabeled samples, HAL then conducts sample querying through a progressive clustering strategy, selecting the most informative instances that jointly satisfy the criteria of scalability, diversity, and uncertainty. This hierarchical process ensures both efficiency and representativeness in sample selection. Extensive experiments on three benchmark RS scene classification datasets, including UCM, AID, and NWPU-RESISC45, demonstrate that HSSAL consistently outperforms SSL- or AL-only baselines. Remarkably, with only 8%, 4%, and 2% labeled training data on UCM, AID, and NWPU-RESISC45, respectively, HSSAL achieves over 95% of fully-supervised accuracy, highlighting its superior label efficiency through informativeness exploitation of unlabeled data. Our code will be released at https://github.com/zhu-xlab/RS-SSAL.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight, Interpretable Deep Learning System for Automated Detection of Cervical Adenocarcinoma In Situ (AIS)</title>
<link>https://arxiv.org/abs/2511.18063</link>
<guid>https://arxiv.org/abs/2511.18063</guid>
<content:encoded><![CDATA[
arXiv:2511.18063v1 Announce Type: new 
Abstract: Cervical adenocarcinoma in situ (AIS) is a critical premalignant lesion whose accurate histopathological diagnosis is challenging. Early detection is essential to prevent progression to invasive cervical adenocarcinoma. In this study, we developed a deep learning-based virtual pathology assistant capable of distinguishing AIS from normal cervical gland histology using the CAISHI dataset, which contains 2240 expert-labeled H&amp;E images (1010 normal and 1230 AIS). All images underwent Macenko stain normalization and patch-based preprocessing to enhance morphological feature representation. An EfficientNet-B3 convolutional neural network was trained using class-balanced sampling and focal loss to address dataset imbalance and emphasize difficult examples. The final model achieved an overall accuracy of 0.7323, with an F1-score of 0.75 for the Abnormal class and 0.71 for the Normal class. Grad-CAM heatmaps demonstrated biologically interpretable activation patterns, highlighting nuclear atypia and glandular crowding consistent with AIS morphology. The trained model was deployed in a Gradio-based virtual diagnostic assistant. These findings demonstrate the feasibility of lightweight, interpretable AI systems for cervical gland pathology, with potential applications in screening workflows, education, and low-resource settings.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VK-Det: Visual Knowledge Guided Prototype Learning for Open-Vocabulary Aerial Object Detection</title>
<link>https://arxiv.org/abs/2511.18075</link>
<guid>https://arxiv.org/abs/2511.18075</guid>
<content:encoded><![CDATA[
arXiv:2511.18075v1 Announce Type: new 
Abstract: To identify objects beyond predefined categories, open-vocabulary aerial object detection (OVAD) leverages the zero-shot capabilities of visual-language models (VLMs) to generalize from base to novel categories. Existing approaches typically utilize self-learning mechanisms with weak text supervision to generate region-level pseudo-labels to align detectors with VLMs semantic spaces. However, text dependence induces semantic bias, restricting open-vocabulary expansion to text-specified concepts. We propose $\textbf{VK-Det}$, a $\textbf{V}$isual $\textbf{K}$nowledge-guided open-vocabulary object $\textbf{Det}$ection framework $\textit{without}$ extra supervision. First, we discover and leverage vision encoder's inherent informative region perception to attain fine-grained localization and adaptive distillation. Second, we introduce a novel prototype-aware pseudo-labeling strategy. It models inter-class decision boundaries through feature clustering and maps detection regions to latent categories via prototype matching. This enhances attention to novel objects while compensating for missing supervision. Extensive experiments show state-of-the-art performance, achieving 30.1 $\mathrm{mAP}^{N}$ on DIOR and 23.3 $\mathrm{mAP}^{N}$ on DOTA, outperforming even extra supervised methods.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2511.18082</link>
<guid>https://arxiv.org/abs/2511.18082</guid>
<content:encoded><![CDATA[
arXiv:2511.18082v1 Announce Type: new 
Abstract: Recent Vision-Language-Action (VLA) models have shown impressive flexibility and generalization, yet their deployment in robotic manipulation remains limited by heavy computational overhead and inference latency. In this work, we present ActDistill, a general action-guided self-derived distillation framework that transfers the action prediction capability of any existing VLA model to a lightweight counterpart. Unlike previous efficiency strategies that primarily emphasize vision-language correlations, ActDistill leverages action priors to guide knowledge transfer and model compression, achieving action-oriented efficiency for VLA models. Specifically, we employ a well-trained VLA model as the teacher and introduce a graph-structured encapsulation strategy to explicitly model the hierarchical evolution of action prediction. The student model, derived from the graph-encapsulated teacher, is further equipped with a dynamic router that adaptively selects computation paths based on action prediction demands, guided by hierarchical graph-informed supervision to ensure smooth and efficient evolution. During inference, graph-related auxiliary components are removed, allowing the student to execute only dynamically routed layers and predict high-precision actions with minimal computation and latency. Experiments on embodied benchmarks demonstrate that ActDistill achieves comparable or superior performance to full-scale VLA models while reducing computation by over 50% with up to 1.67 times speedup, thereby establishing a general paradigm toward efficient embodied intelligence.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More: An Explainable AI Framework for Lightweight Malaria Classification</title>
<link>https://arxiv.org/abs/2511.18083</link>
<guid>https://arxiv.org/abs/2511.18083</guid>
<content:encoded><![CDATA[
arXiv:2511.18083v1 Announce Type: new 
Abstract: Background and Objective: Deep learning models have high computational needs and lack interpretability but are often the first choice for medical image classification tasks. This study addresses whether complex neural networks are essential for the simple binary classification task of malaria. We introduce the Extracted Morphological Feature Engineered (EMFE) pipeline, a transparent, reproducible, and low compute machine learning approach tailored explicitly for simple cell morphology, designed to achieve deep learning performance levels on a simple CPU only setup with the practical aim of real world deployment.
  Methods: The study used the NIH Malaria Cell Images dataset, with two features extracted from each cell image: the number of non background pixels and the number of holes within the cell. Logistic Regression and Random Forest were compared against ResNet18, DenseNet121, MobileNetV2, and EfficientNet across accuracy, model size, and CPU inference time. An ensemble model was created by combining Logistic Regression and Random Forests to achieve higher accuracy while retaining efficiency.
  Results: The single variable Logistic Regression model achieved a test accuracy of 94.80 percent with a file size of 1.2 kB and negligible inference latency (2.3 ms). The two stage ensemble improved accuracy to 97.15 percent. In contrast, the deep learning methods require 13.6 MB to 44.7 MB of storage and show significantly higher inference times (68 ms).
  Conclusion: This study shows that a compact feature engineering approach can produce clinically meaningful classification performance while offering gains in transparency, reproducibility, speed, and deployment feasibility. The proposed pipeline demonstrates that simple interpretable features paired with lightweight models can serve as a practical diagnostic solution for environments with limited computational resources.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Together, Then Apart: Revisiting Multimodal Survival Analysis via a Min-Max Perspective</title>
<link>https://arxiv.org/abs/2511.18089</link>
<guid>https://arxiv.org/abs/2511.18089</guid>
<content:encoded><![CDATA[
arXiv:2511.18089v1 Announce Type: new 
Abstract: Integrating heterogeneous modalities such as histopathology and genomics is central to advancing survival analysis, yet most existing methods prioritize cross-modal alignment through attention-based fusion mechanisms, often at the expense of modality-specific characteristics. This overemphasis on alignment leads to representation collapse and reduced diversity. In this work, we revisit multi-modal survival analysis via the dual lens of alignment and distinctiveness, positing that preserving modality-specific structure is as vital as achieving semantic coherence. In this paper, we introduce Together-Then-Apart (TTA), a unified min-max optimization framework that simultaneously models shared and modality-specific representations. The Together stage minimizes semantic discrepancies by aligning embeddings via shared prototypes, guided by an unbalanced optimal transport objective that adaptively highlights informative tokens. The Apart stage maximizes representational diversity through modality anchors and a contrastive regularizer that preserve unique modality information and prevent feature collapse. Extensive experiments on five TCGA benchmarks show that TTA consistently outperforms state-of-the-art methods. Beyond empirical gains, our formulation provides a new theoretical perspective of how alignment and distinctiveness can be jointly achieved in for robust, interpretable, and biologically meaningful multi-modal survival analysis.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Versatile Recompression-Aware Perceptual Image Super-Resolution</title>
<link>https://arxiv.org/abs/2511.18090</link>
<guid>https://arxiv.org/abs/2511.18090</guid>
<content:encoded><![CDATA[
arXiv:2511.18090v1 Announce Type: new 
Abstract: Perceptual image super-resolution (SR) methods restore degraded images and produce sharp outputs. In practice, those outputs are usually recompressed for storage and transmission. Ignoring recompression is suboptimal as the downstream codec might add additional artifacts to restored images. However, jointly optimizing SR and recompression is challenging, as the codecs are not differentiable and vary in configuration. In this paper, we present Versatile Recompression-Aware Perceptual Super-Resolution (VRPSR), which makes existing perceptual SR aware of versatile compression. First, we formulate compression as conditional text-to-image generation and utilize a pre-trained diffusion model to build a generalizable codec simulator. Next, we propose a set of training techniques tailored for perceptual SR, including optimizing the simulator using perceptual targets and adopting slightly compressed images as the training target. Empirically, our VRPSR saves more than 10\% bitrate based on Real-ESRGAN and S3Diff under H.264/H.265/H.266 compression. Besides, our VRPSR facilitates joint optimization of the SR and post-processing model after recompression.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spotlight: Identifying and Localizing Video Generation Errors Using VLMs</title>
<link>https://arxiv.org/abs/2511.18102</link>
<guid>https://arxiv.org/abs/2511.18102</guid>
<content:encoded><![CDATA[
arXiv:2511.18102v1 Announce Type: new 
Abstract: Current text-to-video models (T2V) can generate high-quality, temporally coherent, and visually realistic videos. Nonetheless, errors still often occur, and are more nuanced and local compared to the previous generation of T2V models. While current evaluation paradigms assess video models across diverse dimensions, they typically evaluate videos holistically without identifying when specific errors occur or describing their nature. We address this gap by introducing Spotlight, a novel task aimed at localizing and explaining video-generation errors. We generate 600 videos using 200 diverse textual prompts and three state-of-the-art video generators (Veo 3, Seedance, and LTX-2), and annotate over 1600 fine-grained errors across six types, including motion, physics, and prompt adherence. We observe that adherence and physics errors are predominant and persist across longer segments, whereas appearance-disappearance and body pose errors manifest in shorter segments. We then evaluate current VLMs on Spotlight and find that VLMs lag significantly behind humans in error identification and localization in videos. We propose inference-time strategies to probe the limits of current VLMs on our task, improving performance by nearly 2x. Our task paves a way forward to building fine-grained evaluation tools and more sophisticated reward models for video generators.
]]></content:encoded>
<pubDate>Tue, 25 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>