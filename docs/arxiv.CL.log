------------------------------------------------------
Started: 2025-04-17 11:13:00.652375
Existing_entries: 0
Fetching from https://arxiv.org/rss/cs.CL
Feed error: 302
append_entries: 0
Finish: 2025-04-17 11:13:01.353527
------------------------------------------------------
Started: 2025-04-17 11:19:18.997494
Existing_entries: 0
Fetching from https://rss.arxiv.org/rss/cs.CL
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://arxiv.org/abs/2504.11468)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [AskQE: Question Answering as Automatic Evaluation for Machine Translation](https://arxiv.org/abs/2504.11582)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters](https://arxiv.org/abs/2504.11770)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Enhancing Web Agents with Explicit Rollback Mechanisms](https://arxiv.org/abs/2504.11788)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification](https://arxiv.org/abs/2504.11793)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture](https://arxiv.org/abs/2504.11809)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ARWI: Arabic Write and Improve](https://arxiv.org/abs/2504.11814)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Could Thinking Multilingually Empower LLM Reasoning?](https://arxiv.org/abs/2504.11833)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations](https://arxiv.org/abs/2504.11837)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection](https://arxiv.org/abs/2504.11900)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation](https://arxiv.org/abs/2504.11934)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA](https://arxiv.org/abs/2504.11972)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2504.11975)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems](https://arxiv.org/abs/2504.11986)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS](https://arxiv.org/abs/2504.12052)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection](https://arxiv.org/abs/2504.12082)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Gauging Overprecision in LLMs: An Empirical Study](https://arxiv.org/abs/2504.12098)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation](https://arxiv.org/abs/2504.12108)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Multilingual Contextualization of Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2504.12140)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task](https://arxiv.org/abs/2504.12172)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube](https://arxiv.org/abs/2504.12177)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification](https://arxiv.org/abs/2504.12180)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://arxiv.org/abs/2504.12185)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure](https://arxiv.org/abs/2504.12187)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [From Conceptual Data Models to Multimodal Representation](https://arxiv.org/abs/2504.11459)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Semantic Matters: Multimodal Features for Affective Analysis](https://arxiv.org/abs/2504.11460)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Language and Knowledge Representation: A Stratified Approach](https://arxiv.org/abs/2504.11492)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment](https://arxiv.org/abs/2504.11515)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation](https://arxiv.org/abs/2504.11524)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [GraphicBench: A Planning Benchmark for Graphic Design with Language Agents](https://arxiv.org/abs/2504.11571)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?](https://arxiv.org/abs/2504.11741)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach](https://arxiv.org/abs/2504.11889)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation](https://arxiv.org/abs/2504.11942)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -](https://arxiv.org/abs/2504.12137)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Watermarking Needs Input Repetition Masking](https://arxiv.org/abs/2504.12229)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning](https://arxiv.org/abs/2504.12254)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Knowledge Graph Reasoning with Self-supervised Reinforcement Learning](https://arxiv.org/abs/2405.13640)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Large Visual-Language Models Are Also Good Classifiers: A Study of In-Context Multimodal Fake News Detection](https://arxiv.org/abs/2407.12879)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Application of AI-based Models for Online Fraud Detection and Analysis](https://arxiv.org/abs/2409.19022)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement](https://arxiv.org/abs/2410.02108)
Append: [Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning](https://arxiv.org/abs/2410.17088)
Append: [Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings](https://arxiv.org/abs/2411.05986)
Append: [Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework](https://arxiv.org/abs/2411.16707)
Append: [Sequence-Level Leakage Risk of Training Data in Large Language Models](https://arxiv.org/abs/2412.11302)
Append: [Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models](https://arxiv.org/abs/2412.12144)
Append: [Enhancing Privacy in the Early Detection of Sexual Predators Through Federated Learning and Differential Privacy](https://arxiv.org/abs/2501.12537)
Append: [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/abs/2502.01568)
Append: [How Inclusively do LMs Perceive Social and Moral Norms?](https://arxiv.org/abs/2502.02696)
Append: [Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation](https://arxiv.org/abs/2502.05151)
Append: [Automatic Input Rewriting Improves Translation with Large Language Models](https://arxiv.org/abs/2502.16682)
Append: [Figurative Archive: an open dataset and web-based application for the study of metaphor](https://arxiv.org/abs/2503.00444)
Append: [FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Append: [What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2503.24235)
Append: [Assessing how hyperparameters impact Large Language Models' sarcasm detection performance](https://arxiv.org/abs/2504.06166)
Append: [Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification](https://arxiv.org/abs/2504.09394)
Append: [Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution](https://arxiv.org/abs/2504.09566)
Append: [LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks](https://arxiv.org/abs/2504.10185)
Append: [Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)
Append: [Automated Python Translation](https://arxiv.org/abs/2504.11290)
Append: [Local Grammar-Based Coding Revisited](https://arxiv.org/abs/2209.13636)
Append: [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773)
Append: [Taming Data and Transformers for Audio Generation](https://arxiv.org/abs/2406.19388)
Append: [Natural Language Outlines for Code: Literate Programming in the LLM Era](https://arxiv.org/abs/2408.04820)
Append: [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)](https://arxiv.org/abs/2409.02920)
Append: [Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models](https://arxiv.org/abs/2410.01795)
Append: [No Need to Talk: Asynchronous Mixture of Language Models](https://arxiv.org/abs/2410.03529)
Append: [Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs](https://arxiv.org/abs/2410.09080)
Append: [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169)
Append: [Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions](https://arxiv.org/abs/2411.09018)
Append: [BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving](https://arxiv.org/abs/2411.17404)
Append: [Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads](https://arxiv.org/abs/2412.00127)
Append: [ChaosEater: Fully Automating Chaos Engineering with Large Language Models](https://arxiv.org/abs/2501.11107)
Append: [FourierNAT: A Fourier-Mixing-Based Non-Autoregressive Transformer for Parallel Sequence Generation](https://arxiv.org/abs/2503.07630)
Append: [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/abs/2503.22675)
Append: [Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks](https://arxiv.org/abs/2504.08525)
Append: [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/abs/2504.11257)
append_entries: 86
Finish: 2025-04-17 11:19:34.176790
------------------------------------------------------
Started: 2025-04-17 11:36:05.707066
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 11:36:05.882828
------------------------------------------------------
Started: 2025-04-17 12:31:42.607215
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 12:31:42.750078
------------------------------------------------------
Started: 2025-04-17 14:15:25.880323
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 14:15:26.002056
------------------------------------------------------
Started: 2025-04-17 16:20:08.581711
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 16:20:08.710971
------------------------------------------------------
Started: 2025-04-17 18:21:33.755192
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 18:21:33.931093
------------------------------------------------------
Started: 2025-04-17 20:17:29.620947
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 20:17:29.774904
------------------------------------------------------
Started: 2025-04-17 22:14:51.190252
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 22:14:51.351165
------------------------------------------------------
Started: 2025-04-18 01:14:25.579571
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 01:14:25.739458
------------------------------------------------------
Started: 2025-04-18 02:57:56.634221
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 02:57:56.791011
------------------------------------------------------
Started: 2025-04-18 04:22:12.545194
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability](https://arxiv.org/abs/2504.12308)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles](https://arxiv.org/abs/2504.12312)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models](https://arxiv.org/abs/2504.12313)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension](https://arxiv.org/abs/2504.12314)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models](https://arxiv.org/abs/2504.12315)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Data Metabolism: An Efficient Data Design Schema For Vision Language Model](https://arxiv.org/abs/2504.12316)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing](https://arxiv.org/abs/2504.12317)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability](https://arxiv.org/abs/2504.12320)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis](https://arxiv.org/abs/2504.12322)
Token length: 1709
Summarized using GPT-3.5-turbo
Append: [The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12323)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media](https://arxiv.org/abs/2504.12325)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis](https://arxiv.org/abs/2504.12326)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Word Embeddings Track Social Group Changes Across 70 Years in China](https://arxiv.org/abs/2504.12327)
Token length: 870
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future](https://arxiv.org/abs/2504.12328)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time](https://arxiv.org/abs/2504.12329)
Token length: 1809
Summarized using GPT-3.5-turbo
Append: [HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2504.12330)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation](https://arxiv.org/abs/2504.12331)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Can the capability of Large Language Models be described by human ability? A Meta Study](https://arxiv.org/abs/2504.12332)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games](https://arxiv.org/abs/2504.12333)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model](https://arxiv.org/abs/2504.12334)
Token length: 944
Summarized using GPT-3.5-turbo
Append: [You've Changed: Detecting Modification of Black-Box Large Language Models](https://arxiv.org/abs/2504.12335)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: ["It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool](https://arxiv.org/abs/2504.12337)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions](https://arxiv.org/abs/2504.12338)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture](https://arxiv.org/abs/2504.12339)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Streamlining Biomedical Research with Specialized LLMs](https://arxiv.org/abs/2504.12341)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.12342)
Token length: 1613
Summarized using GPT-3.5-turbo
Append: [Propaganda via AI? A Study on Semantic Backdoors in Large Language Models](https://arxiv.org/abs/2504.12344)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Token length: 837
Summarized using GPT-3.5-turbo
Append: [Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination](https://arxiv.org/abs/2504.12347)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports](https://arxiv.org/abs/2504.12350)
Token length: 898
Summarized using GPT-3.5-turbo
Append: [Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media](https://arxiv.org/abs/2504.12355)
Token length: 639
Summarized using GPT-3.5-turbo
Append: [Replicating ReLM Results: Validating Large Language Models with ReLM](https://arxiv.org/abs/2504.12357)
Token length: 913
Summarized using GPT-3.5-turbo
Append: [A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version](https://arxiv.org/abs/2504.12360)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Position: The Most Expensive Part of an LLM should be its Training Data](https://arxiv.org/abs/2504.12427)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [On Linear Representations and Pretraining Data Frequency in Language Models](https://arxiv.org/abs/2504.12459)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse](https://arxiv.org/abs/2504.12466)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex](https://arxiv.org/abs/2504.12474)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?](https://arxiv.org/abs/2504.12491)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification](https://arxiv.org/abs/2504.12494)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Beyond Text: Characterizing Domain Expert Needs in Document Research](https://arxiv.org/abs/2504.12495)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents](https://arxiv.org/abs/2504.12516)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Evaluating the Diversity and Quality of LLM Generated Content](https://arxiv.org/abs/2504.12522)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [Memorization vs. Reasoning: Updating LLMs with New Knowledge](https://arxiv.org/abs/2504.12523)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Memorization: A Close Look at Books](https://arxiv.org/abs/2504.12549)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [ELAB: Extensive LLM Alignment Benchmark in Persian Language](https://arxiv.org/abs/2504.12553)
Token length: 1303
Summarized using GPT-3.5-turbo
Append: [CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12560)
Token length: 1595
Summarized using GPT-3.5-turbo
Append: [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)
Append: [Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models](https://arxiv.org/abs/2504.12585)
Append: [GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning](https://arxiv.org/abs/2504.12597)
Append: [Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs](https://arxiv.org/abs/2504.12633)
Append: [Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation](https://arxiv.org/abs/2504.12637)
Append: [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)
Append: [ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2504.12673)
Append: [GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs](https://arxiv.org/abs/2504.12681)
Append: [Data-efficient LLM Fine-tuning for Code Generation](https://arxiv.org/abs/2504.12687)
Append: [Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations](https://arxiv.org/abs/2504.12691)
Append: [KODIS: A Multicultural Dispute Resolution Dialogue Corpus](https://arxiv.org/abs/2504.12723)
Append: [Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge](https://arxiv.org/abs/2504.12734)
Append: [Chinese-Vicuna: A Chinese Instruction-following Llama-based Model](https://arxiv.org/abs/2504.12737)
Append: [Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts](https://arxiv.org/abs/2504.12767)
Append: [Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration](https://arxiv.org/abs/2504.12773)
Append: [Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation](https://arxiv.org/abs/2504.12805)
Append: [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
Append: [Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks](https://arxiv.org/abs/2504.12845)
Append: [ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos](https://arxiv.org/abs/2504.12882)
Append: [Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication](https://arxiv.org/abs/2504.12891)
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Append: [Benchmarking Multi-National Value Alignment for Large Language Models](https://arxiv.org/abs/2504.12911)
Append: [MAIN: Mutual Alignment Is Necessary for instruction tuning](https://arxiv.org/abs/2504.12913)
Append: [ConExion: Concept Extraction with Large Language Models](https://arxiv.org/abs/2504.12915)
Append: [Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback](https://arxiv.org/abs/2504.12951)
Append: [Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization](https://arxiv.org/abs/2504.12972)
Append: [Sparks of Science: Hypothesis Generation Using Structured Paper Data](https://arxiv.org/abs/2504.12976)
Append: [Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild](https://arxiv.org/abs/2504.12982)
Append: [SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation](https://arxiv.org/abs/2504.12996)
Append: [ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images](https://arxiv.org/abs/2504.13023)
Append: [Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation](https://arxiv.org/abs/2504.13054)
Append: [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
Append: [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)
Append: [LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard](https://arxiv.org/abs/2504.13125)
Append: [Energy-Based Reward Models for Robust Language Model Alignment](https://arxiv.org/abs/2504.13134)
Append: [Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo](https://arxiv.org/abs/2504.13139)
Append: [CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training](https://arxiv.org/abs/2504.13161)
Append: [Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective](https://arxiv.org/abs/2504.12309)
Append: [Specialized text classification: an approach to classifying Open Banking transactions](https://arxiv.org/abs/2504.12319)
Append: [A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment](https://arxiv.org/abs/2504.12408)
Append: [Towards Conversational AI for Human-Machine Collaborative MLOps](https://arxiv.org/abs/2504.12477)
Append: [Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice](https://arxiv.org/abs/2504.12545)
Append: [Benchmarking LLM-based Relevance Judgment Methods](https://arxiv.org/abs/2504.12558)
Append: [ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2504.12562)
Append: [Provable Secure Steganography Based on Adaptive Dynamic Sampling](https://arxiv.org/abs/2504.12579)
Append: [Simplifying Graph Transformers](https://arxiv.org/abs/2504.12588)
Append: [VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization](https://arxiv.org/abs/2504.12661)
Append: [WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents](https://arxiv.org/abs/2504.12682)
Append: [Towards Lossless Token Pruning in Late-Interaction Retrieval Models](https://arxiv.org/abs/2504.12778)
Append: [EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting](https://arxiv.org/abs/2504.12867)
Append: [Building Russian Benchmark for Evaluation of Information Retrieval Models](https://arxiv.org/abs/2504.12879)
Append: [A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology](https://arxiv.org/abs/2504.12977)
Append: [How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses](https://arxiv.org/abs/2504.13038)
Append: [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins](https://arxiv.org/abs/2504.13059)
Append: [Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia](https://arxiv.org/abs/2504.13085)
Append: [Probing and Inducing Combinational Creativity in Vision-Language Models](https://arxiv.org/abs/2504.13120)
Append: [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)
Append: [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
Append: [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
Append: [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171)
Append: [SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs](https://arxiv.org/abs/2504.13172)
Append: [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000)
Append: [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305)
Append: [Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers](https://arxiv.org/abs/2402.11700)
Append: [Citation-Enhanced Generation for LLM-based Chatbots](https://arxiv.org/abs/2402.16063)
Append: [MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory](https://arxiv.org/abs/2404.11672)
Append: [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/abs/2405.06691)
Append: [Unipa-GPT: Large Language Models for university-oriented QA in Italian](https://arxiv.org/abs/2407.14246)
Append: [ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities](https://arxiv.org/abs/2408.04682)
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Append: [Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective](https://arxiv.org/abs/2410.10291)
Append: [In-context KV-Cache Eviction for LLMs via Attention-Gate](https://arxiv.org/abs/2410.12876)
Append: [Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities](https://arxiv.org/abs/2410.17385)
Append: [IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs](https://arxiv.org/abs/2411.07466)
Append: [AMPS: ASR with Multimodal Paraphrase Supervision](https://arxiv.org/abs/2411.18368)
Append: [Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Arithmetic Reasoning](https://arxiv.org/abs/2412.01113)
Append: [Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?](https://arxiv.org/abs/2502.20973)
Append: [ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2503.10673)
Append: [Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models](https://arxiv.org/abs/2504.00046)
Append: [OnRL-RAG: Real-Time Personalized Mental Health Dialogue System](https://arxiv.org/abs/2504.02894)
Append: [FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion](https://arxiv.org/abs/2504.06562)
Append: [Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare](https://arxiv.org/abs/2504.08260)
Append: [Taxonomy and Analysis of Sensitive User Queries in Generative AI Search](https://arxiv.org/abs/2404.08672)
Append: [ALCM: Autonomous LLM-Augmented Causal Discovery Framework](https://arxiv.org/abs/2405.01744)
Append: [ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs](https://arxiv.org/abs/2409.09586)
Append: [Multi-Field Adaptive Retrieval](https://arxiv.org/abs/2410.20056)
Append: [Multimodal LLMs Can Reason about Aesthetics in Zero-Shot](https://arxiv.org/abs/2501.09012)
Append: [Contextual Agent Security: A Policy for Every Purpose](https://arxiv.org/abs/2501.17070)
Append: [DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/abs/2504.03160)
Append: [SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning](https://arxiv.org/abs/2504.09081)
Append: [CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography](https://arxiv.org/abs/2504.10090)
Append: [CSPLADE: Learned Sparse Retrieval with Causal Language Models](https://arxiv.org/abs/2504.10816)
append_entries: 141
Finish: 2025-04-18 04:23:56.116842
------------------------------------------------------
Started: 2025-04-18 06:22:49.448041
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 06:22:49.651237
------------------------------------------------------
Started: 2025-04-18 08:20:28.573579
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 08:20:28.802837
------------------------------------------------------
Started: 2025-04-18 10:16:58.114721
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 10:16:58.334868
------------------------------------------------------
Started: 2025-04-18 12:30:48.575312
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 12:30:48.782049
------------------------------------------------------
Started: 2025-04-18 14:14:26.630654
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 14:14:26.886108
------------------------------------------------------
Started: 2025-04-18 16:19:00.392459
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 16:19:00.604683
------------------------------------------------------
Started: 2025-04-18 18:21:10.218530
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 18:21:10.449185
------------------------------------------------------
Started: 2025-04-18 20:16:52.105165
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 20:16:52.304820
------------------------------------------------------
Started: 2025-04-18 22:15:47.891963
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 22:15:48.104332
------------------------------------------------------
Started: 2025-04-19 01:12:23.296762
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 01:12:23.505671
------------------------------------------------------
Started: 2025-04-19 02:54:27.494880
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 02:54:27.699064
------------------------------------------------------
Started: 2025-04-19 04:18:13.814489
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 04:18:13.860699
------------------------------------------------------
Started: 2025-04-19 06:20:49.522606
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 06:20:49.571120
------------------------------------------------------
Started: 2025-04-19 08:18:19.457168
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 08:18:19.506663
------------------------------------------------------
Started: 2025-04-19 10:14:57.449370
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 10:14:57.514971
------------------------------------------------------
Started: 2025-04-19 12:28:10.834622
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 12:28:10.896698
------------------------------------------------------
Started: 2025-04-19 14:13:29.774947
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 14:13:29.817857
------------------------------------------------------
Started: 2025-04-19 16:19:00.848640
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 16:19:00.895567
------------------------------------------------------
Started: 2025-04-19 18:19:24.418417
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 18:19:24.467285
------------------------------------------------------
Started: 2025-04-19 20:16:14.713065
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 20:16:14.756591
------------------------------------------------------
Started: 2025-04-19 22:14:08.761221
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 22:14:08.808443
------------------------------------------------------
Started: 2025-04-20 01:20:16.180253
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 01:20:16.224170
------------------------------------------------------
Started: 2025-04-20 03:05:48.313731
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 03:05:48.376659
------------------------------------------------------
Started: 2025-04-20 04:17:47.627738
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 04:17:47.679948
------------------------------------------------------
Started: 2025-04-20 06:20:54.713747
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 06:20:54.774109
------------------------------------------------------
Started: 2025-04-20 08:18:10.390530
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 08:18:10.441756
------------------------------------------------------
Started: 2025-04-20 10:15:19.146242
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 10:15:19.205367
------------------------------------------------------
Started: 2025-04-20 12:28:45.757837
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 12:28:45.799343
------------------------------------------------------
Started: 2025-04-20 14:13:54.969232
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 14:13:55.015833
------------------------------------------------------
Started: 2025-04-20 16:17:31.583863
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 16:17:31.649257
------------------------------------------------------
Started: 2025-04-20 18:19:58.800955
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 18:19:58.893111
------------------------------------------------------
Started: 2025-04-20 20:16:04.221792
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 20:16:04.296078
------------------------------------------------------
Started: 2025-04-20 22:14:46.880363
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 22:14:46.959585
------------------------------------------------------
Started: 2025-04-21 01:18:44.968624
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 01:18:45.016905
------------------------------------------------------
Started: 2025-04-21 03:07:34.438892
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 03:07:34.487419
------------------------------------------------------
Started: 2025-04-21 04:24:17.719976
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis](https://arxiv.org/abs/2504.13187)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models](https://arxiv.org/abs/2504.13189)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding](https://arxiv.org/abs/2504.13216)
Token length: 1860
Summarized using GPT-3.5-turbo
Append: [Sustainability via LLM Right-sizing](https://arxiv.org/abs/2504.13217)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [DIDS: Domain Impact-aware Data Sampling for Large Language Model Training](https://arxiv.org/abs/2504.13227)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs](https://arxiv.org/abs/2504.13237)
Token length: 1651
Summarized using GPT-3.5-turbo
Append: [CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models](https://arxiv.org/abs/2504.13261)
Token length: 890
Summarized using GPT-3.5-turbo
Append: [Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal](https://arxiv.org/abs/2504.13284)
Token length: 1139
Summarized using GPT-3.5-turbo
Append: [THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models](https://arxiv.org/abs/2504.13367)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering](https://arxiv.org/abs/2504.13425)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model](https://arxiv.org/abs/2504.13439)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [LLM Sensitivity Evaluation Framework for Clinical Diagnosis](https://arxiv.org/abs/2504.13475)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning](https://arxiv.org/abs/2504.13500)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content](https://arxiv.org/abs/2504.13545)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](https://arxiv.org/abs/2504.13562)
Token length: 1308
Summarized using GPT-3.5-turbo
Append: [Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling](https://arxiv.org/abs/2504.13592)
Token length: 1078
Summarized using GPT-3.5-turbo
Append: [Continual Pre-Training is (not) What You Need in Domain Adaption](https://arxiv.org/abs/2504.13603)
Token length: 1840
Summarized using GPT-3.5-turbo
Append: [Long-context Non-factoid Question Answering in Indic Languages](https://arxiv.org/abs/2504.13615)
Token length: 1593
Summarized using GPT-3.5-turbo
Append: [Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models](https://arxiv.org/abs/2504.13626)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing](https://arxiv.org/abs/2504.13629)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling](https://arxiv.org/abs/2504.13630)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning](https://arxiv.org/abs/2504.13643)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Word Embedding Techniques for Classification of Star Ratings](https://arxiv.org/abs/2504.13653)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts](https://arxiv.org/abs/2504.13655)
Token length: 934
Summarized using GPT-3.5-turbo
Append: [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Deep literature reviews: an application of fine-tuned language models to migration research](https://arxiv.org/abs/2504.13685)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence](https://arxiv.org/abs/2504.13730)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models](https://arxiv.org/abs/2504.13775)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/abs/2504.13816)
Token length: 1148
Summarized using GPT-3.5-turbo
Append: [Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2504.13825)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/abs/2504.13828)
Token length: 1914
Summarized using GPT-3.5-turbo
Append: [Science Hierarchography: Hierarchical Organization of Science Literature](https://arxiv.org/abs/2504.13834)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space](https://arxiv.org/abs/2504.13835)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)
Token length: 1635
Summarized using GPT-3.5-turbo
Append: [Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces](https://arxiv.org/abs/2504.13277)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope](https://arxiv.org/abs/2504.13308)
Token length: 1891
Summarized using GPT-3.5-turbo
Append: [Cost-of-Pass: An Economic Framework for Evaluating Language Models](https://arxiv.org/abs/2504.13359)
Token length: 1012
Summarized using GPT-3.5-turbo
Append: [A mean teacher algorithm for unlearning of language models](https://arxiv.org/abs/2504.13388)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [LangCoop: Collaborative Driving with Language](https://arxiv.org/abs/2504.13406)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings](https://arxiv.org/abs/2504.13416)
Token length: 1782
Summarized using GPT-3.5-turbo
Append: [CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2504.13472)
Token length: 1658
Summarized using GPT-3.5-turbo
Append: [Integrating Locality-Aware Attention with Transformers for General Geometry PDEs](https://arxiv.org/abs/2504.13480)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation](https://arxiv.org/abs/2504.13551)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs](https://arxiv.org/abs/2504.13644)
Token length: 601
Summarized using GPT-3.5-turbo
Append: [Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm](https://arxiv.org/abs/2504.13667)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation](https://arxiv.org/abs/2504.13707)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Learning to Attribute with Attention](https://arxiv.org/abs/2504.13752)
Append: [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756)
Append: [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
Append: [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
Append: [Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation](https://arxiv.org/abs/2401.07456)
Append: [A Theory of LLM Sampling: Part Descriptive and Part Prescriptive](https://arxiv.org/abs/2402.11005)
Append: [Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction](https://arxiv.org/abs/2402.12170)
Append: [Argumentative Large Language Models for Explainable and Contestable Claim Verification](https://arxiv.org/abs/2405.02079)
Append: [Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations](https://arxiv.org/abs/2405.13828)
Append: [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://arxiv.org/abs/2405.19874)
Append: [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org/abs/2406.11260)
Append: [Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?](https://arxiv.org/abs/2406.12307)
Append: [The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences of LLM Evaluators](https://arxiv.org/abs/2406.12319)
Append: [Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969)
Append: [Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind](https://arxiv.org/abs/2408.12022)
Append: [Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts](https://arxiv.org/abs/2409.11056)
Append: [Reducing the Scope of Language Models](https://arxiv.org/abs/2410.21597)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation](https://arxiv.org/abs/2412.11831)
Append: [StaICC: Standardized Evaluation for Classification Task in In-context Learning](https://arxiv.org/abs/2501.15708)
Append: [A-MEM: Agentic Memory for LLM Agents](https://arxiv.org/abs/2502.12110)
Append: [Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models](https://arxiv.org/abs/2502.14427)
Append: [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729)
Append: [Adaptive Layer-skipping in Pre-trained LLMs](https://arxiv.org/abs/2503.23798)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [From Token to Line: Enhancing Code Generation with a Long-Term Perspective](https://arxiv.org/abs/2504.07433)
Append: [Can postgraduate translation students identify machine-generated text?](https://arxiv.org/abs/2504.09164)
Append: [C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset](https://arxiv.org/abs/2504.09958)
Append: [The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination](https://arxiv.org/abs/2504.10020)
Append: [SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems](https://arxiv.org/abs/2405.19653)
Append: [LLM-Select: Feature Selection with Large Language Models](https://arxiv.org/abs/2407.02694)
Append: [Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization](https://arxiv.org/abs/2407.07880)
Append: [Spin glass model of in-context learning](https://arxiv.org/abs/2408.02288)
Append: [SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding](https://arxiv.org/abs/2408.15545)
Append: [United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections](https://arxiv.org/abs/2409.09045)
Append: [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024)
Append: [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465)
Append: [PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2411.16750)
Append: [Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant](https://arxiv.org/abs/2501.17176)
Append: [Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs](https://arxiv.org/abs/2502.19413)
Append: [Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review](https://arxiv.org/abs/2503.05992)
Append: [DocAgent: A Multi-Agent System for Automated Code Documentation Generation](https://arxiv.org/abs/2504.08725)
Append: [Assessing Judging Bias in Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2504.09946)
Append: [GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents](https://arxiv.org/abs/2504.10458)
append_entries: 93
Finish: 2025-04-21 04:26:21.470194
------------------------------------------------------
Started: 2025-04-21 06:23:11.446219
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 06:23:11.621175
------------------------------------------------------
Started: 2025-04-21 08:21:32.665323
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 08:21:32.824343
------------------------------------------------------
Started: 2025-04-21 10:17:11.123964
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 10:17:11.292386
------------------------------------------------------
Started: 2025-04-21 12:31:41.885468
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 12:31:42.045876
------------------------------------------------------
Started: 2025-04-21 14:14:45.792935
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 14:14:45.954796
------------------------------------------------------
Started: 2025-04-21 16:19:34.585177
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 16:19:34.784087
------------------------------------------------------
Started: 2025-04-21 18:21:23.595244
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 18:21:23.768949
------------------------------------------------------
Started: 2025-04-21 20:17:45.253500
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 20:17:45.413071
------------------------------------------------------
Started: 2025-04-21 22:16:01.866912
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 22:16:02.025241
------------------------------------------------------
Started: 2025-04-22 01:15:42.061651
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 01:15:42.240146
------------------------------------------------------
Started: 2025-04-22 03:01:08.089373
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 03:01:08.260772
------------------------------------------------------
Started: 2025-04-22 04:23:33.107276
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 940
Summarized using GPT-3.5-turbo
Append: [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
Token length: 855
Summarized using GPT-3.5-turbo
Append: [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
Token length: 1667
Summarized using GPT-3.5-turbo
Append: [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
Token length: 1166
Summarized using GPT-3.5-turbo
Append: [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
Token length: 1819
Summarized using GPT-3.5-turbo
Append: [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
Token length: 976
Summarized using GPT-3.5-turbo
Append: [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
Token length: 1612
Summarized using GPT-3.5-turbo
Append: [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
Token length: 1753
Summarized using GPT-3.5-turbo
Append: [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
Token length: 1764
Summarized using GPT-3.5-turbo
Append: [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
Token length: 1892
Summarized using GPT-3.5-turbo
Append: [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
Token length: 1074
Summarized using GPT-3.5-turbo
Append: [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
Token length: 497
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
Append: [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
Append: [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
Append: [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
Append: [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
Append: [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
Append: [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
Append: [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
Append: [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
Append: [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
Append: [On true empty category](https://arxiv.org/abs/2504.15168)
Append: [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
Append: [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
Append: [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
Append: [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
Append: [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
Append: [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
Append: [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
Append: [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
Append: [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
Append: [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
Append: [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
Append: [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
Append: [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
Append: [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
Append: [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
Append: [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
Append: [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
Append: [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
Append: [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
Append: [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
Append: [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
Append: [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
Append: [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
Append: [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
Append: [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
Append: [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
Append: [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
Append: [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
Append: [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
Append: [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
Append: [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
Append: [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
Append: [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
Append: [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
Append: [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
Append: [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
Append: [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
Append: [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
Append: [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
Append: [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
Append: [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
Append: [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
Append: [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
Append: [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
Append: [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
Append: [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
Append: [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
Append: [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
Append: [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
Append: [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
Append: [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
Append: [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
Append: [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
Append: [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
Append: [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
Append: [Persian Abstract Meaning Representation: Annotation Guidelines and Gold Standard Dataset](https://arxiv.org/abs/2205.07712)
Append: [GLoRE: Evaluating Logical Reasoning of Large Language Models](https://arxiv.org/abs/2310.09107)
Append: [LongStory: Coherent, Complete and Length Controlled Long story Generation](https://arxiv.org/abs/2311.15208)
Append: [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://arxiv.org/abs/2403.10258)
Append: [Aligning Language Models with Demonstrated Feedback](https://arxiv.org/abs/2406.00888)
Append: [Inverse Constitutional AI: Compressing Preferences into Principles](https://arxiv.org/abs/2406.06560)
Append: [Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition](https://arxiv.org/abs/2406.11192)
Append: [Temporal Knowledge Graph Question Answering: A Survey](https://arxiv.org/abs/2406.14191)
Append: [LiveBench: A Challenging, Contamination-Limited LLM Benchmark](https://arxiv.org/abs/2406.19314)
Append: [Training on the Test Task Confounds Evaluation and Emergence](https://arxiv.org/abs/2407.07890)
Append: [IFShip: Interpretable Fine-grained Ship Classification with Domain Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2408.06631)
Append: [DualKanbaFormer: An Efficient Selective Sparse Framework for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2408.15379)
Append: [Self-evolving Agents with reflective and memory-augmented abilities](https://arxiv.org/abs/2409.00872)
Append: [Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning](https://arxiv.org/abs/2409.01035)
Append: [Seek and Solve Reasoning for Table Question Answering](https://arxiv.org/abs/2409.05286)
Append: [Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval](https://arxiv.org/abs/2410.04585)
Append: [Detecting Training Data of Large Language Models via Expectation Maximization](https://arxiv.org/abs/2410.07582)
Append: [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/abs/2410.09300)
Append: [Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus](https://arxiv.org/abs/2410.14815)
Append: [Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment](https://arxiv.org/abs/2410.16714)
Append: [Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation](https://arxiv.org/abs/2410.20941)
Append: [ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents](https://arxiv.org/abs/2411.00927)
Append: [CHATTER: A Character Attribution Dataset for Narrative Understanding](https://arxiv.org/abs/2411.05227)
Append: [FactLens: Benchmarking Fine-Grained Fact Verification](https://arxiv.org/abs/2411.05980)
Append: [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)
Append: [WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation](https://arxiv.org/abs/2412.01626)
Append: [Unanswerability Evaluation for Retrieval Augmented Generation](https://arxiv.org/abs/2412.12300)
Append: [State Space Models are Strong Text Rerankers](https://arxiv.org/abs/2412.14354)
Append: [LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation](https://arxiv.org/abs/2501.05414)
Append: [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)
Append: [Idiom Detection in Sorani Kurdish Texts](https://arxiv.org/abs/2501.14528)
Append: [Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet](https://arxiv.org/abs/2502.05291)
Append: [BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models](https://arxiv.org/abs/2502.07346)
Append: [SparQLe: Speech Queries to Text Translation Through LLMs](https://arxiv.org/abs/2502.09284)
Append: [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)
Append: [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](https://arxiv.org/abs/2502.14866)
Append: [Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores](https://arxiv.org/abs/2502.16358)
Append: [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/abs/2502.18036)
Append: [Semantic Wave Functions: Exploring Meaning in Large Language Models Through Quantum Formalism](https://arxiv.org/abs/2503.10664)
Append: [Halving transcription time: A fast, user-friendly and GDPR-compliant workflow to create AI-assisted transcripts for content analysis](https://arxiv.org/abs/2503.13031)
Append: [A Language Anchor-Guided Method for Robust Noisy Domain Generalization](https://arxiv.org/abs/2503.17211)
Append: [LLM Agents That Act Like Us: Accurate Human Behavior Simulation with Real-World Data](https://arxiv.org/abs/2503.20749)
Append: [ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging](https://arxiv.org/abs/2503.21088)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection](https://arxiv.org/abs/2504.00695)
Append: [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)
Append: [Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers](https://arxiv.org/abs/2504.03595)
Append: [NAACL2025 Tutorial: Adaptation of Large Language Models](https://arxiv.org/abs/2504.03931)
Append: [Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs](https://arxiv.org/abs/2504.04994)
Append: [Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games](https://arxiv.org/abs/2504.06868)
Append: [AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation](https://arxiv.org/abs/2504.07532)
Append: [UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents](https://arxiv.org/abs/2504.09407)
Append: [Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families](https://arxiv.org/abs/2504.10340)
Append: [Characterizing Knowledge Manipulation in a Russian Wikipedia Fork](https://arxiv.org/abs/2504.10663)
Append: [Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge](https://arxiv.org/abs/2312.05693)
Append: [Embedding Ontologies via Incorporating Extensional and Intensional Knowledge](https://arxiv.org/abs/2402.01677)
Append: [SLMRec: Distilling Large Language Models into Small for Sequential Recommendation](https://arxiv.org/abs/2405.17890)
Append: [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)
Append: [Jailbreaking as a Reward Misspecification Problem](https://arxiv.org/abs/2406.14393)
Append: [OpenHands: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/abs/2407.16741)
Append: [MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders](https://arxiv.org/abs/2409.06635)
Append: [DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://arxiv.org/abs/2410.09344)
Append: [Context-Parametric Inversion: Why Instruction Finetuning Can Worsen Context Reliance](https://arxiv.org/abs/2410.10796)
Append: [Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment](https://arxiv.org/abs/2410.14148)
Append: [Aioli: A Unified Optimization Framework for Language Model Data Mixing](https://arxiv.org/abs/2411.05735)
Append: [Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures](https://arxiv.org/abs/2411.16260)
Append: [Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems](https://arxiv.org/abs/2503.21074)
Append: [Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design](https://arxiv.org/abs/2504.01337)
Append: [Self-Resource Allocation in Multi-Agent LLM Systems](https://arxiv.org/abs/2504.02051)
Append: [Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling](https://arxiv.org/abs/2504.05216)
Append: [Fine-tuning a Large Language Model for Automating Computational Fluid Dynamics Simulations](https://arxiv.org/abs/2504.09602)
Append: [EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety](https://arxiv.org/abs/2504.09689)
append_entries: 192
Finish: 2025-04-22 04:25:33.740083
------------------------------------------------------
Started: 2025-04-22 06:23:11.701646
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 06:23:12.073259
------------------------------------------------------
Started: 2025-04-22 08:21:26.118165
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 08:21:26.477918
------------------------------------------------------
Started: 2025-04-22 10:17:12.338506
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 10:17:12.670682
------------------------------------------------------
Started: 2025-04-22 12:31:55.759357
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 12:31:56.094493
------------------------------------------------------
Started: 2025-04-22 14:16:20.423052
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 14:16:20.782122
------------------------------------------------------
Started: 2025-04-22 16:19:48.170928
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 16:19:48.517521
------------------------------------------------------
Started: 2025-04-22 18:21:57.037809
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 18:21:57.372659
------------------------------------------------------
Started: 2025-04-22 20:17:54.394184
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 20:17:54.717448
------------------------------------------------------
Started: 2025-04-22 22:15:20.731987
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 22:15:21.064344
------------------------------------------------------
Started: 2025-04-23 01:16:01.524302
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 01:16:01.926845
------------------------------------------------------
Started: 2025-04-23 03:01:55.044807
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 03:01:55.364725
------------------------------------------------------
Started: 2025-04-23 04:23:47.884309
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
Token length: 777
Summarized using GPT-3.5-turbo
Append: [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
Token length: 770
Summarized using GPT-3.5-turbo
Append: [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
Token length: 1874
Summarized using GPT-3.5-turbo
Append: [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
Token length: 744
Summarized using GPT-3.5-turbo
Append: [Computational Typology](https://arxiv.org/abs/2504.15642)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
Token length: 1800
Summarized using GPT-3.5-turbo
Append: [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
Token length: 1828
Summarized using GPT-3.5-turbo
Append: [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
Token length: 1635
Summarized using GPT-3.5-turbo
Append: [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
Token length: 682
Summarized using GPT-3.5-turbo
Append: [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
Token length: 1441
Summarized using GPT-3.5-turbo
Append: [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
Token length: 1838
Summarized using GPT-3.5-turbo
Append: [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
Token length: 1692
Summarized using GPT-3.5-turbo
Append: [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Med-CoDE: Medical Critique based Disagreement Evaluation Framework](https://arxiv.org/abs/2504.15330)
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data](https://arxiv.org/abs/2504.15448)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
Token length: 1846
Summarized using GPT-3.5-turbo
Append: [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
Token length: 1949
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
Append: [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
Append: [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)
Append: [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
Append: [Aggregating Soft Labels from Crowd Annotations Improves Uncertainty Estimation Under Distribution Shift](https://arxiv.org/abs/2212.09409)
Append: [On the Low-Rank Parametrization of Reward Models for Controlled Language Generation](https://arxiv.org/abs/2407.04615)
Append: [Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation](https://arxiv.org/abs/2408.06276)
Append: [Open-World Evaluation for Retrieving Diverse Perspectives](https://arxiv.org/abs/2409.18110)
Append: [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://arxiv.org/abs/2410.02355)
Append: [SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2410.19503)
Append: [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)
Append: [Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree](https://arxiv.org/abs/2412.12639)
Append: [Fine-tuning Whisper on Low-Resource Languages for Real-World Applications](https://arxiv.org/abs/2412.15726)
Append: [Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs](https://arxiv.org/abs/2412.15993)
Append: [Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks](https://arxiv.org/abs/2502.13053)
Append: [Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review](https://arxiv.org/abs/2503.04797)
Append: [Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks](https://arxiv.org/abs/2503.09572)
Append: [Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques](https://arxiv.org/abs/2503.11816)
Append: [FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages](https://arxiv.org/abs/2504.00021)
Append: [Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training](https://arxiv.org/abs/2504.01801)
Append: [VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation](https://arxiv.org/abs/2504.04060)
Append: [Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance](https://arxiv.org/abs/2504.07989)
Append: [Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol](https://arxiv.org/abs/2504.10284)
Append: [Certifying Knowledge Comprehension in LLMs](https://arxiv.org/abs/2402.15929)
Append: [Optimizing RLHF Training for Large Language Models with Stage Fusion](https://arxiv.org/abs/2409.13221)
Append: [Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct](https://arxiv.org/abs/2410.02064)
Append: [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828)
Append: [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669)
Append: [Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments](https://arxiv.org/abs/2410.21131)
Append: [Codenames as a Benchmark for Large Language Models](https://arxiv.org/abs/2412.11373)
Append: [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/abs/2502.19676)
Append: [AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents](https://arxiv.org/abs/2504.09723)
append_entries: 81
Finish: 2025-04-23 04:25:49.528895
------------------------------------------------------
Started: 2025-04-23 06:23:05.510321
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 06:23:05.708699
------------------------------------------------------
Started: 2025-04-23 08:21:12.383997
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 08:21:12.583563
------------------------------------------------------
Started: 2025-04-23 10:18:03.041403
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 10:18:03.218827
------------------------------------------------------
Started: 2025-04-23 12:32:26.453256
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 12:32:26.631945
------------------------------------------------------
Started: 2025-04-23 14:16:58.304240
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 14:16:58.503814
------------------------------------------------------
Started: 2025-04-23 16:20:48.513946
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 16:20:48.687699
------------------------------------------------------
Started: 2025-04-23 18:23:17.886460
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 18:23:18.072062
------------------------------------------------------
Started: 2025-04-23 20:18:04.155370
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 20:18:04.350490
------------------------------------------------------
Started: 2025-04-23 22:15:40.121407
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 22:15:40.325700
------------------------------------------------------
Started: 2025-04-24 01:16:03.671826
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 01:16:03.917041
------------------------------------------------------
Started: 2025-04-24 03:03:43.859659
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 03:03:44.034993
------------------------------------------------------
Started: 2025-04-24 04:23:41.148106
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 912
Summarized using GPT-3.5-turbo
Append: [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
Token length: 806
Summarized using GPT-3.5-turbo
Append: [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
Token length: 734
Summarized using GPT-3.5-turbo
Append: [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
Token length: 748
Summarized using GPT-3.5-turbo
Append: [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Cooperative Speech, Semantic Competence, and AI](https://arxiv.org/abs/2504.16092)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval](https://arxiv.org/abs/2504.16121)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
Token length: 1921
Summarized using GPT-3.5-turbo
Append: [Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design](https://arxiv.org/abs/2504.16204)
Token length: 757
Summarized using GPT-3.5-turbo
Append: [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents](https://arxiv.org/abs/2504.16264)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
Token length: 687
Summarized using GPT-3.5-turbo
Append: [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
Token length: 1514
Summarized using GPT-3.5-turbo
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion](https://arxiv.org/abs/2111.15473)
Append: [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge](https://arxiv.org/abs/2307.08813)
Append: [A dataset and benchmark for hospital course summarization with adapted large language models](https://arxiv.org/abs/2403.05720)
Append: [NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens](https://arxiv.org/abs/2403.12766)
Append: [Large Language Model Sentinel: LLM Agent for Adversarial Purification](https://arxiv.org/abs/2405.20770)
Append: [Synthetic Lyrics Detection Across Languages and Genres](https://arxiv.org/abs/2406.15231)
Append: [SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy](https://arxiv.org/abs/2407.03004)
Append: [ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation](https://arxiv.org/abs/2407.12022)
Append: [Lawma: The Power of Specialization for Legal Annotation](https://arxiv.org/abs/2407.16615)
Append: [Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models](https://arxiv.org/abs/2407.17914)
Append: [The advantages of context specific language models: the case of the Erasmian Language Model](https://arxiv.org/abs/2408.06931)
Append: [lamss: when large language models meet self-skepticism](https://arxiv.org/abs/2409.06601)
Append: [ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems](https://arxiv.org/abs/2410.19572)
Append: [MEG: Medical Knowledge-Augmented Large Language Models for Question Answering](https://arxiv.org/abs/2411.03883)
Append: [Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/abs/2411.05000)
Append: [Sufficient Context: A New Lens on Retrieval Augmented Generation Systems](https://arxiv.org/abs/2411.06037)
Append: [7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/abs/2412.06845)
Append: [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)
Append: [Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?](https://arxiv.org/abs/2502.04718)
Append: [Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization](https://arxiv.org/abs/2502.13108)
Append: [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](https://arxiv.org/abs/2503.01840)
Append: [Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations](https://arxiv.org/abs/2503.14477)
Append: [Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models](https://arxiv.org/abs/2503.16419)
Append: [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](https://arxiv.org/abs/2503.20533)
Append: [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)
Append: [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2404.04545)
Append: [Multimodal Situational Safety](https://arxiv.org/abs/2410.06172)
Append: [Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers](https://arxiv.org/abs/2410.22663)
Append: [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/abs/2411.18203)
Append: [StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist](https://arxiv.org/abs/2412.06412)
Append: [MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants](https://arxiv.org/abs/2412.12661)
Append: [Lorecast: Layout-Aware Performance and Power Forecasting from Natural Language](https://arxiv.org/abs/2503.11662)
Append: [Dynamic hashtag recommendation in social media with trend shift detection and adaptation](https://arxiv.org/abs/2504.00044)
Append: [Analyzing 16,193 LLM Papers for Fun and Profits](https://arxiv.org/abs/2504.08619)
append_entries: 83
Finish: 2025-04-24 04:25:54.807250
------------------------------------------------------
Started: 2025-04-24 06:23:36.982518
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 06:23:37.215010
------------------------------------------------------
Started: 2025-04-24 08:21:31.629261
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 08:21:31.818649
------------------------------------------------------
Started: 2025-04-24 10:17:44.427132
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 10:17:44.644702
------------------------------------------------------
Started: 2025-04-24 12:33:41.557085
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 12:33:41.775548
------------------------------------------------------
Started: 2025-04-24 14:15:17.148893
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 14:15:17.336330
------------------------------------------------------
Started: 2025-04-24 16:20:00.365624
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 16:20:00.579474
------------------------------------------------------
Started: 2025-04-24 18:21:39.126113
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 18:21:39.315127
------------------------------------------------------
Started: 2025-04-24 20:18:03.965115
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 20:18:04.153653
------------------------------------------------------
Started: 2025-04-24 22:15:20.157299
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 22:15:20.375474
------------------------------------------------------
Started: 2025-04-25 01:16:38.936972
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 01:16:39.121732
------------------------------------------------------
Started: 2025-04-25 03:03:50.691430
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 03:03:50.878160
------------------------------------------------------
Started: 2025-04-25 04:21:30.354646
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1362
Summarized using GPT-3.5-turbo
Append: [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
Token length: 1408
Summarized using GPT-3.5-turbo
Append: [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)
Token length: 1883
Summarized using GPT-3.5-turbo
Append: [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)
Token length: 1101
Summarized using GPT-3.5-turbo
Append: [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)
Token length: 992
Summarized using GPT-3.5-turbo
Append: [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)
Token length: 1584
Summarized using GPT-3.5-turbo
Append: [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
Token length: 1656
Summarized using GPT-3.5-turbo
Append: [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)
Token length: 836
Summarized using GPT-3.5-turbo
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Token length: 1053
Summarized using GPT-3.5-turbo
Append: [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)
Token length: 1702
Summarized using GPT-3.5-turbo
Append: [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
Token length: 1890
Summarized using GPT-3.5-turbo
Append: [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
Token length: 874
Summarized using GPT-3.5-turbo
Append: [SCALAR: A Part-of-speech Tagger for Identifiers](https://arxiv.org/abs/2504.17038)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
Token length: 1960
Summarized using GPT-3.5-turbo
Append: [CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization](https://arxiv.org/abs/2406.07494)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure](https://arxiv.org/abs/2406.17276)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine](https://arxiv.org/abs/2409.18986)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://arxiv.org/abs/2409.19151)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://arxiv.org/abs/2410.01952)
Append: [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)
Append: [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)
Append: [Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies](https://arxiv.org/abs/2410.19878)
Append: [Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach](https://arxiv.org/abs/2411.04950)
Append: [jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images](https://arxiv.org/abs/2412.08802)
Append: [Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing](https://arxiv.org/abs/2501.14936)
Append: [Multilingual State Space Models for Structured Question Answering in Indic Languages](https://arxiv.org/abs/2502.01673)
Append: [Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models](https://arxiv.org/abs/2502.05346)
Append: [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569)
Append: [PSCon: Product Search Through Conversations](https://arxiv.org/abs/2502.13881)
Append: [Automatically Evaluating the Paper Reviewing Capability of Large Language Models](https://arxiv.org/abs/2502.17086)
Append: [SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection](https://arxiv.org/abs/2503.07269)
Append: [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)
Append: [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)
Append: [Cognitive Memory in Large Language Models](https://arxiv.org/abs/2504.02441)
Append: [Not All Data Are Unlearned Equally](https://arxiv.org/abs/2504.05058)
Append: [Multilingual MFA: Forced Alignment on Low-Resource Related Languages](https://arxiv.org/abs/2504.07315)
Append: [Transferable text data distillation by trajectory matching](https://arxiv.org/abs/2504.09818)
Append: [ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation](https://arxiv.org/abs/2406.14088)
Append: [Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF](https://arxiv.org/abs/2410.04612)
Append: [CallNavi, A Challenge and Empirical Study on LLM Function Calling and Routing](https://arxiv.org/abs/2501.05255)
Append: [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
Append: [Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](https://arxiv.org/abs/2503.10742)
Append: [Looking beyond the next token](https://arxiv.org/abs/2504.11336)
Append: [Teaching Large Language Models to Reason through Learning and Forgetting](https://arxiv.org/abs/2504.11364)
append_entries: 75
Finish: 2025-04-25 04:23:26.404476
------------------------------------------------------
Started: 2025-04-25 06:22:52.147618
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 06:22:52.370816
------------------------------------------------------
Started: 2025-04-25 08:21:37.045192
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 08:21:37.296065
------------------------------------------------------
Started: 2025-04-25 10:17:06.515090
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 10:17:06.699985
------------------------------------------------------
Started: 2025-04-25 12:32:33.596439
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 12:32:33.777022
------------------------------------------------------
Started: 2025-04-25 14:15:10.371478
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 14:15:10.584149
------------------------------------------------------
Started: 2025-04-25 16:19:37.968500
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 16:19:38.156762
------------------------------------------------------
Started: 2025-04-25 18:21:48.634558
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 18:21:48.842632
------------------------------------------------------
Started: 2025-04-25 20:17:46.391295
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 20:17:46.637887
------------------------------------------------------
Started: 2025-04-25 22:15:39.484283
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 22:15:39.724851
------------------------------------------------------
Started: 2025-04-26 01:13:49.773938
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 01:13:49.988037
------------------------------------------------------
Started: 2025-04-26 02:57:28.791726
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 02:57:29.025375
------------------------------------------------------
Started: 2025-04-26 04:18:23.245536
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 04:18:23.302458
------------------------------------------------------
Started: 2025-04-26 06:20:46.878442
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 06:20:46.961783
------------------------------------------------------
Started: 2025-04-26 08:18:43.599009
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 08:18:43.670760
------------------------------------------------------
Started: 2025-04-26 10:15:17.019775
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 10:15:17.091715
------------------------------------------------------
Started: 2025-04-26 12:28:37.289104
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 12:28:37.359790
------------------------------------------------------
Started: 2025-04-26 14:13:16.409050
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 14:13:16.482023
------------------------------------------------------
Started: 2025-04-26 16:18:26.092728
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 16:18:26.166364
------------------------------------------------------
Started: 2025-04-26 18:19:44.152571
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 18:19:44.240748
------------------------------------------------------
Started: 2025-04-26 20:15:43.516778
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 20:15:43.577322
------------------------------------------------------
Started: 2025-04-26 22:14:03.104258
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 22:14:03.192620
------------------------------------------------------
Started: 2025-04-27 01:20:37.743123
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 01:20:37.796413
------------------------------------------------------
Started: 2025-04-27 03:06:29.498566
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 03:06:29.552011
------------------------------------------------------
Started: 2025-04-27 04:18:17.123855
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 04:18:17.182030
------------------------------------------------------
Started: 2025-04-27 06:20:58.862543
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 06:20:58.920000
------------------------------------------------------
Started: 2025-04-27 08:18:50.720727
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 08:18:50.817021
------------------------------------------------------
Started: 2025-04-27 10:16:12.208862
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 10:16:12.277405
------------------------------------------------------
Started: 2025-04-27 12:28:55.734421
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 12:28:55.816515
------------------------------------------------------
Started: 2025-04-27 14:13:24.908627
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 14:13:24.965676
------------------------------------------------------
Started: 2025-04-27 16:18:11.271664
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 16:18:11.329220
------------------------------------------------------
Started: 2025-04-27 18:19:52.750001
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 18:19:52.802594
------------------------------------------------------
Started: 2025-04-27 20:16:35.406210
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 20:16:35.458989
------------------------------------------------------
Started: 2025-04-27 22:14:45.710258
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 22:14:45.793573
------------------------------------------------------
Started: 2025-04-28 01:18:28.296301
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 01:18:28.377376
------------------------------------------------------
Started: 2025-04-28 03:08:03.661980
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 03:08:03.718214
------------------------------------------------------
Started: 2025-04-28 04:24:01.274055
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/abs/2504.17974)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Improving LLM Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation](https://arxiv.org/abs/2504.18012)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Exploring Personality-Aware Interactions in Salesperson Dialogue Agents](https://arxiv.org/abs/2504.18058)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization](https://arxiv.org/abs/2504.18080)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [Random-Set Large Language Models](https://arxiv.org/abs/2504.18085)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation](https://arxiv.org/abs/2504.18104)
Token length: 928
Summarized using GPT-3.5-turbo
Append: [Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering](https://arxiv.org/abs/2504.18106)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Temporal Entailment Pretraining for Clinical Language Models over EHR Data](https://arxiv.org/abs/2504.18128)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)](https://arxiv.org/abs/2504.18142)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Aligning Language Models for Icelandic Legal Text Summarization](https://arxiv.org/abs/2504.18180)
Token length: 718
Summarized using GPT-3.5-turbo
Append: [Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish](https://arxiv.org/abs/2504.18221)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Efficient Single-Pass Training for Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [MAGI: Multi-Agent Guided Interview for Psychiatric Assessment](https://arxiv.org/abs/2504.18260)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation](https://arxiv.org/abs/2504.18269)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Pushing the boundary on Natural Language Inference](https://arxiv.org/abs/2504.18376)
Token length: 829
Summarized using GPT-3.5-turbo
Append: [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412)
Token length: 900
Summarized using GPT-3.5-turbo
Append: [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
Token length: 1075
Summarized using GPT-3.5-turbo
Append: [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions](https://arxiv.org/abs/2504.18474)
Token length: 1182
Summarized using GPT-3.5-turbo
Append: [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
Token length: 1913
Summarized using GPT-3.5-turbo
Append: [Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval](https://arxiv.org/abs/2504.17884)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Token Sequence Compression for Efficient Multimodal Computing](https://arxiv.org/abs/2504.17892)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [CAMU: Context Augmentation for Meme Understanding](https://arxiv.org/abs/2504.17902)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning](https://arxiv.org/abs/2504.17950)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [SMARTFinRAG: Interactive Modularized Financial RAG Benchmark](https://arxiv.org/abs/2504.18024)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture](https://arxiv.org/abs/2504.18099)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Kimi-Audio Technical Report](https://arxiv.org/abs/2504.18425)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [PRobELM: Plausibility Ranking Evaluation for Language Models](https://arxiv.org/abs/2404.03818)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arxiv.org/abs/2405.19325)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction](https://arxiv.org/abs/2406.10432)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Multilingual Large Language Models and Curse of Multilinguality](https://arxiv.org/abs/2406.10602)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings](https://arxiv.org/abs/2408.16073)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Your Weak LLM is Secretly a Strong Teacher for Alignment](https://arxiv.org/abs/2409.08813)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning](https://arxiv.org/abs/2409.12059)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"](https://arxiv.org/abs/2410.03727)
Append: [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
Append: [ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data](https://arxiv.org/abs/2412.11704)
Append: [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)
Append: [EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2502.12486)
Append: [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)
Append: [LRAGE: Legal Retrieval Augmented Generation Evaluation Tool](https://arxiv.org/abs/2504.01840)
Append: [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810)
Append: [Can Reasoning LLMs Enhance Clinical Document Classification?](https://arxiv.org/abs/2504.08040)
Append: [Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio](https://arxiv.org/abs/1711.08058)
Append: [M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models](https://arxiv.org/abs/2405.15638)
Append: [Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition](https://arxiv.org/abs/2406.02566)
Append: [GOFA: A Generative One-For-All Model for Joint Graph Language Modeling](https://arxiv.org/abs/2407.09709)
Append: [Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs](https://arxiv.org/abs/2408.06621)
Append: [Adaptive Uncertainty Quantification for Generative AI](https://arxiv.org/abs/2408.08990)
Append: [MIND: Math Informed syNthetic Dialogues for Pretraining LLMs](https://arxiv.org/abs/2410.12881)
Append: [Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification](https://arxiv.org/abs/2411.01841)
Append: [Repurposing the scientific literature with vision-language models](https://arxiv.org/abs/2502.19546)
Append: [Spatial Audio Processing with Large Language Model on Wearable Devices](https://arxiv.org/abs/2504.08907)
append_entries: 68
Finish: 2025-04-28 04:25:53.968059
------------------------------------------------------
Started: 2025-04-28 06:30:42.394218
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 06:30:42.588139
------------------------------------------------------
Started: 2025-04-28 08:57:44.055561
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 08:57:44.246486
------------------------------------------------------
Started: 2025-04-28 11:03:55.719388
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 11:03:55.907081
------------------------------------------------------
Started: 2025-04-28 12:33:56.928650
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 12:33:57.121104
------------------------------------------------------
Started: 2025-04-28 14:17:38.964210
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 14:17:39.207577
------------------------------------------------------
Started: 2025-04-28 16:19:30.632004
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 16:19:30.821051
------------------------------------------------------
Started: 2025-04-28 18:23:05.315985
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 18:23:05.536692
------------------------------------------------------
Started: 2025-04-28 20:17:28.413307
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 20:17:28.603960
------------------------------------------------------
Started: 2025-04-28 22:15:51.625514
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 22:15:51.814415
------------------------------------------------------
Started: 2025-04-29 01:16:35.727114
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 01:16:35.922910
------------------------------------------------------
Started: 2025-04-29 03:04:03.381857
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 03:04:03.574013
------------------------------------------------------
Started: 2025-04-29 04:22:36.669946
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 751
Summarized using GPT-3.5-turbo
Append: [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Can Third-parties Read Our Emotions?](https://arxiv.org/abs/2504.18673)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation](https://arxiv.org/abs/2504.18805)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks](https://arxiv.org/abs/2504.18838)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning](https://arxiv.org/abs/2504.18839)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [When2Call: When (not) to Call Tools](https://arxiv.org/abs/2504.18851)
Token length: 1615
Summarized using GPT-3.5-turbo
Append: [Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation](https://arxiv.org/abs/2504.18857)
Token length: 1706
Summarized using GPT-3.5-turbo
Append: [Latent Adversarial Training Improves the Representation of Refusal](https://arxiv.org/abs/2504.18872)
Token length: 701
Summarized using GPT-3.5-turbo
Append: [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/abs/2504.18884)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)
Token length: 1904
Summarized using GPT-3.5-turbo
Append: [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://arxiv.org/abs/2504.18992)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)
Token length: 817
Summarized using GPT-3.5-turbo
Append: [KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation](https://arxiv.org/abs/2504.19024)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics](https://arxiv.org/abs/2504.19066)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Sample-Efficient Language Model for Hinglish Conversational AI](https://arxiv.org/abs/2504.19070)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
Token length: 1726
Summarized using GPT-3.5-turbo
Append: [Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19101)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [WuNeng: Hybrid State with Attention](https://arxiv.org/abs/2504.19191)
Token length: 777
Summarized using GPT-3.5-turbo
Append: [Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora](https://arxiv.org/abs/2504.19209)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [AndroidGen: Building an Android Language Agent under Data Scarcity](https://arxiv.org/abs/2504.19298)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [Context Selection and Rewriting for Video-based EducationalQuestion Generation](https://arxiv.org/abs/2504.19406)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models](https://arxiv.org/abs/2504.19436)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks](https://arxiv.org/abs/2504.19445)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [Conflicts in Texts: Data, Implications and Challenges](https://arxiv.org/abs/2504.19472)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
Token length: 662
Summarized using GPT-3.5-turbo
Append: [Arabic Metaphor Sentiment Classification Using Semantic Information](https://arxiv.org/abs/2504.19590)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Coreference Resolution for Vietnamese Narrative Texts](https://arxiv.org/abs/2504.19606)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks](https://arxiv.org/abs/2504.19645)
Append: [Multimodal Conditioned Diffusive Time Series Forecasting](https://arxiv.org/abs/2504.19669)
Append: [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)
Append: [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)
Append: [LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding](https://arxiv.org/abs/2504.19734)
Append: [Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs](https://arxiv.org/abs/2504.19759)
Append: [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)
Append: [To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels](https://arxiv.org/abs/2504.19850)
Append: [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
Append: [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](https://arxiv.org/abs/2504.19867)
Append: [GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets](https://arxiv.org/abs/2504.19898)
Append: [Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking](https://arxiv.org/abs/2504.19940)
Append: [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)
Append: [Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom](https://arxiv.org/abs/2504.20000)
Append: [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)
Append: [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)
Append: [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)
Append: [Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines](https://arxiv.org/abs/2504.18596)
Append: [Generative Product Recommendations for Implicit Superlative Queries](https://arxiv.org/abs/2504.18748)
Append: [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919)
Append: [LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings](https://arxiv.org/abs/2504.18988)
Append: [Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions](https://arxiv.org/abs/2504.19056)
Append: [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
Append: [Hierarchical Attention Generates Better Proofs](https://arxiv.org/abs/2504.19188)
Append: [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://arxiv.org/abs/2504.19276)
Append: [Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks](https://arxiv.org/abs/2504.19444)
Append: [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
Append: [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://arxiv.org/abs/2504.19483)
Append: [Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2504.19500)
Append: [FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation](https://arxiv.org/abs/2504.19519)
Append: [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
Append: [Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge](https://arxiv.org/abs/2504.19730)
Append: [Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19754)
Append: [A Bayesian approach to modeling topic-metadata relationships](https://arxiv.org/abs/2104.02496)
Append: [Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information](https://arxiv.org/abs/2110.08420)
Append: [Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach](https://arxiv.org/abs/2112.06876)
Append: [Generative Meta-Learning for Zero-Shot Relation Triplet Extraction](https://arxiv.org/abs/2305.01920)
Append: [Benchmarking large language models for biomedical natural language processing applications and recommendations](https://arxiv.org/abs/2305.16326)
Append: [Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)
Append: [Large language models for newspaper sentiment analysis during COVID-19: The Guardian](https://arxiv.org/abs/2405.13056)
Append: [Fake News Detection: It's All in the Data!](https://arxiv.org/abs/2407.02122)
Append: [Pula: Training Large Language Models for Setswana](https://arxiv.org/abs/2408.02239)
Append: [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://arxiv.org/abs/2408.05906)
Append: [W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering](https://arxiv.org/abs/2408.08444)
Append: [Data Processing for the OpenGPT-X Model Family](https://arxiv.org/abs/2410.08800)
Append: [Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling](https://arxiv.org/abs/2410.11325)
Append: [Open Domain Question Answering with Conflicting Contexts](https://arxiv.org/abs/2410.12311)
Append: [From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization](https://arxiv.org/abs/2410.13961)
Append: [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030)
Append: [An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese](https://arxiv.org/abs/2411.17270)
Append: [Investigating Length Issues in Document-level Machine Translation](https://arxiv.org/abs/2412.17592)
Append: [LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context](https://arxiv.org/abs/2412.17596)
Append: [Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora](https://arxiv.org/abs/2502.00090)
Append: [InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context](https://arxiv.org/abs/2502.12257)
Append: [Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13019)
Append: [LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning](https://arxiv.org/abs/2502.14644)
Append: [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://arxiv.org/abs/2502.15147)
Append: [Protecting multimodal large language models against misleading visualizations](https://arxiv.org/abs/2502.20503)
Append: [Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models](https://arxiv.org/abs/2503.10617)
Append: [Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish](https://arxiv.org/abs/2504.09714)
Append: [The Semantic Scholar Open Data Platform](https://arxiv.org/abs/2301.10140)
Append: [NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models](https://arxiv.org/abs/2303.10430)
Append: [Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans](https://arxiv.org/abs/2307.12369)
Append: [Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation](https://arxiv.org/abs/2403.19103)
Append: [Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/abs/2403.20331)
Append: [GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation](https://arxiv.org/abs/2409.03140)
Append: [AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://arxiv.org/abs/2409.09013)
Append: [Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization](https://arxiv.org/abs/2410.08847)
Append: [CREAM: Consistency Regularized Self-Rewarding Language Models](https://arxiv.org/abs/2410.12735)
Append: [Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models](https://arxiv.org/abs/2410.18252)
Append: [A Guide to Misinformation Detection Data and Evaluation](https://arxiv.org/abs/2411.05060)
Append: [An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training](https://arxiv.org/abs/2501.15579)
Append: [Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering](https://arxiv.org/abs/2502.09573)
Append: [Exploring LLM-based Student Simulation for Metacognitive Cultivation](https://arxiv.org/abs/2502.11678)
Append: [NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence](https://arxiv.org/abs/2502.20601)
Append: [Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking](https://arxiv.org/abs/2504.03947)
Append: [Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use](https://arxiv.org/abs/2504.04736)
Append: [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089)
Append: [Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection](https://arxiv.org/abs/2504.17834)
append_entries: 129
Finish: 2025-04-29 04:24:34.291533
------------------------------------------------------
Started: 2025-04-29 06:24:30.171459
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 06:24:30.486505
------------------------------------------------------
Started: 2025-04-29 08:22:47.034261
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 08:22:47.357448
------------------------------------------------------
Started: 2025-04-29 10:18:27.720219
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 10:18:28.043674
------------------------------------------------------
Started: 2025-04-29 12:34:40.885194
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 12:34:41.210261
------------------------------------------------------
Started: 2025-04-29 14:16:39.217356
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 14:16:39.544472
------------------------------------------------------
Started: 2025-04-29 16:20:02.492784
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 16:20:02.808175
------------------------------------------------------
Started: 2025-04-29 18:22:13.455317
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 18:22:13.839323
------------------------------------------------------
Started: 2025-04-29 20:18:07.124266
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 20:18:07.444309
------------------------------------------------------
Started: 2025-04-29 22:15:17.365462
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 22:15:17.718552
------------------------------------------------------
Started: 2025-04-30 01:16:44.203366
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 01:16:44.552175
------------------------------------------------------
Started: 2025-04-30 03:03:54.454656
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 03:03:54.786597
------------------------------------------------------
Started: 2025-04-30 04:23:40.425225
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1831
Summarized using GPT-3.5-turbo
Append: [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
Token length: 419
Summarized using GPT-3.5-turbo
Append: [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
Token length: 653
Summarized using GPT-3.5-turbo
Append: [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
Token length: 1121
Summarized using GPT-3.5-turbo
Append: [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
Token length: 1032
Summarized using GPT-3.5-turbo
Append: [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
Token length: 801
Summarized using GPT-3.5-turbo
Append: [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
Token length: 967
Summarized using GPT-3.5-turbo
Append: [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
Token length: 827
Summarized using GPT-3.5-turbo
Append: [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
Token length: 493
Summarized using GPT-3.5-turbo
Append: [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
Token length: 855
Summarized using GPT-3.5-turbo
Append: [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
Token length: 1465
Summarized using GPT-3.5-turbo
Append: [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
Token length: 833
Summarized using GPT-3.5-turbo
Append: [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence](https://arxiv.org/abs/2504.20059)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
Token length: 1895
Summarized using GPT-3.5-turbo
Append: [AI Awareness](https://arxiv.org/abs/2504.20084)
Token length: 1091
Summarized using GPT-3.5-turbo
Append: [MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?](https://arxiv.org/abs/2504.20094)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies](https://arxiv.org/abs/2504.20117)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
Token length: 1530
Summarized using GPT-3.5-turbo
Append: [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
Token length: 1884
Summarized using GPT-3.5-turbo
Append: [Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](https://arxiv.org/abs/2504.20458)
Append: [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
Append: [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
Append: [X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.20859)
Append: [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
Append: [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
Append: [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
Append: [Semantic Consistency for Assuring Reliability of Large Language Models](https://arxiv.org/abs/2308.09138)
Append: [LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models](https://arxiv.org/abs/2310.03903)
Append: [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education](https://arxiv.org/abs/2310.12059)
Append: [Agentic AI: The Era of Semantic Decoding](https://arxiv.org/abs/2403.14562)
Append: [A Practical Analysis of Human Alignment with *PO](https://arxiv.org/abs/2407.15229)
Append: [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://arxiv.org/abs/2408.00137)
Append: [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://arxiv.org/abs/2409.07394)
Append: [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://arxiv.org/abs/2410.02102)
Append: [Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context](https://arxiv.org/abs/2410.07103)
Append: [MDCure: A Scalable Pipeline for Multi-Document Instruction-Following](https://arxiv.org/abs/2410.23463)
Append: [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175)
Append: [Benchmarking LLMs' Judgments with No Gold Standard](https://arxiv.org/abs/2411.07127)
Append: [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)
Append: [A Bayesian Optimization Approach to Machine Translation Reranking](https://arxiv.org/abs/2411.09694)
Append: [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)
Append: [An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation](https://arxiv.org/abs/2502.12836)
Append: [MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation](https://arxiv.org/abs/2502.17163)
Append: [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/abs/2503.15358)
Append: [CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation](https://arxiv.org/abs/2503.19878)
Append: [Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users](https://arxiv.org/abs/2504.00799)
Append: [LLM-based Automated Grading with Human-in-the-Loop](https://arxiv.org/abs/2504.05239)
Append: [Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation](https://arxiv.org/abs/2504.07072)
Append: [Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2303.01903)
Append: [Pose-Based Sign Language Appearance Transfer](https://arxiv.org/abs/2410.13675)
Append: [DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators](https://arxiv.org/abs/2412.02467)
Append: [SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering](https://arxiv.org/abs/2412.06832)
Append: [Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models](https://arxiv.org/abs/2501.12433)
Append: [From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors](https://arxiv.org/abs/2501.18045)
Append: [REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations](https://arxiv.org/abs/2502.03629)
Append: [The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation](https://arxiv.org/abs/2503.04606)
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Append: [LocAgent: Graph-Guided LLM Agents for Code Localization](https://arxiv.org/abs/2503.09089)
append_entries: 88
Finish: 2025-04-30 04:25:44.527424
------------------------------------------------------
Started: 2025-04-30 06:23:15.361193
Existing_entries: 1036
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 06:23:15.618642
------------------------------------------------------
Started: 2025-04-30 08:21:50.001071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 08:21:50.237415
------------------------------------------------------
Started: 2025-04-30 10:17:34.865107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 10:17:35.137225
------------------------------------------------------
Started: 2025-04-30 12:32:19.416250
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 12:32:19.691571
------------------------------------------------------
Started: 2025-04-30 14:15:51.805005
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 14:15:52.072578
------------------------------------------------------
Started: 2025-04-30 16:20:13.711852
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 16:20:13.952481
------------------------------------------------------
Started: 2025-04-30 18:22:26.118946
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 18:22:26.394410
------------------------------------------------------
Started: 2025-04-30 20:17:52.505808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 20:17:52.744465
------------------------------------------------------
Started: 2025-04-30 22:15:17.634986
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 22:15:17.869587
------------------------------------------------------
Started: 2025-05-01 01:23:25.222117
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 01:23:25.488610
------------------------------------------------------
Started: 2025-05-01 03:14:18.921097
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 03:14:19.188706
------------------------------------------------------
Started: 2025-05-01 04:25:19.936055
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
Token length: 664
Summarized using GPT-3.5-turbo
Append: [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
Token length: 1174
Summarized using GPT-3.5-turbo
Append: [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Param$\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
Token length: 967
Summarized using GPT-3.5-turbo
Append: [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
Token length: 1767
Summarized using GPT-3.5-turbo
Append: [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
Token length: 1922
Summarized using GPT-3.5-turbo
Append: [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
Token length: 866
Summarized using GPT-3.5-turbo
Append: [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
Token length: 811
Summarized using GPT-3.5-turbo
Append: [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
Token length: 834
Summarized using GPT-3.5-turbo
Append: [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
Token length: 924
Summarized using GPT-3.5-turbo
Append: [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
Token length: 778
Summarized using GPT-3.5-turbo
Append: [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
Token length: 807
Summarized using GPT-3.5-turbo
Append: [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
Token length: 867
Summarized using GPT-3.5-turbo
Append: [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval](https://arxiv.org/abs/2504.21015)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage](https://arxiv.org/abs/2504.21035)
Append: [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
Append: [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
Append: [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400)
Append: [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
Append: [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
Append: [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
Append: [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
Append: [LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics](https://arxiv.org/abs/2504.21716)
Append: [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
Append: [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
Append: [LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection](https://arxiv.org/abs/2310.18964)
Append: [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)
Append: [Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs](https://arxiv.org/abs/2404.19442)
Append: [Emergence of a High-Dimensional Abstraction Phase in Language Transformers](https://arxiv.org/abs/2405.15471)
Append: [Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models](https://arxiv.org/abs/2410.07825)
Append: [Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent](https://arxiv.org/abs/2410.16658)
Append: [KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities](https://arxiv.org/abs/2501.00571)
Append: [Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification](https://arxiv.org/abs/2502.11258)
Append: [Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice](https://arxiv.org/abs/2503.04785)
Append: [JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System](https://arxiv.org/abs/2503.14258)
Append: [Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/abs/2503.21934)
Append: [VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge](https://arxiv.org/abs/2504.10342)
Append: [Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding](https://arxiv.org/abs/2301.11564)
Append: [SignLLM: Sign Language Production Large Language Models](https://arxiv.org/abs/2405.10718)
Append: [HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes](https://arxiv.org/abs/2408.05794)
Append: [Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance](https://arxiv.org/abs/2409.15545)
Append: [Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction](https://arxiv.org/abs/2409.15551)
Append: [Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models](https://arxiv.org/abs/2409.16920)
Append: [Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling](https://arxiv.org/abs/2409.16937)
Append: [Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations](https://arxiv.org/abs/2409.17899)
Append: [How to Construct Random Unitaries](https://arxiv.org/abs/2410.10116)
Append: [Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion](https://arxiv.org/abs/2411.08165)
Append: [All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages](https://arxiv.org/abs/2411.16508)
Append: [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/abs/2412.12119)
Append: [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723)
Append: [Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews](https://arxiv.org/abs/2502.05439)
Append: [Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training](https://arxiv.org/abs/2502.12734)
Append: [SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations](https://arxiv.org/abs/2502.16949)
Append: [Learning Code-Edit Embedding to Model Student Debugging Behavior](https://arxiv.org/abs/2502.19407)
Append: [AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language](https://arxiv.org/abs/2503.01453)
Append: [Urban Computing in the Era of Large Language Models](https://arxiv.org/abs/2504.02009)
append_entries: 91
Finish: 2025-05-01 04:27:19.501001
------------------------------------------------------
Started: 2025-05-01 06:24:07.644520
Existing_entries: 1091
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 06:24:07.915664
------------------------------------------------------
Started: 2025-05-01 08:21:25.162194
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 08:21:25.453475
------------------------------------------------------
Started: 2025-05-01 10:17:37.752923
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 10:17:37.987974
------------------------------------------------------
Started: 2025-05-01 12:31:33.382774
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 12:31:33.655939
------------------------------------------------------
Started: 2025-05-01 14:14:59.420908
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 14:14:59.676124
------------------------------------------------------
Started: 2025-05-01 16:20:18.825099
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 16:20:19.095232
------------------------------------------------------
Started: 2025-05-01 18:22:26.998973
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 18:22:27.231121
------------------------------------------------------
Started: 2025-05-01 20:16:40.740301
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 20:16:41.003808
------------------------------------------------------
Started: 2025-05-01 22:15:22.977906
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 22:15:23.214372
------------------------------------------------------
Started: 2025-05-02 01:17:00.594983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 01:17:00.874513
------------------------------------------------------
Started: 2025-05-02 03:05:38.010694
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 03:05:38.274249
------------------------------------------------------
Started: 2025-05-02 04:22:47.078995
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/abs/2505.00001)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [Symbol grounding in computational systems: A paradox of intentions](https://arxiv.org/abs/2505.00002)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs](https://arxiv.org/abs/2505.00003)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [LangVAE and LangSpace: Building and Probing for Language Model VAEs](https://arxiv.org/abs/2505.00004)
Token length: 979
Summarized using GPT-3.5-turbo
Append: [Toward a digital twin of U.S. Congress](https://arxiv.org/abs/2505.00006)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination](https://arxiv.org/abs/2505.00008)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation](https://arxiv.org/abs/2505.00009)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)
Token length: 595
Summarized using GPT-3.5-turbo
Append: [The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?](https://arxiv.org/abs/2505.00012)
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa](https://arxiv.org/abs/2505.00013)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and M\"obius Strips](https://arxiv.org/abs/2505.00014)
Token length: 1884
Summarized using GPT-3.5-turbo
Append: [Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation](https://arxiv.org/abs/2505.00015)
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning](https://arxiv.org/abs/2505.00016)
Token length: 574
Summarized using GPT-3.5-turbo
Append: [ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation](https://arxiv.org/abs/2505.00017)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [An Empirical Study on Prompt Compression for Large Language Models](https://arxiv.org/abs/2505.00019)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [Beyond Public Access in LLM Pre-Training Data](https://arxiv.org/abs/2505.00020)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss](https://arxiv.org/abs/2505.00021)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [CORG: Generating Answers from Complex, Interrelated Contexts](https://arxiv.org/abs/2505.00023)
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://arxiv.org/abs/2505.00024)
Token length: 1728
Summarized using GPT-3.5-turbo
Append: [A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1](https://arxiv.org/abs/2505.00025)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [Theory of Mind in Large Language Models: Assessment and Enhancement](https://arxiv.org/abs/2505.00026)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts](https://arxiv.org/abs/2505.00027)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation](https://arxiv.org/abs/2505.00028)
Token length: 1544
Summarized using GPT-3.5-turbo
Append: [Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting](https://arxiv.org/abs/2505.00029)
Token length: 636
Summarized using GPT-3.5-turbo
Append: [Can Language Models Represent the Past without Anachronism?](https://arxiv.org/abs/2505.00030)
Token length: 1610
Summarized using GPT-3.5-turbo
Append: [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://arxiv.org/abs/2505.00031)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2505.00032)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models](https://arxiv.org/abs/2505.00033)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Improving Phishing Email Detection Performance of Small Large Language Models](https://arxiv.org/abs/2505.00034)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics](https://arxiv.org/abs/2505.00035)
Token length: 1810
Summarized using GPT-3.5-turbo
Append: [A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies](https://arxiv.org/abs/2505.00036)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: [HyPerAlign: Hypotheses-driven Personalized Alignment](https://arxiv.org/abs/2505.00038)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Base Models Beat Aligned Models at Randomness and Creativity](https://arxiv.org/abs/2505.00047)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting](https://arxiv.org/abs/2505.00050)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity](https://arxiv.org/abs/2505.00056)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [A Report on the llms evaluating the high school questions](https://arxiv.org/abs/2505.00057)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition](https://arxiv.org/abs/2505.00059)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5](https://arxiv.org/abs/2505.00060)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems](https://arxiv.org/abs/2505.00061)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [ConSens: Assessing context grounding in open-book question answering](https://arxiv.org/abs/2505.00065)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese](https://arxiv.org/abs/2505.00114)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs](https://arxiv.org/abs/2505.00127)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models](https://arxiv.org/abs/2505.00147)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports](https://arxiv.org/abs/2505.00191)
Token length: 828
Summarized using GPT-3.5-turbo
Append: [Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring](https://arxiv.org/abs/2505.00261)
Token length: 998
Summarized using GPT-3.5-turbo
Append: [Consistency in Language Models: Current Landscape, Challenges, and Future Directions](https://arxiv.org/abs/2505.00268)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation](https://arxiv.org/abs/2505.00339)
Append: [KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis](https://arxiv.org/abs/2505.00367)
Append: [CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass](https://arxiv.org/abs/2505.00389)
Append: [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)
Append: [Computational Identification of Regulatory Statements in EU Legislation](https://arxiv.org/abs/2505.00479)
Append: [HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection](https://arxiv.org/abs/2505.00506)
Append: [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)
Append: [Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models](https://arxiv.org/abs/2505.00557)
Append: [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
Append: [Block Circulant Adapter for Large Language Models](https://arxiv.org/abs/2505.00582)
Append: [FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation](https://arxiv.org/abs/2505.00624)
Append: [The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)](https://arxiv.org/abs/2505.00626)
Append: [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/abs/2505.00654)
Append: [On the generalization of language models from in-context learning and finetuning: a controlled study](https://arxiv.org/abs/2505.00661)
Append: [DeepCritic: Deliberate Critique with Large Language Models](https://arxiv.org/abs/2505.00662)
Append: [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
Append: [Steering Large Language Models with Register Analysis for Arbitrary Style Transfer](https://arxiv.org/abs/2505.00679)
Append: [Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications](https://arxiv.org/abs/2505.00049)
Append: [Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques](https://arxiv.org/abs/2505.00105)
Append: [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/abs/2505.00150)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
Append: [EnronQA: Towards Personalized RAG over Private Documents](https://arxiv.org/abs/2505.00263)
Append: [Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing](https://arxiv.org/abs/2505.00315)
Append: [T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation](https://arxiv.org/abs/2505.00337)
Append: [R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training](https://arxiv.org/abs/2505.00358)
Append: [Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training](https://arxiv.org/abs/2505.00422)
Append: [Investigating Task Arithmetic for Zero-Shot Information Retrieval](https://arxiv.org/abs/2505.00649)
Append: [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)
Append: [EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532)
Append: [LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning](https://arxiv.org/abs/2401.15371)
Append: ["Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments](https://arxiv.org/abs/2402.08498)
Append: [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532)
Append: [(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts](https://arxiv.org/abs/2405.11804)
Append: [Automated Review Generation Method Based on Large Language Models](https://arxiv.org/abs/2407.20906)
Append: [Challenges and Future Directions of Data-Centric AI Alignment](https://arxiv.org/abs/2410.01957)
Append: [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://arxiv.org/abs/2410.20774)
Append: [Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis](https://arxiv.org/abs/2412.05862)
Append: [A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://arxiv.org/abs/2501.13947)
Append: [HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models](https://arxiv.org/abs/2502.05945)
Append: [Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?](https://arxiv.org/abs/2502.07963)
Append: [UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation](https://arxiv.org/abs/2502.20984)
Append: [Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](https://arxiv.org/abs/2503.23895)
Append: [Opioid Named Entity Recognition (ONER-2025) from Reddit](https://arxiv.org/abs/2504.00027)
Append: [LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem](https://arxiv.org/abs/2403.00108)
Append: [Large Language Model Agent as a Mechanical Designer](https://arxiv.org/abs/2404.17525)
Append: [Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers](https://arxiv.org/abs/2405.04620)
Append: [TaeBench: Improving Quality of Toxic Adversarial Examples](https://arxiv.org/abs/2410.05573)
Append: [Bridging Personalization and Control in Scientific Personalized Search](https://arxiv.org/abs/2411.02790)
Append: [Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner](https://arxiv.org/abs/2412.18086)
Append: [Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking](https://arxiv.org/abs/2501.18265)
Append: [Efficient Reinforcement Finetuning via Adaptive Curriculum Learning](https://arxiv.org/abs/2504.05520)
append_entries: 101
Finish: 2025-05-02 04:24:50.973466
------------------------------------------------------
Started: 2025-05-02 06:22:58.565597
Existing_entries: 1101
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 06:22:58.831768
------------------------------------------------------
Started: 2025-05-02 08:21:03.015241
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 08:21:03.309858
------------------------------------------------------
Started: 2025-05-02 10:17:17.122492
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 10:17:17.382094
------------------------------------------------------
Started: 2025-05-02 12:31:57.572343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 12:31:57.835419
------------------------------------------------------
Started: 2025-05-02 14:15:36.330689
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 14:15:36.613397
------------------------------------------------------
Started: 2025-05-02 16:20:00.940575
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 16:20:01.202289
------------------------------------------------------
Started: 2025-05-02 18:21:56.113792
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 18:21:56.364424
------------------------------------------------------
Started: 2025-05-02 20:17:57.421515
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 20:17:57.684845
------------------------------------------------------
Started: 2025-05-02 22:15:24.006114
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 22:15:24.290948
------------------------------------------------------
Started: 2025-05-03 01:15:13.934457
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 01:15:14.192303
------------------------------------------------------
Started: 2025-05-03 03:01:18.688971
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 03:01:18.959935
------------------------------------------------------
Started: 2025-05-03 04:18:37.330220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 04:18:37.392968
------------------------------------------------------
Started: 2025-05-03 06:20:53.828651
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 06:20:53.905038
------------------------------------------------------
Started: 2025-05-03 08:18:55.653907
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 08:18:55.739166
------------------------------------------------------
Started: 2025-05-03 10:15:17.902925
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 10:15:17.958709
------------------------------------------------------
Started: 2025-05-03 12:29:35.037106
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 12:29:35.119349
------------------------------------------------------
Started: 2025-05-03 14:14:12.630838
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 14:14:12.715767
------------------------------------------------------
Started: 2025-05-03 16:18:19.315741
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 16:18:19.396727
------------------------------------------------------
Started: 2025-05-03 18:20:16.425313
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 18:20:16.490246
------------------------------------------------------
Started: 2025-05-03 20:16:21.203195
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 20:16:21.263042
------------------------------------------------------
Started: 2025-05-03 22:14:21.941006
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 22:14:22.013179
------------------------------------------------------
Started: 2025-05-04 01:24:07.894825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 01:24:07.950782
------------------------------------------------------
Started: 2025-05-04 03:14:23.995746
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 03:14:24.054440
------------------------------------------------------
Started: 2025-05-04 04:19:51.392192
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 04:19:51.452550
------------------------------------------------------
Started: 2025-05-04 06:22:05.880652
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 06:22:05.939796
------------------------------------------------------
Started: 2025-05-04 08:19:11.811528
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 08:19:11.872311
------------------------------------------------------
Started: 2025-05-04 10:15:51.226512
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 10:15:51.288132
------------------------------------------------------
Started: 2025-05-04 12:29:52.291566
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 12:29:52.404019
------------------------------------------------------
Started: 2025-05-04 14:13:39.705622
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 14:13:39.767489
------------------------------------------------------
Started: 2025-05-04 16:18:20.734342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 16:18:20.799536
------------------------------------------------------
Started: 2025-05-04 18:20:17.390516
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 18:20:17.446402
------------------------------------------------------
Started: 2025-05-04 20:16:38.507668
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 20:16:38.584513
------------------------------------------------------
Started: 2025-05-04 22:14:35.751245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 22:14:35.829778
------------------------------------------------------
Started: 2025-05-05 01:20:10.431233
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 01:20:10.513727
------------------------------------------------------
Started: 2025-05-05 03:10:31.585894
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 03:10:31.645818
------------------------------------------------------
Started: 2025-05-05 04:24:16.722852
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
Token length: 1907
Summarized using GPT-3.5-turbo
Append: [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
Token length: 846
Summarized using GPT-3.5-turbo
Append: [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
Token length: 1615
Summarized using GPT-3.5-turbo
Append: [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
Token length: 1327
Summarized using GPT-3.5-turbo
Append: [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
Token length: 823
Summarized using GPT-3.5-turbo
Append: [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
Token length: 928
Summarized using GPT-3.5-turbo
Append: [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
Token length: 875
Summarized using GPT-3.5-turbo
Append: [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
Token length: 1965
Summarized using GPT-3.5-turbo
Append: [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark](https://arxiv.org/abs/2402.14359)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?](https://arxiv.org/abs/2404.18624)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [REFFLY: Melody-Constrained Lyrics Editing Model](https://arxiv.org/abs/2409.00292)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Does Self-Attention Need Separate Weights in Transformers?](https://arxiv.org/abs/2412.00359)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [When Every Token Counts: Optimal Segmentation for Low-Resource Language Models](https://arxiv.org/abs/2412.06926)
Token length: 1803
Summarized using GPT-3.5-turbo
Append: [AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework](https://arxiv.org/abs/2412.10422)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [ICLR: In-Context Learning of Representations](https://arxiv.org/abs/2501.00070)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention](https://arxiv.org/abs/2501.06382)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [TableMaster: A Recipe to Advance Table Understanding with Language Models](https://arxiv.org/abs/2501.19378)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/abs/2502.01976)
Token length: 1152
Summarized using GPT-3.5-turbo
Append: [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning](https://arxiv.org/abs/2311.09830)
Token length: 1249
Summarized using GPT-3.5-turbo
Append: [FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning](https://arxiv.org/abs/2402.18789)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables](https://arxiv.org/abs/2403.04577)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [MoDeGPT: Modular Decomposition for Large Language Model Compression](https://arxiv.org/abs/2408.09632)
Token length: 1710
Summarized using GPT-3.5-turbo
Append: [Competition Dynamics Shape Algorithmic Phases of In-Context Learning](https://arxiv.org/abs/2412.01003)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment](https://arxiv.org/abs/2412.07446)
Append: [A Rate-Distortion Framework for Summarization](https://arxiv.org/abs/2501.13100)
Append: [Activation Steering in Neural Theorem Provers](https://arxiv.org/abs/2502.15507)
append_entries: 52
Finish: 2025-05-05 04:26:15.688257
------------------------------------------------------
Started: 2025-05-05 06:24:33.721452
Existing_entries: 1052
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 06:24:33.936195
------------------------------------------------------
Started: 2025-05-05 08:23:01.354074
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 08:23:01.521425
------------------------------------------------------
Started: 2025-05-05 10:18:15.542958
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 10:18:15.735466
------------------------------------------------------
Started: 2025-05-05 12:33:08.883418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 12:33:09.086435
------------------------------------------------------
Started: 2025-05-05 14:16:43.864983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 14:16:44.062886
------------------------------------------------------
Started: 2025-05-05 16:21:10.141364
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 16:21:10.304225
------------------------------------------------------
Started: 2025-05-05 18:19:23.508124
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 18:19:23.705744
------------------------------------------------------
Started: 2025-05-05 20:18:03.120230
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 20:18:03.292202
------------------------------------------------------
Started: 2025-05-05 22:15:56.872759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 22:15:57.034904
------------------------------------------------------
Started: 2025-05-06 01:17:38.706983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 01:17:38.877576
------------------------------------------------------
Started: 2025-05-06 03:06:12.461245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 03:06:12.638391
------------------------------------------------------
Started: 2025-05-06 04:24:00.678404
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1642
Summarized using GPT-3.5-turbo
Append: [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling](https://arxiv.org/abs/2505.01459)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [SymPlanner: Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/abs/2505.01479)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [On the effectiveness of Large Language Models in the mechanical design domain](https://arxiv.org/abs/2505.01559)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains](https://arxiv.org/abs/2505.01560)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents](https://arxiv.org/abs/2505.01592)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Always Tell Me The Odds: Fine-grained Conditional Probability Estimation](https://arxiv.org/abs/2505.01595)
Token length: 1665
Summarized using GPT-3.5-turbo
Append: [A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/abs/2505.01658)
Token length: 1735
Summarized using GPT-3.5-turbo
Append: [High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers](https://arxiv.org/abs/2505.01693)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models](https://arxiv.org/abs/2505.01761)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments](https://arxiv.org/abs/2505.01794)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis](https://arxiv.org/abs/2505.01800)
Token length: 1668
Summarized using GPT-3.5-turbo
Append: [$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge](https://arxiv.org/abs/2505.01812)
Token length: 787
Summarized using GPT-3.5-turbo
Append: [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Positional Attention for Efficient BERT-Based Named Entity Recognition](https://arxiv.org/abs/2505.01868)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/abs/2505.01877)
Token length: 847
Summarized using GPT-3.5-turbo
Append: [Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams](https://arxiv.org/abs/2505.01883)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation](https://arxiv.org/abs/2505.01900)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview](https://arxiv.org/abs/2505.01967)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load](https://arxiv.org/abs/2505.01980)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [An overview of artificial intelligence in computer-assisted language learning](https://arxiv.org/abs/2505.02032)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction](https://arxiv.org/abs/2505.02072)
Token length: 1029
Summarized using GPT-3.5-turbo
Append: [LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning](https://arxiv.org/abs/2505.02078)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications](https://arxiv.org/abs/2505.02091)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study](https://arxiv.org/abs/2505.02142)
Token length: 1860
Summarized using GPT-3.5-turbo
Append: [QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach](https://arxiv.org/abs/2505.02146)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents](https://arxiv.org/abs/2505.02156)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use](https://arxiv.org/abs/2505.02164)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking](https://arxiv.org/abs/2505.02171)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Token length: 1594
Summarized using GPT-3.5-turbo
Append: [Measuring Hong Kong Massive Multi-Task Language Understanding](https://arxiv.org/abs/2505.02177)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation](https://arxiv.org/abs/2505.02235)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models](https://arxiv.org/abs/2505.02252)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Parameter-Efficient Transformer Embeddings](https://arxiv.org/abs/2505.02266)
Token length: 847
Summarized using GPT-3.5-turbo
Append: [Demystifying optimized prompts in language models](https://arxiv.org/abs/2505.02273)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition](https://arxiv.org/abs/2505.02304)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering](https://arxiv.org/abs/2505.02311)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning](https://arxiv.org/abs/2505.02363)
Token length: 1954
Summarized using GPT-3.5-turbo
Append: [JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2505.02366)
Token length: 1892
Summarized using GPT-3.5-turbo
Append: [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Bielik 11B v2 Technical Report](https://arxiv.org/abs/2505.02410)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs](https://arxiv.org/abs/2505.02456)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda](https://arxiv.org/abs/2505.02463)
Token length: 477
Summarized using GPT-3.5-turbo
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Token length: 1356
Summarized using GPT-3.5-turbo
Append: [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/abs/2505.02579)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [Ensemble Kalman filter for uncertainty in human language comprehension](https://arxiv.org/abs/2505.02590)
Token length: 1075
Summarized using GPT-3.5-turbo
Append: [Automatic Proficiency Assessment in L2 English Learners](https://arxiv.org/abs/2505.02615)
Token length: 942
Summarized using GPT-3.5-turbo
Append: [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis](https://arxiv.org/abs/2505.02625)
Append: [Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/abs/2505.02656)
Append: [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/abs/2505.02666)
Append: [Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686)
Append: [fastabx: A library for efficient computation of ABX discriminability](https://arxiv.org/abs/2505.02692)
Append: [Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models](https://arxiv.org/abs/2505.02763)
Append: [ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations](https://arxiv.org/abs/2505.02819)
Append: [Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations](https://arxiv.org/abs/2505.01433)
Append: [AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine](https://arxiv.org/abs/2505.01435)
Append: [CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code](https://arxiv.org/abs/2505.01485)
Append: [Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation](https://arxiv.org/abs/2505.01636)
Append: [Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm](https://arxiv.org/abs/2505.01706)
Append: [Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias](https://arxiv.org/abs/2505.01754)
Append: [Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos](https://arxiv.org/abs/2505.01790)
Append: [Explainability by design: an experimental analysis of the legal coding process](https://arxiv.org/abs/2505.01944)
Append: [A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.01958)
Append: [Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data](https://arxiv.org/abs/2505.02130)
Append: [Exploring new Approaches for Information Retrieval through Natural Language Processing](https://arxiv.org/abs/2505.02199)
Append: [DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units](https://arxiv.org/abs/2505.02206)
Append: [Interpretable Emergent Language Using Inter-Agent Transformers](https://arxiv.org/abs/2505.02215)
Append: [Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques](https://arxiv.org/abs/2505.02309)
Append: [Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL](https://arxiv.org/abs/2505.02391)
Append: [Incentivizing Inclusive Contributions in Model Sharing Markets](https://arxiv.org/abs/2505.02462)
Append: [Bielik v3 Small: Technical Report](https://arxiv.org/abs/2505.02550)
Append: [Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning](https://arxiv.org/abs/2505.02639)
Append: [Predicting Movie Hits Before They Happen with LLMs](https://arxiv.org/abs/2505.02693)
Append: [Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play](https://arxiv.org/abs/2505.02707)
Append: [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
Append: [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/abs/2505.02811)
Append: [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/abs/2505.02820)
Append: [AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation](https://arxiv.org/abs/2505.02830)
Append: [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/abs/2505.02835)
Append: [Transformadores: Fundamentos teoricos y Aplicaciones](https://arxiv.org/abs/2302.09327)
Append: [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/abs/2402.01685)
Append: [DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation](https://arxiv.org/abs/2403.01954)
Append: [ParaICL: Towards Parallel In-Context Learning](https://arxiv.org/abs/2404.00570)
Append: [Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind](https://arxiv.org/abs/2404.04748)
Append: [From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences](https://arxiv.org/abs/2405.05572)
Append: [Large Language Models as Carriers of Hidden Messages](https://arxiv.org/abs/2406.02481)
Append: [LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models](https://arxiv.org/abs/2407.12772)
Append: [A Logical Fallacy-Informed Framework for Argument Generation](https://arxiv.org/abs/2408.03618)
Append: [LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library](https://arxiv.org/abs/2408.06150)
Append: [Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence](https://arxiv.org/abs/2409.09413)
Append: [ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions](https://arxiv.org/abs/2410.14567)
Append: [LLMs for Extremely Low-Resource Finno-Ugric Languages](https://arxiv.org/abs/2410.18902)
Append: [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://arxiv.org/abs/2412.04454)
Append: [AD-LLM: Benchmarking Large Language Models for Anomaly Detection](https://arxiv.org/abs/2412.11142)
Append: [ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis](https://arxiv.org/abs/2501.00062)
Append: [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/abs/2501.00874)
Append: [Towards the Anonymization of the Language Modeling](https://arxiv.org/abs/2501.02407)
Append: [How do Humans and Language Models Reason About Creativity? A Comparative Analysis](https://arxiv.org/abs/2502.03253)
Append: [SpeechT: Findings of the First Mentorship in Speech Translation](https://arxiv.org/abs/2502.12050)
Append: [Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing](https://arxiv.org/abs/2502.15666)
Append: [Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data](https://arxiv.org/abs/2502.16892)
Append: [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424)
Append: [Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs](https://arxiv.org/abs/2502.21239)
Append: [A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications](https://arxiv.org/abs/2503.17003)
Append: [A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?](https://arxiv.org/abs/2503.24235)
Append: [Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2504.03302)
Append: [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/abs/2504.03601)
Append: [Better Estimation of the KL Divergence Between Language Models](https://arxiv.org/abs/2504.10637)
Append: [MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection](https://arxiv.org/abs/2309.15670)
Append: [Impact of Noisy Supervision in Foundation Model Learning](https://arxiv.org/abs/2403.06869)
Append: [Tailored Design of Audio-Visual Speech Recognition Models using Branchformers](https://arxiv.org/abs/2407.06606)
Append: [Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant](https://arxiv.org/abs/2501.17176)
Append: [Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization](https://arxiv.org/abs/2502.04667)
Append: [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/abs/2502.20490)
Append: [The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats](https://arxiv.org/abs/2503.02650)
Append: [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
append_entries: 118
Finish: 2025-05-06 04:26:11.843811
------------------------------------------------------
Started: 2025-05-06 06:23:59.634339
Existing_entries: 1118
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 06:23:59.940227
------------------------------------------------------
Started: 2025-05-06 08:22:21.601435
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 08:22:21.904152
------------------------------------------------------
Started: 2025-05-06 10:17:45.917359
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 10:17:46.216210
------------------------------------------------------
Started: 2025-05-06 12:35:05.056216
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 12:35:05.356908
------------------------------------------------------
Started: 2025-05-06 14:16:47.151709
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 14:16:47.449708
------------------------------------------------------
Started: 2025-05-06 16:19:03.156137
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 16:19:03.484089
------------------------------------------------------
Started: 2025-05-06 18:22:47.924209
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 18:22:48.258386
------------------------------------------------------
Started: 2025-05-06 20:18:16.547581
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 20:18:16.881116
------------------------------------------------------
Started: 2025-05-06 22:15:32.302078
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 22:15:32.596858
------------------------------------------------------
Started: 2025-05-07 01:18:02.364414
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 01:18:02.694941
------------------------------------------------------
Started: 2025-05-07 03:07:36.694136
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 03:07:36.996131
------------------------------------------------------
Started: 2025-05-07 04:21:39.613376
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
Token length: 1956
Summarized using GPT-3.5-turbo
Append: [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
Token length: 725
Summarized using GPT-3.5-turbo
Append: [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
Token length: 656
Summarized using GPT-3.5-turbo
Append: [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
Token length: 944
Summarized using GPT-3.5-turbo
Append: [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
Token length: 921
Summarized using GPT-3.5-turbo
Append: [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
Token length: 1383
Summarized using GPT-3.5-turbo
Append: [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [{\Psi}-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
Token length: 763
Summarized using GPT-3.5-turbo
Append: [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
Token length: 849
Summarized using GPT-3.5-turbo
Append: [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
Token length: 929
Summarized using GPT-3.5-turbo
Append: [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
Token length: 1784
Summarized using GPT-3.5-turbo
Append: [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
Token length: 779
Summarized using GPT-3.5-turbo
Append: [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
Token length: 692
Summarized using GPT-3.5-turbo
Append: [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Token length: 974
Summarized using GPT-3.5-turbo
Append: [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
Token length: 1760
Summarized using GPT-3.5-turbo
Append: [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
Token length: 941
Summarized using GPT-3.5-turbo
Append: [Incoherent Probability Judgments in Large Language Models](https://arxiv.org/abs/2401.16646)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors](https://arxiv.org/abs/2406.14498)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [CFBench: A Comprehensive Constraints-Following Benchmark for LLMs](https://arxiv.org/abs/2408.01122)
Token length: 1787
Summarized using GPT-3.5-turbo
Append: [LLM-3D Print: Large Language Models To Monitor and Control 3D Printing](https://arxiv.org/abs/2408.14307)
Append: [Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution](https://arxiv.org/abs/2410.00153)
Append: [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://arxiv.org/abs/2410.09580)
Append: [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)
Append: [LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment](https://arxiv.org/abs/2412.18135)
Append: [Self-reflecting Large Language Models: A Hegelian Dialectical Approach](https://arxiv.org/abs/2501.14917)
Append: [Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech](https://arxiv.org/abs/2501.15858)
Append: [Predicting potentially abusive clauses in Chilean terms of services with natural language processing](https://arxiv.org/abs/2502.00865)
Append: [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)
Append: [English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports](https://arxiv.org/abs/2502.14338)
Append: [BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187)
Append: [CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models](https://arxiv.org/abs/2503.10707)
Append: [Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models](https://arxiv.org/abs/2503.13551)
Append: [CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement](https://arxiv.org/abs/2503.17279)
Append: [HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment](https://arxiv.org/abs/2503.18991)
Append: [Clean & Clear: Feasibility of Safe LLM Clinical Guidance](https://arxiv.org/abs/2503.20953)
Append: [SEAL: Steerable Reasoning Calibration of Large Language Models for Free](https://arxiv.org/abs/2504.07986)
Append: [FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback](https://arxiv.org/abs/2404.05046)
Append: [OAC: Output-adaptive Calibration for Accurate Post-training Quantization](https://arxiv.org/abs/2405.15025)
Append: [Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice](https://arxiv.org/abs/2405.19313)
Append: [AudioBench: A Universal Benchmark for Audio Large Language Models](https://arxiv.org/abs/2406.16020)
Append: [Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate](https://arxiv.org/abs/2410.22086)
Append: [MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications](https://arxiv.org/abs/2411.18915)
Append: [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/abs/2412.17739)
Append: [Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models](https://arxiv.org/abs/2502.07328)
Append: [BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling](https://arxiv.org/abs/2503.02445)
Append: [LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications](https://arxiv.org/abs/2503.02950)
Append: [UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction](https://arxiv.org/abs/2503.15661)
Append: [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
append_entries: 78
Finish: 2025-05-07 04:23:43.413339
------------------------------------------------------
Started: 2025-05-07 06:23:49.623941
Existing_entries: 1078
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 06:23:49.867404
------------------------------------------------------
Started: 2025-05-07 08:23:01.933170
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 08:23:02.182829
------------------------------------------------------
Started: 2025-05-07 10:18:03.800220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 10:18:04.018132
------------------------------------------------------
Started: 2025-05-07 12:35:11.670684
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 12:35:11.904580
------------------------------------------------------
Started: 2025-05-07 14:17:11.945548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 14:17:12.164682
------------------------------------------------------
Started: 2025-05-07 16:21:15.650555
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 16:21:15.918321
------------------------------------------------------
Started: 2025-05-07 18:23:24.636034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 18:23:24.898600
------------------------------------------------------
Started: 2025-05-07 20:18:26.448800
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 20:18:26.669037
------------------------------------------------------
Started: 2025-05-07 22:16:09.694027
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 22:16:09.912166
------------------------------------------------------
Started: 2025-05-08 01:18:41.723176
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 01:18:41.945235
------------------------------------------------------
Started: 2025-05-08 03:11:21.572356
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 03:11:21.804024
------------------------------------------------------
Started: 2025-05-08 04:25:11.789348
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
Token length: 1111
Summarized using GPT-3.5-turbo
Append: [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
Token length: 1600
Summarized using GPT-3.5-turbo
Append: [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
Token length: 1428
Summarized using GPT-3.5-turbo
Append: [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
Token length: 1894
Summarized using GPT-3.5-turbo
Append: [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
Token length: 652
Summarized using GPT-3.5-turbo
Append: [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
Token length: 895
Summarized using GPT-3.5-turbo
Append: [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
Token length: 1955
Summarized using GPT-3.5-turbo
Append: [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
Token length: 1905
Summarized using GPT-3.5-turbo
Append: [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
Token length: 1542
Summarized using GPT-3.5-turbo
Append: [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
Token length: 1696
Summarized using GPT-3.5-turbo
Append: [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
Token length: 1684
Summarized using GPT-3.5-turbo
Append: [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
Token length: 959
Summarized using GPT-3.5-turbo
Append: [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [Playing repeated games with Large Language Models](https://arxiv.org/abs/2305.16867)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)
Token length: 1007
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are Struggle to Cope with Unreasonability in Math Problems](https://arxiv.org/abs/2403.19346)
Token length: 1865
Summarized using GPT-3.5-turbo
Append: [Re-ReST: Reflection-Reinforced Self-Training for Language Agents](https://arxiv.org/abs/2406.01495)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA](https://arxiv.org/abs/2406.02044)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://arxiv.org/abs/2406.18501)
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning](https://arxiv.org/abs/2408.13184)
Token length: 1719
Summarized using GPT-3.5-turbo
Append: [Advancements and limitations of LLMs in replicating human color-word associations](https://arxiv.org/abs/2411.02116)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution](https://arxiv.org/abs/2501.05040)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Estimating LLM Uncertainty with Logits](https://arxiv.org/abs/2502.00290)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [Liger: Linearizing Large Language Models to Gated Recurrent Structures](https://arxiv.org/abs/2503.01496)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation](https://arxiv.org/abs/2503.03186)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/abs/2503.11280)
Token length: 745
Summarized using GPT-3.5-turbo
Append: [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/abs/2503.21813)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance](https://arxiv.org/abs/2311.18681)
Append: [Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners](https://arxiv.org/abs/2410.02131)
Append: [Vision-Language Models Create Cross-Modal Task Representations](https://arxiv.org/abs/2410.22330)
Append: [LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation](https://arxiv.org/abs/2411.04997)
Append: [Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation](https://arxiv.org/abs/2411.05261)
Append: [Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT](https://arxiv.org/abs/2411.10246)
Append: [SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild](https://arxiv.org/abs/2503.18892)
append_entries: 56
Finish: 2025-05-08 04:27:25.060534
------------------------------------------------------
Started: 2025-05-08 06:24:50.458891
Existing_entries: 1056
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 06:24:50.651650
------------------------------------------------------
Started: 2025-05-08 08:21:53.195102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 08:21:53.365599
------------------------------------------------------
Started: 2025-05-08 10:18:17.035680
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 10:18:17.232308
------------------------------------------------------
Started: 2025-05-08 12:33:06.354143
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 12:33:06.529342
------------------------------------------------------
Started: 2025-05-08 14:15:00.054341
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 14:15:00.260603
------------------------------------------------------
Started: 2025-05-08 16:21:01.117397
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 16:21:01.305577
------------------------------------------------------
Started: 2025-05-08 18:22:58.306924
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 18:22:58.483787
------------------------------------------------------
Started: 2025-05-08 20:18:32.451493
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 20:18:32.642931
------------------------------------------------------
Started: 2025-05-08 22:15:42.386952
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 22:15:42.592826
------------------------------------------------------
Started: 2025-05-09 01:17:54.167988
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 01:17:54.356910
------------------------------------------------------
Started: 2025-05-09 03:07:40.451032
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 03:07:40.674312
------------------------------------------------------
Started: 2025-05-09 04:30:56.020834
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks](https://arxiv.org/abs/2505.04628)
Token length: 1524
Summarized using GPT-3.5-turbo
Append: [Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs](https://arxiv.org/abs/2505.04637)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Language translation, and change of accent for speech-to-speech task using diffusion model](https://arxiv.org/abs/2505.04639)
Token length: 872
Summarized using GPT-3.5-turbo
Append: [A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)](https://arxiv.org/abs/2505.04640)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture](https://arxiv.org/abs/2505.04642)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation](https://arxiv.org/abs/2505.04643)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [ChatGPT for automated grading of short answer questions in mechanical ventilation](https://arxiv.org/abs/2505.04645)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights](https://arxiv.org/abs/2505.04649)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions](https://arxiv.org/abs/2505.04651)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [Advancing Conversational Diagnostic AI with Multimodal Reasoning](https://arxiv.org/abs/2505.04653)
Token length: 854
Summarized using GPT-3.5-turbo
Append: [A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient](https://arxiv.org/abs/2505.04654)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction](https://arxiv.org/abs/2505.04655)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection](https://arxiv.org/abs/2505.04660)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising](https://arxiv.org/abs/2505.04665)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes](https://arxiv.org/abs/2505.04666)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards](https://arxiv.org/abs/2505.04671)
Token length: 1697
Summarized using GPT-3.5-turbo
Append: [REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM](https://arxiv.org/abs/2505.04673)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols](https://arxiv.org/abs/2505.04678)
Token length: 1781
Summarized using GPT-3.5-turbo
Append: [SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding](https://arxiv.org/abs/2505.04723)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence](https://arxiv.org/abs/2505.04785)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [Osiris: A Lightweight Open-Source Hallucination Detection System](https://arxiv.org/abs/2505.04844)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards](https://arxiv.org/abs/2505.04847)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education](https://arxiv.org/abs/2505.04916)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Chain-of-Thought Tokens are Computer Program Variables](https://arxiv.org/abs/2505.04955)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Rethinking the Relationship between the Power Law and Hierarchical Structures](https://arxiv.org/abs/2505.04984)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes](https://arxiv.org/abs/2505.04993)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Rethinking Invariance in In-context Learning](https://arxiv.org/abs/2505.04994)
Token length: 1714
Summarized using GPT-3.5-turbo
Append: [The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations](https://arxiv.org/abs/2505.05016)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization](https://arxiv.org/abs/2505.05017)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/abs/2505.05026)
Token length: 731
Summarized using GPT-3.5-turbo
Append: [Image-Text Relation Prediction for Multilingual Tweets](https://arxiv.org/abs/2505.05040)
Token length: 788
Summarized using GPT-3.5-turbo
Append: [Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations](https://arxiv.org/abs/2505.05056)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization](https://arxiv.org/abs/2505.05070)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction](https://arxiv.org/abs/2505.05084)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2505.05111)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition](https://arxiv.org/abs/2505.05148)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation](https://arxiv.org/abs/2505.05225)
Token length: 1704
Summarized using GPT-3.5-turbo
Append: [T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction](https://arxiv.org/abs/2505.05271)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design](https://arxiv.org/abs/2505.05298)
Token length: 1467
Summarized using GPT-3.5-turbo
Append: [ICon: In-Context Contribution for Automatic Data Selection](https://arxiv.org/abs/2505.05327)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?](https://arxiv.org/abs/2505.05406)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Crosslingual Reasoning through Test-Time Scaling](https://arxiv.org/abs/2505.05408)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410)
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/abs/2505.05423)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data](https://arxiv.org/abs/2505.05427)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations](https://arxiv.org/abs/2505.05445)
Token length: 897
Summarized using GPT-3.5-turbo
Append: [UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections](https://arxiv.org/abs/2505.05459)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging](https://arxiv.org/abs/2505.05464)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [ComPO: Preference Alignment via Comparison Oracles](https://arxiv.org/abs/2505.05465)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification](https://arxiv.org/abs/2505.04629)
Append: [Towards Artificial Intelligence Research Assistant for Expert-Involved Learning](https://arxiv.org/abs/2505.04638)
Append: [When Bad Data Leads to Good Models](https://arxiv.org/abs/2505.04741)
Append: [Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)
Append: [HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights](https://arxiv.org/abs/2505.04846)
Append: [CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation](https://arxiv.org/abs/2505.04851)
Append: [ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning](https://arxiv.org/abs/2505.04881)
Append: [SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models](https://arxiv.org/abs/2505.04911)
Append: [Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models](https://arxiv.org/abs/2505.04914)
Append: [Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://arxiv.org/abs/2505.04921)
Append: [T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models](https://arxiv.org/abs/2505.04946)
Append: [Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations](https://arxiv.org/abs/2505.04948)
Append: [General Transform: A Unified Framework for Adaptive Transform to Enhance Representations](https://arxiv.org/abs/2505.04969)
Append: [CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts](https://arxiv.org/abs/2505.05063)
Append: [X-Driver: Explainable Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2505.05098)
Append: [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/abs/2505.05145)
Append: [Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks](https://arxiv.org/abs/2505.05190)
Append: [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315)
Append: [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/abs/2505.05422)
Append: [Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding](https://arxiv.org/abs/2505.05446)
Append: [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/abs/2505.05467)
Append: [Position: AI Evaluation Should Learn from How We Test Humans](https://arxiv.org/abs/2306.10512)
Append: [DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying](https://arxiv.org/abs/2405.13325)
Append: [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://arxiv.org/abs/2406.17746)
Append: [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/abs/2406.20094)
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Append: [To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/abs/2409.12183)
Append: [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/abs/2410.04055)
Append: [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://arxiv.org/abs/2410.12705)
Append: [E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2411.00437)
Append: [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996)
Append: [ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning](https://arxiv.org/abs/2501.01031)
Append: [Communicating Activations Between Language Model Agents](https://arxiv.org/abs/2501.14082)
Append: [Safety Evaluation of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2502.11137)
Append: [Drift: Decoding-time Personalized Alignments with Implicit User Preferences](https://arxiv.org/abs/2502.14289)
Append: [Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework](https://arxiv.org/abs/2503.05505)
Append: [Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges](https://arxiv.org/abs/2503.08292)
Append: [Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization](https://arxiv.org/abs/2503.13690)
Append: [Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks](https://arxiv.org/abs/2503.15169)
Append: [Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment](https://arxiv.org/abs/2307.02075)
Append: [HORAE: A Domain-Agnostic Language for Automated Service Regulation](https://arxiv.org/abs/2406.06600)
Append: [Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](https://arxiv.org/abs/2409.03757)
Append: [Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play](https://arxiv.org/abs/2411.08884)
Append: [Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems](https://arxiv.org/abs/2502.18635)
Append: [Re-evaluating Open-ended Evaluation of Large Language Models](https://arxiv.org/abs/2502.20170)
Append: [TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining](https://arxiv.org/abs/2504.02107)
Append: [Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets](https://arxiv.org/abs/2504.19981)
append_entries: 96
Finish: 2025-05-09 04:36:36.837588
------------------------------------------------------
Started: 2025-05-09 06:24:29.845477
Existing_entries: 1096
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2404.04545)
append_entries: 1
Finish: 2025-05-09 06:24:39.843162
------------------------------------------------------
Started: 2025-05-09 08:21:41.261413
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 08:21:41.534147
------------------------------------------------------
Started: 2025-05-09 10:17:26.988457
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 10:17:27.239872
------------------------------------------------------
Started: 2025-05-09 12:32:30.636984
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 12:32:30.888378
------------------------------------------------------
Started: 2025-05-09 14:16:10.248188
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 14:16:10.497113
------------------------------------------------------
Started: 2025-05-09 16:20:05.494920
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 16:20:05.834049
------------------------------------------------------
Started: 2025-05-09 18:22:28.070092
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 18:22:28.352192
------------------------------------------------------
Started: 2025-05-09 20:17:58.394100
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 20:17:58.730561
------------------------------------------------------
Started: 2025-05-09 22:15:19.357945
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 22:15:19.637458
------------------------------------------------------
Started: 2025-05-10 01:15:15.764777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 01:15:16.048224
------------------------------------------------------
Started: 2025-05-10 03:01:31.343550
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 03:01:31.690190
------------------------------------------------------
Started: 2025-05-10 04:18:20.956078
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 04:18:21.014676
------------------------------------------------------
Started: 2025-05-10 06:20:29.195808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 06:20:29.287884
------------------------------------------------------
Started: 2025-05-10 08:19:00.888975
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 08:19:00.955350
------------------------------------------------------
Started: 2025-05-10 10:15:51.112993
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 10:15:51.172193
------------------------------------------------------
Started: 2025-05-10 12:29:21.996055
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 12:29:22.090206
------------------------------------------------------
Started: 2025-05-10 14:13:36.155397
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 14:13:36.265321
------------------------------------------------------
Started: 2025-05-10 16:18:20.922105
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 16:18:20.986543
------------------------------------------------------
Started: 2025-05-10 18:19:50.477117
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 18:19:50.552951
------------------------------------------------------
Started: 2025-05-10 20:16:11.045855
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 20:16:11.106975
------------------------------------------------------
Started: 2025-05-10 22:14:06.690930
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 22:14:06.823538
------------------------------------------------------
Started: 2025-05-11 01:22:45.037232
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 01:22:45.097800
------------------------------------------------------
Started: 2025-05-11 03:12:35.847149
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 03:12:35.963953
------------------------------------------------------
Started: 2025-05-11 04:19:17.590073
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 04:19:17.741930
------------------------------------------------------
Started: 2025-05-11 06:21:36.442033
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 06:21:36.551911
------------------------------------------------------
Started: 2025-05-11 08:19:26.124793
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 08:19:26.194978
------------------------------------------------------
Started: 2025-05-11 10:15:21.828016
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 10:15:21.895291
------------------------------------------------------
Started: 2025-05-11 12:29:50.747601
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 12:29:50.874298
------------------------------------------------------
Started: 2025-05-11 14:14:05.915839
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 14:14:06.026804
------------------------------------------------------
Started: 2025-05-11 16:18:10.414755
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 16:18:10.479890
------------------------------------------------------
Started: 2025-05-11 18:20:18.644728
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 18:20:18.706304
------------------------------------------------------
Started: 2025-05-11 20:16:23.343317
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 20:16:23.401726
------------------------------------------------------
Started: 2025-05-11 22:14:52.226102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 22:14:52.308179
------------------------------------------------------
Started: 2025-05-12 01:20:55.561761
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 01:20:55.681904
------------------------------------------------------
Started: 2025-05-12 03:11:58.255368
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 03:11:58.318219
------------------------------------------------------
Started: 2025-05-12 04:25:06.023151
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/abs/2505.05583)
Token length: 692
Summarized using GPT-3.5-turbo
Append: [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/abs/2505.05648)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/abs/2505.05687)
Token length: 1416
Summarized using GPT-3.5-turbo
Append: [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/abs/2505.05704)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/abs/2505.05714)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/abs/2505.05772)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI](https://arxiv.org/abs/2505.05864)
Token length: 776
Summarized using GPT-3.5-turbo
Append: [Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2](https://arxiv.org/abs/2505.05946)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [Summarisation of German Judgments in conjunction with a Class-based Evaluation](https://arxiv.org/abs/2505.05947)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [NeoQA: Evidence-based Question Answering with Generated News Events](https://arxiv.org/abs/2505.05949)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models](https://arxiv.org/abs/2505.05970)
Token length: 1689
Summarized using GPT-3.5-turbo
Append: [An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition](https://arxiv.org/abs/2505.05973)
Token length: 882
Summarized using GPT-3.5-turbo
Append: [Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models](https://arxiv.org/abs/2505.06004)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective](https://arxiv.org/abs/2505.06010)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation](https://arxiv.org/abs/2505.06027)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/abs/2505.06046)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax](https://arxiv.org/abs/2505.06062)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/abs/2505.06110)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies](https://arxiv.org/abs/2505.06145)
Token length: 984
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 840
Summarized using GPT-3.5-turbo
Append: [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)
Token length: 1915
Summarized using GPT-3.5-turbo
Append: [Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework](https://arxiv.org/abs/2505.06151)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [Adaptive Stress Testing Black-Box LLM Planners](https://arxiv.org/abs/2505.05665)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Prompted Meta-Learning for Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2505.05684)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications](https://arxiv.org/abs/2505.05736)
Token length: 1576
Summarized using GPT-3.5-turbo
Append: [Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification](https://arxiv.org/abs/2505.05744)
Token length: 1094
Summarized using GPT-3.5-turbo
Append: [BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection](https://arxiv.org/abs/2505.05763)
Token length: 953
Summarized using GPT-3.5-turbo
Append: [An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers](https://arxiv.org/abs/2505.05828)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Evolutionary ecology of words](https://arxiv.org/abs/2505.05863)
Token length: 986
Summarized using GPT-3.5-turbo
Append: [Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification](https://arxiv.org/abs/2505.06032)
Token length: 1926
Summarized using GPT-3.5-turbo
Append: [Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models](https://arxiv.org/abs/2505.06107)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling](https://arxiv.org/abs/2505.06184)
Token length: 1000
Summarized using GPT-3.5-turbo
Append: [Neuro-Symbolic Concepts](https://arxiv.org/abs/2505.06191)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [PART: Pre-trained Authorship Representation Transformer](https://arxiv.org/abs/2209.15373)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [Talking Heads: Understanding Inter-layer Communication in Transformer Language Models](https://arxiv.org/abs/2406.09519)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?](https://arxiv.org/abs/2407.11963)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget](https://arxiv.org/abs/2408.00103)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits](https://arxiv.org/abs/2410.18234)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2411.11053)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes](https://arxiv.org/abs/2501.12106)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models](https://arxiv.org/abs/2501.14851)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought](https://arxiv.org/abs/2501.16154)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [Phonetic accommodation and inhibition in a dynamic neural field model](https://arxiv.org/abs/2502.01210)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs](https://arxiv.org/abs/2502.04134)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids](https://arxiv.org/abs/2502.09667)
Append: [Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](https://arxiv.org/abs/2502.20364)
Append: [ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach](https://arxiv.org/abs/2503.17460)
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Append: [Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security](https://arxiv.org/abs/2406.09831)
Append: [Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning](https://arxiv.org/abs/2503.24289)
append_entries: 55
Finish: 2025-05-12 04:27:05.983439
------------------------------------------------------
Started: 2025-05-12 06:25:09.140121
Existing_entries: 1055
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 06:25:09.327037
------------------------------------------------------
Started: 2025-05-12 08:23:08.138885
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 08:23:08.403509
------------------------------------------------------
Started: 2025-05-12 10:18:49.206756
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 10:18:49.430031
------------------------------------------------------
Started: 2025-05-12 12:34:24.594379
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 12:34:24.815123
------------------------------------------------------
Started: 2025-05-12 14:17:23.442187
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 14:17:23.709236
------------------------------------------------------
Started: 2025-05-12 16:21:13.967442
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 16:21:14.155998
------------------------------------------------------
Started: 2025-05-12 18:22:49.634977
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 18:22:49.845269
------------------------------------------------------
Started: 2025-05-12 20:18:31.152441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 20:18:31.345055
------------------------------------------------------
Started: 2025-05-12 22:15:59.147892
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 22:15:59.364834
------------------------------------------------------
Started: 2025-05-13 01:19:19.194204
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 01:19:19.410942
------------------------------------------------------
Started: 2025-05-13 03:09:51.376951
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 03:09:51.583310
------------------------------------------------------
Started: 2025-05-13 04:25:03.357222
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents](https://arxiv.org/abs/2505.06416)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Is your multimodal large language model a good science tutor?](https://arxiv.org/abs/2505.06418)
Token length: 593
Summarized using GPT-3.5-turbo
Append: [xGen-small Technical Report](https://arxiv.org/abs/2505.06496)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback](https://arxiv.org/abs/2505.06548)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/abs/2505.06552)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
Token length: 800
Summarized using GPT-3.5-turbo
Append: [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/abs/2505.06591)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation](https://arxiv.org/abs/2505.06594)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation](https://arxiv.org/abs/2505.06599)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Using External knowledge to Enhanced PLM for Semantic Matching](https://arxiv.org/abs/2505.06605)
Token length: 1753
Summarized using GPT-3.5-turbo
Append: [Boosting Neural Language Inference via Cascaded Interactive Reasoning](https://arxiv.org/abs/2505.06607)
Token length: 948
Summarized using GPT-3.5-turbo
Append: [The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification](https://arxiv.org/abs/2505.06624)
Token length: 1755
Summarized using GPT-3.5-turbo
Append: [Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis](https://arxiv.org/abs/2505.06630)
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models](https://arxiv.org/abs/2505.06633)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models](https://arxiv.org/abs/2505.06660)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Enhancing BERTopic with Intermediate Layer Representations](https://arxiv.org/abs/2505.06696)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback](https://arxiv.org/abs/2505.06698)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK](https://arxiv.org/abs/2505.06782)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting](https://arxiv.org/abs/2505.06862)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method](https://arxiv.org/abs/2505.06889)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation](https://arxiv.org/abs/2505.06904)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [The Distracting Effect: Understanding Irrelevant Passages in RAG](https://arxiv.org/abs/2505.06914)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire](https://arxiv.org/abs/2505.06974)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
Token length: 1605
Summarized using GPT-3.5-turbo
Append: [HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling](https://arxiv.org/abs/2505.07157)
Token length: 1922
Summarized using GPT-3.5-turbo
Append: [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](https://arxiv.org/abs/2505.07161)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification](https://arxiv.org/abs/2505.07162)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs](https://arxiv.org/abs/2505.07184)
Token length: 945
Summarized using GPT-3.5-turbo
Append: [On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud](https://arxiv.org/abs/2505.07202)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030](https://arxiv.org/abs/2505.07205)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.07233)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/abs/2505.07247)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [No Query, No Access](https://arxiv.org/abs/2505.07258)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [On the Robustness of Reward Models for Language Model Alignment](https://arxiv.org/abs/2505.07271)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/abs/2505.07289)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/abs/2505.07313)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines](https://arxiv.org/abs/2505.07345)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles](https://arxiv.org/abs/2505.07409)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](https://arxiv.org/abs/2505.07416)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights](https://arxiv.org/abs/2505.07430)
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge](https://arxiv.org/abs/2505.07440)
Token length: 913
Summarized using GPT-3.5-turbo
Append: [Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions](https://arxiv.org/abs/2505.07495)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution](https://arxiv.org/abs/2505.07512)
Token length: 1640
Summarized using GPT-3.5-turbo
Append: [SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion](https://arxiv.org/abs/2505.07528)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models](https://arxiv.org/abs/2505.07591)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596)
Token length: 1688
Summarized using GPT-3.5-turbo
Append: [Characterizing the Investigative Methods of Fictional Detectives with Large Language Models](https://arxiv.org/abs/2505.07601)
Append: [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)
Append: [Concept-Level Explainability for Auditing & Steering LLM Responses](https://arxiv.org/abs/2505.07610)
Append: [Chronocept: Instilling a Sense of Time in Machines](https://arxiv.org/abs/2505.07637)
Append: [JobHop: A Large-Scale Dataset of Career Trajectories](https://arxiv.org/abs/2505.07653)
Append: [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/abs/2505.07659)
Append: [Benchmarking Retrieval-Augmented Generation for Chemistry](https://arxiv.org/abs/2505.07671)
Append: [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/abs/2505.07672)
Append: [Codifying Character Logic in Role-Playing](https://arxiv.org/abs/2505.07705)
Append: [Spoken Language Understanding on Unseen Tasks With In-Context Learning](https://arxiv.org/abs/2505.07731)
Append: [Must Read: A Systematic Survey of Computational Persuasion](https://arxiv.org/abs/2505.07775)
Append: [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/abs/2505.07784)
Append: [Learning from Peers in Reasoning Models](https://arxiv.org/abs/2505.07787)
Append: [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)
Append: [A Comparative Analysis of Static Word Embeddings for Hungarian](https://arxiv.org/abs/2505.07809)
Append: [Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction](https://arxiv.org/abs/2505.06297)
Append: [AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity](https://arxiv.org/abs/2505.06313)
Append: [Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution](https://arxiv.org/abs/2505.06320)
Append: [Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations](https://arxiv.org/abs/2505.06653)
Append: [Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation](https://arxiv.org/abs/2505.06803)
Append: [Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge](https://arxiv.org/abs/2505.06814)
Append: [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843)
Append: [Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration](https://arxiv.org/abs/2505.06898)
Append: [A digital perspective on the role of a stemma in material-philological transmission studies](https://arxiv.org/abs/2505.06938)
Append: [Web Page Classification using LLMs for Crawling Support](https://arxiv.org/abs/2505.06972)
Append: [Towards the Three-Phase Dynamics of Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
Append: [LLM-Augmented Chemical Synthesis and Design Decision Programs](https://arxiv.org/abs/2505.07027)
Append: [Reassessing Large Language Model Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2505.07155)
Append: [Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition](https://arxiv.org/abs/2505.07166)
Append: [One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](https://arxiv.org/abs/2505.07167)
Append: [Securing Genomic Data Against Inference Attacks in Federated Learning Environments](https://arxiv.org/abs/2505.07188)
Append: [Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge](https://arxiv.org/abs/2505.07365)
Append: [A Survey on Collaborative Mechanisms Between Large and Small Language Models](https://arxiv.org/abs/2505.07460)
Append: [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/abs/2505.07558)
Append: [Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images](https://arxiv.org/abs/2505.07704)
Append: [Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding](https://arxiv.org/abs/2505.07768)
Append: [Clickbait Detection via Large Language Models](https://arxiv.org/abs/2306.09597)
Append: [Towards Understanding Sycophancy in Language Models](https://arxiv.org/abs/2310.13548)
Append: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)
Append: [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/abs/2405.06691)
Append: [A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis](https://arxiv.org/abs/2406.15163)
Append: [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://arxiv.org/abs/2406.17692)
Append: [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)
Append: [MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge](https://arxiv.org/abs/2408.05497)
Append: [Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2408.09701)
Append: [Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy](https://arxiv.org/abs/2409.13746)
Append: [Endless Jailbreaks with Bijection Learning](https://arxiv.org/abs/2410.01294)
Append: [Isolated Causal Effects of Natural Language](https://arxiv.org/abs/2410.14812)
Append: [Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions](https://arxiv.org/abs/2410.18966)
Append: [Evaluating Creative Short Story Generation in Humans and Large Language Models](https://arxiv.org/abs/2411.02316)
Append: [The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models](https://arxiv.org/abs/2411.03700)
Append: [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)
Append: [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](https://arxiv.org/abs/2411.15100)
Append: [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/abs/2412.05342)
Append: [Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning](https://arxiv.org/abs/2412.08587)
Append: [Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain](https://arxiv.org/abs/2412.20309)
Append: [MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments](https://arxiv.org/abs/2501.01652)
Append: [Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective](https://arxiv.org/abs/2501.11110)
Append: [Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models](https://arxiv.org/abs/2501.13428)
Append: [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/abs/2502.01568)
Append: [VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/abs/2502.12084)
Append: [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)
Append: [SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking](https://arxiv.org/abs/2503.00955)
Append: [HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition](https://arxiv.org/abs/2503.01217)
Append: [Can (A)I Change Your Mind?](https://arxiv.org/abs/2503.01844)
Append: [NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT](https://arxiv.org/abs/2503.01921)
Append: [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/abs/2503.03122)
Append: [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/abs/2503.10354)
Append: [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
Append: [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
Append: [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
Append: [Semantic and Expressive Variation in Image Captions Across Languages](https://arxiv.org/abs/2310.14356)
Append: [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)
Append: [AIOS: LLM Agent Operating System](https://arxiv.org/abs/2403.16971)
Append: [EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization](https://arxiv.org/abs/2405.15189)
Append: [MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning](https://arxiv.org/abs/2406.06620)
Append: [CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2408.14419)
Append: [Self-Data Distillation for Recovering Quality in Pruned Large Language Models](https://arxiv.org/abs/2410.09982)
Append: [Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning](https://arxiv.org/abs/2410.13439)
Append: [Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-based Prompt Injection Attacks via the Fine-Tuning Interface](https://arxiv.org/abs/2501.09798)
Append: [Training and Evaluating with Human Label Variation: An Empirical Study](https://arxiv.org/abs/2502.01891)
Append: [Integrating Expert Knowledge into Logical Programs via LLMs](https://arxiv.org/abs/2502.12275)
Append: [A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/abs/2502.14581)
Append: [SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models](https://arxiv.org/abs/2502.14908)
Append: [I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?](https://arxiv.org/abs/2503.08980)
Append: [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382)
Append: [Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines](https://arxiv.org/abs/2504.07840)
append_entries: 136
Finish: 2025-05-13 04:27:34.565166
------------------------------------------------------
Started: 2025-05-13 06:24:19.088746
Existing_entries: 1136
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 06:24:19.440681
------------------------------------------------------
Started: 2025-05-13 08:22:56.297543
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 08:22:56.697694
------------------------------------------------------
Started: 2025-05-13 10:18:16.790316
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 10:18:17.111899
------------------------------------------------------
Started: 2025-05-13 12:34:53.868107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 12:34:54.181889
------------------------------------------------------
Started: 2025-05-13 14:17:17.905696
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 14:17:18.309672
------------------------------------------------------
Started: 2025-05-13 16:21:12.955734
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 16:21:13.273154
------------------------------------------------------
Started: 2025-05-13 18:23:32.109942
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 18:23:32.452073
------------------------------------------------------
Started: 2025-05-13 20:18:24.456400
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 20:18:24.768735
------------------------------------------------------
Started: 2025-05-13 22:15:57.896985
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 22:15:58.261927
------------------------------------------------------
Started: 2025-05-14 01:18:02.163443
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 01:18:02.519141
------------------------------------------------------
Started: 2025-05-14 03:08:00.942377
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 03:08:01.312006
------------------------------------------------------
Started: 2025-05-14 04:26:11.661705
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 818
Summarized using GPT-3.5-turbo
Append: [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/abs/2505.07831)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/abs/2505.07850)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/abs/2505.07852)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/abs/2505.07853)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/abs/2505.07856)
Token length: 1758
Summarized using GPT-3.5-turbo
Append: [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/abs/2505.07857)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Scaling Laws for Speculative Decoding](https://arxiv.org/abs/2505.07858)
Token length: 1060
Summarized using GPT-3.5-turbo
Append: [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)
Token length: 681
Summarized using GPT-3.5-turbo
Append: [Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition](https://arxiv.org/abs/2505.07862)
Token length: 1723
Summarized using GPT-3.5-turbo
Append: [QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](https://arxiv.org/abs/2505.07863)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/abs/2505.07870)
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/abs/2505.07871)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [The Sound of Populism: Distinct Linguistic Features Across Populist Variants](https://arxiv.org/abs/2505.07874)
Token length: 1341
Summarized using GPT-3.5-turbo
Append: [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/abs/2505.07883)
Token length: 1836
Summarized using GPT-3.5-turbo
Append: [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/abs/2505.07884)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/abs/2505.07886)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/abs/2505.07888)
Token length: 1844
Summarized using GPT-3.5-turbo
Append: [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/abs/2505.07899)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/abs/2505.07903)
Token length: 1632
Summarized using GPT-3.5-turbo
Append: [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/abs/2505.07920)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration](https://arxiv.org/abs/2505.07980)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Large Language Models and Arabic Content: A Review](https://arxiv.org/abs/2505.08004)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/abs/2505.08037)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)
Token length: 758
Summarized using GPT-3.5-turbo
Append: [HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method](https://arxiv.org/abs/2505.08058)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/abs/2505.08106)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [Putting It All into Context: Simplifying Agents with LCLMs](https://arxiv.org/abs/2505.08120)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/abs/2505.08130)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/abs/2505.08168)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/abs/2505.08261)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow](https://arxiv.org/abs/2505.08303)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [On the Geometry of Semantics in Next-token Prediction](https://arxiv.org/abs/2505.08348)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Towards Contamination Resistant Benchmarks](https://arxiv.org/abs/2505.08389)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/abs/2505.08392)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers](https://arxiv.org/abs/2505.08402)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)
Token length: 1060
Summarized using GPT-3.5-turbo
Append: [A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court](https://arxiv.org/abs/2505.08439)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/abs/2505.08450)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
Append: [Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions](https://arxiv.org/abs/2505.08464)
Append: [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)
Append: [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)
Append: [Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding](https://arxiv.org/abs/2505.08504)
Append: [Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation](https://arxiv.org/abs/2505.08546)
Append: [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)
Append: [Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models](https://arxiv.org/abs/2505.08590)
Append: [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](https://arxiv.org/abs/2505.08600)
Append: [Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing](https://arxiv.org/abs/2505.08651)
Append: [Revealing economic facts: LLMs know more than they say](https://arxiv.org/abs/2505.08662)
Append: [Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08690)
Append: [NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context](https://arxiv.org/abs/2505.08734)
Append: [Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies](https://arxiv.org/abs/2505.08739)
Append: [AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models](https://arxiv.org/abs/2505.08750)
Append: [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/abs/2505.08751)
Append: [HealthBench: Evaluating Large Language Models Towards Improved Human Health](https://arxiv.org/abs/2505.08775)
Append: [Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding](https://arxiv.org/abs/2505.07864)
Append: [CellVerse: Do Large Language Models Really Understand Cell Biology?](https://arxiv.org/abs/2505.07865)
Append: [Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach](https://arxiv.org/abs/2505.07902)
Append: [A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny](https://arxiv.org/abs/2505.07908)
Append: [SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts](https://arxiv.org/abs/2505.07912)
Append: [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
Append: [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/abs/2505.08080)
Append: [Large Language Models for Computer-Aided Design: A Survey](https://arxiv.org/abs/2505.08137)
Append: [A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem](https://arxiv.org/abs/2505.08148)
Append: [Not that Groove: Zero-Shot Symbolic Music Editing](https://arxiv.org/abs/2505.08203)
Append: [Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency](https://arxiv.org/abs/2505.08445)
Append: [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
Append: [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/abs/2505.08638)
Append: [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
Append: [Memorization-Compression Cycles Improve Generalization](https://arxiv.org/abs/2505.08727)
Append: [CodePDE: An Inference Framework for LLM-driven PDE Solver Generation](https://arxiv.org/abs/2505.08783)
Append: [Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions](https://arxiv.org/abs/2408.06787)
Append: [Studying the Effects of Collaboration in Interactive Theme Discovery Systems](https://arxiv.org/abs/2408.09030)
Append: [From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks](https://arxiv.org/abs/2409.04168)
Append: [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/abs/2410.06205)
Append: [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/abs/2410.07002)
Append: [No Preference Left Behind: Group Distributional Preference Optimization](https://arxiv.org/abs/2412.20299)
Append: [FutureVision: A methodology for the investigation of future cognition](https://arxiv.org/abs/2502.01597)
Append: [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/abs/2502.04066)
Append: [Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data](https://arxiv.org/abs/2502.18679)
Append: [Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents](https://arxiv.org/abs/2503.04830)
Append: [CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning](https://arxiv.org/abs/2503.13517)
Append: [Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes](https://arxiv.org/abs/2503.24027)
Append: [Why do LLMs attend to the first token?](https://arxiv.org/abs/2504.02732)
Append: [AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening](https://arxiv.org/abs/2504.02870)
Append: [Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models](https://arxiv.org/abs/2504.04717)
Append: [DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning](https://arxiv.org/abs/2504.07128)
Append: [LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
Append: [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
Append: [MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation](https://arxiv.org/abs/2410.13757)
Append: [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/abs/2501.00958)
Append: [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
Append: [Vision-Language Models Do Not Understand Negation](https://arxiv.org/abs/2501.09425)
Append: [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
Append: [Adaptive Integrated Layered Attention (AILA)](https://arxiv.org/abs/2503.22742)
Append: [Efficient Adaptation For Remote Sensing Visual Grounding](https://arxiv.org/abs/2503.23083)
Append: [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
Append: [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
append_entries: 109
Finish: 2025-05-14 04:28:19.156852
------------------------------------------------------
Started: 2025-05-14 06:24:10.695034
Existing_entries: 1109
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 06:24:10.983477
------------------------------------------------------
Started: 2025-05-14 08:22:14.629183
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 08:22:14.905203
------------------------------------------------------
Started: 2025-05-14 10:17:47.481171
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 10:17:47.781346
------------------------------------------------------
Started: 2025-05-14 12:33:37.080492
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 12:33:37.355053
------------------------------------------------------
Started: 2025-05-14 14:16:51.916410
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 14:16:52.262249
------------------------------------------------------
Started: 2025-05-14 16:20:32.820366
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 16:20:33.180875
------------------------------------------------------
Started: 2025-05-14 18:21:00.601168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 18:21:00.889546
------------------------------------------------------
Started: 2025-05-14 20:15:25.088274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 20:15:25.384824
------------------------------------------------------
Started: 2025-05-14 22:13:34.172426
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 22:13:34.461730
------------------------------------------------------
Started: 2025-05-15 01:16:13.662011
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 01:16:14.030464
------------------------------------------------------
Started: 2025-05-15 03:07:33.869065
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 03:07:34.175994
------------------------------------------------------
Started: 2025-05-15 04:23:42.465084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
Token length: 1005
Summarized using GPT-3.5-turbo
Append: [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
Token length: 852
Summarized using GPT-3.5-turbo
Append: [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
Token length: 721
Summarized using GPT-3.5-turbo
Append: [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
Token length: 793
Summarized using GPT-3.5-turbo
Append: [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
Token length: 828
Summarized using GPT-3.5-turbo
Append: [LLM-based NLG Evaluation: Current Status and Challenges](https://arxiv.org/abs/2402.01383)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model](https://arxiv.org/abs/2404.03080)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering](https://arxiv.org/abs/2410.04526)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs](https://arxiv.org/abs/2411.09116)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples](https://arxiv.org/abs/2502.09650)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [PropNet: a White-Box and Human-Like Network for Sentence Representation](https://arxiv.org/abs/2502.10725)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference](https://arxiv.org/abs/2503.10652)
Token length: 1093
Summarized using GPT-3.5-turbo
Append: [Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark](https://arxiv.org/abs/2503.17599)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](https://arxiv.org/abs/2503.21696)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Is analogy enough to draw novel adjective-noun inferences?](https://arxiv.org/abs/2503.24293)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks](https://arxiv.org/abs/2411.03343)
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [FAS: Fast ANN-SNN Conversion for Spiking Large Language Models](https://arxiv.org/abs/2502.04405)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [InductionBench: LLMs Fail in the Simplest Complexity Class](https://arxiv.org/abs/2502.15823)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [An Analytical Emotion Framework of Rumour Threads on Social Media](https://arxiv.org/abs/2502.16560)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/abs/2503.11197)
append_entries: 46
Finish: 2025-05-15 04:25:30.181142
------------------------------------------------------
Started: 2025-05-15 06:24:21.829988
Existing_entries: 1046
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 06:24:21.987811
------------------------------------------------------
Started: 2025-05-15 08:22:29.976329
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 08:22:30.158590
------------------------------------------------------
Started: 2025-05-15 10:18:15.633083
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 10:18:15.809404
------------------------------------------------------
Started: 2025-05-15 12:33:45.423237
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 12:33:45.579638
------------------------------------------------------
Started: 2025-05-15 14:17:05.928336
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 14:17:06.101991
------------------------------------------------------
Started: 2025-05-15 16:20:49.317551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 16:20:49.475292
------------------------------------------------------
Started: 2025-05-15 18:22:53.030341
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 18:22:53.214931
------------------------------------------------------
Started: 2025-05-15 20:18:17.266258
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 20:18:17.436020
------------------------------------------------------
Started: 2025-05-15 22:15:13.908506
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 22:15:14.073473
------------------------------------------------------
Started: 2025-05-16 01:19:05.234892
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 01:19:05.386928
------------------------------------------------------
Started: 2025-05-16 03:10:25.697402
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 03:10:25.902579
------------------------------------------------------
Started: 2025-05-16 04:25:39.740599
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 841
Summarized using GPT-3.5-turbo
Append: [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
Token length: 1406
Summarized using GPT-3.5-turbo
Append: [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
Token length: 1558
Summarized using GPT-3.5-turbo
Append: [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
Token length: 919
Summarized using GPT-3.5-turbo
Append: [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
Token length: 1537
Summarized using GPT-3.5-turbo
Append: [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
Token length: 1156
Summarized using GPT-3.5-turbo
Append: [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
Token length: 587
Summarized using GPT-3.5-turbo
Append: [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
Token length: 1829
Summarized using GPT-3.5-turbo
Append: [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
Token length: 1961
Summarized using GPT-3.5-turbo
Append: [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [A Survey on Large Language Models in Multimodal Recommender Systems](https://arxiv.org/abs/2505.09777)
Token length: 1847
Summarized using GPT-3.5-turbo
Append: [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
Token length: 1886
Summarized using GPT-3.5-turbo
Append: [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
Append: [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
Append: [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
Append: [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
Append: [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
Append: [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
Append: [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
Append: [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
Append: [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
Append: [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
Append: [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
Append: [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
Append: [Temporal Scaling Law for Large Language Models](https://arxiv.org/abs/2404.17785)
Append: [The Mosaic Memory of Large Language Models](https://arxiv.org/abs/2405.15523)
Append: [Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization](https://arxiv.org/abs/2405.17067)
Append: [RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis](https://arxiv.org/abs/2406.00367)
Append: [PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling](https://arxiv.org/abs/2406.02069)
Append: [uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes](https://arxiv.org/abs/2407.01257)
Append: [Conversational Query Reformulation with the Guidance of Retrieved Documents](https://arxiv.org/abs/2407.12363)
Append: [PersLLM: A Personified Training Approach for Large Language Models](https://arxiv.org/abs/2407.12393)
Append: [Beyond Next Token Prediction: Patch-Level Training for Large Language Models](https://arxiv.org/abs/2407.12665)
Append: [Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners](https://arxiv.org/abs/2407.15508)
Append: [Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time](https://arxiv.org/abs/2409.13338)
Append: [MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder](https://arxiv.org/abs/2409.14074)
Append: [TopoLM: brain-like spatio-functional organization in a topographic language model](https://arxiv.org/abs/2410.11516)
Append: [How Does Knowledge Selection Help Retrieval Augmented Generation?](https://arxiv.org/abs/2410.13258)
Append: [ChronoFact: Timeline-based Temporal Fact Verification](https://arxiv.org/abs/2410.14964)
Append: [SceneGenAgent: Precise Industrial Scene Generation with Coding Agent](https://arxiv.org/abs/2410.21909)
Append: [Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction](https://arxiv.org/abs/2411.00646)
Append: [Disentangling Memory and Reasoning Ability in Large Language Models](https://arxiv.org/abs/2411.13504)
Append: [KBAlign: Efficient Self Adaptation on Specific Knowledge Bases](https://arxiv.org/abs/2411.14790)
Append: [Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477)
Append: [Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2412.03587)
Append: [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/abs/2501.00777)
Append: [Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)](https://arxiv.org/abs/2501.13957)
Append: [TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs](https://arxiv.org/abs/2501.15674)
Append: [ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning](https://arxiv.org/abs/2502.04689)
Append: [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/abs/2502.18036)
Append: [KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus](https://arxiv.org/abs/2503.06899)
Append: [Concise Reasoning via Reinforcement Learning](https://arxiv.org/abs/2504.05185)
Append: [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/abs/2504.07440)
Append: [CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives](https://arxiv.org/abs/2504.10823)
Append: [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
Append: [Pose Priors from Language Models](https://arxiv.org/abs/2405.03689)
Append: [Latent Action Pretraining from Videos](https://arxiv.org/abs/2410.11758)
Append: [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)
Append: [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
append_entries: 96
Finish: 2025-05-16 04:28:00.376203
------------------------------------------------------
Started: 2025-05-16 06:24:50.159901
Existing_entries: 1096
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 06:24:50.403957
------------------------------------------------------
Started: 2025-05-16 08:22:04.392653
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 08:22:04.648402
------------------------------------------------------
Started: 2025-05-16 10:17:38.361279
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 10:17:38.613920
------------------------------------------------------
Started: 2025-05-16 12:33:50.559468
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 12:33:50.840442
------------------------------------------------------
Started: 2025-05-16 14:16:01.577993
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 14:16:01.823176
------------------------------------------------------
Started: 2025-05-16 16:20:17.493465
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 16:20:17.738159
------------------------------------------------------
Started: 2025-05-16 18:22:27.241999
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 18:22:27.530125
------------------------------------------------------
Started: 2025-05-16 20:17:56.667442
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 20:17:56.984374
------------------------------------------------------
Started: 2025-05-16 22:15:23.444669
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 22:15:23.691005
------------------------------------------------------
Started: 2025-05-17 01:17:14.205421
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 01:17:14.518592
------------------------------------------------------
Started: 2025-05-17 03:05:25.955719
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 03:05:26.211248
------------------------------------------------------
Started: 2025-05-17 04:19:23.134244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 04:19:23.214468
------------------------------------------------------
Started: 2025-05-17 06:21:23.492830
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 06:21:23.560662
------------------------------------------------------
Started: 2025-05-17 08:19:27.543713
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 08:19:27.627887
------------------------------------------------------
Started: 2025-05-17 10:16:34.255954
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 10:16:34.318065
------------------------------------------------------
Started: 2025-05-17 12:29:58.284037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 12:29:58.347130
------------------------------------------------------
Started: 2025-05-17 14:13:39.650637
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 14:13:39.720559
------------------------------------------------------
Started: 2025-05-17 16:18:42.753874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 16:18:42.813312
------------------------------------------------------
Started: 2025-05-17 18:20:45.399746
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 18:20:45.490750
------------------------------------------------------
Started: 2025-05-17 20:16:43.439064
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 20:16:43.504094
------------------------------------------------------
Started: 2025-05-17 22:14:06.814953
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 22:14:06.893712
------------------------------------------------------
Started: 2025-05-18 01:23:51.398096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 01:23:51.496391
------------------------------------------------------
Started: 2025-05-18 03:15:15.477630
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 03:15:15.544305
------------------------------------------------------
Started: 2025-05-18 04:21:52.888062
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 04:21:52.953720
------------------------------------------------------
Started: 2025-05-18 06:21:58.213143
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 06:21:58.275032
------------------------------------------------------
Started: 2025-05-18 08:19:06.838699
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 08:19:06.930797
------------------------------------------------------
Started: 2025-05-18 10:16:00.678084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 10:16:00.772593
------------------------------------------------------
Started: 2025-05-18 12:30:22.155034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 12:30:22.219013
------------------------------------------------------
Started: 2025-05-18 14:13:42.887998
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 14:13:42.947718
------------------------------------------------------
Started: 2025-05-18 16:18:03.880707
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 16:18:03.959610
------------------------------------------------------
Started: 2025-05-18 18:20:41.159262
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 18:20:41.220329
------------------------------------------------------
Started: 2025-05-18 20:17:29.277522
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 20:17:29.338751
------------------------------------------------------
Started: 2025-05-18 22:14:39.650678
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 22:14:39.709852
------------------------------------------------------
Started: 2025-05-19 01:22:19.392978
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 01:22:19.454334
------------------------------------------------------
Started: 2025-05-19 03:15:52.418636
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 03:15:52.478690
------------------------------------------------------
Started: 2025-05-19 04:29:32.914640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/abs/2505.10643)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
Token length: 1532
Summarized using GPT-3.5-turbo
Append: [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/abs/2505.10717)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/abs/2505.10718)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.10740)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [Ranked Voting based Self-Consistency of Large Language Models](https://arxiv.org/abs/2505.10772)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [A Systematic Analysis of Base Model Choice for Reward Modeling](https://arxiv.org/abs/2505.10775)
Token length: 891
Summarized using GPT-3.5-turbo
Append: [Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10792)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets](https://arxiv.org/abs/2505.10798)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances](https://arxiv.org/abs/2505.10829)
Token length: 1575
Summarized using GPT-3.5-turbo
Append: [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs](https://arxiv.org/abs/2505.10836)
Token length: 755
Summarized using GPT-3.5-turbo
Append: [Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?](https://arxiv.org/abs/2505.10862)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](https://arxiv.org/abs/2505.10870)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents](https://arxiv.org/abs/2505.10936)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations](https://arxiv.org/abs/2505.10937)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Accurate KV Cache Quantization with Outlier Tokens Tracing](https://arxiv.org/abs/2505.10938)
Token length: 1367
Summarized using GPT-3.5-turbo
Append: [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/abs/2505.10939)
Token length: 1357
Summarized using GPT-3.5-turbo
Append: [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs](https://arxiv.org/abs/2505.10948)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio](https://arxiv.org/abs/2505.10975)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs](https://arxiv.org/abs/2505.11008)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/abs/2505.11010)
Token length: 896
Summarized using GPT-3.5-turbo
Append: [StRuCom: A Novel Dataset of Structured Code Comments in Russian](https://arxiv.org/abs/2505.11026)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/abs/2505.11031)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/abs/2505.11051)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Towards Better Evaluation for Generated Patent Claims](https://arxiv.org/abs/2505.11095)
Token length: 1793
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization](https://arxiv.org/abs/2505.11166)
Token length: 864
Summarized using GPT-3.5-turbo
Append: [Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline](https://arxiv.org/abs/2505.11177)
Token length: 1698
Summarized using GPT-3.5-turbo
Append: [NoPE: The Counting Power of Transformers with No Positional Encodings](https://arxiv.org/abs/2505.11199)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/abs/2505.11271)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Temporal fine-tuning for early risk detection](https://arxiv.org/abs/2505.11280)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision](https://arxiv.org/abs/2505.11336)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/abs/2505.11341)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors](https://arxiv.org/abs/2505.11352)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)
Token length: 1608
Summarized using GPT-3.5-turbo
Append: [A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography](https://arxiv.org/abs/2505.11379)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://arxiv.org/abs/2505.11413)
Token length: 1725
Summarized using GPT-3.5-turbo
Append: [Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model](https://arxiv.org/abs/2505.11421)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/abs/2505.11423)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/abs/2505.11462)
Append: [No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies](https://arxiv.org/abs/2505.11470)
Append: [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475)
Append: [Improving Assembly Code Performance with Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.11480)
Append: [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
Append: [Modeling cognitive processes of natural reading with transformer-based Language Models](https://arxiv.org/abs/2505.11485)
Append: [Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models](https://arxiv.org/abs/2505.10583)
Append: [Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports](https://arxiv.org/abs/2505.10586)
Append: [Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation](https://arxiv.org/abs/2505.10588)
Append: [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/abs/2505.10597)
Append: [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
Append: [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
Append: [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)
Append: [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838)
Append: [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
Append: [MatTools: Benchmarking Large Language Models for Materials Science Tools](https://arxiv.org/abs/2505.10852)
Append: [REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?](https://arxiv.org/abs/2505.10872)
Append: [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
Append: [$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection](https://arxiv.org/abs/2505.11079)
Append: [MPMA: Preference Manipulation Attack Against Model Context Protocol](https://arxiv.org/abs/2505.11154)
Append: [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/abs/2505.11165)
Append: [CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback](https://arxiv.org/abs/2505.11178)
Append: [On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms](https://arxiv.org/abs/2505.11183)
Append: [Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese](https://arxiv.org/abs/2505.11200)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks](https://arxiv.org/abs/2505.11314)
Append: [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
Append: [EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2505.11405)
Append: [Large Language Model Use Impact Locus of Control](https://arxiv.org/abs/2505.11406)
Append: [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
Append: [Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator](https://arxiv.org/abs/2305.15099)
Append: [Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?](https://arxiv.org/abs/2311.07564)
Append: [COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models](https://arxiv.org/abs/2402.14889)
Append: [ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images](https://arxiv.org/abs/2404.10652)
Append: [Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](https://arxiv.org/abs/2405.00715)
Append: [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/abs/2406.06326)
Append: [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://arxiv.org/abs/2408.06518)
Append: [Towards understanding evolution of science through language model series](https://arxiv.org/abs/2409.09636)
Append: [Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach](https://arxiv.org/abs/2409.20204)
Append: [Training of Scaffolded Language Models with Language Supervision: A Survey](https://arxiv.org/abs/2410.16392)
Append: [ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework](https://arxiv.org/abs/2410.19453)
Append: [How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP](https://arxiv.org/abs/2411.05527)
Append: [UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction](https://arxiv.org/abs/2411.07019)
Append: [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://arxiv.org/abs/2411.08135)
Append: [When to Speak, When to Abstain: Contrastive Decoding with Abstention](https://arxiv.org/abs/2412.12527)
Append: [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context](https://arxiv.org/abs/2412.12632)
Append: [XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation](https://arxiv.org/abs/2412.15529)
Append: [Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines](https://arxiv.org/abs/2501.00745)
Append: [LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena](https://arxiv.org/abs/2501.03266)
Append: [TreeKV: Smooth Key-Value Cache Compression with Tree Structures](https://arxiv.org/abs/2501.04987)
Append: [Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling](https://arxiv.org/abs/2501.10316)
Append: [Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine](https://arxiv.org/abs/2501.11885)
Append: [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/abs/2501.13977)
Append: [Do we really have to filter out random noise in pre-training data for language models?](https://arxiv.org/abs/2502.06604)
Append: [Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging](https://arxiv.org/abs/2502.06876)
Append: [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490)
Append: [Hallucination, Monofacts, and Miscalibration: An Empirical Investigation](https://arxiv.org/abs/2502.08666)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [Can Your Uncertainty Scores Detect Hallucinated Entity?](https://arxiv.org/abs/2502.11948)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing](https://arxiv.org/abs/2502.15208)
Append: [Call for Rigor in Reporting Quality of Instruction Tuning Data](https://arxiv.org/abs/2503.04807)
Append: [TigerLLM -- A Family of Bangla Large Language Models](https://arxiv.org/abs/2503.10995)
Append: [KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse](https://arxiv.org/abs/2503.16525)
Append: [Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2503.16529)
Append: [Mixture of Routers](https://arxiv.org/abs/2503.23362)
Append: [Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?](https://arxiv.org/abs/2504.01698)
Append: [Parameterized Synthetic Text Generation with SimpleStories](https://arxiv.org/abs/2504.09184)
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Append: [Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning](https://arxiv.org/abs/2403.11083)
Append: [Linear Attention Sequence Parallelism](https://arxiv.org/abs/2404.02882)
Append: [Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective](https://arxiv.org/abs/2405.13522)
Append: [Item-Language Model for Conversational Recommendation](https://arxiv.org/abs/2406.02844)
Append: [Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers](https://arxiv.org/abs/2406.11624)
Append: [Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions](https://arxiv.org/abs/2410.00031)
Append: [TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction](https://arxiv.org/abs/2410.11507)
Append: [DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search](https://arxiv.org/abs/2410.14609)
Append: [MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection](https://arxiv.org/abs/2410.14731)
Append: [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/abs/2411.02335)
Append: [Evaluating Vision-Language Models as Evaluators in Path Planning](https://arxiv.org/abs/2411.18711)
Append: [Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities](https://arxiv.org/abs/2501.02406)
Append: [Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods](https://arxiv.org/abs/2502.01384)
Append: [Shuttle Between the Instructions and the Parameters of Large Language Models](https://arxiv.org/abs/2502.02315)
Append: [MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?](https://arxiv.org/abs/2502.09933)
Append: [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/abs/2502.19676)
Append: [Structured Preference Optimization for Vision-Language Long-Horizon Task Planning](https://arxiv.org/abs/2502.20742)
Append: [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
Append: [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
append_entries: 140
Finish: 2025-05-19 04:32:19.711995
------------------------------------------------------
Started: 2025-05-19 06:25:43.413949
Existing_entries: 1140
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 06:25:43.746012
------------------------------------------------------
Started: 2025-05-19 08:24:40.082193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 08:24:40.418120
------------------------------------------------------
Started: 2025-05-19 10:18:42.642578
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 10:18:42.972156
------------------------------------------------------
Started: 2025-05-19 12:35:18.764670
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 12:35:19.088869
------------------------------------------------------
Started: 2025-05-19 14:16:53.787825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 14:16:54.128196
------------------------------------------------------
Started: 2025-05-19 16:20:44.459170
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 16:20:44.833076
------------------------------------------------------
Started: 2025-05-19 18:23:30.942168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 18:23:31.278604
------------------------------------------------------
Started: 2025-05-19 20:18:25.401477
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 20:18:25.784530
------------------------------------------------------
Started: 2025-05-19 22:16:03.787886
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 22:16:04.129114
------------------------------------------------------
Started: 2025-05-20 01:20:08.980858
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 01:20:09.369961
------------------------------------------------------
Started: 2025-05-20 03:10:13.816305
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 03:10:14.185486
------------------------------------------------------
Started: 2025-05-20 04:24:54.654973
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism](https://arxiv.org/abs/2505.11533)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification](https://arxiv.org/abs/2505.11550)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models](https://arxiv.org/abs/2505.11604)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models](https://arxiv.org/abs/2505.11613)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations](https://arxiv.org/abs/2505.11615)
Token length: 733
Summarized using GPT-3.5-turbo
Append: [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/abs/2505.11628)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2](https://arxiv.org/abs/2505.11643)
Token length: 1783
Summarized using GPT-3.5-turbo
Append: [Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks](https://arxiv.org/abs/2505.11665)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation](https://arxiv.org/abs/2505.11683)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions](https://arxiv.org/abs/2505.11690)
Token length: 706
Summarized using GPT-3.5-turbo
Append: [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/abs/2505.11733)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)
Token length: 983
Summarized using GPT-3.5-turbo
Append: [Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/abs/2505.11746)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation](https://arxiv.org/abs/2505.11754)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
Token length: 1657
Summarized using GPT-3.5-turbo
Append: [BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2505.11811)
Token length: 1793
Summarized using GPT-3.5-turbo
Append: [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
Token length: 1679
Summarized using GPT-3.5-turbo
Append: [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks](https://arxiv.org/abs/2505.11829)
Token length: 1717
Summarized using GPT-3.5-turbo
Append: [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855)
Token length: 794
Summarized using GPT-3.5-turbo
Append: [NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization](https://arxiv.org/abs/2505.11876)
Token length: 1451
Summarized using GPT-3.5-turbo
Append: [AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation](https://arxiv.org/abs/2505.11887)
Token length: 1584
Summarized using GPT-3.5-turbo
Append: [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving](https://arxiv.org/abs/2505.11893)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data](https://arxiv.org/abs/2505.11900)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [ELITE: Embedding-Less retrieval with Iterative Text Exploration](https://arxiv.org/abs/2505.11908)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning](https://arxiv.org/abs/2505.11922)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts](https://arxiv.org/abs/2505.11924)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
Token length: 938
Summarized using GPT-3.5-turbo
Append: [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English](https://arxiv.org/abs/2505.11959)
Token length: 930
Summarized using GPT-3.5-turbo
Append: [CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation](https://arxiv.org/abs/2505.11965)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
Token length: 1852
Summarized using GPT-3.5-turbo
Append: [Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation](https://arxiv.org/abs/2505.11995)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method](https://arxiv.org/abs/2505.12028)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/abs/2505.12043)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [ABoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [GenderBench: Evaluation Suite for Gender Biases in LLMs](https://arxiv.org/abs/2505.12054)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach](https://arxiv.org/abs/2505.12071)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/abs/2505.12075)
Append: [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
Append: [Personalized Author Obfuscation with Large Language Models](https://arxiv.org/abs/2505.12090)
Append: [Improving Fairness in LLMs Through Testing-Time Adversaries](https://arxiv.org/abs/2505.12100)
Append: [A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings](https://arxiv.org/abs/2505.12116)
Append: [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
Append: [Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse](https://arxiv.org/abs/2505.12160)
Append: [Truth Neurons](https://arxiv.org/abs/2505.12182)
Append: [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)
Append: [Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled](https://arxiv.org/abs/2505.12196)
Append: [How Reliable is Multilingual LLM-as-a-Judge?](https://arxiv.org/abs/2505.12201)
Append: [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
Append: [GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment](https://arxiv.org/abs/2505.12215)
Append: [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
Append: [Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers](https://arxiv.org/abs/2505.12218)
Append: [Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training](https://arxiv.org/abs/2505.12236)
Append: [PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs](https://arxiv.org/abs/2505.12238)
Append: [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)
Append: [Not All Documents Are What You Need for Extracting Instruction Tuning Data](https://arxiv.org/abs/2505.12250)
Append: [Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches](https://arxiv.org/abs/2505.12259)
Append: [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)
Append: [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
Append: [LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark](https://arxiv.org/abs/2505.12273)
Append: [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)
Append: [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
Append: [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)
Append: [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306)
Append: [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)
Append: [LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning](https://arxiv.org/abs/2505.12328)
Append: [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
Append: [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)
Append: [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
Append: [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)
Append: [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
Append: [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)
Append: [The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT](https://arxiv.org/abs/2505.12405)
Append: [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
Append: [PSC: Extending Context Window of Large Language Models via Phase Shift Calibration](https://arxiv.org/abs/2505.12423)
Append: [Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games](https://arxiv.org/abs/2505.12439)
Append: [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
Append: [Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations](https://arxiv.org/abs/2505.12454)
Append: [What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)
Append: [Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering](https://arxiv.org/abs/2505.12476)
Append: [KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation](https://arxiv.org/abs/2505.12495)
Append: [LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection](https://arxiv.org/abs/2505.12507)
Append: [DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design](https://arxiv.org/abs/2505.12511)
Append: [ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents](https://arxiv.org/abs/2505.12531)
Append: [Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE](https://arxiv.org/abs/2505.12533)
Append: [Disambiguation in Conversational Question Answering in the Era of LLM: A Survey](https://arxiv.org/abs/2505.12543)
Append: [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/abs/2505.12545)
Append: [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
Append: [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)
Append: [Enriching Patent Claim Generation with European Patent Dataset](https://arxiv.org/abs/2505.12568)
Append: [Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)
Append: [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)
Append: [CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling](https://arxiv.org/abs/2505.12587)
Append: [PromptPrism: A Linguistically-Inspired Taxonomy for Prompts](https://arxiv.org/abs/2505.12592)
Append: [AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection](https://arxiv.org/abs/2505.12594)
Append: [Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.12616)
Append: [Think Before You Attribute: Improving the Performance of LLMs Attribution Systems](https://arxiv.org/abs/2505.12621)
Append: [R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model](https://arxiv.org/abs/2505.12625)
Append: [Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing](https://arxiv.org/abs/2505.12636)
Append: [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/abs/2505.12654)
Append: [Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/abs/2505.12662)
Append: [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
Append: [ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](https://arxiv.org/abs/2505.12717)
Append: [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)
Append: [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)
Append: [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
Append: [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2505.12768)
Append: [A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/abs/2505.12781)
Append: [EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs](https://arxiv.org/abs/2505.12792)
Append: [Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2505.12808)
Append: [PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs](https://arxiv.org/abs/2505.12814)
Append: [SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models](https://arxiv.org/abs/2505.12821)
Append: [Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering](https://arxiv.org/abs/2505.12831)
Append: [FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models](https://arxiv.org/abs/2505.12835)
Append: [The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting](https://arxiv.org/abs/2505.12837)
Append: [Re-identification of De-identified Documents with Autoregressive Infilling](https://arxiv.org/abs/2505.12859)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation](https://arxiv.org/abs/2505.12888)
Append: [On the Thinking-Language Modeling Gap in Large Language Models](https://arxiv.org/abs/2505.12896)
Append: [PyFCG: Fluid Construction Grammar in Python](https://arxiv.org/abs/2505.12920)
Append: [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)
Append: [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
Append: [Neural Morphological Tagging for Nguni Languages](https://arxiv.org/abs/2505.12949)
Append: [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)
Append: [MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition](https://arxiv.org/abs/2505.12964)
Append: [Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down](https://arxiv.org/abs/2505.12969)
Append: [A Structured Literature Review on Traditional Approaches in Current Natural Language Processing](https://arxiv.org/abs/2505.12970)
Append: [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973)
Append: [An Empirical Study of Many-to-Many Summarization with Large Language Models](https://arxiv.org/abs/2505.12983)
Append: [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996)
Append: [EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code](https://arxiv.org/abs/2505.13004)
Append: [Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://arxiv.org/abs/2505.13006)
Append: [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)
Append: [topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation](https://arxiv.org/abs/2505.13034)
Append: [KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025](https://arxiv.org/abs/2505.13036)
Append: [SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation](https://arxiv.org/abs/2505.13053)
Append: [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
Append: [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
Append: [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
Append: [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)
Append: [Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning](https://arxiv.org/abs/2505.13115)
Append: [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136)
Append: [Understanding Cross-Lingual Inconsistency in Large Language Models](https://arxiv.org/abs/2505.13141)
Append: [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/abs/2505.13147)
Append: [Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice](https://arxiv.org/abs/2505.13156)
Append: [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157)
Append: [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
Append: [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
Append: [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
Append: [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)
Append: [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)
Append: [Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry](https://arxiv.org/abs/2505.13210)
Append: [SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science](https://arxiv.org/abs/2505.13220)
Append: [JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models](https://arxiv.org/abs/2505.13244)
Append: [Stronger Together: Unleashing the Social Impact of Hate Speech Research](https://arxiv.org/abs/2505.13251)
Append: [Natural Language Planning via Coding and Inference Scaling](https://arxiv.org/abs/2505.13252)
Append: [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)
Append: [WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?](https://arxiv.org/abs/2505.13257)
Append: [Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability](https://arxiv.org/abs/2505.13258)
Append: [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
Append: [Representation of perceived prosodic similarity of conversational feedback](https://arxiv.org/abs/2505.13268)
Append: [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)
Append: [$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
Append: [RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.13307)
Append: [GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)
Append: [Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges](https://arxiv.org/abs/2505.13328)
Append: [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
Append: [J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization](https://arxiv.org/abs/2505.13346)
Append: [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)
Append: [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/abs/2505.13353)
Append: [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)
Append: [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)
Append: [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
Append: [MR. Judge: Multimodal Reasoner as a Judge](https://arxiv.org/abs/2505.13403)
Append: [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/abs/2505.13404)
Append: [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417)
Append: [Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness](https://arxiv.org/abs/2505.13418)
Append: [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/abs/2505.13434)
Append: [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)
Append: [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)
Append: [TARGET: Benchmarking Table Retrieval for Generative Tasks](https://arxiv.org/abs/2505.11545)
Append: [ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems](https://arxiv.org/abs/2505.11572)
Append: [Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
Append: [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://arxiv.org/abs/2505.11611)
Append: [Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions](https://arxiv.org/abs/2505.11614)
Append: [EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents](https://arxiv.org/abs/2505.11717)
Append: [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
Append: [Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
Append: [Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders](https://arxiv.org/abs/2505.11756)
Append: [Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors](https://arxiv.org/abs/2505.11770)
Append: [VenusX: Unlocking Fine-Grained Functional Understanding of Proteins](https://arxiv.org/abs/2505.11812)
Append: [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842)
Append: [Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity](https://arxiv.org/abs/2505.11861)
Append: [J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge](https://arxiv.org/abs/2505.11875)
Append: [Introduction to Analytical Software Engineering Design Paradigm](https://arxiv.org/abs/2505.11979)
Append: [AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research](https://arxiv.org/abs/2505.12039)
Append: [Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation](https://arxiv.org/abs/2505.12058)
Append: [Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents](https://arxiv.org/abs/2505.12065)
Append: [LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs](https://arxiv.org/abs/2505.12135)
Append: [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
Append: [Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering](https://arxiv.org/abs/2505.12189)
Append: [Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling](https://arxiv.org/abs/2505.12225)
Append: [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
Append: [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
Append: [Efficient RL Training for Reasoning Models via Length-Aware Optimization](https://arxiv.org/abs/2505.12284)
Append: [Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge](https://arxiv.org/abs/2505.12301)
Append: [LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?](https://arxiv.org/abs/2505.12307)
Append: [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
Append: [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
Append: [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/abs/2505.12371)
Append: [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
Append: [UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection](https://arxiv.org/abs/2505.12457)
Append: [mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model](https://arxiv.org/abs/2505.12565)
Append: [Enhancing Latent Computation in Transformers with Latent Tokens](https://arxiv.org/abs/2505.12629)
Append: [Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents](https://arxiv.org/abs/2505.12632)
Append: [Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities](https://arxiv.org/abs/2505.12680)
Append: [Bullying the Machine: How Personas Increase LLM Vulnerability](https://arxiv.org/abs/2505.12692)
Append: [Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization](https://arxiv.org/abs/2505.12763)
Append: [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
Append: [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871)
Append: [Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/abs/2505.12886)
Append: [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
Append: [AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models](https://arxiv.org/abs/2505.12900)
Append: [Leveraging LLM Inconsistency to Boost Pass@k Performance](https://arxiv.org/abs/2505.12938)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset](https://arxiv.org/abs/2505.13028)
Append: [MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix](https://arxiv.org/abs/2505.13032)
Append: [LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs](https://arxiv.org/abs/2505.13098)
Append: [FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference](https://arxiv.org/abs/2505.13109)
Append: [Zero-Shot Iterative Formalization and Planning in Partially Observable Environments](https://arxiv.org/abs/2505.13126)
Append: [Efficient Generation of Parameterised Quantum Circuits from Large Texts](https://arxiv.org/abs/2505.13208)
Append: [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
Append: [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
Append: [Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space](https://arxiv.org/abs/2505.13308)
Append: [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition](https://arxiv.org/abs/2505.13380)
Append: [IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar](https://arxiv.org/abs/2505.13393)
Append: [A Minimum Description Length Approach to Regularization in Neural Networks](https://arxiv.org/abs/2505.13398)
Append: [CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process](https://arxiv.org/abs/2505.13408)
Append: [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430)
Append: [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
Append: [Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.13445)
Append: [Large Linguistic Models: Investigating LLMs' metalinguistic abilities](https://arxiv.org/abs/2305.00948)
Append: [Physics of Language Models: Part 1, Learning Hierarchical Language Structures](https://arxiv.org/abs/2305.13673)
Append: [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://arxiv.org/abs/2310.10378)
Append: [Automatically generating Riddles aiding Concept Attainment](https://arxiv.org/abs/2310.18290)
Append: [Streaming Sequence Transduction through Dynamic Compression](https://arxiv.org/abs/2402.01172)
Append: [Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528)
Append: [FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning](https://arxiv.org/abs/2402.12692)
Append: [Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance](https://arxiv.org/abs/2402.12819)
Append: [From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets](https://arxiv.org/abs/2404.17874)
Append: [Sparse Matrix in Large Language Model Fine-tuning](https://arxiv.org/abs/2405.15525)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation](https://arxiv.org/abs/2406.10785)
Append: [Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging](https://arxiv.org/abs/2406.16330)
Append: [Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models](https://arxiv.org/abs/2406.17513)
Append: [DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising](https://arxiv.org/abs/2407.00248)
Append: [A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding](https://arxiv.org/abs/2407.01976)
Append: [ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks](https://arxiv.org/abs/2407.18525)
Append: [SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning](https://arxiv.org/abs/2408.05517)
Append: [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](https://arxiv.org/abs/2408.08696)
Append: [Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions](https://arxiv.org/abs/2408.08780)
Append: [LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction](https://arxiv.org/abs/2408.12249)
Append: [What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices](https://arxiv.org/abs/2409.01893)
Append: [Learning Efficient Recursive Numeral Systems via Reinforcement Learning](https://arxiv.org/abs/2409.07170)
Append: [PACE: Abstractions for Communicating Efficiently](https://arxiv.org/abs/2409.20120)
Append: [SSR: Alignment-Aware Modality Connector for Speech Language Models](https://arxiv.org/abs/2410.00168)
Append: [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707)
Append: [Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions](https://arxiv.org/abs/2410.06577)
Append: [MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses](https://arxiv.org/abs/2410.07076)
Append: [Enhancing LLM Evaluations: The Garbling Trick](https://arxiv.org/abs/2411.01533)
Append: [VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs](https://arxiv.org/abs/2411.11266)
Append: [Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework](https://arxiv.org/abs/2411.16707)
Append: [Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths](https://arxiv.org/abs/2412.02466)
Append: [Intention Knowledge Graph Construction for User Intention Relation Modeling](https://arxiv.org/abs/2412.11500)
Append: [DateLogicQA: Benchmarking Temporal Biases in Large Language Models](https://arxiv.org/abs/2412.13377)
Append: [Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set](https://arxiv.org/abs/2412.14872)
Append: [Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration](https://arxiv.org/abs/2412.17061)
Append: [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506)
Append: [Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios](https://arxiv.org/abs/2501.11269)
Append: [Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning](https://arxiv.org/abs/2501.11292)
Append: [Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges](https://arxiv.org/abs/2501.11496)
Append: [AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding](https://arxiv.org/abs/2501.12162)
Append: [Option-ID Based Elimination For Multiple Choice Questions](https://arxiv.org/abs/2501.15175)
Append: [How Linguistics Learned to Stop Worrying and Love the Language Models](https://arxiv.org/abs/2501.17047)
Append: [Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models](https://arxiv.org/abs/2501.18280)
Append: [Vision-centric Token Compression in Large Language Model](https://arxiv.org/abs/2502.00791)
Append: [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/abs/2502.01179)
Append: [Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637)
Append: [Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models](https://arxiv.org/abs/2502.02444)
Append: [Reformulation for Pretraining Data Augmentation](https://arxiv.org/abs/2502.04235)
Append: [ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data](https://arxiv.org/abs/2502.05567)
Append: [Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization](https://arxiv.org/abs/2502.05605)
Append: [Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement](https://arxiv.org/abs/2502.06207)
Append: [Who Taught You That? Tracing Teachers in Model Distillation](https://arxiv.org/abs/2502.06659)
Append: [Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues](https://arxiv.org/abs/2502.09120)
Append: [RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation](https://arxiv.org/abs/2502.10996)
Append: [Beyond Pairwise: Global Zero-shot Temporal Graph Generation](https://arxiv.org/abs/2502.11114)
Append: [Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures](https://arxiv.org/abs/2502.11150)
Append: [The Mirage of Model Editing: Revisiting Evaluation in the Wild](https://arxiv.org/abs/2502.11177)
Append: [From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs](https://arxiv.org/abs/2502.11380)
Append: [Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs](https://arxiv.org/abs/2502.11525)
Append: [FaMTEB: Massive Text Embedding Benchmark in Persian Language](https://arxiv.org/abs/2502.11571)
Append: [To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models](https://arxiv.org/abs/2502.12202)
Append: [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)
Append: [Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora](https://arxiv.org/abs/2502.13691)
Append: [Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach](https://arxiv.org/abs/2502.14285)
Append: [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)
Append: [FANformer: Improving Large Language Models Through Effective Periodicity Modeling](https://arxiv.org/abs/2502.21309)
Append: [MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings](https://arxiv.org/abs/2503.03008)
Append: [Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions](https://arxiv.org/abs/2503.03261)
Append: [SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios](https://arxiv.org/abs/2503.06218)
Append: [PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs](https://arxiv.org/abs/2503.09543)
Append: [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)
Append: [UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality](https://arxiv.org/abs/2503.10669)
Append: [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)
Append: [Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models](https://arxiv.org/abs/2503.14411)
Append: [Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](https://arxiv.org/abs/2503.21380)
Append: [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729)
Append: [ImF: Implicit Fingerprint for Large Language Models](https://arxiv.org/abs/2503.21805)
Append: [Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](https://arxiv.org/abs/2503.22388)
Append: [RARE: Retrieval-Augmented Reasoning Modeling](https://arxiv.org/abs/2503.23513)
Append: [FISH-Tuning: Enhancing PEFT Methods with Fisher Information](https://arxiv.org/abs/2504.04050)
Append: [Leveraging Robust Optimization for LLM Alignment under Distribution Shifts](https://arxiv.org/abs/2504.05831)
Append: [LSR-MCTS: Alleviating Long Range Dependency in Code Generation](https://arxiv.org/abs/2504.07433)
Append: [Large Language Models Could Be Rote Learners](https://arxiv.org/abs/2504.08300)
Append: [DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation](https://arxiv.org/abs/2504.10198)
Append: [Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events](https://arxiv.org/abs/2504.12052)
Append: [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
Append: [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
Append: [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
Append: [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
Append: [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
Append: [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
Append: [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
Append: [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
Append: [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
Append: [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
Append: [UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
Append: [Computational Reasoning of Large Language Models](https://arxiv.org/abs/2504.20771)
Append: [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
Append: [PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent](https://arxiv.org/abs/2309.12555)
Append: [BAT: Learning to Reason about Spatial Sounds with Large Language Models](https://arxiv.org/abs/2402.01591)
Append: [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/abs/2403.08946)
Append: [Controlled Training Data Generation with Diffusion Models](https://arxiv.org/abs/2403.15309)
Append: [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
Append: [$S^3$ -- Semantic Signal Separation](https://arxiv.org/abs/2406.09556)
Append: [Watermarking Language Models with Error Correcting Codes](https://arxiv.org/abs/2406.10281)
Append: [Task Facet Learning: A Structured Approach to Prompt Optimization](https://arxiv.org/abs/2406.10504)
Append: [Gradient descent with generalized Newton's method](https://arxiv.org/abs/2407.02772)
Append: [EfficientQAT: Efficient Quantization-Aware Training for Large Language Models](https://arxiv.org/abs/2407.11062)
Append: ["Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation](https://arxiv.org/abs/2409.02718)
Append: [Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection](https://arxiv.org/abs/2410.02647)
Append: [Inference and Verbalization Functions During In-Context Learning](https://arxiv.org/abs/2410.09349)
Append: [Bias Similarity Across Large Language Models](https://arxiv.org/abs/2410.12010)
Append: [BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation](https://arxiv.org/abs/2410.14971)
Append: [LLMScan: Causal Scan for LLM Misbehavior Detection](https://arxiv.org/abs/2410.16638)
Append: [Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation](https://arxiv.org/abs/2410.17462)
Append: [VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939)
Append: [Training-Free Bayesianization for Low-Rank Adapters of Large Language Models](https://arxiv.org/abs/2412.05723)
Append: [MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization](https://arxiv.org/abs/2412.06141)
Append: [Superhuman performance of a large language model on the reasoning tasks of a physician](https://arxiv.org/abs/2412.10849)
Append: [Feedback-Driven Vision-Language Alignment with Minimal Human Supervision](https://arxiv.org/abs/2501.04568)
Append: [Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling](https://arxiv.org/abs/2502.00814)
Append: [Explaining Context Length Scaling and Bounds for Language Models](https://arxiv.org/abs/2502.01481)
Append: [Leveraging the true depth of LLMs](https://arxiv.org/abs/2502.02790)
Append: [Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620)
Append: [Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning](https://arxiv.org/abs/2502.11799)
Append: [ARS: Automatic Routing Solver with Large Language Models](https://arxiv.org/abs/2502.15359)
Append: [The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems](https://arxiv.org/abs/2502.16565)
Append: [Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore](https://arxiv.org/abs/2502.20034)
Append: [A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation](https://arxiv.org/abs/2502.20854)
Append: [MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment](https://arxiv.org/abs/2503.01711)
Append: [Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding](https://arxiv.org/abs/2503.13139)
Append: [DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation](https://arxiv.org/abs/2503.18225)
Append: [MaintainCoder: Maintainable Code Generation Under Dynamic Requirements](https://arxiv.org/abs/2503.24260)
Append: [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370)
Append: [Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation](https://arxiv.org/abs/2504.04453)
Append: [Signatures of human-like processing in Transformer forward passes](https://arxiv.org/abs/2504.14107)
Append: [AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning](https://arxiv.org/abs/2504.14858)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Append: [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
append_entries: 395
Finish: 2025-05-20 04:27:00.270035
------------------------------------------------------
Started: 2025-05-20 06:26:04.397935
Existing_entries: 1395
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
Token length: 1645
Summarized using GPT-3.5-turbo
Append: [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [RICo: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection](https://arxiv.org/abs/2505.05327)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
append_entries: 5
Finish: 2025-05-20 06:26:19.666697
------------------------------------------------------
Started: 2025-05-20 08:23:02.950582
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 08:23:03.790412
------------------------------------------------------
Started: 2025-05-20 10:18:44.050375
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 10:18:44.903192
------------------------------------------------------
Started: 2025-05-20 12:35:16.406592
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 12:35:17.219973
------------------------------------------------------
Started: 2025-05-20 14:16:46.580116
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 14:16:47.490675
------------------------------------------------------
Started: 2025-05-20 16:20:48.673500
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 16:20:49.482040
------------------------------------------------------
Started: 2025-05-20 18:23:29.680254
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 18:23:30.534037
------------------------------------------------------
Started: 2025-05-20 20:18:39.656533
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 20:18:40.506488
------------------------------------------------------
Started: 2025-05-20 22:15:27.733243
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 22:15:28.534784
------------------------------------------------------
Started: 2025-05-21 01:19:46.369245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 01:19:47.210846
------------------------------------------------------
Started: 2025-05-21 03:09:49.567441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 03:09:50.512521
------------------------------------------------------
Started: 2025-05-21 04:24:16.225321
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
Token length: 1103
Summarized using GPT-3.5-turbo
Append: [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
Token length: 730
Summarized using GPT-3.5-turbo
Append: [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
Token length: 900
Summarized using GPT-3.5-turbo
Append: [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
Token length: 963
Summarized using GPT-3.5-turbo
Append: [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
Token length: 1933
Summarized using GPT-3.5-turbo
Append: [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
Token length: 811
Summarized using GPT-3.5-turbo
Append: [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
Token length: 1559
Summarized using GPT-3.5-turbo
Append: [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
Token length: 1158
Summarized using GPT-3.5-turbo
Append: [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
Append: [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
Append: [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
Append: [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
Append: [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
Append: [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
Append: [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
Append: [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
Append: [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
Append: [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
Append: [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
Append: [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
Append: [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
Append: [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
Append: [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
Append: [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
Append: [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
Append: [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
Append: [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
Append: [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
Append: [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
Append: [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
Append: [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
Append: [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
Append: [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
Append: ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
Append: [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
Append: [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
Append: [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
Append: [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
Append: [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
Append: [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
Append: [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
Append: [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
Append: [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
Append: [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
Append: [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
Append: [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
Append: [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
Append: [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
Append: [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
Append: [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
Append: [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
Append: [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
Append: [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
Append: [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
Append: [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
Append: [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
Append: [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
Append: [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
Append: [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
Append: [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
Append: [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
Append: [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
Append: [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
Append: [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
Append: [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
Append: [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
Append: [Void in Language Models](https://arxiv.org/abs/2505.14467)
Append: [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
Append: [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
Append: [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
Append: [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
Append: [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
Append: [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
Append: [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
Append: [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
Append: [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
Append: [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
Append: [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
Append: [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
Append: [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
Append: [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
Append: [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
Append: [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
Append: [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
Append: [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
Append: [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
Append: [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
Append: [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
Append: [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
Append: [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
Append: [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
Append: [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
Append: [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
Append: [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
Append: [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
Append: [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
Append: [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
Append: [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
Append: [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
Append: [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
Append: [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
Append: [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
Append: [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
Append: [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
Append: [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
Append: [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
Append: [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
Append: [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
Append: [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
Append: [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
Append: [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
Append: [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
Append: [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
Append: [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
Append: [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
Append: [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
Append: [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
Append: [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
Append: [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
Append: [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
Append: [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
Append: [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
Append: [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
Append: [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
Append: [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
Append: [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
Append: [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
Append: [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
Append: [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
Append: [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
Append: [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
Append: [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
Append: [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
Append: [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
Append: [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
Append: [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
Append: [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
Append: [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
Append: [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
Append: [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
Append: [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
Append: [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
Append: [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
Append: [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
Append: [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
Append: [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
Append: [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
Append: [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
Append: [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
Append: [Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy](https://arxiv.org/abs/2312.10097)
Append: [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)
Append: [PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games](https://arxiv.org/abs/2404.17662)
Append: [Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs](https://arxiv.org/abs/2407.01082)
Append: [PersonaGym: Evaluating Persona Agents and LLMs](https://arxiv.org/abs/2407.18416)
Append: [Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework](https://arxiv.org/abs/2409.00054)
Append: [RoMath: A Mathematical Reasoning Benchmark in Romanian](https://arxiv.org/abs/2409.11074)
Append: [Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing](https://arxiv.org/abs/2409.11726)
Append: [Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review](https://arxiv.org/abs/2410.03663)
Append: [SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition](https://arxiv.org/abs/2410.10624)
Append: [RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals](https://arxiv.org/abs/2410.11348)
Append: [Interpreting token compositionality in LLMs: A robustness analysis](https://arxiv.org/abs/2410.12924)
Append: [The Mystery of the Pathological Path-star Task for Language Models](https://arxiv.org/abs/2410.13779)
Append: [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)
Append: [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/abs/2410.15522)
Append: [Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics](https://arxiv.org/abs/2410.21272)
Append: [Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models](https://arxiv.org/abs/2411.02448)
Append: [Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training](https://arxiv.org/abs/2411.14318)
Append: [Can LLMs be Good Graph Judge for Knowledge Graph Construction?](https://arxiv.org/abs/2411.17388)
Append: [A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension](https://arxiv.org/abs/2412.06245)
Append: [A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges](https://arxiv.org/abs/2412.11936)
Append: [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161)
Append: [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)
Append: [SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs](https://arxiv.org/abs/2412.16783)
Append: [Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts](https://arxiv.org/abs/2501.02009)
Append: [ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting](https://arxiv.org/abs/2501.06582)
Append: [TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time](https://arxiv.org/abs/2501.07482)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection](https://arxiv.org/abs/2501.15451)
Append: [People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text](https://arxiv.org/abs/2501.15654)
Append: [Improving LLM Unlearning Robustness via Random Perturbations](https://arxiv.org/abs/2501.19202)
Append: [LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information](https://arxiv.org/abs/2502.02095)
Append: [Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs](https://arxiv.org/abs/2502.02362)
Append: [A comparison of translation performance between DeepL and Supertext](https://arxiv.org/abs/2502.02577)
Append: [Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation](https://arxiv.org/abs/2502.02789)
Append: [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)
Append: [CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment](https://arxiv.org/abs/2502.11066)
Append: [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/abs/2502.11100)
Append: [Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment](https://arxiv.org/abs/2502.11733)
Append: [FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2502.11811)
Append: [EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models](https://arxiv.org/abs/2502.11916)
Append: [R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](https://arxiv.org/abs/2502.12767)
Append: [Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection](https://arxiv.org/abs/2502.13061)
Append: [TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation](https://arxiv.org/abs/2502.13442)
Append: [DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation](https://arxiv.org/abs/2502.14037)
Append: [Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction](https://arxiv.org/abs/2502.14171)
Append: [Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare](https://arxiv.org/abs/2502.16051)
Append: [SQLong: Enhanced NL2SQL for Longer Contexts with LLMs](https://arxiv.org/abs/2502.16747)
Append: [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/abs/2502.16894)
Append: [Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs](https://arxiv.org/abs/2502.16901)
Append: [Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models](https://arxiv.org/abs/2502.19982)
Append: [Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing](https://arxiv.org/abs/2502.20592)
Append: [CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation](https://arxiv.org/abs/2502.21074)
Append: [Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey](https://arxiv.org/abs/2503.01513)
Append: [MCiteBench: A Multimodal Benchmark for Generating Text with Citations](https://arxiv.org/abs/2503.02589)
Append: [Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms](https://arxiv.org/abs/2503.04372)
Append: [Cost-Optimal Grouped-Query Attention for Long-Context Modeling](https://arxiv.org/abs/2503.09579)
Append: [RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs](https://arxiv.org/abs/2503.10657)
Append: [MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection](https://arxiv.org/abs/2503.18132)
Append: [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)
Append: [Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models](https://arxiv.org/abs/2504.08399)
Append: [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)
Append: [Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
Append: [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
Append: [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
Append: [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
Append: [HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis Generation](https://arxiv.org/abs/2505.00038)
Append: [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
Append: [Adaptive Thinking via Mode Policy Optimization for Social Language Agents](https://arxiv.org/abs/2505.02156)
Append: [From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)
Append: [Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech](https://arxiv.org/abs/2410.01162)
Append: [IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models](https://arxiv.org/abs/2410.02429)
Append: [Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment](https://arxiv.org/abs/2410.09083)
Append: [Large Continual Instruction Assistant](https://arxiv.org/abs/2410.10868)
Append: [Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study](https://arxiv.org/abs/2410.17980)
Append: [ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models](https://arxiv.org/abs/2412.04756)
Append: [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559)
Append: [MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents](https://arxiv.org/abs/2501.08828)
Append: [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.12368)
Append: [Fairshare Data Pricing via Data Valuation for Large Language Models](https://arxiv.org/abs/2502.00198)
Append: [From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios](https://arxiv.org/abs/2502.02145)
Append: [Training Language Models to Reason Efficiently](https://arxiv.org/abs/2502.04463)
Append: [VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks](https://arxiv.org/abs/2502.11163)
Append: [EquiBench: Benchmarking Large Language Models' Understanding of Program Semantics via Equivalence Checking](https://arxiv.org/abs/2502.12466)
Append: [DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning](https://arxiv.org/abs/2502.12623)
Append: [Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study](https://arxiv.org/abs/2503.06794)
Append: [VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models](https://arxiv.org/abs/2503.07575)
Append: [Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More](https://arxiv.org/abs/2503.10542)
Append: [CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.14232)
Append: [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)
Append: [Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions](https://arxiv.org/abs/2503.16505)
Append: [Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding](https://arxiv.org/abs/2504.01281)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
Append: [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
append_entries: 291
Finish: 2025-05-21 04:26:10.770767
------------------------------------------------------
Started: 2025-05-21 06:25:14.275548
Existing_entries: 1291
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 988
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [Technical Report: Quantifying and Analyzing the Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/abs/2505.07558)
append_entries: 3
Finish: 2025-05-21 06:25:21.275814
------------------------------------------------------
Started: 2025-05-21 08:22:12.489475
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 08:22:13.116071
------------------------------------------------------
Started: 2025-05-21 10:18:11.354685
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 10:18:11.974912
------------------------------------------------------
Started: 2025-05-21 12:34:07.703990
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 12:34:08.326567
------------------------------------------------------
Started: 2025-05-21 14:17:26.799262
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 14:17:27.506976
------------------------------------------------------
Started: 2025-05-21 16:21:12.123448
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 16:21:12.759748
------------------------------------------------------
Started: 2025-05-21 18:23:37.410084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 18:23:38.036958
------------------------------------------------------
Started: 2025-05-21 20:18:20.404359
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 20:18:21.088953
------------------------------------------------------
Started: 2025-05-21 22:15:39.845177
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 22:15:40.493513
------------------------------------------------------
Started: 2025-05-22 01:18:51.220002
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 01:18:51.929571
------------------------------------------------------
Started: 2025-05-22 03:09:58.222323
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 03:09:58.868096
------------------------------------------------------
Started: 2025-05-22 04:25:00.150692
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 758
Summarized using GPT-3.5-turbo
Append: [Addressing the Challenges of Planning Language Generation](https://arxiv.org/abs/2505.14763)
Token length: 696
Summarized using GPT-3.5-turbo
Append: [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/abs/2505.14824)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/abs/2505.14845)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)
Token length: 536
Summarized using GPT-3.5-turbo
Append: [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/abs/2505.14852)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/abs/2505.14871)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Incorporating Token Usage into Prompting Strategy Evaluation](https://arxiv.org/abs/2505.14880)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters](https://arxiv.org/abs/2505.14886)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties](https://arxiv.org/abs/2505.14887)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/abs/2505.14892)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [Concept Incongruence: An Exploration of Time and Death in Role Playing](https://arxiv.org/abs/2505.14905)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain](https://arxiv.org/abs/2505.14906)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories](https://arxiv.org/abs/2505.14917)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/abs/2505.14918)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/abs/2505.14925)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [MedBrowseComp: Benchmarking Medical Deep Research and Computer Use](https://arxiv.org/abs/2505.14963)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies](https://arxiv.org/abs/2505.14972)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [CRAFT: Training-Free Cascaded Retrieval for Tabular QA](https://arxiv.org/abs/2505.14984)
Token length: 1780
Summarized using GPT-3.5-turbo
Append: [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](https://arxiv.org/abs/2505.14992)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/abs/2505.14996)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems](https://arxiv.org/abs/2505.15000)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/abs/2505.15024)
Token length: 909
Summarized using GPT-3.5-turbo
Append: [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)
Token length: 726
Summarized using GPT-3.5-turbo
Append: [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective](https://arxiv.org/abs/2505.15045)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)
Token length: 1745
Summarized using GPT-3.5-turbo
Append: [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/abs/2505.15050)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/abs/2505.15055)
Token length: 1873
Summarized using GPT-3.5-turbo
Append: [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking](https://arxiv.org/abs/2505.15063)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [In-Domain African Languages Translation Using LLMs and Multi-armed Bandits](https://arxiv.org/abs/2505.15069)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Understand Internet Buzzwords Through User-Generated Content](https://arxiv.org/abs/2505.15071)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
Token length: 1027
Summarized using GPT-3.5-turbo
Append: [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora](https://arxiv.org/abs/2505.15087)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2505.15090)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models](https://arxiv.org/abs/2505.15094)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
Append: [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
Append: [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)
Append: [RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals](https://arxiv.org/abs/2505.15110)
Append: [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/abs/2505.15117)
Append: [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154)
Append: [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)
Append: [EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association](https://arxiv.org/abs/2505.15196)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs](https://arxiv.org/abs/2505.15210)
Append: [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
Append: [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)
Append: [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/abs/2505.15245)
Append: [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/abs/2505.15249)
Append: [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/abs/2505.15255)
Append: [When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners](https://arxiv.org/abs/2505.15257)
Append: [AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection](https://arxiv.org/abs/2505.15261)
Append: [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/abs/2505.15277)
Append: [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/abs/2505.15282)
Append: [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/abs/2505.15291)
Append: [Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites](https://arxiv.org/abs/2505.15297)
Append: [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/abs/2505.15299)
Append: [Emotional Supporters often Use Multiple Strategies in a Single Turn](https://arxiv.org/abs/2505.15316)
Append: [Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack](https://arxiv.org/abs/2505.15323)
Append: [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/abs/2505.15333)
Append: [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
Append: [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/abs/2505.15347)
Append: [The Super Emotion Dataset](https://arxiv.org/abs/2505.15348)
Append: [Revealing Language Model Trajectories via Kullback-Leibler Divergence](https://arxiv.org/abs/2505.15353)
Append: [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/abs/2505.15355)
Append: [NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging](https://arxiv.org/abs/2505.15356)
Append: [X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System](https://arxiv.org/abs/2505.15372)
Append: [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)
Append: [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)
Append: [An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations](https://arxiv.org/abs/2505.15392)
Append: [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404)
Append: [Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches](https://arxiv.org/abs/2505.15422)
Append: [Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models](https://arxiv.org/abs/2505.15424)
Append: [NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish](https://arxiv.org/abs/2505.15426)
Append: [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/abs/2505.15427)
Append: [Likelihood Variance as Text Importance for Resampling Texts to Map Language Models](https://arxiv.org/abs/2505.15428)
Append: [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431)
Append: [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/abs/2505.15442)
Append: [AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs](https://arxiv.org/abs/2505.15443)
Append: [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/abs/2505.15444)
Append: [Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment](https://arxiv.org/abs/2505.15456)
Append: [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/abs/2505.15467)
Append: [CoLA: Collaborative Low-Rank Adaptation](https://arxiv.org/abs/2505.15471)
Append: [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/abs/2505.15472)
Append: [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2505.15475)
Append: [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
Append: [Collaborative Problem-Solving in an Optimization Game](https://arxiv.org/abs/2505.15490)
Append: [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/abs/2505.15501)
Append: [Multilingual Test-Time Scaling via Initial Thought Transfer](https://arxiv.org/abs/2505.15508)
Append: [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/abs/2505.15524)
Append: [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/abs/2505.15553)
Append: [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/abs/2505.15554)
Append: [A Survey on Multilingual Mental Disorders Detection from Social Media Data](https://arxiv.org/abs/2505.15556)
Append: [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561)
Append: [Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis](https://arxiv.org/abs/2505.15563)
Append: [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/abs/2505.15607)
Append: [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612)
Append: [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/abs/2505.15623)
Append: [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/abs/2505.15633)
Append: [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
Append: [Word Level Timestamp Generation for Automatic Speech Recognition and Translation](https://arxiv.org/abs/2505.15646)
Append: [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656)
Append: [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
Append: [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674)
Append: [The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect](https://arxiv.org/abs/2505.15682)
Append: [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683)
Append: [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/abs/2505.15684)
Append: [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
Append: [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/abs/2505.15695)
Append: [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/abs/2505.15696)
Append: ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
Append: [LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing](https://arxiv.org/abs/2505.15702)
Append: [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710)
Append: [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/abs/2505.15712)
Append: [Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling](https://arxiv.org/abs/2505.15715)
Append: [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/abs/2505.15722)
Append: [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/abs/2505.15727)
Append: [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)
Append: [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/abs/2505.15769)
Append: [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774)
Append: [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776)
Append: [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)
Append: [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781)
Append: [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/abs/2505.15792)
Append: [Reverse Engineering Human Preferences with Reinforcement Learning](https://arxiv.org/abs/2505.15795)
Append: [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
Append: [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805)
Append: [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/abs/2505.15807)
Append: [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/abs/2505.15810)
Append: [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)
Append: [Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers](https://arxiv.org/abs/2505.14692)
Append: [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/abs/2505.14699)
Append: [QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding](https://arxiv.org/abs/2505.14723)
Append: [MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models](https://arxiv.org/abs/2505.14728)
Append: [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/abs/2505.14826)
Append: [Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs](https://arxiv.org/abs/2505.14899)
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
Append: [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
Append: [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
Append: [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/abs/2505.15068)
Append: [An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc](https://arxiv.org/abs/2505.15070)
Append: [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
Append: [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
Append: [ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://arxiv.org/abs/2505.15158)
Append: [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
Append: [BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems](https://arxiv.org/abs/2505.15216)
Append: [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
Append: [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/abs/2505.15276)
Append: [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
Append: [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
Append: [AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals](https://arxiv.org/abs/2505.15365)
Append: [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
Append: [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/abs/2505.15400)
Append: [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/abs/2505.15410)
Append: [Set-LLM: A Permutation-Invariant LLM](https://arxiv.org/abs/2505.15433)
Append: [A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach](https://arxiv.org/abs/2505.15466)
Append: [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
Append: [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.15510)
Append: [Explainable embeddings with Distance Explainer](https://arxiv.org/abs/2505.15516)
Append: [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
Append: [MIRB: Mathematical Information Retrieval Benchmark](https://arxiv.org/abs/2505.15585)
Append: [Mechanistic Insights into Grokking from the Embedding Layer](https://arxiv.org/abs/2505.15624)
Append: [Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information](https://arxiv.org/abs/2505.15667)
Append: [HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases](https://arxiv.org/abs/2505.15701)
Append: [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738)
Append: [Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications](https://arxiv.org/abs/2505.15741)
Append: [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753)
Append: [MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling](https://arxiv.org/abs/2505.15772)
Append: [ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality](https://arxiv.org/abs/2505.15773)
Append: [Large Language Models as Computable Approximations to Solomonoff Induction](https://arxiv.org/abs/2505.15784)
Append: [Predicting generalization performance with correctness discriminators](https://arxiv.org/abs/2311.09422)
Append: [A Framework for Real-time Safeguarding the Text Generation of Large Language Model](https://arxiv.org/abs/2404.19048)
Append: [MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset](https://arxiv.org/abs/2406.02106)
Append: [Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis](https://arxiv.org/abs/2406.12719)
Append: [Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior](https://arxiv.org/abs/2407.02099)
Append: [A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting](https://arxiv.org/abs/2407.11638)
Append: [dMel: Speech Tokenization made Simple](https://arxiv.org/abs/2407.15835)
Append: [Fine-tuning Large Language Models for Entity Matching](https://arxiv.org/abs/2409.08185)
Append: [A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/abs/2410.08109)
Append: [Meta-Chunking: Learning Text Segmentation and Semantic Completion via Logical Perception](https://arxiv.org/abs/2410.12788)
Append: [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852)
Append: [GATEAU: Selecting Influential Samples for Long Context Alignment](https://arxiv.org/abs/2410.15633)
Append: [Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models](https://arxiv.org/abs/2410.16168)
Append: [Robust and Minimally Invasive Watermarking for EaaS](https://arxiv.org/abs/2410.17552)
Append: [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/abs/2410.21728)
Append: [Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains](https://arxiv.org/abs/2411.07417)
Append: [FastDraft: How to Train Your Draft](https://arxiv.org/abs/2411.11055)
Append: [Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues](https://arxiv.org/abs/2412.09049)
Append: [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/abs/2412.10138)
Append: [DARWIN 1.5: Large Language Models as Materials Science Adapted Learners](https://arxiv.org/abs/2412.11970)
Append: [Exploring Cross-lingual Latent Transplantation: Mutual Opportunities and Open Challenges](https://arxiv.org/abs/2412.12686)
Append: [MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering](https://arxiv.org/abs/2412.15540)
Append: [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)
Append: [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)
Append: [Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter](https://arxiv.org/abs/2501.14491)
Append: [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2502.00299)
Append: [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)
Append: [Lifelong Knowledge Editing requires Better Regularization](https://arxiv.org/abs/2502.01636)
Append: [BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation](https://arxiv.org/abs/2502.01697)
Append: [Can LLMs Maintain Fundamental Abilities under KV Cache Compression?](https://arxiv.org/abs/2502.01941)
Append: [Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization](https://arxiv.org/abs/2502.04295)
Append: [An Analysis for Reasoning Bias of Language Models with Small Initialization](https://arxiv.org/abs/2502.04375)
Append: [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/abs/2502.04964)
Append: [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316)
Append: [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/abs/2502.11441)
Append: [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/abs/2502.11471)
Append: [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/abs/2502.12562)
Append: [Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs](https://arxiv.org/abs/2502.13195)
Append: [FineEdit: Unlock Instruction-Based Text Editing for LLMs](https://arxiv.org/abs/2502.13358)
Append: [Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval](https://arxiv.org/abs/2502.13369)
Append: [UniKnow: A Unified Framework for Reliable Language Model Behavior across Parametric and External Knowledge](https://arxiv.org/abs/2502.13648)
Append: [Rapid Word Learning Through Meta In-Context Learning](https://arxiv.org/abs/2502.14791)
Append: [Sparsity May Be All You Need: Sparse Random Parameter Adaptation](https://arxiv.org/abs/2502.15975)
Append: [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/abs/2502.16825)
Append: [Spontaneous Giving and Calculated Greed in Language Models](https://arxiv.org/abs/2502.17720)
Append: [Stay Focused: Problem Drift in Multi-Agent Debate](https://arxiv.org/abs/2502.19559)
Append: [Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs](https://arxiv.org/abs/2502.19721)
Append: [Adaptively profiling models with task elicitation](https://arxiv.org/abs/2503.01986)
Append: [Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations](https://arxiv.org/abs/2503.03062)
Append: [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/abs/2503.03122)
Append: [DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL](https://arxiv.org/abs/2503.04959)
Append: [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/abs/2503.05179)
Append: [Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning](https://arxiv.org/abs/2503.06072)
Append: [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)
Append: [Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning](https://arxiv.org/abs/2503.15952)
Append: [AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text](https://arxiv.org/abs/2503.18247)
Append: [A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications](https://arxiv.org/abs/2503.20302)
Append: [Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/abs/2503.24245)
Append: [GLiNER-BioMed: A Suite of Efficient Models for Open Biomedical Named Entity Recognition](https://arxiv.org/abs/2504.00676)
Append: [Think When You Need: Self-Adaptive Chain-of-Thought Learning](https://arxiv.org/abs/2504.03234)
Append: [Thinking Out Loud: Do Reasoning Models Know When They're Right?](https://arxiv.org/abs/2504.06564)
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Append: [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
Append: [Improving Language Model Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
Append: [Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Append: [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
Append: [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
Append: [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
Append: [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
Append: [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
Append: [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
Append: [SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network](https://arxiv.org/abs/2310.06488)
Append: [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264)
Append: [DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/abs/2404.18922)
Append: [Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing](https://arxiv.org/abs/2405.11783)
Append: [SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model](https://arxiv.org/abs/2406.12030)
Append: [Parameter-Efficient Fine-Tuning via Circular Convolution](https://arxiv.org/abs/2407.19342)
Append: [An In-Depth Investigation of Data Collection in LLM App Ecosystems](https://arxiv.org/abs/2408.13247)
Append: [NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls](https://arxiv.org/abs/2409.03797)
Append: [Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2410.06981)
Append: [Parameter Efficient Fine-tuning via Explained Variance Adaptation](https://arxiv.org/abs/2410.07170)
Append: [How to Construct Random Unitaries](https://arxiv.org/abs/2410.10116)
Append: [WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning](https://arxiv.org/abs/2501.16344)
Append: [PixelWorld: Towards Perceiving Everything as Pixels](https://arxiv.org/abs/2501.19339)
Append: [How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence](https://arxiv.org/abs/2502.00678)
Append: [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)
Append: [The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles](https://arxiv.org/abs/2502.01081)
Append: [Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution](https://arxiv.org/abs/2502.06809)
Append: [MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition](https://arxiv.org/abs/2502.10447)
Append: [Probing Semantic Routing in Large Mixture-of-Expert Models](https://arxiv.org/abs/2502.10928)
Append: [Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization](https://arxiv.org/abs/2502.11140)
Append: [GiFT: Gibbs Fine-Tuning for Code Generation](https://arxiv.org/abs/2502.11466)
Append: [Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation](https://arxiv.org/abs/2502.14846)
Append: [Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM Reasoning](https://arxiv.org/abs/2502.17216)
Append: [Large Language Models are Powerful Electronic Health Record Encoders](https://arxiv.org/abs/2502.17403)
Append: [ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction](https://arxiv.org/abs/2502.18744)
Append: [SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic](https://arxiv.org/abs/2503.07996)
Append: [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Design and Implementation of an FPGA-Based Hardware Accelerator for Transformer](https://arxiv.org/abs/2503.16731)
Append: [Plain Transformers Can be Powerful Graph Learners](https://arxiv.org/abs/2504.12588)
Append: [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
Append: [Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
Append: [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
Append: [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315)
append_entries: 288
Finish: 2025-05-22 04:26:48.503834
------------------------------------------------------
Started: 2025-05-22 06:24:39.848926
Existing_entries: 1288
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
Token length: 1547
Summarized using GPT-3.5-turbo
Append: [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
append_entries: 3
Finish: 2025-05-22 06:24:47.496843
------------------------------------------------------
Started: 2025-05-22 08:25:06.511999
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 08:25:07.187998
------------------------------------------------------
Started: 2025-05-22 10:18:16.315037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 10:18:17.025357
------------------------------------------------------
Started: 2025-05-22 12:34:57.470534
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 12:34:58.070158
------------------------------------------------------
Started: 2025-05-22 14:16:38.128724
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 14:16:38.741083
------------------------------------------------------
Started: 2025-05-22 16:20:57.760111
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 16:20:58.471178
------------------------------------------------------
Started: 2025-05-22 18:23:07.590818
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 18:23:08.208239
------------------------------------------------------
Started: 2025-05-22 20:18:25.348077
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 20:18:25.981756
------------------------------------------------------
Started: 2025-05-22 22:16:05.593090
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 22:16:06.192508
------------------------------------------------------
Started: 2025-05-23 01:19:00.778553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 01:19:01.476595
------------------------------------------------------
Started: 2025-05-23 03:09:21.731424
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 03:09:22.381982
------------------------------------------------------
Started: 2025-05-23 04:30:47.266131
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/abs/2505.15922)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
Token length: 1721
Summarized using GPT-3.5-turbo
Append: [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/abs/2505.15960)
Token length: 1021
Summarized using GPT-3.5-turbo
Append: [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
Token length: 846
Summarized using GPT-3.5-turbo
Append: [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/abs/2505.15993)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
Token length: 1743
Summarized using GPT-3.5-turbo
Append: [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
Token length: 1274
Summarized using GPT-3.5-turbo
Append: [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/abs/2505.16036)
Token length: 936
Summarized using GPT-3.5-turbo
Append: [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/abs/2505.16061)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/abs/2505.16102)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/abs/2505.16107)
Token length: 812
Summarized using GPT-3.5-turbo
Append: [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/abs/2505.16118)
Token length: 1591
Summarized using GPT-3.5-turbo
Append: [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/abs/2505.16125)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/abs/2505.16128)
Token length: 932
Summarized using GPT-3.5-turbo
Append: [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/abs/2505.16129)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
Token length: 1038
Summarized using GPT-3.5-turbo
Append: [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/abs/2505.16162)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/abs/2505.16164)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/abs/2505.16178)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/abs/2505.16188)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/abs/2505.16189)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
Token length: 952
Summarized using GPT-3.5-turbo
Append: [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/abs/2505.16212)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/abs/2505.16277)
Append: [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
Append: [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293)
Append: [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
Append: [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/abs/2505.16303)
Append: [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
Append: [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
Append: [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
Append: [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/abs/2505.16348)
Append: [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/abs/2505.16349)
Append: [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
Append: [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2505.16385)
Append: [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
Append: [On the reliability of feature attribution methods for speech classification](https://arxiv.org/abs/2505.16406)
Append: [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
Append: [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
Append: [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
Append: [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/abs/2505.16418)
Append: [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
Append: [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
Append: [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
Append: [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
Append: [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
Append: [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
Append: [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
Append: [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
Append: [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
Append: [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
Append: [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
Append: [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/abs/2505.16526)
Append: [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/abs/2505.16566)
Append: [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570)
Append: [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
Append: [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
Append: [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/abs/2505.16591)
Append: [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
Append: [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/abs/2505.16610)
Append: [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
Append: [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
Append: [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
Append: [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
Append: [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
Append: [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
Append: [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/abs/2505.16703)
Append: [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
Append: [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
Append: [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/abs/2505.16774)
Append: [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.16782)
Append: [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
Append: [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/abs/2505.16800)
Append: [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/abs/2505.16806)
Append: [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
Append: [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
Append: [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
Append: [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/abs/2505.16838)
Append: [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/abs/2505.16847)
Append: [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
Append: [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/abs/2505.16868)
Append: [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869)
Append: [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
Append: [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
Append: [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
Append: [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
Append: [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/abs/2505.16931)
Append: [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934)
Append: [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/abs/2505.16956)
Append: [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
Append: [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/abs/2505.16972)
Append: [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/abs/2505.16973)
Append: [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
Append: [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
Append: [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
Append: [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/abs/2505.16995)
Append: [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
Append: [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
Append: [InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872)
Append: [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
Append: [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
Append: [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
Append: [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
Append: [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
Append: [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
Append: [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
Append: [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
Append: [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
Append: [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
Append: [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
Append: [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
Append: [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
Append: [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
Append: [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
Append: [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
Append: [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
Append: [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
Append: [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
Append: [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
Append: [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
Append: [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
Append: [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
Append: [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/abs/2505.16220)
Append: [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
Append: [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
Append: [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
Append: [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
Append: [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
Append: [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
Append: [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
Append: [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)
Append: [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
Append: [MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries](https://arxiv.org/abs/2505.16631)
Append: [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
Append: [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
Append: [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
Append: [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
Append: [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
Append: [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
Append: [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
Append: [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888)
Append: [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
Append: [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
Append: [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
Append: [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/abs/2505.16944)
Append: [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/abs/2505.16964)
Append: [Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](https://arxiv.org/abs/2505.16967)
Append: [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
Append: [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
Append: [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16984)
Append: [$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning](https://arxiv.org/abs/2505.16994)
Append: [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/abs/2505.16997)
Append: [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015)
Append: [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
Append: [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/abs/2505.17022)
Append: [Language Models are Universal Embedders](https://arxiv.org/abs/2310.08232)
Append: [Large Language Models are Miscalibrated In-Context Learners](https://arxiv.org/abs/2312.13772)
Append: [EntGPT: Entity Linking with Generative Large Language Models](https://arxiv.org/abs/2402.06738)
Append: [Red-Teaming for Inducing Societal Bias in Large Language Models](https://arxiv.org/abs/2405.04756)
Append: [BlockPruner: Fine-grained Pruning for Large Language Models](https://arxiv.org/abs/2406.10594)
Append: [Determination of language families using deep learning](https://arxiv.org/abs/2409.02393)
Append: [MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2409.02813)
Append: [Normal forms in Virus Machines](https://arxiv.org/abs/2409.03327)
Append: [LangSAMP: Language-Script Aware Multilingual Pretraining](https://arxiv.org/abs/2409.18199)
Append: [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/abs/2410.05254)
Append: [Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering](https://arxiv.org/abs/2410.08085)
Append: [Keys to Robust Edits: from Theoretical Insights to Practical Advances](https://arxiv.org/abs/2410.09338)
Append: [A Unified Approach to Routing and Cascading for LLMs](https://arxiv.org/abs/2410.10347)
Append: [Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination](https://arxiv.org/abs/2410.17477)
Append: [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
Append: [AAAR-1.0: Assessing AI's Potential to Assist Research](https://arxiv.org/abs/2410.22394)
Append: [Graph-based Confidence Calibration for Large Language Models](https://arxiv.org/abs/2411.02454)
Append: [Prompt-Guided Internal States for Hallucination Detection of Large Language Models](https://arxiv.org/abs/2411.04847)
Append: [Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings](https://arxiv.org/abs/2412.01031)
Append: [Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study](https://arxiv.org/abs/2412.06272)
Append: [My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement for Personalized Implicit Emotion Analysis](https://arxiv.org/abs/2412.07367)
Append: [LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework](https://arxiv.org/abs/2412.12459)
Append: [DocFusion: A Unified Framework for Document Parsing Tasks](https://arxiv.org/abs/2412.12505)
Append: [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
Append: [BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism](https://arxiv.org/abs/2412.17933)
Append: [Diverse Preference Optimization](https://arxiv.org/abs/2501.18101)
Append: [ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration](https://arxiv.org/abs/2502.00675)
Append: [FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training](https://arxiv.org/abs/2502.00761)
Append: [Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data](https://arxiv.org/abs/2502.04380)
Append: [Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type](https://arxiv.org/abs/2502.06086)
Append: [LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs](https://arxiv.org/abs/2502.06139)
Append: [C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation](https://arxiv.org/abs/2502.06205)
Append: [No Need for Explanations: LLMs can implicitly learn from mistakes in-context](https://arxiv.org/abs/2502.08550)
Append: [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/abs/2502.09604)
Append: [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
Append: [GRIFFIN: Effective Token Alignment for Faster Speculative Decoding](https://arxiv.org/abs/2502.11018)
Append: [SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL](https://arxiv.org/abs/2502.11438)
Append: [M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2502.11824)
Append: [Whose story is it? Personalizing story generation by inferring author styles](https://arxiv.org/abs/2502.13028)
Append: [Transferring Textual Preferences to Vision-Language Understanding through Model Merging](https://arxiv.org/abs/2502.13487)
Append: [CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations](https://arxiv.org/abs/2502.15132)
Append: [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](https://arxiv.org/abs/2502.16002)
Append: [FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks](https://arxiv.org/abs/2502.17775)
Append: [Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments](https://arxiv.org/abs/2502.17956)
Append: [Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents](https://arxiv.org/abs/2502.20073)
Append: [HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization](https://arxiv.org/abs/2503.04598)
Append: [ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews](https://arxiv.org/abs/2503.08506)
Append: [Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence](https://arxiv.org/abs/2503.14749)
Append: [From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment](https://arxiv.org/abs/2503.15463)
Append: [Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants, and Markets](https://arxiv.org/abs/2503.16674)
Append: [Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning](https://arxiv.org/abs/2503.16965)
Append: [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Append: [DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](https://arxiv.org/abs/2503.19498)
Append: [Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching](https://arxiv.org/abs/2503.20083)
Append: [TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling](https://arxiv.org/abs/2504.07053)
Append: [Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance](https://arxiv.org/abs/2504.09753)
Append: [Hallucination Detection in LLMs with Topological Divergence on Attention Graphs](https://arxiv.org/abs/2504.10063)
Append: [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
Append: [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
Append: [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
Append: [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
Append: [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
Append: [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
Append: [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Append: [Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework](https://arxiv.org/abs/2505.03563)
Append: [LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/abs/2505.05423)
Append: [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
Append: [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
Append: [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization](https://arxiv.org/abs/2305.04971)
Append: [CodeMind: Evaluating Large Language Models for Code Reasoning](https://arxiv.org/abs/2402.09664)
Append: [FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering](https://arxiv.org/abs/2405.13873)
Append: [How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities](https://arxiv.org/abs/2407.08112)
Append: [More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding](https://arxiv.org/abs/2408.15966)
Append: [Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for Exploration and Exploitation in Recommender Systems](https://arxiv.org/abs/2411.13865)
Append: [Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts](https://arxiv.org/abs/2412.04614)
Append: [Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian](https://arxiv.org/abs/2501.11264)
Append: [To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization](https://arxiv.org/abs/2502.00691)
Append: [Slamming: Training a Speech Language Model on One GPU in a Day](https://arxiv.org/abs/2502.15814)
Append: [Similarity-Distance-Magnitude Universal Verification](https://arxiv.org/abs/2502.20167)
Append: [Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG](https://arxiv.org/abs/2503.01222)
Append: [Steer LLM Latents for Hallucination Detection](https://arxiv.org/abs/2503.01917)
Append: [Transformers for molecular property prediction: Domain adaptation efficiently improves performance](https://arxiv.org/abs/2503.03360)
Append: [Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts](https://arxiv.org/abs/2503.05066)
Append: [MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?](https://arxiv.org/abs/2503.09499)
Append: [Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models](https://arxiv.org/abs/2503.20576)
append_entries: 271
Finish: 2025-05-23 04:35:33.824028
------------------------------------------------------
Started: 2025-05-23 06:24:29.000999
Existing_entries: 1271
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
append_entries: 4
Finish: 2025-05-23 06:24:37.080464
------------------------------------------------------
Started: 2025-05-23 08:22:14.266211
Existing_entries: 1004
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 08:22:14.846135
------------------------------------------------------
Started: 2025-05-23 10:17:46.897739
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 10:17:47.478582
------------------------------------------------------
Started: 2025-05-23 12:33:10.922271
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 12:33:11.590726
------------------------------------------------------
Started: 2025-05-23 14:15:59.605118
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 14:16:00.225100
------------------------------------------------------
Started: 2025-05-23 16:20:00.026904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 16:20:00.669140
------------------------------------------------------
Started: 2025-05-23 18:21:27.355076
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 18:21:27.938311
------------------------------------------------------
Started: 2025-05-23 20:18:20.074644
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 20:18:20.702629
------------------------------------------------------
Started: 2025-05-23 22:15:46.373433
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 22:15:46.946569
------------------------------------------------------
Started: 2025-05-24 01:16:09.705148
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 01:16:10.289553
------------------------------------------------------
Started: 2025-05-24 03:03:51.247139
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 03:03:51.916913
------------------------------------------------------
Started: 2025-05-24 04:18:59.578741
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 04:18:59.643109
------------------------------------------------------
Started: 2025-05-24 06:21:09.447247
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 06:21:09.524598
------------------------------------------------------
Started: 2025-05-24 08:19:02.227821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 08:19:02.345400
------------------------------------------------------
Started: 2025-05-24 10:15:39.260693
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 10:15:39.320388
------------------------------------------------------
Started: 2025-05-24 12:30:09.592412
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 12:30:09.657835
------------------------------------------------------
Started: 2025-05-24 14:13:47.071589
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 14:13:47.133103
------------------------------------------------------
Started: 2025-05-24 16:18:12.897787
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 16:18:12.950882
------------------------------------------------------
Started: 2025-05-24 18:20:31.315128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 18:20:31.369411
------------------------------------------------------
Started: 2025-05-24 20:16:41.143313
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 20:16:41.196972
------------------------------------------------------
Started: 2025-05-24 22:14:41.105882
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 22:14:41.164455
------------------------------------------------------
Started: 2025-05-25 01:25:47.440750
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 01:25:47.521681
------------------------------------------------------
Started: 2025-05-25 03:17:43.175539
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 03:17:43.244043
------------------------------------------------------
Started: 2025-05-25 04:23:13.399152
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 04:23:13.474157
------------------------------------------------------
Started: 2025-05-25 06:21:29.464935
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 06:21:29.535799
------------------------------------------------------
Started: 2025-05-25 08:19:15.733380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 08:19:15.789991
------------------------------------------------------
Started: 2025-05-25 10:16:04.479884
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 10:16:04.539247
------------------------------------------------------
Started: 2025-05-25 12:30:25.520204
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 12:30:25.591784
------------------------------------------------------
Started: 2025-05-25 14:13:56.524611
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 14:13:56.599616
------------------------------------------------------
Started: 2025-05-25 16:18:30.668141
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 16:18:30.719751
------------------------------------------------------
Started: 2025-05-25 18:20:29.639196
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 18:20:29.694466
------------------------------------------------------
Started: 2025-05-25 20:16:44.848153
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 20:16:44.963651
------------------------------------------------------
Started: 2025-05-25 22:15:01.068796
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 22:15:01.127998
------------------------------------------------------
Started: 2025-05-26 01:21:06.135810
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 01:21:06.218666
------------------------------------------------------
Started: 2025-05-26 03:14:34.396552
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 03:14:34.457337
------------------------------------------------------
Started: 2025-05-26 04:26:27.806251
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
Token length: 1595
Summarized using GPT-3.5-turbo
Append: [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
Token length: 1506
Summarized using GPT-3.5-turbo
Append: [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
Token length: 1803
Summarized using GPT-3.5-turbo
Append: [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\'esum\'e Evaluations](https://arxiv.org/abs/2505.17049)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
Token length: 1757
Summarized using GPT-3.5-turbo
Append: [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
Token length: 948
Summarized using GPT-3.5-turbo
Append: [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
Token length: 1758
Summarized using GPT-3.5-turbo
Append: [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
Token length: 878
Summarized using GPT-3.5-turbo
Append: [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/abs/2505.17071)
Token length: 1253
Summarized using GPT-3.5-turbo
Append: [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)
Token length: 1650
Summarized using GPT-3.5-turbo
Append: [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)
Token length: 820
Summarized using GPT-3.5-turbo
Append: [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/abs/2505.17089)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/abs/2505.17095)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/abs/2505.17099)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/abs/2505.17102)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/abs/2505.17110)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek](https://arxiv.org/abs/2505.17112)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
Append: [Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data](https://arxiv.org/abs/2505.17116)
Append: [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
Append: [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/abs/2505.17118)
Append: [Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models](https://arxiv.org/abs/2505.17119)
Append: [Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training](https://arxiv.org/abs/2505.17120)
Append: [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/abs/2505.17121)
Append: [Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?](https://arxiv.org/abs/2505.17122)
Append: [MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation](https://arxiv.org/abs/2505.17123)
Append: [Conformal Language Model Reasoning with Coherent Factuality](https://arxiv.org/abs/2505.17126)
Append: [Relative Bias: A Comparative Framework for Quantifying Bias in LLMs](https://arxiv.org/abs/2505.17131)
Append: [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)
Append: [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)
Append: [Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations](https://arxiv.org/abs/2505.17136)
Append: [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)
Append: [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
Append: [Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs](https://arxiv.org/abs/2505.17140)
Append: [MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models](https://arxiv.org/abs/2505.17144)
Append: [Large Language Models for Predictive Analysis: How Far Are They?](https://arxiv.org/abs/2505.17149)
Append: [Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions](https://arxiv.org/abs/2505.17151)
Append: [Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN](https://arxiv.org/abs/2505.17153)
Append: [PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG](https://arxiv.org/abs/2505.17156)
Append: [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160)
Append: [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/abs/2505.17167)
Append: [Next Token Perception Score: Analytical Assessment of your LLM Perception Skills](https://arxiv.org/abs/2505.17169)
Append: [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/abs/2505.17206)
Append: [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/abs/2505.17217)
Append: [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/abs/2505.17222)
Append: [ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects](https://arxiv.org/abs/2505.17231)
Append: [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2505.17238)
Append: [ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models](https://arxiv.org/abs/2505.17244)
Append: [ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models](https://arxiv.org/abs/2505.17250)
Append: [The Rise of Parameter Specialization for Knowledge Storage in Large Language Models](https://arxiv.org/abs/2505.17260)
Append: [CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports](https://arxiv.org/abs/2505.17265)
Append: [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/abs/2505.17266)
Append: [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/abs/2505.17267)
Append: [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/abs/2505.17281)
Append: [SELF: Self-Extend the Context Length With Logistic Growth Function](https://arxiv.org/abs/2505.17296)
Append: [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
Append: [Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2](https://arxiv.org/abs/2505.17320)
Append: [From Compression to Expansion: A Layerwise Analysis of In-Context Learning](https://arxiv.org/abs/2505.17322)
Append: [GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints](https://arxiv.org/abs/2505.17327)
Append: [SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332)
Append: [Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking](https://arxiv.org/abs/2505.17345)
Append: [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)
Append: [AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing](https://arxiv.org/abs/2505.17380)
Append: [WiNGPT-3.0 Technical Report](https://arxiv.org/abs/2505.17387)
Append: [Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting](https://arxiv.org/abs/2505.17390)
Append: [Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation](https://arxiv.org/abs/2505.17391)
Append: [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](https://arxiv.org/abs/2505.17399)
Append: [Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?](https://arxiv.org/abs/2505.17407)
Append: [Conversations: Love Them, Hate Them, Steer Them](https://arxiv.org/abs/2505.17413)
Append: [DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](https://arxiv.org/abs/2505.17420)
Append: [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/abs/2505.17427)
Append: [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)
Append: [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/abs/2505.17446)
Append: [LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization](https://arxiv.org/abs/2505.17447)
Append: [Towards Evaluating Proactive Risk Awareness of Multimodal Language Models](https://arxiv.org/abs/2505.17455)
Append: [Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.17464)
Append: [A Position Paper on the Automatic Generation of Machine Learning Leaderboards](https://arxiv.org/abs/2505.17465)
Append: [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://arxiv.org/abs/2505.17470)
Append: [FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain](https://arxiv.org/abs/2505.17471)
Append: [MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning](https://arxiv.org/abs/2505.17481)
Append: [keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection](https://arxiv.org/abs/2505.17485)
Append: [Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models](https://arxiv.org/abs/2505.17496)
Append: [CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents](https://arxiv.org/abs/2505.17503)
Append: [L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](https://arxiv.org/abs/2505.17505)
Append: [Large Language Models Do Multi-Label Classification Differently](https://arxiv.org/abs/2505.17510)
Append: [Multimodal Conversation Structure Understanding](https://arxiv.org/abs/2505.17536)
Append: [How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception](https://arxiv.org/abs/2505.17537)
Append: [Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition](https://arxiv.org/abs/2505.17538)
Append: [Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection](https://arxiv.org/abs/2505.17558)
Append: [PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models](https://arxiv.org/abs/2505.17565)
Append: [Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation](https://arxiv.org/abs/2505.17571)
Append: [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
Append: [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/abs/2505.17612)
Append: [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/abs/2505.17616)
Append: [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/abs/2505.17625)
Append: [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/abs/2505.17630)
Append: [Stereotype Detection in Natural Language Processing](https://arxiv.org/abs/2505.17642)
Append: [Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks](https://arxiv.org/abs/2505.17643)
Append: [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)
Append: [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
Append: [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)
Append: [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)
Append: [MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis](https://arxiv.org/abs/2505.17671)
Append: [Tuning Language Models for Robust Prediction of Diverse User Behaviors](https://arxiv.org/abs/2505.17682)
Append: [ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction](https://arxiv.org/abs/2505.17691)
Append: [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/abs/2505.17697)
Append: [SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus](https://arxiv.org/abs/2505.17704)
Append: [Understanding How Value Neurons Shape the Generation of Specified Values in LLMs](https://arxiv.org/abs/2505.17712)
Append: [The Pilot Corpus of the English Semantic Sketches](https://arxiv.org/abs/2505.17733)
Append: [Fast Quiet-STaR: Thinking Without Thought Tokens](https://arxiv.org/abs/2505.17746)
Append: [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)
Append: [Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.17762)
Append: [The Real Barrier to LLM Agent Usability is Agentic ROI](https://arxiv.org/abs/2505.17767)
Append: [EXECUTE: A Multilingual Benchmark for LLM Token Understanding](https://arxiv.org/abs/2505.17784)
Append: [Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion](https://arxiv.org/abs/2505.17793)
Append: [DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors](https://arxiv.org/abs/2505.17795)
Append: [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning](https://arxiv.org/abs/2505.17813)
Append: [Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong](https://arxiv.org/abs/2505.17816)
Append: [Not All Tokens Are What You Need In Thinking](https://arxiv.org/abs/2505.17827)
Append: [Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning](https://arxiv.org/abs/2505.17829)
Append: [Emerging categories in scientific explanations](https://arxiv.org/abs/2505.17832)
Append: [Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus](https://arxiv.org/abs/2505.17833)
Append: [Explaining Sources of Uncertainty in Automated Fact-Checking](https://arxiv.org/abs/2505.17855)
Append: [Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods](https://arxiv.org/abs/2505.17870)
Append: [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)
Append: [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/abs/2505.17894)
Append: [Language models can learn implicit multi-hop reasoning, but only if they have lots of training data](https://arxiv.org/abs/2505.17923)
Append: [Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models](https://arxiv.org/abs/2505.17950)
Append: [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952)
Append: [Counting Cycles with Deepseek](https://arxiv.org/abs/2505.17964)
Append: [AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web](https://arxiv.org/abs/2505.17978)
Append: [TRACE for Tracking the Emergence of Semantic Representations in Transformers](https://arxiv.org/abs/2505.17998)
Append: [Training with Pseudo-Code for Instruction Following](https://arxiv.org/abs/2505.18011)
Append: [Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition](https://arxiv.org/abs/2505.18040)
Append: [MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving](https://arxiv.org/abs/2505.18056)
Append: [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/abs/2505.18071)
Append: [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)
Append: [Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL](https://arxiv.org/abs/2505.18098)
Append: [ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework](https://arxiv.org/abs/2505.18105)
Append: [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/abs/2505.18110)
Append: [UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification](https://arxiv.org/abs/2505.18122)
Append: [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
Append: [Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection](https://arxiv.org/abs/2505.18136)
Append: [Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find](https://arxiv.org/abs/2505.18148)
Append: [First Finish Search: Efficient Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2505.18149)
Append: [Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs](https://arxiv.org/abs/2505.18152)
Append: [The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas](https://arxiv.org/abs/2505.18154)
Append: [Generalizing Large Language Model Usability Across Resource-Constrained](https://arxiv.org/abs/2505.17040)
Append: [Exploring EFL Secondary Students' AI-generated Text Editing While Composition Writing](https://arxiv.org/abs/2505.17041)
Append: [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)
Append: [GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis](https://arxiv.org/abs/2505.17085)
Append: [From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data](https://arxiv.org/abs/2505.17088)
Append: [Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech](https://arxiv.org/abs/2505.17093)
Append: [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
Append: [Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities](https://arxiv.org/abs/2505.17109)
Append: [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
Append: [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
Append: [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/abs/2505.17163)
Append: [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)
Append: [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
Append: [Zebra-Llama: Towards Extremely Efficient Hybrid Models](https://arxiv.org/abs/2505.17272)
Append: [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
Append: [Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning](https://arxiv.org/abs/2505.17315)
Append: [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
Append: [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
Append: [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/abs/2505.17331)
Append: [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/abs/2505.17348)
Append: [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
Append: [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.17373)
Append: [Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts](https://arxiv.org/abs/2505.17374)
Append: [LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context](https://arxiv.org/abs/2505.17410)
Append: [Speechless: Speech Instruction Training Without Speech for Low Resource Languages](https://arxiv.org/abs/2505.17417)
Append: [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/abs/2505.17425)
Append: [Self-Training Large Language Models with Confident Reasoning](https://arxiv.org/abs/2505.17454)
Append: [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/abs/2505.17461)
Append: [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
Append: [From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark](https://arxiv.org/abs/2505.17482)
Append: [PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate](https://arxiv.org/abs/2505.17492)
Append: [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)
Append: [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
Append: [Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs](https://arxiv.org/abs/2505.17512)
Append: [What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection](https://arxiv.org/abs/2505.17513)
Append: [Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models](https://arxiv.org/abs/2505.17519)
Append: [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
Append: [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.17553)
Append: [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
Append: [One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs](https://arxiv.org/abs/2505.17598)
Append: [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/abs/2505.17607)
Append: [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/abs/2505.17613)
Append: [Large language model as user daily behavior data generator: balancing population diversity and individual personality](https://arxiv.org/abs/2505.17615)
Append: [Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis](https://arxiv.org/abs/2505.17636)
Append: [HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning](https://arxiv.org/abs/2505.17645)
Append: [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
Append: [PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization](https://arxiv.org/abs/2505.17714)
Append: [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions](https://arxiv.org/abs/2505.17818)
Append: [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
Append: [T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation](https://arxiv.org/abs/2505.17897)
Append: [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
Append: [Understanding Gated Neurons in Transformers from Their Input-Output Functionality](https://arxiv.org/abs/2505.17936)
Append: [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](https://arxiv.org/abs/2505.17968)
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
Append: [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
Append: [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
Append: [Data Mixing Can Induce Phase Transitions in Knowledge Acquisition](https://arxiv.org/abs/2505.18091)
Append: [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
Append: [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
Append: [ProgRM: Build Better GUI Agents with Progress Rewards](https://arxiv.org/abs/2505.18121)
Append: [TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations](https://arxiv.org/abs/2505.18125)
Append: [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/abs/2505.18126)
Append: [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
Append: [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
Append: [Gaming Tool Preferences in Agentic LLMs](https://arxiv.org/abs/2505.18135)
Append: [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources](https://arxiv.org/abs/2310.07147)
Append: [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)
Append: [Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy](https://arxiv.org/abs/2405.03111)
Append: [ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios](https://arxiv.org/abs/2405.10808)
Append: [Mitigate Position Bias in Large Language Models via Scaling a Single Dimension](https://arxiv.org/abs/2406.02536)
Append: [ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods](https://arxiv.org/abs/2406.15968)
Append: [Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks](https://arxiv.org/abs/2407.00869)
Append: [Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation](https://arxiv.org/abs/2407.01796)
Append: [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)
Append: [Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models](https://arxiv.org/abs/2407.21077)
Append: [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/abs/2408.10615)
Append: [Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models](https://arxiv.org/abs/2409.10999)
Append: [Task Arithmetic for Language Expansion in Speech Translation](https://arxiv.org/abs/2409.11274)
Append: [From Lists to Emojis: How Format Bias Affects Model Alignment](https://arxiv.org/abs/2409.11704)
Append: [Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning](https://arxiv.org/abs/2409.12887)
Append: [Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models](https://arxiv.org/abs/2409.14364)
Append: [Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization](https://arxiv.org/abs/2409.17673)
Append: [KCIF: Knowledge-Conditioned Instruction Following](https://arxiv.org/abs/2410.12972)
Append: [TrendFact: A Benchmark for Explainable Hotspot Perception in Fact-Checking with Natural Language Explanation](https://arxiv.org/abs/2410.15135)
Append: [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/abs/2410.15639)
Append: [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)
Append: [Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning](https://arxiv.org/abs/2410.20926)
Append: [LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines](https://arxiv.org/abs/2411.16365)
Append: [Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning](https://arxiv.org/abs/2411.19557)
Append: [MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines](https://arxiv.org/abs/2412.02271)
Append: [UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models](https://arxiv.org/abs/2412.11803)
Append: [Boosting Long-Context Management via Query-Guided Activation Refilling](https://arxiv.org/abs/2412.12486)
Append: [TrustRAG: Enhancing Robustness and Trustworthiness in Retrieval-Augmented Generation](https://arxiv.org/abs/2501.00879)
Append: [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/abs/2501.04686)
Append: [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694)
Append: [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346)
Append: [TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection](https://arxiv.org/abs/2501.11960)
Append: [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/abs/2501.16975)
Append: [CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality](https://arxiv.org/abs/2502.08923)
Append: [Beyond One-Size-Fits-All Pruning via Evolutionary Metric Search for Large Language Models](https://arxiv.org/abs/2502.10735)
Append: [1bit-Merging: Dynamic Quantized Merging for Large Language Models](https://arxiv.org/abs/2502.10743)
Append: [LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging](https://arxiv.org/abs/2502.10749)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs](https://arxiv.org/abs/2502.11228)
Append: [System Message Generation for User Preferences using Open-Source Models](https://arxiv.org/abs/2502.11330)
Append: [Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI](https://arxiv.org/abs/2502.11614)
Append: [Personality Editing for Language Models through Relevant Knowledge Editing](https://arxiv.org/abs/2502.11789)
Append: [PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery](https://arxiv.org/abs/2502.12594)
Append: [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)
Append: [Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests](https://arxiv.org/abs/2502.14359)
Append: [ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis](https://arxiv.org/abs/2502.14614)
Append: [Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs](https://arxiv.org/abs/2502.14645)
Append: [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/abs/2502.15594)
Append: [Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation](https://arxiv.org/abs/2502.16529)
Append: [PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance](https://arxiv.org/abs/2502.17041)
Append: [Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews](https://arxiv.org/abs/2502.17086)
Append: [Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective](https://arxiv.org/abs/2502.17262)
Append: [Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review](https://arxiv.org/abs/2502.19614)
Append: [Do Retrieval-Augmented Language Models Adapt to Varying User Needs?](https://arxiv.org/abs/2502.19779)
Append: [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/abs/2503.02003)
Append: [Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models](https://arxiv.org/abs/2503.02623)
Append: [SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection](https://arxiv.org/abs/2503.03303)
Append: [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/abs/2503.04556)
Append: [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)
Append: [MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation](https://arxiv.org/abs/2503.17900)
Append: [A Retrieval-Based Approach to Medical Procedure Matching in Romanian](https://arxiv.org/abs/2503.20556)
Append: [Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging](https://arxiv.org/abs/2503.20641)
Append: [Cognitive Debiasing Large Language Models for Decision-Making](https://arxiv.org/abs/2504.04141)
Append: [SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog](https://arxiv.org/abs/2504.07199)
Append: [ConceptCarve: Dynamic Realization of Evidence](https://arxiv.org/abs/2504.07228)
Append: [Playpen: An Environment for Exploring Learning Through Conversational Interaction](https://arxiv.org/abs/2504.08590)
Append: [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456)
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Append: [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
Append: [Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property](https://arxiv.org/abs/2405.14522)
Append: [Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity](https://arxiv.org/abs/2406.14479)
Append: [Fundamental Limitations on Subquadratic Alternatives to Transformers](https://arxiv.org/abs/2410.04271)
Append: [MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling](https://arxiv.org/abs/2410.13610)
Append: [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/abs/2412.12119)
Append: [FBQuant: FeedBack Quantization for Large Language Models](https://arxiv.org/abs/2501.16385)
Append: [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2501.17116)
Append: [WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training](https://arxiv.org/abs/2501.18511)
Append: [Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533)
Append: [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/abs/2501.19306)
Append: [GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models](https://arxiv.org/abs/2502.01406)
Append: [Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs](https://arxiv.org/abs/2502.01926)
Append: [Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Reasoning](https://arxiv.org/abs/2502.10440)
Append: [Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning](https://arxiv.org/abs/2502.11799)
Append: [Provably Correct Automata Embeddings for Optimal Automata-Conditioned Reinforcement Learning](https://arxiv.org/abs/2503.05042)
Append: [StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization](https://arxiv.org/abs/2504.05804)
Append: [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261)
Append: [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
Append: [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
Append: [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
append_entries: 338
Finish: 2025-05-26 04:28:03.495728
------------------------------------------------------
Started: 2025-05-26 06:26:06.199943
Existing_entries: 1338
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
append_entries: 5
Finish: 2025-05-26 06:26:19.018598
------------------------------------------------------
Started: 2025-05-26 08:35:39.721530
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1430
Summarized using GPT-3.5-turbo
Append: [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
append_entries: 1
Finish: 2025-05-26 08:35:43.786190
------------------------------------------------------
Started: 2025-05-26 10:21:02.873200
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 10:21:03.613807
------------------------------------------------------
Started: 2025-05-26 12:33:12.007667
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 12:33:12.736202
------------------------------------------------------
Started: 2025-05-26 14:16:38.766128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 14:16:39.494597
------------------------------------------------------
Started: 2025-05-26 16:19:50.194213
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 16:19:50.893869
------------------------------------------------------
Started: 2025-05-26 18:21:52.577873
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 18:21:53.320226
------------------------------------------------------
Started: 2025-05-26 20:18:08.211632
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 20:18:08.899947
------------------------------------------------------
Started: 2025-05-26 22:15:15.323000
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 22:15:16.097041
------------------------------------------------------
Started: 2025-05-27 01:18:19.299410
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 01:18:19.995835
------------------------------------------------------
Started: 2025-05-27 03:09:03.308809
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 03:09:04.073008
------------------------------------------------------
Started: 2025-05-27 04:26:38.070620
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
Token length: 1752
Summarized using GPT-3.5-turbo
Append: [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards](https://arxiv.org/abs/2505.18298)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)
Token length: 1154
Summarized using GPT-3.5-turbo
Append: [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Model Editing with Graph-Based External Memory](https://arxiv.org/abs/2505.18343)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [RaDeR: Reasoning-aware Dense Retrieval Models](https://arxiv.org/abs/2505.18405)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [BRIT: Bidirectional Retrieval over Unified Image-Text Graph](https://arxiv.org/abs/2505.18450)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [MedScore: Factuality Evaluation of Free-Form Medical Answers](https://arxiv.org/abs/2505.18452)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Hybrid Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.18454)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Measuring South Asian Biases in Large Language Models](https://arxiv.org/abs/2505.18466)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models](https://arxiv.org/abs/2505.18497)
Token length: 1897
Summarized using GPT-3.5-turbo
Append: [How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation](https://arxiv.org/abs/2505.18522)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [metaTextGrad: Automatically optimizing language model optimizers](https://arxiv.org/abs/2505.18524)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [Business as \textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs](https://arxiv.org/abs/2505.18542)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Composable Cross-prompt Essay Scoring by Merging Models](https://arxiv.org/abs/2505.18548)
Token length: 933
Summarized using GPT-3.5-turbo
Append: [MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors](https://arxiv.org/abs/2505.18549)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)
Token length: 1491
Summarized using GPT-3.5-turbo
Append: [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations](https://arxiv.org/abs/2505.18609)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)
Token length: 980
Summarized using GPT-3.5-turbo
Append: [Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models](https://arxiv.org/abs/2505.18638)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster](https://arxiv.org/abs/2505.18642)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)
Append: [Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change](https://arxiv.org/abs/2505.18653)
Append: [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)
Append: [Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models](https://arxiv.org/abs/2505.18673)
Append: [Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts](https://arxiv.org/abs/2505.18677)
Append: [TULUN: Transparent and Adaptable Low-resource Machine Translation](https://arxiv.org/abs/2505.18683)
Append: [From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation](https://arxiv.org/abs/2505.18685)
Append: [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
Append: [Benchmarking and Rethinking Knowledge Editing for Large Language Models](https://arxiv.org/abs/2505.18690)
Append: [Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task](https://arxiv.org/abs/2505.18703)
Append: [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)
Append: [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)
Append: [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)
Append: [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges](https://arxiv.org/abs/2505.18744)
Append: [Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning](https://arxiv.org/abs/2505.18752)
Append: [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
Append: [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
Append: [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)
Append: [Disentangling Knowledge Representations for Large Language Model Editing](https://arxiv.org/abs/2505.18774)
Append: [A generalised editor calculus (Short Paper)](https://arxiv.org/abs/2505.18778)
Append: [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
Append: [Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation](https://arxiv.org/abs/2505.18842)
Append: [Multi-Party Conversational Agents: A Survey](https://arxiv.org/abs/2505.18845)
Append: [Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation](https://arxiv.org/abs/2505.18853)
Append: [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)
Append: [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)
Append: [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)
Append: [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)
Append: [StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos](https://arxiv.org/abs/2505.18903)
Append: [Building a Functional Machine Translation Corpus for Kpelle](https://arxiv.org/abs/2505.18905)
Append: [Federated Retrieval-Augmented Generation: A Systematic Mapping Study](https://arxiv.org/abs/2505.18906)
Append: [SCRum-9: Multilingual Stance Classification over Rumours on Social Media](https://arxiv.org/abs/2505.18916)
Append: [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)
Append: [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems](https://arxiv.org/abs/2505.18943)
Append: [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)
Append: [BnMMLU: Measuring Massive Multitask Language Understanding in Bengali](https://arxiv.org/abs/2505.18951)
Append: [Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?](https://arxiv.org/abs/2505.18953)
Append: [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)
Append: [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)
Append: [Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE](https://arxiv.org/abs/2505.18971)
Append: [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)
Append: [AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2505.18978)
Append: [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)
Append: [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization](https://arxiv.org/abs/2505.19000)
Append: [CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language](https://arxiv.org/abs/2505.19018)
Append: [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)
Append: [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
Append: [UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models](https://arxiv.org/abs/2505.19060)
Append: [Towards Harmonized Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2505.19073)
Append: [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)
Append: [ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning](https://arxiv.org/abs/2505.19100)
Append: [WHISTRESS: Enriching Transcriptions with Sentence Stress Detection](https://arxiv.org/abs/2505.19103)
Append: [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)
Append: [Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering](https://arxiv.org/abs/2505.19112)
Append: [Controlling Language Confusion in Multilingual LLMs](https://arxiv.org/abs/2505.19116)
Append: [Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models](https://arxiv.org/abs/2505.19121)
Append: [MMATH: A Multilingual Benchmark for Mathematical Reasoning](https://arxiv.org/abs/2505.19126)
Append: [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)
Append: [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
Append: [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)
Append: [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
Append: [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)
Append: [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)
Append: [Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection](https://arxiv.org/abs/2505.19191)
Append: [DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](https://arxiv.org/abs/2505.19201)
Append: [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)
Append: [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)
Append: [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)
Append: [The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training](https://arxiv.org/abs/2505.19217)
Append: [Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator](https://arxiv.org/abs/2505.19236)
Append: [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
Append: [PATS: Process-Level Adaptive Thinking Mode Switching](https://arxiv.org/abs/2505.19250)
Append: [Unveiling Dual Quality in Product Reviews: An NLP-Based Approach](https://arxiv.org/abs/2505.19254)
Append: [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)
Append: [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)
Append: [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
Append: [SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking](https://arxiv.org/abs/2505.19300)
Append: [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
Append: [GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance](https://arxiv.org/abs/2505.19354)
Append: [Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement](https://arxiv.org/abs/2505.19355)
Append: [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360)
Append: [Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality](https://arxiv.org/abs/2505.19376)
Append: [GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor](https://arxiv.org/abs/2505.19384)
Append: [gec-metrics: A Unified Library for Grammatical Error Correction Evaluation](https://arxiv.org/abs/2505.19388)
Append: [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)
Append: [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)
Append: [Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering](https://arxiv.org/abs/2505.19410)
Append: [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
Append: [Frictional Agent Alignment Framework: Slow Down and Don't Break Things](https://arxiv.org/abs/2505.19428)
Append: [Rhapsody: A Dataset for Highlight Detection in Podcasts](https://arxiv.org/abs/2505.19429)
Append: [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
Append: [Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection](https://arxiv.org/abs/2505.19435)
Append: [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)
Append: [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
Append: [Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks](https://arxiv.org/abs/2505.19472)
Append: [Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection](https://arxiv.org/abs/2505.19475)
Append: [CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis](https://arxiv.org/abs/2505.19484)
Append: [Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents](https://arxiv.org/abs/2505.19494)
Append: [LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](https://arxiv.org/abs/2505.19510)
Append: [Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models](https://arxiv.org/abs/2505.19511)
Append: [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
Append: [Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework](https://arxiv.org/abs/2505.19515)
Append: [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
Append: [Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation](https://arxiv.org/abs/2505.19529)
Append: [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)
Append: [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)
Append: [Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents](https://arxiv.org/abs/2505.19549)
Append: [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)
Append: [TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](https://arxiv.org/abs/2505.19586)
Append: [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)
Append: [Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study](https://arxiv.org/abs/2505.19598)
Append: [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)
Append: [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)
Append: [Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically](https://arxiv.org/abs/2505.19606)
Append: [HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices](https://arxiv.org/abs/2505.19628)
Append: [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630)
Append: [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)
Append: [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)
Append: [Interleaved Reasoning for Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.19640)
Append: [Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation](https://arxiv.org/abs/2505.19647)
Append: [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)
Append: [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)
Append: [Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models](https://arxiv.org/abs/2505.19670)
Append: [Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations](https://arxiv.org/abs/2505.19674)
Append: [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
Append: [Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs](https://arxiv.org/abs/2505.19678)
Append: [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)
Append: [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)
Append: [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)
Append: [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
Append: [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)
Append: [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)
Append: [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
Append: [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
Append: [Efficient Reasoning via Chain of Unconscious Thought](https://arxiv.org/abs/2505.19756)
Append: [SGM: A Framework for Building Specification-Guided Moderation Filters](https://arxiv.org/abs/2505.19766)
Append: [T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search](https://arxiv.org/abs/2505.19768)
Append: [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)
Append: [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)
Append: [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)
Append: [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800)
Append: [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)
Append: [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)
Append: [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)
Append: [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)
Append: [Improving Multilingual Math Reasoning for African Languages](https://arxiv.org/abs/2505.19848)
Append: [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)
Append: [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)
Append: [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)
Append: [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
Append: [ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs](https://arxiv.org/abs/2505.19937)
Append: [MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2505.19959)
Append: [CP-Router: An Uncertainty-Aware Router Between LLM and LRM](https://arxiv.org/abs/2505.19970)
Append: [Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language](https://arxiv.org/abs/2505.19971)
Append: [DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset](https://arxiv.org/abs/2505.19978)
Append: [How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation](https://arxiv.org/abs/2505.19987)
Append: [Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition](https://arxiv.org/abs/2505.20006)
Append: [WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback](https://arxiv.org/abs/2505.20013)
Append: [Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation](https://arxiv.org/abs/2505.20014)
Append: [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
Append: [TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation](https://arxiv.org/abs/2505.20016)
Append: [Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking](https://arxiv.org/abs/2505.20023)
Append: [Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs](https://arxiv.org/abs/2505.20045)
Append: [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)
Append: [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)
Append: [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
Append: [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
Append: [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)
Append: [S2LPP: Small-to-Large Prompt Prediction across LLMs](https://arxiv.org/abs/2505.20097)
Append: [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
Append: [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/abs/2505.20101)
Append: [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)
Append: [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
Append: [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)
Append: [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)
Append: [Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers](https://arxiv.org/abs/2505.20128)
Append: [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)
Append: [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)
Append: [UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.20154)
Append: [Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs](https://arxiv.org/abs/2505.20155)
Append: [Exploring Generative Error Correction for Dysarthric Speech Recognition](https://arxiv.org/abs/2505.20163)
Append: [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)
Append: ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)
Append: [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)
Append: [Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning](https://arxiv.org/abs/2505.20195)
Append: [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199)
Append: [Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations](https://arxiv.org/abs/2505.20201)
Append: [How to Improve the Robustness of Closed-Source Models on NLI](https://arxiv.org/abs/2505.20209)
Append: [Dependency Parsing is More Parameter-Efficient with Normalization](https://arxiv.org/abs/2505.20215)
Append: [FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](https://arxiv.org/abs/2505.20225)
Append: [Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue](https://arxiv.org/abs/2505.20231)
Append: [Efficient Speech Translation through Model Compression and Knowledge Distillation](https://arxiv.org/abs/2505.20237)
Append: [It's High Time: A Survey of Temporal Information Retrieval and Question Answering](https://arxiv.org/abs/2505.20243)
Append: [KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing](https://arxiv.org/abs/2505.20245)
Append: [WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2505.20249)
Append: [ARM: Adaptive Reasoning Model](https://arxiv.org/abs/2505.20258)
Append: [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)
Append: [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)
Append: [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)
Append: [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)
Append: [MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability](https://arxiv.org/abs/2505.20285)
Append: [Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery](https://arxiv.org/abs/2505.20293)
Append: [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
Append: [Reasoning LLMs are Wandering Solution Explorers](https://arxiv.org/abs/2505.20296)
Append: [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding](https://arxiv.org/abs/2505.20298)
Append: [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization](https://arxiv.org/abs/2309.03824)
Append: [Improving Resnet-9 Generalization Trained on Small Datasets](https://arxiv.org/abs/2309.03965)
Append: [GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values](https://arxiv.org/abs/2311.03426)
Append: [News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation](https://arxiv.org/abs/2406.12634)
Append: [Accelerating the Low-Rank Decomposed Models](https://arxiv.org/abs/2407.20266)
Append: [Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks](https://arxiv.org/abs/2505.16849)
Append: [Towards medical AI misalignment: a preliminary study](https://arxiv.org/abs/2505.18212)
Append: [Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs](https://arxiv.org/abs/2505.18221)
Append: [ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning](https://arxiv.org/abs/2505.18232)
Append: [Will Large Language Models Transform Clinical Prediction?](https://arxiv.org/abs/2505.18246)
Append: [Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control](https://arxiv.org/abs/2505.18279)
Append: [Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?](https://arxiv.org/abs/2505.18350)
Append: [Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems](https://arxiv.org/abs/2505.18366)
Append: [LatentLLM: Attention-Aware Joint Tensor Compression](https://arxiv.org/abs/2505.18413)
Append: [$\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts](https://arxiv.org/abs/2505.18451)
Append: [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
Append: [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)
Append: [Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark](https://arxiv.org/abs/2505.18467)
Append: [Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications](https://arxiv.org/abs/2505.18488)
Append: [Knowledge Grafting of Large Language Models](https://arxiv.org/abs/2505.18502)
Append: [AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking](https://arxiv.org/abs/2505.18512)
Append: [B-score: Detecting biases in large language models using response history](https://arxiv.org/abs/2505.18545)
Append: [Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs](https://arxiv.org/abs/2505.18573)
Append: [RvLLM: LLM Runtime Verification with Domain Knowledge](https://arxiv.org/abs/2505.18585)
Append: [Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving](https://arxiv.org/abs/2505.18644)
Append: [SEW: Self-Evolving Agentic Workflows for Automated Code Generation](https://arxiv.org/abs/2505.18646)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
Append: [$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models](https://arxiv.org/abs/2505.18680)
Append: [Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer](https://arxiv.org/abs/2505.18713)
Append: [Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers](https://arxiv.org/abs/2505.18722)
Append: [From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?](https://arxiv.org/abs/2505.18789)
Append: [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822)
Append: [On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization](https://arxiv.org/abs/2505.18830)
Append: [Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework](https://arxiv.org/abs/2505.18847)
Append: [Inference Compute-Optimal Video Vision Language Models](https://arxiv.org/abs/2505.18855)
Append: [Meta-aware Learning in text-to-SQL Large Language Model](https://arxiv.org/abs/2505.18929)
Append: [Can Large Language Models Infer Causal Relationships from Real-World Text?](https://arxiv.org/abs/2505.18931)
Append: [REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2505.18933)
Append: [Language Models Surface the Unwritten Code of Science and Society](https://arxiv.org/abs/2505.18942)
Append: [STRICT: Stress Test of Rendering Images Containing Text](https://arxiv.org/abs/2505.18985)
Append: [Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection](https://arxiv.org/abs/2505.19010)
Append: [SQUiD: Synthesizing Relational Databases from Unstructured Text](https://arxiv.org/abs/2505.19025)
Append: [Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models](https://arxiv.org/abs/2505.19037)
Append: [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs](https://arxiv.org/abs/2505.19075)
Append: [Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs](https://arxiv.org/abs/2505.19155)
Append: [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234)
Append: [Next Token Prediction Is a Dead End for Creativity](https://arxiv.org/abs/2505.19277)
Append: [Towards Reliable Large Audio Language Model](https://arxiv.org/abs/2505.19294)
Append: [ODIN: A NL2SQL Recommender to Handle Schema Ambiguity](https://arxiv.org/abs/2505.19302)
Append: [Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation](https://arxiv.org/abs/2505.19353)
Append: [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
Append: [Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents](https://arxiv.org/abs/2505.19436)
Append: [Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI](https://arxiv.org/abs/2505.19443)
Append: [BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs](https://arxiv.org/abs/2505.19457)
Append: [DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation](https://arxiv.org/abs/2505.19504)
Append: [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
Append: [Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights](https://arxiv.org/abs/2505.19563)
Append: [Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing](https://arxiv.org/abs/2505.19578)
Append: [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
Append: [Preference Optimization by Estimating the Ratio of the Data Distribution](https://arxiv.org/abs/2505.19601)
Append: [Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621)
Append: [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
Append: [Large Language Models for Planning: A Comprehensive and Systematic Survey](https://arxiv.org/abs/2505.19683)
Append: [Discrete Markov Bridge](https://arxiv.org/abs/2505.19752)
Append: [CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement](https://arxiv.org/abs/2505.19757)
Append: [Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO](https://arxiv.org/abs/2505.19770)
Append: [HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation](https://arxiv.org/abs/2505.19866)
Append: [ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining](https://arxiv.org/abs/2505.19893)
Append: [Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program](https://arxiv.org/abs/2505.19896)
Append: [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897)
Append: [Can Visual Encoder Learn to See Arrows?](https://arxiv.org/abs/2505.19944)
Append: [An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning](https://arxiv.org/abs/2505.19954)
Append: [MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research](https://arxiv.org/abs/2505.19955)
Append: [DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph](https://arxiv.org/abs/2505.19956)
Append: [The Limits of Preference Data for Post-Training](https://arxiv.org/abs/2505.19964)
Append: [Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents](https://arxiv.org/abs/2505.19997)
Append: [Multi-modal brain encoding models for multi-modal stimuli](https://arxiv.org/abs/2505.20027)
Append: [REARANK: Reasoning Re-ranking Agent via Reinforcement Learning](https://arxiv.org/abs/2505.20046)
Append: [MVP: Multi-source Voice Pathology detection](https://arxiv.org/abs/2505.20050)
Append: [Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion](https://arxiv.org/abs/2505.20053)
Append: [SAEs Are Good for Steering -- If You Select the Right Features](https://arxiv.org/abs/2505.20063)
Append: [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087)
Append: [SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment](https://arxiv.org/abs/2505.20103)
Append: [StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs](https://arxiv.org/abs/2505.20139)
Append: [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
Append: [Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning](https://arxiv.org/abs/2505.20161)
Append: [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/abs/2505.20166)
Append: [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
Append: [Learning Extrapolative Sequence Transformations from Markov Chains](https://arxiv.org/abs/2505.20251)
Append: [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
Append: [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)
Append: [The Coverage Principle: A Framework for Understanding Compositional Generalization](https://arxiv.org/abs/2505.20278)
Append: [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/abs/2505.20279)
Append: [Visualized Text-to-Image Retrieval](https://arxiv.org/abs/2505.20291)
Append: [DiSA: Diffusion Step Annealing in Autoregressive Image Generation](https://arxiv.org/abs/2505.20297)
Append: [AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection](https://arxiv.org/abs/2112.11479)
Append: [ADEPT: A DEbiasing PrompT Framework](https://arxiv.org/abs/2211.05414)
Append: [The More Similar, the Better? Associations between Latent Semantic Similarity and Emotional Experiences Differ across Conversation Contexts](https://arxiv.org/abs/2309.12646)
Append: [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models](https://arxiv.org/abs/2310.13312)
Append: [Unearthing Large Scale Domain-Specific Knowledge from Public Corpora](https://arxiv.org/abs/2401.14624)
Append: [Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation](https://arxiv.org/abs/2402.13211)
Append: [A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion](https://arxiv.org/abs/2402.13405)
Append: [MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models](https://arxiv.org/abs/2402.13606)
Append: [Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency](https://arxiv.org/abs/2402.15481)
Append: [MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263)
Append: [Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation](https://arxiv.org/abs/2405.13984)
Append: [UniICL: An Efficient Unified Framework Unifying Compression, Selection, and Generation](https://arxiv.org/abs/2405.17062)
Append: [Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets](https://arxiv.org/abs/2406.05348)
Append: [Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation](https://arxiv.org/abs/2406.11632)
Append: [USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations](https://arxiv.org/abs/2406.16833)
Append: [Can Large Language Models Generate High-quality Patent Claims?](https://arxiv.org/abs/2406.19465)
Append: [The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations](https://arxiv.org/abs/2407.19299)
Append: [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](https://arxiv.org/abs/2408.08696)
Append: [CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts](https://arxiv.org/abs/2408.09070)
Append: [Language Models Benefit from Preparation with Elicited Knowledge](https://arxiv.org/abs/2409.01345)
Append: [The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language](https://arxiv.org/abs/2409.08103)
Append: [Identifying Knowledge Editing Types in Large Language Models](https://arxiv.org/abs/2409.19663)
Append: [QAEncoder: Towards Aligned Representation Learning in Question Answering System](https://arxiv.org/abs/2409.20434)
Append: [Do Vision-Language Models Really Understand Visual Language?](https://arxiv.org/abs/2410.00193)
Append: [In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement](https://arxiv.org/abs/2410.03124)
Append: [Lens: Rethinking Multilingual Enhancement for Large Language Models](https://arxiv.org/abs/2410.04407)
Append: [PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs](https://arxiv.org/abs/2410.06704)
Append: [Stuffed Mamba: Oversized States Lead to the Inability to Forget](https://arxiv.org/abs/2410.07145)
Append: [LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](https://arxiv.org/abs/2410.10700)
Append: [Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up](https://arxiv.org/abs/2410.12323)
Append: [Conformity in Large Language Models](https://arxiv.org/abs/2410.12428)
Append: [SynapticRAG: Enhancing Temporal Memory Retrieval in Large Language Models through Synaptic Mechanisms](https://arxiv.org/abs/2410.13553)
Append: [RESTOR: Knowledge Recovery in Machine Unlearning](https://arxiv.org/abs/2411.00204)
Append: [Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models](https://arxiv.org/abs/2411.02083)
Append: [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/abs/2411.02391)
Append: [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://arxiv.org/abs/2411.02937)
Append: [Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries](https://arxiv.org/abs/2411.07237)
Append: [SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs](https://arxiv.org/abs/2411.07965)
Append: [On the Compatibility of Generative AI and Generative Linguistics](https://arxiv.org/abs/2411.10533)
Append: [Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?](https://arxiv.org/abs/2411.15821)
Append: [Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning](https://arxiv.org/abs/2411.17679)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Patent-CR: A Dataset for Patent Claim Revision](https://arxiv.org/abs/2412.02549)
Append: [Interpretable Company Similarity with Sparse Autoencoders](https://arxiv.org/abs/2412.02605)
Append: [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://arxiv.org/abs/2412.07282)
Append: [On the Limit of Language Models as Planning Formalizers](https://arxiv.org/abs/2412.09879)
Append: [MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset](https://arxiv.org/abs/2412.10105)
Append: [Rethinking Chain-of-Thought from the Perspective of Self-Training](https://arxiv.org/abs/2412.10827)
Append: [Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models](https://arxiv.org/abs/2412.11041)
Append: [Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models](https://arxiv.org/abs/2412.11333)
Append: [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context for Multi-Hop QA](https://arxiv.org/abs/2412.12632)
Append: [Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models](https://arxiv.org/abs/2412.13328)
Append: [EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents](https://arxiv.org/abs/2412.13549)
Append: [Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](https://arxiv.org/abs/2412.13879)
Append: [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/abs/2412.14689)
Append: [DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs](https://arxiv.org/abs/2412.14838)
Append: [ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty](https://arxiv.org/abs/2412.20251)
Append: [Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation](https://arxiv.org/abs/2501.02979)
Append: [OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis](https://arxiv.org/abs/2501.04561)
Append: [A partition cover approach to tokenization](https://arxiv.org/abs/2501.06246)
Append: [Language Fusion for Parameter-Efficient Cross-lingual Transfer](https://arxiv.org/abs/2501.06892)
Append: [Domain Adaptation of Foundation LLMs for e-Commerce](https://arxiv.org/abs/2501.09706)
Append: [iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use](https://arxiv.org/abs/2501.09766)
Append: [Each Graph is a New Language: Graph Learning with LLMs](https://arxiv.org/abs/2501.11478)
Append: [NExtLong: Toward Effective Long-Context Training without Long Documents](https://arxiv.org/abs/2501.12766)
Append: [Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages](https://arxiv.org/abs/2501.13836)
Append: [Token Sampling Uncertainty Does Not Explain Homogeneity Bias in Large Language Models](https://arxiv.org/abs/2501.19337)
Append: [A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment](https://arxiv.org/abs/2502.00136)
Append: [UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models](https://arxiv.org/abs/2502.00334)
Append: [A statistically consistent measure of semantic uncertainty using Language Models](https://arxiv.org/abs/2502.00507)
Append: [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/abs/2502.04066)
Append: [JingFang: An Expert-Level Large Language Model for Traditional Chinese Medicine Clinical Consultation and Syndrome Differentiation-Based Treatment](https://arxiv.org/abs/2502.04345)
Append: [DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2502.04394)
Append: [Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection](https://arxiv.org/abs/2502.04528)
Append: [Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering](https://arxiv.org/abs/2502.07340)
Append: [What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations](https://arxiv.org/abs/2502.08279)
Append: [SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence](https://arxiv.org/abs/2502.08767)
Append: [A Survey of LLM-based Agents in Medicine: How far are we from Baymax?](https://arxiv.org/abs/2502.11211)
Append: [HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning](https://arxiv.org/abs/2502.11393)
Append: [Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study](https://arxiv.org/abs/2502.11514)
Append: [Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?](https://arxiv.org/abs/2502.11598)
Append: [ReviewEval: An Evaluation Framework for AI-Generated Reviews](https://arxiv.org/abs/2502.11736)
Append: [Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning](https://arxiv.org/abs/2502.11962)
Append: [How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines](https://arxiv.org/abs/2502.12051)
Append: [TokenSkip: Controllable Chain-of-Thought Compression in LLMs](https://arxiv.org/abs/2502.12067)
Append: [Evaluating Step-by-step Reasoning Traces: A Survey](https://arxiv.org/abs/2502.12289)
Append: [A Cognitive Writing Perspective for Constrained Long-Form Text Generation](https://arxiv.org/abs/2502.12568)
Append: [Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization](https://arxiv.org/abs/2502.12672)
Append: [Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements](https://arxiv.org/abs/2502.12904)
Append: [Conditioning LLMs to Generate Code-Switched Text](https://arxiv.org/abs/2502.12924)
Append: [Natural Language Generation from Visual Events: Challenges and Future Directions](https://arxiv.org/abs/2502.13034)
Append: [Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors](https://arxiv.org/abs/2502.13311)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?](https://arxiv.org/abs/2502.14924)
Append: [Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models](https://arxiv.org/abs/2502.15094)
Append: [Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning](https://arxiv.org/abs/2502.15361)
Append: [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/abs/2502.16514)
Append: [CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter](https://arxiv.org/abs/2502.16880)
Append: [Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch](https://arxiv.org/abs/2502.17173)
Append: [Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning](https://arxiv.org/abs/2502.18001)
Append: [Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs](https://arxiv.org/abs/2502.18791)
Append: [Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization](https://arxiv.org/abs/2502.19127)
Append: [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/abs/2502.19148)
Append: [R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning](https://arxiv.org/abs/2502.19735)
Append: [GeoEdit: Geometric Knowledge Editing for Large Language Models](https://arxiv.org/abs/2502.19953)
Append: [PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues](https://arxiv.org/abs/2502.21017)
Append: [Detecting LLM-Generated Korean Text through Linguistic Feature Analysis](https://arxiv.org/abs/2503.00032)
Append: [Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models](https://arxiv.org/abs/2503.01763)
Append: [SteerConf: Steering LLMs for Confidence Elicitation](https://arxiv.org/abs/2503.02863)
Append: [LINGOLY-TOO: Disentangling Memorisation from Knowledge with Linguistic Templatisation and Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)
Append: [Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions](https://arxiv.org/abs/2503.03862)
Append: [DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models](https://arxiv.org/abs/2503.04240)
Append: [One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](https://arxiv.org/abs/2503.04856)
Append: [InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2503.06692)
Append: [MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System](https://arxiv.org/abs/2503.09600)
Append: [MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation](https://arxiv.org/abs/2503.10497)
Append: [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/abs/2503.10688)
Append: [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
Append: [RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning](https://arxiv.org/abs/2503.12759)
Append: [Optimizing Decomposition for Optimal Claim Verification](https://arxiv.org/abs/2503.15354)
Append: [Prompting is Not All You Need! Evaluating LLM Agent Simulation Methodologies with Real-World Online Customer Behavior Data](https://arxiv.org/abs/2503.20749)
Append: [GTR: Graph-Table-RAG for Cross-Table Question Answering](https://arxiv.org/abs/2504.01346)
Append: [FISH-Tuning: Enhancing PEFT Methods with Fisher Information](https://arxiv.org/abs/2504.04050)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [NoveltyBench: Evaluating Language Models for Humanlike Diversity](https://arxiv.org/abs/2504.05228)
Append: [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/abs/2504.07440)
Append: [MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations](https://arxiv.org/abs/2504.07830)
Append: [PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims](https://arxiv.org/abs/2504.09866)
Append: [TextArena](https://arxiv.org/abs/2504.11442)
Append: [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Append: [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
Append: [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
Append: [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
Append: [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Append: [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
Append: [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
Append: [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
Append: [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
Append: [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing](https://arxiv.org/abs/2505.11604)
Append: [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
Append: [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
Append: [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
Append: [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
Append: [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
Append: [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
Append: [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
Append: [Towards End-to-End Training of Automatic Speech Recognition for Nigerian Pidgin](https://arxiv.org/abs/2010.11123)
Append: [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299)
Append: [JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)
Append: [Query Performance Prediction using Relevance Judgments Generated by Large Language Models](https://arxiv.org/abs/2404.01012)
Append: [Model Extrapolation Expedites Alignment](https://arxiv.org/abs/2404.16792)
Append: [Constructing a BPE Tokenization DFA](https://arxiv.org/abs/2405.07671)
Append: [AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments](https://arxiv.org/abs/2405.07960)
Append: [SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2405.14917)
Append: [Explaining the role of Intrinsic Dimensionality in Adversarial Training](https://arxiv.org/abs/2405.17130)
Append: [Little Data, Big Impact: Privacy-Aware Visual Language Models via Minimal Tuning](https://arxiv.org/abs/2405.17423)
Append: [Parrot: Multilingual Visual Instruction Tuning](https://arxiv.org/abs/2406.02539)
Append: [Algorithmic Language Models with Neurally Compiled Libraries](https://arxiv.org/abs/2407.04899)
Append: [MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation](https://arxiv.org/abs/2408.09865)
Append: [ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework](https://arxiv.org/abs/2409.10289)
Append: [Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis](https://arxiv.org/abs/2410.02167)
Append: [Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System](https://arxiv.org/abs/2410.09403)
Append: [AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents](https://arxiv.org/abs/2410.13825)
Append: [LLMScan: Causal Scan for LLM Misbehavior Detection](https://arxiv.org/abs/2410.16638)
Append: [Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers](https://arxiv.org/abs/2410.22663)
Append: [P$^2$ Law: Scaling Law for Post-Training After Model Pruning](https://arxiv.org/abs/2411.10272)
Append: [FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers](https://arxiv.org/abs/2411.14507)
Append: [BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving](https://arxiv.org/abs/2411.17404)
Append: [Demonstration Selection for In-Context Learning via Reinforcement Learning](https://arxiv.org/abs/2412.03966)
Append: [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559)
Append: [Visual Program Distillation with Template-Based Augmentation](https://arxiv.org/abs/2412.08564)
Append: [GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration](https://arxiv.org/abs/2412.16216)
Append: [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639)
Append: [Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation](https://arxiv.org/abs/2501.12432)
Append: [Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach](https://arxiv.org/abs/2502.00577)
Append: [Polynomial, trigonometric, and tropical activations](https://arxiv.org/abs/2502.01247)
Append: [Preference Leakage: A Contamination Problem in LLM-as-a-judge](https://arxiv.org/abs/2502.01534)
Append: [ACECODER: Acing Coder RL via Automated Test-Case Synthesis](https://arxiv.org/abs/2502.01718)
Append: [DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation](https://arxiv.org/abs/2502.03930)
Append: [When More is Less: Understanding Chain-of-Thought Length in LLMs](https://arxiv.org/abs/2502.07266)
Append: [QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language](https://arxiv.org/abs/2502.09723)
Append: [SMART: Self-Aware Agent for Tool Overuse Mitigation](https://arxiv.org/abs/2502.11435)
Append: [Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding](https://arxiv.org/abs/2502.11492)
Append: [APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs](https://arxiv.org/abs/2502.12085)
Append: [HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2502.12442)
Append: [Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees](https://arxiv.org/abs/2502.12678)
Append: [K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](https://arxiv.org/abs/2502.13344)
Append: [Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems](https://arxiv.org/abs/2502.18632)
Append: [TheoremExplainAgent: Towards Video-based Multimodal Explanations for LLM Theorem Understanding](https://arxiv.org/abs/2502.19400)
Append: [Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models](https://arxiv.org/abs/2502.19883)
Append: [Generalizable Prompt Learning of CLIP: A Brief Overview](https://arxiv.org/abs/2503.01263)
Append: [Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More](https://arxiv.org/abs/2503.10542)
Append: [Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding](https://arxiv.org/abs/2503.13377)
Append: [Mixture of Lookup Experts](https://arxiv.org/abs/2503.15798)
Append: [MoLAE: Mixture of Latent Experts for Parameter-Efficient Language Models](https://arxiv.org/abs/2503.23100)
Append: [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382)
Append: [Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](https://arxiv.org/abs/2504.05652)
Append: [FamilyTool: A Multi-hop Personalized Tool Use Benchmark](https://arxiv.org/abs/2504.06766)
Append: [DocAgent: A Multi-Agent System for Automated Code Documentation Generation](https://arxiv.org/abs/2504.08725)
Append: [ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search](https://arxiv.org/abs/2504.10893)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
Append: [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
Append: [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
Append: [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843)
Append: [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
Append: [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
Append: [PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
Append: [GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples](https://arxiv.org/abs/2505.14814)
append_entries: 568
Finish: 2025-05-27 04:28:29.763551
------------------------------------------------------
Started: 2025-05-27 06:26:35.991993
Existing_entries: 1568
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [AAAR-1.0: Assessing AI's Potential to Assist Research](https://arxiv.org/abs/2410.22394)
Token length: 1597
Summarized using GPT-3.5-turbo
Append: [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/abs/2412.10138)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Token length: 988
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
Token length: 1747
Summarized using GPT-3.5-turbo
Append: [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision](https://arxiv.org/abs/2505.14996)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
Token length: 943
Summarized using GPT-3.5-turbo
Append: ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
Token length: 1855
Summarized using GPT-3.5-turbo
Append: [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
append_entries: 25
Finish: 2025-05-27 06:27:36.820994
------------------------------------------------------
Started: 2025-05-27 08:22:28.722995
Existing_entries: 1025
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
append_entries: 2
Finish: 2025-05-27 08:22:33.710128
------------------------------------------------------
Started: 2025-05-27 10:18:35.287357
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 10:18:36.515366
------------------------------------------------------
Started: 2025-05-27 12:35:18.008371
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 12:35:19.218601
------------------------------------------------------
Started: 2025-05-27 14:16:23.343462
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 14:16:24.474794
------------------------------------------------------
Started: 2025-05-27 16:21:03.442434
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 16:21:04.618719
------------------------------------------------------
Started: 2025-05-27 18:22:09.997551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 18:22:11.236560
------------------------------------------------------
Started: 2025-05-27 20:18:37.073028
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 20:18:38.191566
------------------------------------------------------
Started: 2025-05-27 22:15:37.517071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 22:15:38.707862
------------------------------------------------------
Started: 2025-05-28 01:19:45.327893
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 01:19:46.475260
------------------------------------------------------
Started: 2025-05-28 03:11:16.977459
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 03:11:18.268401
------------------------------------------------------
Started: 2025-05-28 04:25:12.991782
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/abs/2505.20315)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/abs/2505.20318)
Token length: 1127
Summarized using GPT-3.5-turbo
Append: [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/abs/2505.20320)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/abs/2505.20323)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/abs/2505.20333)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/abs/2505.20334)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/abs/2505.20335)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/abs/2505.20336)
Token length: 1885
Summarized using GPT-3.5-turbo
Append: [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/abs/2505.20338)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/abs/2505.20340)
Token length: 1748
Summarized using GPT-3.5-turbo
Append: [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/abs/2505.20343)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision](https://arxiv.org/abs/2505.20415)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/abs/2505.20416)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project](https://arxiv.org/abs/2505.20428)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/abs/2505.20429)
Token length: 1588
Summarized using GPT-3.5-turbo
Append: [HAMburger: Accelerating LLM Inference via Token Smashing](https://arxiv.org/abs/2505.20438)
Token length: 1011
Summarized using GPT-3.5-turbo
Append: [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries](https://arxiv.org/abs/2505.20451)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/abs/2505.20482)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/abs/2505.20487)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages](https://arxiv.org/abs/2505.20496)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/abs/2505.20500)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [Gatsby Without the 'E': Crafting Lipograms with LLMs](https://arxiv.org/abs/2505.20501)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Large Language Models for IT Automation Tasks: Are We There Yet?](https://arxiv.org/abs/2505.20505)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/abs/2505.20506)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects](https://arxiv.org/abs/2505.20511)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline](https://arxiv.org/abs/2505.20546)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Emotion Classification In-Context in Spanish](https://arxiv.org/abs/2505.20571)
Token length: 1666
Summarized using GPT-3.5-turbo
Append: [Effectiveness of Prompt Optimization in NL2SQL Systems](https://arxiv.org/abs/2505.20591)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation](https://arxiv.org/abs/2505.20606)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization](https://arxiv.org/abs/2505.20624)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/abs/2505.20625)
Token length: 1555
Summarized using GPT-3.5-turbo
Append: [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/abs/2505.20645)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/abs/2505.20650)
Token length: 1156
Summarized using GPT-3.5-turbo
Append: [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/abs/2505.20654)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge](https://arxiv.org/abs/2505.20658)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/abs/2505.20660)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/abs/2505.20664)
Append: [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)
Append: [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)
Append: [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/abs/2505.20693)
Append: [Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration](https://arxiv.org/abs/2505.20700)
Append: [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/abs/2505.20707)
Append: [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
Append: [Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator](https://arxiv.org/abs/2505.20738)
Append: [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)
Append: [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
Append: [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/abs/2505.20779)
Append: [Improved Representation Steering for Language Models](https://arxiv.org/abs/2505.20809)
Append: [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
Append: [Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective](https://arxiv.org/abs/2505.20816)
Append: [Tracing and Reversing Rank-One Model Edits](https://arxiv.org/abs/2505.20819)
Append: [Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation](https://arxiv.org/abs/2505.20825)
Append: [AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset](https://arxiv.org/abs/2505.20826)
Append: [Concealment of Intent: A Game-Theoretic Analysis](https://arxiv.org/abs/2505.20841)
Append: [Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG](https://arxiv.org/abs/2505.20871)
Append: [Can LLMs Learn to Map the World from Local Descriptions?](https://arxiv.org/abs/2505.20874)
Append: [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
Append: [MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection](https://arxiv.org/abs/2505.20880)
Append: [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)
Append: [Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing](https://arxiv.org/abs/2505.20899)
Append: [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/abs/2505.20901)
Append: [Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?](https://arxiv.org/abs/2505.20903)
Append: [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/abs/2505.20910)
Append: [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/abs/2505.20921)
Append: [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/abs/2505.20925)
Append: [Information-Theoretic Complementary Prompts for Improved Continual Text Classification](https://arxiv.org/abs/2505.20933)
Append: [On VLMs for Diverse Tasks in Multimodal Meme Classification](https://arxiv.org/abs/2505.20937)
Append: [Research Community Perspectives on "Intelligence" and Large Language Models](https://arxiv.org/abs/2505.20959)
Append: [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/abs/2505.20963)
Append: [Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation](https://arxiv.org/abs/2505.20966)
Append: [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/abs/2505.20971)
Append: [Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing](https://arxiv.org/abs/2505.20976)
Append: [Evaluating and Steering Modality Preferences in Multimodal Large Language Model](https://arxiv.org/abs/2505.20977)
Append: [Who Reasons in the Large Language Models?](https://arxiv.org/abs/2505.20993)
Append: [Articulatory strategy in vowel production as a basis for speaker discrimination](https://arxiv.org/abs/2505.20995)
Append: [Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?](https://arxiv.org/abs/2505.21003)
Append: [LLMs are Frequency Pattern Learners in Natural Language Inference](https://arxiv.org/abs/2505.21011)
Append: [Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation](https://arxiv.org/abs/2505.21033)
Append: [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/abs/2505.21040)
Append: [Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction](https://arxiv.org/abs/2505.21043)
Append: [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/abs/2505.21068)
Append: [Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation](https://arxiv.org/abs/2505.21072)
Append: [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/abs/2505.21082)
Append: [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/abs/2505.21092)
Append: [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
Append: [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/abs/2505.21109)
Append: [Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA](https://arxiv.org/abs/2505.21115)
Append: [Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2505.21137)
Append: [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/abs/2505.21138)
Append: [Assessment of L2 Oral Proficiency using Speech Large Language Models](https://arxiv.org/abs/2505.21148)
Append: [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/abs/2505.21171)
Append: [TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment](https://arxiv.org/abs/2505.21172)
Append: [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.21178)
Append: [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189)
Append: [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/abs/2505.21190)
Append: [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191)
Append: [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/abs/2505.21218)
Append: [A Representation Level Analysis of NMT Model Robustness to Grammatical Errors](https://arxiv.org/abs/2505.21224)
Append: [LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners](https://arxiv.org/abs/2505.21239)
Append: [Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings](https://arxiv.org/abs/2505.21242)
Append: [ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://arxiv.org/abs/2505.21250)
Append: [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/abs/2505.21265)
Append: [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset](https://arxiv.org/abs/2505.21297)
Append: [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/abs/2505.21301)
Append: [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/abs/2505.21315)
Append: [Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts](https://arxiv.org/abs/2505.21324)
Append: [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/abs/2505.21342)
Append: [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)
Append: [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)
Append: [Analyzing values about gendered language reform in LLMs' revisions](https://arxiv.org/abs/2505.21378)
Append: [PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense](https://arxiv.org/abs/2505.21380)
Append: [AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs](https://arxiv.org/abs/2505.21389)
Append: [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)
Append: [DecisionFlow: Advancing Large Language Model as Principled Decision Maker](https://arxiv.org/abs/2505.21397)
Append: [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/abs/2505.21399)
Append: [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/abs/2505.21409)
Append: [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411)
Append: [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/abs/2505.21413)
Append: [Towards Better Instruction Following Retrieval Models](https://arxiv.org/abs/2505.21439)
Append: [Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication](https://arxiv.org/abs/2505.21451)
Append: [Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance](https://arxiv.org/abs/2505.21458)
Append: [Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion](https://arxiv.org/abs/2505.21467)
Append: [Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration](https://arxiv.org/abs/2505.21471)
Append: [Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/abs/2505.21479)
Append: [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)
Append: [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/abs/2505.21503)
Append: [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505)
Append: [InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning](https://arxiv.org/abs/2505.18291)
Append: [Cultural Awareness in Vision-Language Models: A Cross-Country Exploration](https://arxiv.org/abs/2505.20326)
Append: [Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset](https://arxiv.org/abs/2505.20341)
Append: [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
Append: [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/abs/2505.20405)
Append: [SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents](https://arxiv.org/abs/2505.20411)
Append: [The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions](https://arxiv.org/abs/2505.20464)
Append: [BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics](https://arxiv.org/abs/2505.20480)
Append: [Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](https://arxiv.org/abs/2505.20503)
Append: [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/abs/2505.20521)
Append: [Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
Append: [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/abs/2505.20561)
Append: [Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients](https://arxiv.org/abs/2505.20609)
Append: [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
Append: [SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis](https://arxiv.org/abs/2505.20630)
Append: [TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research](https://arxiv.org/abs/2505.20663)
Append: [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)
Append: [MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding](https://arxiv.org/abs/2505.20715)
Append: [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
Append: [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks](https://arxiv.org/abs/2505.20854)
Append: [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
Append: [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
Append: [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
Append: [Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers](https://arxiv.org/abs/2505.21024)
Append: [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
Append: [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)
Append: [Leveraging GANs for citation intent classification and its impact on citation network analysis](https://arxiv.org/abs/2505.21162)
Append: [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)
Append: [PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems](https://arxiv.org/abs/2505.21230)
Append: [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://arxiv.org/abs/2505.21277)
Append: [Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants](https://arxiv.org/abs/2505.21304)
Append: [Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks](https://arxiv.org/abs/2505.21329)
Append: [The Multilingual Divide and Its Impact on Global AI Safety](https://arxiv.org/abs/2505.21344)
Append: [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/abs/2505.21465)
Append: [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472)
Append: [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487)
Append: [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
Append: [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/abs/2505.21497)
Append: [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
Append: [WizardLM: Empowering large pre-trained language models to follow complex instructions](https://arxiv.org/abs/2304.12244)
Append: [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568)
Append: [Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods](https://arxiv.org/abs/2401.16332)
Append: [LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey](https://arxiv.org/abs/2402.14558)
Append: [An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment](https://arxiv.org/abs/2403.04963)
Append: [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518)
Append: [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [Predicting drug-gene relations via analogy tasks with word embeddings](https://arxiv.org/abs/2406.00984)
Append: [NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human](https://arxiv.org/abs/2406.03749)
Append: [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/abs/2406.14230)
Append: [Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?](https://arxiv.org/abs/2407.00996)
Append: [Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs](https://arxiv.org/abs/2407.03181)
Append: [Autoregressive Speech Synthesis without Vector Quantization](https://arxiv.org/abs/2407.08551)
Append: [Sentiment Reasoning for Healthcare](https://arxiv.org/abs/2407.21054)
Append: [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/abs/2408.13533)
Append: [GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding](https://arxiv.org/abs/2409.04183)
Append: [Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints](https://arxiv.org/abs/2409.14469)
Append: [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/abs/2410.01651)
Append: [Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing](https://arxiv.org/abs/2410.06638)
Append: [Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles](https://arxiv.org/abs/2410.09829)
Append: [The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph](https://arxiv.org/abs/2410.12458)
Append: [BQA: Body Language Question Answering Dataset for Video Large Language Models](https://arxiv.org/abs/2410.13206)
Append: [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://arxiv.org/abs/2410.13776)
Append: [Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs](https://arxiv.org/abs/2410.14641)
Append: [Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch](https://arxiv.org/abs/2410.18693)
Append: [Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers](https://arxiv.org/abs/2410.21013)
Append: [STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing](https://arxiv.org/abs/2411.00387)
Append: [Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2411.01834)
Append: [Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection](https://arxiv.org/abs/2411.07446)
Append: [Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation](https://arxiv.org/abs/2411.12719)
Append: [DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization](https://arxiv.org/abs/2411.14055)
Append: [How Private are Language Models in Abstractive Summarization?](https://arxiv.org/abs/2412.12040)
Append: [Knowledge Boundary of Large Language Models: A Survey](https://arxiv.org/abs/2412.12472)
Append: [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/abs/2501.01264)
Append: [VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models](https://arxiv.org/abs/2501.04962)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [Tuning LLM Judge Design Decisions for 1/1000 of the Cost](https://arxiv.org/abs/2501.17178)
Append: [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/abs/2501.18922)
Append: [Thinking beyond the anthropomorphic paradigm benefits LLM research](https://arxiv.org/abs/2502.09192)
Append: [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
Append: [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)
Append: [ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition](https://arxiv.org/abs/2502.11198)
Append: [SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/abs/2502.12134)
Append: [Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues](https://arxiv.org/abs/2502.12187)
Append: [Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text](https://arxiv.org/abs/2502.12953)
Append: [Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/abs/2502.13010)
Append: [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/abs/2502.14132)
Append: [Behavioral Analysis of Information Salience in Large Language Models](https://arxiv.org/abs/2502.14613)
Append: [Predicting Through Generation: Why Generation Is Better for Prediction](https://arxiv.org/abs/2502.17817)
Append: [Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework](https://arxiv.org/abs/2502.18874)
Append: [Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases](https://arxiv.org/abs/2502.19249)
Append: [Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models](https://arxiv.org/abs/2502.20795)
Append: [The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents](https://arxiv.org/abs/2502.20859)
Append: [Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs](https://arxiv.org/abs/2502.20968)
Append: [MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving](https://arxiv.org/abs/2503.03205)
Append: [HalluCounter: Reference-free LLM Hallucination Detection in the Wild!](https://arxiv.org/abs/2503.04615)
Append: [How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation](https://arxiv.org/abs/2503.09598)
Append: [No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models](https://arxiv.org/abs/2503.11985)
Append: [Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations](https://arxiv.org/abs/2503.13857)
Append: [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)
Append: [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
Append: [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2505.05111)
Append: [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
Append: [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
Append: [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
Append: [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
Append: [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
Append: [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
Append: [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
Append: [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
Append: [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
Append: [CLEVRER-Humans: Describing Physical and Causal Events the Human Way](https://arxiv.org/abs/2310.03635)
Append: [Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification](https://arxiv.org/abs/2402.04068)
Append: [GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement](https://arxiv.org/abs/2406.11546)
Append: [Can Large Language Models Understand Symbolic Graphics Programs?](https://arxiv.org/abs/2408.08313)
Append: ["Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree": Zero-Shot Decision Tree Induction and Embedding with Large Language Models](https://arxiv.org/abs/2409.18594)
Append: [EPIC: Efficient Position-Independent Caching for Serving Large Language Models](https://arxiv.org/abs/2410.15332)
Append: [Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models](https://arxiv.org/abs/2410.18122)
Append: [Interlocking-free Selective Rationalization Through Genetic-based Learning](https://arxiv.org/abs/2412.10312)
Append: [Leveraging Large Language Models for Active Merchant Non-player Characters](https://arxiv.org/abs/2412.11189)
Append: [Transparent and Coherent Procedural Mistake Detection](https://arxiv.org/abs/2412.11927)
Append: [Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey](https://arxiv.org/abs/2412.20367)
Append: [Efficiently Scaling LLM Reasoning with Certaindex](https://arxiv.org/abs/2412.20993)
Append: [More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives](https://arxiv.org/abs/2501.04070)
Append: [vCache: Verified Semantic Prompt Caching](https://arxiv.org/abs/2502.03771)
Append: [A Lightweight Method to Disrupt Memorized Sequences in LLM](https://arxiv.org/abs/2502.05159)
Append: [Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](https://arxiv.org/abs/2502.05374)
Append: [Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection](https://arxiv.org/abs/2502.06042)
Append: [GeLLMO: Generalizing Large Language Models for Multi-property Molecule Optimization](https://arxiv.org/abs/2502.13398)
Append: [Training a Generally Curious Agent](https://arxiv.org/abs/2502.17543)
Append: [Voting or Consensus? Decision-Making in Multi-Agent Debate](https://arxiv.org/abs/2502.19130)
Append: [Path Pooling: Training-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2503.05203)
Append: [ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2503.09501)
Append: [Exploring the Necessity of Reasoning in LLM-based Agent Scenarios](https://arxiv.org/abs/2503.11074)
Append: [SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models](https://arxiv.org/abs/2503.13503)
Append: [LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers](https://arxiv.org/abs/2503.14434)
Append: [Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models](https://arxiv.org/abs/2503.20576)
Append: [Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations](https://arxiv.org/abs/2505.04948)
Append: [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
Append: [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
Append: [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
Append: [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
append_entries: 291
Finish: 2025-05-28 04:27:04.742668
------------------------------------------------------
Started: 2025-05-28 06:25:15.080654
Existing_entries: 1291
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
Token length: 1465
Summarized using GPT-3.5-turbo
Append: [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
append_entries: 7
Finish: 2025-05-28 06:25:32.168821
------------------------------------------------------
Started: 2025-05-28 08:22:24.863839
Existing_entries: 1007
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 08:22:25.514336
------------------------------------------------------
Started: 2025-05-28 10:18:49.376337
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 10:18:50.115461
------------------------------------------------------
Started: 2025-05-28 12:35:04.812962
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 12:35:05.455127
------------------------------------------------------
Started: 2025-05-28 14:17:45.260354
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 14:17:45.923769
------------------------------------------------------
Started: 2025-05-28 16:19:39.824501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 16:19:40.545935
------------------------------------------------------
Started: 2025-05-28 18:22:08.933407
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 18:22:09.589879
------------------------------------------------------
Started: 2025-05-28 20:18:47.335795
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 20:18:47.984769
------------------------------------------------------
Started: 2025-05-28 22:15:33.698776
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 22:15:34.344525
------------------------------------------------------
Started: 2025-05-29 01:19:39.834538
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 01:19:40.479882
------------------------------------------------------
Started: 2025-05-29 03:11:33.079014
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 03:11:33.738525
------------------------------------------------------
Started: 2025-05-29 04:26:33.202518
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/abs/2505.21578)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/abs/2505.21598)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/abs/2505.21701)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/abs/2505.21710)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/abs/2505.21757)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/abs/2505.21781)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/abs/2505.21816)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Representative Language Generation](https://arxiv.org/abs/2505.21819)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/abs/2505.21859)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/abs/2505.21889)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
Token length: 1933
Summarized using GPT-3.5-turbo
Append: [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/abs/2505.21937)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/abs/2505.21940)
Token length: 774
Summarized using GPT-3.5-turbo
Append: [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/abs/2505.21941)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/abs/2505.21958)
Token length: 1573
Summarized using GPT-3.5-turbo
Append: [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/abs/2505.21967)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/abs/2505.21997)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/abs/2505.21999)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/abs/2505.22017)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/abs/2505.22018)
Token length: 1958
Summarized using GPT-3.5-turbo
Append: [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
Token length: 751
Summarized using GPT-3.5-turbo
Append: [Voice Adaptation for Swiss German](https://arxiv.org/abs/2505.22054)
Token length: 949
Summarized using GPT-3.5-turbo
Append: [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/abs/2505.22076)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.22095)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
Token length: 1537
Summarized using GPT-3.5-turbo
Append: [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/abs/2505.22101)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
Token length: 1158
Summarized using GPT-3.5-turbo
Append: [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/abs/2505.22113)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)
Append: [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/abs/2505.22120)
Append: [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/abs/2505.22131)
Append: [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
Append: [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
Append: [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)
Append: [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)
Append: [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
Append: [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)
Append: [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/abs/2505.22172)
Append: [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
Append: [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
Append: [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
Append: [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
Append: [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
Append: [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/abs/2505.22236)
Append: [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
Append: [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
Append: [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/abs/2505.22273)
Append: [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
Append: [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/abs/2505.22293)
Append: [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
Append: [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)
Append: [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
Append: [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)
Append: [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)
Append: [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
Append: [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
Append: [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/abs/2505.22354)
Append: [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)
Append: [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/abs/2505.22430)
Append: [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
Append: [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/abs/2505.22501)
Append: [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)
Append: [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)
Append: [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
Append: [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/abs/2505.22563)
Append: [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
Append: [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
Append: [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/abs/2505.22582)
Append: [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/abs/2505.22586)
Append: [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
Append: [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)
Append: [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
Append: [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
Append: [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
Append: [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
Append: [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)
Append: [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
Append: [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/abs/2505.22653)
Append: [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/abs/2505.22661)
Append: [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
Append: [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
Append: [Complexity counts: global and local perspectives on Indo-Aryan numeral systems](https://arxiv.org/abs/2505.21510)
Append: [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
Append: [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
Append: [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
Append: [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
Append: [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
Append: [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
Append: [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
Append: [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
Append: [From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism](https://arxiv.org/abs/2505.21753)
Append: [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
Append: [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
Append: [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
Append: [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
Append: [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
Append: [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
Append: [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
Append: [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
Append: [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
Append: [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
Append: [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
Append: [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
Append: [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)
Append: [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
Append: [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
Append: [Visual Cues Support Robust Turn-taking Prediction in Noise](https://arxiv.org/abs/2505.22088)
Append: [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
Append: [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
Append: [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
Append: [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
Append: [Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis](https://arxiv.org/abs/2505.22231)
Append: [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
Append: [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
Append: [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
Append: [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
Append: [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
Append: [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
Append: [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
Append: [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
Append: [Effective Context in Neural Speech Models](https://arxiv.org/abs/2505.22487)
Append: [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
Append: [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
Append: [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
Append: [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
Append: [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
Append: [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
Append: [Machine Translation Models are Zero-Shot Detectors of Translation Direction](https://arxiv.org/abs/2401.06769)
Append: [Tracking Semantic Change in Slovene: A Novel Dataset and Optimal Transport-Based Distance](https://arxiv.org/abs/2402.16596)
Append: [Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems](https://arxiv.org/abs/2404.06762)
Append: [Mitigating Text Toxicity with Counterfactual Generation](https://arxiv.org/abs/2405.09948)
Append: [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)
Append: [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)
Append: [Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?](https://arxiv.org/abs/2406.14737)
Append: [Large Vocabulary Size Improves Large Language Models](https://arxiv.org/abs/2406.16508)
Append: [Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification](https://arxiv.org/abs/2407.07004)
Append: [Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering](https://arxiv.org/abs/2409.04122)
Append: [Nonlinear second-order dynamics describe labial constriction trajectories across languages and contexts](https://arxiv.org/abs/2410.08351)
Append: [Which Demographics do LLMs Default to During Annotation?](https://arxiv.org/abs/2410.08820)
Append: [Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2410.13080)
Append: [SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior](https://arxiv.org/abs/2410.16665)
Append: [Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching](https://arxiv.org/abs/2410.18436)
Append: [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
Append: [Controllable Context Sensitivity and the Knob Behind It](https://arxiv.org/abs/2411.07404)
Append: [LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [Overcoming Non-monotonicity in Transducer-based Streaming Generation](https://arxiv.org/abs/2411.17170)
Append: [ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning](https://arxiv.org/abs/2412.11418)
Append: [Core Context Aware Transformers for Long Context Language Modeling](https://arxiv.org/abs/2412.12465)
Append: [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/abs/2412.16926)
Append: [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/abs/2501.00777)
Append: [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/abs/2501.03124)
Append: [LLMs Reproduce Stereotypes of Sexual and Gender Minorities](https://arxiv.org/abs/2501.05926)
Append: [Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts](https://arxiv.org/abs/2501.06365)
Append: [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://arxiv.org/abs/2501.13567)
Append: [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/abs/2501.13953)
Append: [Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains](https://arxiv.org/abs/2501.14431)
Append: [Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2502.00602)
Append: [Advancing Reasoning in Large Language Models: Promising Methods and Approaches](https://arxiv.org/abs/2502.03671)
Append: [Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring](https://arxiv.org/abs/2502.05242)
Append: [LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation](https://arxiv.org/abs/2502.07365)
Append: [CoSER: Coordinating LLM-Based Persona Simulation of Established Roles](https://arxiv.org/abs/2502.09082)
Append: [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/abs/2502.11100)
Append: [ReLearn: Unlearning via Learning for Large Language Models](https://arxiv.org/abs/2502.11190)
Append: [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/abs/2502.11441)
Append: [BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages](https://arxiv.org/abs/2502.11926)
Append: [ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails](https://arxiv.org/abs/2502.13458)
Append: [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913)
Append: [Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering](https://arxiv.org/abs/2502.14245)
Append: [Self-Taught Agentic Long Context Understanding](https://arxiv.org/abs/2502.15920)
Append: [Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations](https://arxiv.org/abs/2503.00134)
Append: [Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication](https://arxiv.org/abs/2503.04395)
Append: [Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation](https://arxiv.org/abs/2503.08057)
Append: [Explicit Learning and the LLM in Machine Translation](https://arxiv.org/abs/2503.09454)
Append: [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)
Append: [Constrained Discrete Diffusion](https://arxiv.org/abs/2503.09790)
Append: [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond](https://arxiv.org/abs/2503.10460)
Append: [TLUE: A Tibetan Language Understanding Evaluation Benchmark](https://arxiv.org/abs/2503.12051)
Append: [Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA](https://arxiv.org/abs/2503.17933)
Append: [Sun-Shine: A Foundation Large Language Model for Tibetan Culture and Heritage](https://arxiv.org/abs/2503.18288)
Append: [Token embeddings violate the manifold hypothesis](https://arxiv.org/abs/2504.01002)
Append: [Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices](https://arxiv.org/abs/2504.03312)
Append: [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/abs/2504.03561)
Append: [AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments](https://arxiv.org/abs/2504.05104)
Append: [Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations](https://arxiv.org/abs/2504.06792)
Append: [Layers at Similar Depths Generate Similar Activations Across LLM Architectures](https://arxiv.org/abs/2504.08775)
Append: [GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM](https://arxiv.org/abs/2504.12339)
Append: [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
Append: [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Append: [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
Append: [Language-Specific Latent Process Hinders Cross-Lingual Performance](https://arxiv.org/abs/2505.13141)
Append: [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
Append: [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
Append: [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
Append: [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
Append: [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
Append: [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
Append: [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://arxiv.org/abs/2311.04378)
Append: [Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures](https://arxiv.org/abs/2407.19580)
Append: [VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing](https://arxiv.org/abs/2408.05758)
Append: [Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game](https://arxiv.org/abs/2408.09946)
Append: [On the Within-class Variation Issue in Alzheimer's Disease Detection](https://arxiv.org/abs/2409.16322)
Append: [Exploring the Limitations of Mamba in COPY and CoT Reasoning](https://arxiv.org/abs/2410.03810)
Append: [AdvAgent: Controllable Blackbox Red-teaming on Web Agents](https://arxiv.org/abs/2410.17401)
Append: [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)
Append: [AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling](https://arxiv.org/abs/2411.17284)
Append: [Preference Adaptive and Sequential Text-to-Image Generation](https://arxiv.org/abs/2412.10419)
Append: [You Do Not Fully Utilize Transformer's Representation Capacity](https://arxiv.org/abs/2502.09245)
Append: [Non-Markovian Discrete Diffusion with Causal Language Models](https://arxiv.org/abs/2502.09767)
Append: [Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models](https://arxiv.org/abs/2502.09863)
Append: [Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration](https://arxiv.org/abs/2502.11882)
Append: [MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections](https://arxiv.org/abs/2502.12170)
Append: [WiseMind: Recontextualizing AI with a Knowledge-Guided, Theory-Informed Multi-Agent Framework for Instrumental and Humanistic Benefits](https://arxiv.org/abs/2502.20689)
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Append: [WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation](https://arxiv.org/abs/2503.07265)
Append: [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
Append: [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
Append: [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
Append: [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
Append: [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
append_entries: 247
Finish: 2025-05-29 04:28:44.012511
------------------------------------------------------
Started: 2025-05-29 06:25:38.653807
Existing_entries: 1247
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/abs/2502.16514)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [LINGOLY-TOO: Disentangling Reasoning from Knowledge with Templatised Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/abs/2503.10688)
append_entries: 3
Finish: 2025-05-29 06:25:48.119924
------------------------------------------------------
Started: 2025-05-29 08:22:27.216220
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 08:22:27.795545
------------------------------------------------------
Started: 2025-05-29 10:18:44.271650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 10:18:44.822579
------------------------------------------------------
Started: 2025-05-29 12:34:00.300977
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 12:34:00.858918
------------------------------------------------------
Started: 2025-05-29 14:16:13.619860
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 14:16:14.203877
------------------------------------------------------
Started: 2025-05-29 16:20:19.301069
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 16:20:19.908487
------------------------------------------------------
Started: 2025-05-29 18:22:50.090030
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 18:22:50.694769
------------------------------------------------------
Started: 2025-05-29 20:18:51.623273
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 20:18:52.274538
------------------------------------------------------
Started: 2025-05-29 22:15:53.687654
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 22:15:54.281277
------------------------------------------------------
Started: 2025-05-30 01:17:33.438446
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 01:17:34.049622
------------------------------------------------------
Started: 2025-05-30 03:09:40.627553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 03:09:41.185059
------------------------------------------------------
Started: 2025-05-30 04:24:45.166820
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
Token length: 801
Summarized using GPT-3.5-turbo
Append: [Climate Finance Bench](https://arxiv.org/abs/2505.22752)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
Token length: 1621
Summarized using GPT-3.5-turbo
Append: [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages](https://arxiv.org/abs/2505.22774)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Match the Conclusions of Systematic Reviews?](https://arxiv.org/abs/2505.22787)
Token length: 1016
Summarized using GPT-3.5-turbo
Append: [Towards a More Generalized Approach in Open Relation Extraction](https://arxiv.org/abs/2505.22801)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/abs/2505.22823)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
Token length: 927
Summarized using GPT-3.5-turbo
Append: [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification](https://arxiv.org/abs/2505.22867)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models](https://arxiv.org/abs/2505.22897)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [Talent or Luck? Evaluating Attribution Bias in Large Language Models](https://arxiv.org/abs/2505.22910)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room](https://arxiv.org/abs/2505.22919)
Token length: 1693
Summarized using GPT-3.5-turbo
Append: [Structured Memory Mechanisms for Stable Context Representation in Large Language Models](https://arxiv.org/abs/2505.22921)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs](https://arxiv.org/abs/2505.22937)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
Token length: 982
Summarized using GPT-3.5-turbo
Append: [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs](https://arxiv.org/abs/2505.22950)
Token length: 1061
Summarized using GPT-3.5-turbo
Append: [LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments](https://arxiv.org/abs/2505.22956)
Token length: 1710
Summarized using GPT-3.5-turbo
Append: [LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://arxiv.org/abs/2505.22959)
Token length: 1810
Summarized using GPT-3.5-turbo
Append: [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac](https://arxiv.org/abs/2505.23029)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset](https://arxiv.org/abs/2505.23030)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse](https://arxiv.org/abs/2505.23035)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Improving Multilingual Social Media Insights: Aspect-based Comment Analysis](https://arxiv.org/abs/2505.23037)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models](https://arxiv.org/abs/2505.23038)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [Query Routing for Retrieval-Augmented Language Models](https://arxiv.org/abs/2505.23052)
Token length: 1445
Summarized using GPT-3.5-turbo
Append: [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/abs/2505.23060)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services](https://arxiv.org/abs/2505.23065)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [Generating Diverse Training Samples for Relation Extraction with Large Language Models](https://arxiv.org/abs/2505.23108)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
Append: [PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics](https://arxiv.org/abs/2505.23126)
Append: [Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring](https://arxiv.org/abs/2505.23140)
Append: [Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models](https://arxiv.org/abs/2505.23146)
Append: [Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes](https://arxiv.org/abs/2505.23166)
Append: [ZIPA: A family of efficient models for multilingual phone recognition](https://arxiv.org/abs/2505.23170)
Append: [Map&Make: Schema Guided Text to Table Generation](https://arxiv.org/abs/2505.23174)
Append: [Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification](https://arxiv.org/abs/2505.23177)
Append: [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
Append: [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
Append: [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
Append: [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
Append: [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
Append: [ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering](https://arxiv.org/abs/2505.23242)
Append: [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)
Append: [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
Append: [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
Append: [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/abs/2505.23291)
Append: [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
Append: [EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian](https://arxiv.org/abs/2505.23297)
Append: [Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs](https://arxiv.org/abs/2505.23299)
Append: [Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs](https://arxiv.org/abs/2505.23304)
Append: [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
Append: [Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO](https://arxiv.org/abs/2505.23316)
Append: [Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors](https://arxiv.org/abs/2505.23323)
Append: [Discriminative Policy Optimization for Token-Level Reward Models](https://arxiv.org/abs/2505.23363)
Append: [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
Append: [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404)
Append: [From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs](https://arxiv.org/abs/2505.23410)
Append: [The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence](https://arxiv.org/abs/2505.23420)
Append: [UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions](https://arxiv.org/abs/2505.23461)
Append: [Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons](https://arxiv.org/abs/2505.23477)
Append: [Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt](https://arxiv.org/abs/2505.23480)
Append: [Spoken Language Modeling with Duration-Penalized Self-Supervised Units](https://arxiv.org/abs/2505.23494)
Append: [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
Append: [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
Append: [Probability-Consistent Preference Optimization for Enhanced LLM Reasoning](https://arxiv.org/abs/2505.23540)
Append: [Translation in the Wild](https://arxiv.org/abs/2505.23548)
Append: [Understanding Refusal in Language Models with Sparse Autoencoders](https://arxiv.org/abs/2505.23556)
Append: [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/abs/2505.23570)
Append: [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
Append: [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/abs/2505.23621)
Append: [Characterizing the Expressivity of Transformer Language Models](https://arxiv.org/abs/2505.23623)
Append: [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
Append: [GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns](https://arxiv.org/abs/2505.23630)
Append: [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
Append: [ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs](https://arxiv.org/abs/2505.23654)
Append: [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
Append: [ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions](https://arxiv.org/abs/2505.23662)
Append: [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
Append: [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/abs/2505.23688)
Append: [Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models](https://arxiv.org/abs/2505.23689)
Append: [Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation](https://arxiv.org/abs/2505.23701)
Append: [SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models](https://arxiv.org/abs/2505.23713)
Append: [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
Append: [Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models](https://arxiv.org/abs/2505.23715)
Append: [Label-Guided In-Context Learning for Named Entity Recognition](https://arxiv.org/abs/2505.23722)
Append: [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
Append: [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
Append: [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
Append: [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
Append: [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
Append: [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
Append: [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
Append: [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
Append: [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
Append: [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/abs/2505.22793)
Append: [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
Append: [Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge](https://arxiv.org/abs/2505.22863)
Append: [Conversational Alignment with Artificial Intelligence in Context](https://arxiv.org/abs/2505.22907)
Append: [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
Append: [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
Append: [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
Append: [TailorSQL: An NL2SQL System Tailored to Your Query Workload](https://arxiv.org/abs/2505.23039)
Append: [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
Append: [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
Append: [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
Append: [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
Append: [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
Append: [Nosey: Open-source hardware for acoustic nasalance](https://arxiv.org/abs/2505.23339)
Append: [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
Append: [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
Append: [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
Append: [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.23493)
Append: [Identity resolution of software metadata using Large Language Models](https://arxiv.org/abs/2505.23500)
Append: [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
Append: [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
Append: [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
Append: [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
Append: [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
Append: [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
Append: [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
Append: [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
Append: [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
Append: [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
Append: [ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation](https://arxiv.org/abs/2405.17057)
Append: [mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus](https://arxiv.org/abs/2406.08707)
Append: [Neuro-symbolic Training for Reasoning over Spatial Language](https://arxiv.org/abs/2406.13828)
Append: [CLEME2.0: Towards Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction](https://arxiv.org/abs/2407.00934)
Append: [ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts](https://arxiv.org/abs/2407.09447)
Append: [$T^5Score$: A Methodology for Automatically Assessing the Quality of LLM Generated Multi-Document Topic Sets](https://arxiv.org/abs/2407.17390)
Append: [BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models](https://arxiv.org/abs/2408.04556)
Append: [X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term Dialogue Agents](https://arxiv.org/abs/2408.09853)
Append: [Resolving Lexical Bias in Model Editing](https://arxiv.org/abs/2408.10411)
Append: [Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding with LLMs](https://arxiv.org/abs/2410.03553)
Append: [Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs](https://arxiv.org/abs/2410.11001)
Append: [On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of LLMs](https://arxiv.org/abs/2410.12600)
Append: [BenchmarkCards: Large Language Model and Risk Reporting](https://arxiv.org/abs/2410.12974)
Append: [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/abs/2410.16502)
Append: [Reducing Tool Hallucination via Reliability Alignment](https://arxiv.org/abs/2412.04141)
Append: [C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model Evaluation](https://arxiv.org/abs/2412.04947)
Append: [EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation](https://arxiv.org/abs/2412.12559)
Append: [FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](https://arxiv.org/abs/2412.12567)
Append: [AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge](https://arxiv.org/abs/2412.13670)
Append: [SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation](https://arxiv.org/abs/2412.15272)
Append: [Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](https://arxiv.org/abs/2412.16359)
Append: [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
Append: [A Reality Check on Context Utilisation for Retrieval-Augmented Generation](https://arxiv.org/abs/2412.17031)
Append: [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)
Append: [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319)
Append: [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/abs/2501.13977)
Append: [Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation](https://arxiv.org/abs/2501.13978)
Append: [Decomposed Opinion Summarization with Verified Aspect-Aware Modules](https://arxiv.org/abs/2501.17191)
Append: [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/abs/2502.01179)
Append: [Fast Large Language Model Collaborative Decoding via Speculation](https://arxiv.org/abs/2502.01662)
Append: [SPRI: Aligning Large Language Models with Context-Situated Principles](https://arxiv.org/abs/2502.03397)
Append: [Toward universal steering and monitoring of AI models](https://arxiv.org/abs/2502.03708)
Append: [CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance](https://arxiv.org/abs/2502.04350)
Append: [Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives](https://arxiv.org/abs/2502.04358)
Append: [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/abs/2502.04964)
Append: [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
Append: [Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages](https://arxiv.org/abs/2502.10852)
Append: [Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability](https://arxiv.org/abs/2502.11115)
Append: [Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?](https://arxiv.org/abs/2502.11501)
Append: [Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu](https://arxiv.org/abs/2502.11862)
Append: [LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data](https://arxiv.org/abs/2502.12583)
Append: [Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](https://arxiv.org/abs/2502.12970)
Append: [Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction](https://arxiv.org/abs/2502.13044)
Append: [FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems](https://arxiv.org/abs/2502.13472)
Append: [LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization](https://arxiv.org/abs/2502.14538)
Append: [Length-Controlled Margin-Based Preference Optimization without Reference Model](https://arxiv.org/abs/2502.14643)
Append: [SOTOPIA-$\Omega$: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents](https://arxiv.org/abs/2502.15538)
Append: [ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2502.15543)
Append: [DReSD: Dense Retrieval for Speculative Decoding](https://arxiv.org/abs/2502.15572)
Append: [Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines](https://arxiv.org/abs/2502.16377)
Append: [A Survey of Uncertainty Estimation Methods on Large Language Models](https://arxiv.org/abs/2503.00172)
Append: [What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2503.09894)
Append: [DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2503.10452)
Append: [Enhancing Retrieval for ESGLLM via ESG-CID -- A Disclosure Content Index Finetuning Dataset for Mapping GRI and ESRS](https://arxiv.org/abs/2503.10674)
Append: [HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model](https://arxiv.org/abs/2503.12941)
Append: [FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article](https://arxiv.org/abs/2503.16561)
Append: [Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach](https://arxiv.org/abs/2503.18085)
Append: [Multi-Modal Framing Analysis of News](https://arxiv.org/abs/2503.20960)
Append: [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/abs/2503.22353)
Append: [Agentic Knowledgeable Self-awareness](https://arxiv.org/abs/2504.03553)
Append: [NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables](https://arxiv.org/abs/2504.06560)
Append: [DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/abs/2504.08120)
Append: [LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline](https://arxiv.org/abs/2504.09570)
Append: [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
Append: [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
Append: [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Append: [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
Append: [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
Append: [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
Append: [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
Append: [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
Append: [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/abs/2311.09948)
Append: [Theoretical guarantees on the best-of-n alignment policy](https://arxiv.org/abs/2401.01879)
Append: [Learning to Poison Large Language Models for Downstream Manipulation](https://arxiv.org/abs/2402.13459)
Append: [BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes](https://arxiv.org/abs/2404.03161)
Append: [Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts](https://arxiv.org/abs/2404.05019)
Append: [Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition](https://arxiv.org/abs/2404.08008)
Append: [Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning](https://arxiv.org/abs/2408.14774)
Append: [Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents](https://arxiv.org/abs/2408.16081)
Append: [On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists](https://arxiv.org/abs/2409.13931)
Append: [CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning](https://arxiv.org/abs/2410.02229)
Append: [GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation](https://arxiv.org/abs/2410.08475)
Append: [Can We Predict Performance of Large Models across Vision-Language Tasks?](https://arxiv.org/abs/2410.10112)
Append: [GraphNarrator: Generating Textual Explanations for Graph Neural Networks](https://arxiv.org/abs/2410.15268)
Append: [Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces](https://arxiv.org/abs/2410.15625)
Append: [GWQ: Gradient-Aware Weight Quantization for Large Language Models](https://arxiv.org/abs/2411.00850)
Append: [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426)
Append: [VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://arxiv.org/abs/2501.05874)
Append: [Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection](https://arxiv.org/abs/2502.01699)
Append: [Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View](https://arxiv.org/abs/2502.11256)
Append: [GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning](https://arxiv.org/abs/2502.12913)
Append: [K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](https://arxiv.org/abs/2502.13344)
Append: [STeCa: Step-level Trajectory Calibration for LLM Agent Learning](https://arxiv.org/abs/2502.14276)
Append: [Learning to Reason from Feedback at Test-Time](https://arxiv.org/abs/2502.15771)
Append: [Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction](https://arxiv.org/abs/2502.17541)
Append: [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/abs/2503.16814)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
append_entries: 245
Finish: 2025-05-30 04:26:30.689264
------------------------------------------------------
Started: 2025-05-30 06:24:32.402844
Existing_entries: 1245
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
append_entries: 1
Finish: 2025-05-30 06:24:35.163004
------------------------------------------------------
Started: 2025-05-30 08:21:46.605609
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 08:21:47.187615
------------------------------------------------------
Started: 2025-05-30 10:17:39.524530
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 10:17:40.120347
------------------------------------------------------
Started: 2025-05-30 12:33:50.138874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 12:33:50.749088
------------------------------------------------------
Started: 2025-05-30 14:17:03.695096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 14:17:04.344905
------------------------------------------------------
Started: 2025-05-30 16:20:42.470038
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 16:20:43.105595
------------------------------------------------------
Started: 2025-05-30 18:22:43.112111
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 18:22:43.744087
------------------------------------------------------
Started: 2025-05-30 20:18:13.976261
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 20:18:14.510861
------------------------------------------------------
Started: 2025-05-30 22:15:35.697715
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 22:15:36.259501
------------------------------------------------------
Started: 2025-05-31 01:17:44.922081
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 01:17:45.492159
------------------------------------------------------
Started: 2025-05-31 03:08:38.585071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 03:08:39.127535
------------------------------------------------------
Started: 2025-05-31 04:20:30.513549
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 04:20:30.590506
------------------------------------------------------
Started: 2025-05-31 06:22:02.359108
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 06:22:02.415079
------------------------------------------------------
Started: 2025-05-31 08:20:00.796072
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 08:20:00.854968
------------------------------------------------------
Started: 2025-05-31 10:16:15.967913
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 10:16:16.026289
------------------------------------------------------
Started: 2025-05-31 12:30:39.237802
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 12:30:39.312090
------------------------------------------------------
Started: 2025-05-31 14:14:14.855388
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 14:14:14.962627
------------------------------------------------------
Started: 2025-05-31 16:18:36.607553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 16:18:36.715546
------------------------------------------------------
Started: 2025-05-31 18:21:13.424259
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 18:21:13.479007
------------------------------------------------------
Started: 2025-05-31 20:16:59.111552
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 20:16:59.175262
