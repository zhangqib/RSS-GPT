------------------------------------------------------
Started: 2025-04-17 11:13:00.652375
Existing_entries: 0
Fetching from https://arxiv.org/rss/cs.CL
Feed error: 302
append_entries: 0
Finish: 2025-04-17 11:13:01.353527
------------------------------------------------------
Started: 2025-04-17 11:19:18.997494
Existing_entries: 0
Fetching from https://rss.arxiv.org/rss/cs.CL
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SFT or RL? An Early Investigation into Training R1-Like Reasoning Large Vision-Language Models](https://arxiv.org/abs/2504.11468)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ReTool: Reinforcement Learning for Strategic Tool Use in LLMs](https://arxiv.org/abs/2504.11536)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [AskQE: Question Answering as Automatic Evaluation for Machine Translation](https://arxiv.org/abs/2504.11582)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Improving Instruct Models for Free: A Study on Partial Adaptation](https://arxiv.org/abs/2504.11626)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Higher-Order Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters](https://arxiv.org/abs/2504.11770)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Enhancing Web Agents with Explicit Rollback Mechanisms](https://arxiv.org/abs/2504.11788)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Selective Attention Federated Learning: Improving Privacy and Efficiency for Clinical Text Classification](https://arxiv.org/abs/2504.11793)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Efficient and Adaptive Simultaneous Speech Translation with Fully Unidirectional Architecture](https://arxiv.org/abs/2504.11809)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ARWI: Arabic Write and Improve](https://arxiv.org/abs/2504.11814)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [D\'ej\`a Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Could Thinking Multilingually Empower LLM Reasoning?](https://arxiv.org/abs/2504.11833)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [FiSMiness: A Finite State Machine Based Paradigm for Emotional Support Conversations](https://arxiv.org/abs/2504.11837)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Finding Flawed Fictions: Evaluating Complex Reasoning in Language Models via Plot Hole Detection](https://arxiv.org/abs/2504.11900)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [An LLM-as-a-judge Approach for Scalable Gender-Neutral Translation Evaluation](https://arxiv.org/abs/2504.11934)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA](https://arxiv.org/abs/2504.11972)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2504.11975)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Language Models as Quasi-Crystalline Thought: Structure, Constraint, and Emergence in Generative Systems](https://arxiv.org/abs/2504.11986)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Bayesian dynamic borrowing considering semantic similarity between outcomes for disproportionality analysis in FAERS](https://arxiv.org/abs/2504.12052)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Selective Demonstration Retrieval for Improved Implicit Hate Speech Detection](https://arxiv.org/abs/2504.12082)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Gauging Overprecision in LLMs: An Empirical Study](https://arxiv.org/abs/2504.12098)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation](https://arxiv.org/abs/2504.12108)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Multilingual Contextualization of Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2504.12140)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task](https://arxiv.org/abs/2504.12172)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube](https://arxiv.org/abs/2504.12177)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification](https://arxiv.org/abs/2504.12180)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data](https://arxiv.org/abs/2504.12185)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure](https://arxiv.org/abs/2504.12187)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [From Conceptual Data Models to Multimodal Representation](https://arxiv.org/abs/2504.11459)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Semantic Matters: Multimodal Features for Affective Analysis](https://arxiv.org/abs/2504.11460)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Language and Knowledge Representation: A Stratified Approach](https://arxiv.org/abs/2504.11492)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment](https://arxiv.org/abs/2504.11515)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation](https://arxiv.org/abs/2504.11524)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [GraphicBench: A Planning Benchmark for Graphic Design with Language Agents](https://arxiv.org/abs/2504.11571)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Climbing the Ladder of Reasoning: What LLMs Can-and Still Can't-Solve after SFT?](https://arxiv.org/abs/2504.11741)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/abs/2504.11844)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Rethinking LLM-Based Recommendations: A Query Generation-Based, Training-Free Approach](https://arxiv.org/abs/2504.11889)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ADAT: Time-Series-Aware Adaptive Transformer Architecture for Sign Language Translation](https://arxiv.org/abs/2504.11942)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -](https://arxiv.org/abs/2504.12137)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Watermarking Needs Input Repetition Masking](https://arxiv.org/abs/2504.12229)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning](https://arxiv.org/abs/2504.12254)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Knowledge Graph Reasoning with Self-supervised Reinforcement Learning](https://arxiv.org/abs/2405.13640)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Large Visual-Language Models Are Also Good Classifiers: A Study of In-Context Multimodal Fake News Detection](https://arxiv.org/abs/2407.12879)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [Application of AI-based Models for Online Fraud Detection and Analysis](https://arxiv.org/abs/2409.19022)
Summarization failed, append the original article
error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-mys6F***************************************sVQE. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}
Append: [ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement](https://arxiv.org/abs/2410.02108)
Append: [Science Out of Its Ivory Tower: Improving Accessibility with Reinforcement Learning](https://arxiv.org/abs/2410.17088)
Append: [Fine-Grained Reward Optimization for Machine Translation using Error Severity Mappings](https://arxiv.org/abs/2411.05986)
Append: [Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework](https://arxiv.org/abs/2411.16707)
Append: [Sequence-Level Leakage Risk of Training Data in Large Language Models](https://arxiv.org/abs/2412.11302)
Append: [Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models](https://arxiv.org/abs/2412.12144)
Append: [Enhancing Privacy in the Early Detection of Sexual Predators Through Federated Learning and Differential Privacy](https://arxiv.org/abs/2501.12537)
Append: [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/abs/2502.01568)
Append: [How Inclusively do LMs Perceive Social and Moral Norms?](https://arxiv.org/abs/2502.02696)
Append: [Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation](https://arxiv.org/abs/2502.05151)
Append: [Automatic Input Rewriting Improves Translation with Large Language Models](https://arxiv.org/abs/2502.16682)
Append: [Figurative Archive: an open dataset and web-based application for the study of metaphor](https://arxiv.org/abs/2503.00444)
Append: [FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Append: [What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2503.24235)
Append: [Assessing how hyperparameters impact Large Language Models' sarcasm detection performance](https://arxiv.org/abs/2504.06166)
Append: [Evaluation Under Imperfect Benchmarks and Ratings: A Case Study in Text Simplification](https://arxiv.org/abs/2504.09394)
Append: [Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution](https://arxiv.org/abs/2504.09566)
Append: [LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks](https://arxiv.org/abs/2504.10185)
Append: [Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)
Append: [Automated Python Translation](https://arxiv.org/abs/2504.11290)
Append: [Local Grammar-Based Coding Revisited](https://arxiv.org/abs/2209.13636)
Append: [StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text](https://arxiv.org/abs/2403.14773)
Append: [Taming Data and Transformers for Audio Generation](https://arxiv.org/abs/2406.19388)
Append: [Natural Language Outlines for Code: Literate Programming in the LLM Era](https://arxiv.org/abs/2408.04820)
Append: [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins (early version)](https://arxiv.org/abs/2409.02920)
Append: [Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models](https://arxiv.org/abs/2410.01795)
Append: [No Need to Talk: Asynchronous Mixture of Language Models](https://arxiv.org/abs/2410.03529)
Append: [Leveraging Social Determinants of Health in Alzheimer's Research Using LLM-Augmented Literature Mining and Knowledge Graphs](https://arxiv.org/abs/2410.09080)
Append: [Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction](https://arxiv.org/abs/2410.21169)
Append: [Bridging the Visual Gap: Fine-Tuning Multimodal Models with Knowledge-Adapted Captions](https://arxiv.org/abs/2411.09018)
Append: [BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving](https://arxiv.org/abs/2411.17404)
Append: [Orthus: Autoregressive Interleaved Image-Text Generation with Modality-Specific Heads](https://arxiv.org/abs/2412.00127)
Append: [ChaosEater: Fully Automating Chaos Engineering with Large Language Models](https://arxiv.org/abs/2501.11107)
Append: [FourierNAT: A Fourier-Mixing-Based Non-Autoregressive Transformer for Parallel Sequence Generation](https://arxiv.org/abs/2503.07630)
Append: [Think Before Recommend: Unleashing the Latent Reasoning Power for Sequential Recommendation](https://arxiv.org/abs/2503.22675)
Append: [Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks](https://arxiv.org/abs/2504.08525)
Append: [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/abs/2504.11257)
append_entries: 86
Finish: 2025-04-17 11:19:34.176790
------------------------------------------------------
Started: 2025-04-17 11:36:05.707066
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 11:36:05.882828
------------------------------------------------------
Started: 2025-04-17 12:31:42.607215
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 12:31:42.750078
------------------------------------------------------
Started: 2025-04-17 14:15:25.880323
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 14:15:26.002056
------------------------------------------------------
Started: 2025-04-17 16:20:08.581711
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 16:20:08.710971
------------------------------------------------------
Started: 2025-04-17 18:21:33.755192
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 18:21:33.931093
------------------------------------------------------
Started: 2025-04-17 20:17:29.620947
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 20:17:29.774904
------------------------------------------------------
Started: 2025-04-17 22:14:51.190252
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-17 22:14:51.351165
------------------------------------------------------
Started: 2025-04-18 01:14:25.579571
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 01:14:25.739458
------------------------------------------------------
Started: 2025-04-18 02:57:56.634221
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 02:57:56.791011
------------------------------------------------------
Started: 2025-04-18 04:22:12.545194
Existing_entries: 86
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Unmasking the Reality of PII Masking Models: Performance Gaps and the Call for Accountability](https://arxiv.org/abs/2504.12308)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large Language Models with Logic Programming-based Test Oracles](https://arxiv.org/abs/2504.12312)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Exploring the Impact of Personality Traits on Conversational Recommender Systems: A Simulation with Large Language Models](https://arxiv.org/abs/2504.12313)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [How to Detect and Defeat Molecular Mirage: A Metric-Driven Benchmark for Hallucination in LLM-based Molecular Comprehension](https://arxiv.org/abs/2504.12314)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Capybara-OMNI: An Efficient Paradigm for Building Omni-Modal Language Models](https://arxiv.org/abs/2504.12315)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Data Metabolism: An Efficient Data Design Schema For Vision Language Model](https://arxiv.org/abs/2504.12316)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [ChatGPT as Linguistic Equalizer? Quantifying LLM-Driven Lexical Shifts in Academic Writing](https://arxiv.org/abs/2504.12317)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Has the Creativity of Large-Language Models peaked? An analysis of inter- and intra-LLM variability](https://arxiv.org/abs/2504.12320)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [AttentionDefense: Leveraging System Prompt Attention for Explainable Defense Against Novel Jailbreaks](https://arxiv.org/abs/2504.12321)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis](https://arxiv.org/abs/2504.12322)
Token length: 1709
Summarized using GPT-3.5-turbo
Append: [The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12323)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [Cross-Document Cross-Lingual Natural Language Inference via RST-enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media](https://arxiv.org/abs/2504.12325)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis](https://arxiv.org/abs/2504.12326)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Word Embeddings Track Social Group Changes Across 70 Years in China](https://arxiv.org/abs/2504.12327)
Token length: 870
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future](https://arxiv.org/abs/2504.12328)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Speculative Thinking: Enhancing Small-Model Reasoning with Large Model Guidance at Inference Time](https://arxiv.org/abs/2504.12329)
Token length: 1809
Summarized using GPT-3.5-turbo
Append: [HM-RAG: Hierarchical Multi-Agent Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2504.12330)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation](https://arxiv.org/abs/2504.12331)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Can the capability of Large Language Models be described by human ability? A Meta Study](https://arxiv.org/abs/2504.12332)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Meta-Evaluating Local LLMs: Rethinking Performance Metrics for Serious Games](https://arxiv.org/abs/2504.12333)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [QM-ToT: A Medical Tree of Thoughts Reasoning Framework for Quantized Model](https://arxiv.org/abs/2504.12334)
Token length: 944
Summarized using GPT-3.5-turbo
Append: [You've Changed: Detecting Modification of Black-Box Large Language Models](https://arxiv.org/abs/2504.12335)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: ["It Listens Better Than My Therapist": Exploring Social Media Discourse on LLMs as Mental Health Tool](https://arxiv.org/abs/2504.12337)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Paging Dr. GPT: Extracting Information from Clinical Notes to Enhance Patient Predictions](https://arxiv.org/abs/2504.12338)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [GOAT-TTS: LLM-based Text-To-Speech Generation Optimized via A Dual-Branch Architecture](https://arxiv.org/abs/2504.12339)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Streamlining Biomedical Research with Specialized LLMs](https://arxiv.org/abs/2504.12341)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Benchmarking Biopharmaceuticals Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.12342)
Token length: 1613
Summarized using GPT-3.5-turbo
Append: [Propaganda via AI? A Study on Semantic Backdoors in Large Language Models](https://arxiv.org/abs/2504.12344)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Token length: 837
Summarized using GPT-3.5-turbo
Append: [Mathematical Capabilities of Large Language Models in Finnish Matriculation Examination](https://arxiv.org/abs/2504.12347)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [A Large-Language Model Framework for Relative Timeline Extraction from PubMed Case Reports](https://arxiv.org/abs/2504.12350)
Token length: 898
Summarized using GPT-3.5-turbo
Append: [Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media](https://arxiv.org/abs/2504.12355)
Token length: 639
Summarized using GPT-3.5-turbo
Append: [Replicating ReLM Results: Validating Large Language Models with ReLM](https://arxiv.org/abs/2504.12357)
Token length: 913
Summarized using GPT-3.5-turbo
Append: [A Method for Handling Negative Similarities in Explainable Graph Spectral Clustering of Text Documents -- Extended Version](https://arxiv.org/abs/2504.12360)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Position: The Most Expensive Part of an LLM should be its Training Data](https://arxiv.org/abs/2504.12427)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [On Linear Representations and Pretraining Data Frequency in Language Models](https://arxiv.org/abs/2504.12459)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [SLURG: Investigating the Feasibility of Generating Synthetic Online Fallacious Discourse](https://arxiv.org/abs/2504.12466)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex](https://arxiv.org/abs/2504.12474)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [Can Pre-training Indicators Reliably Predict Fine-tuning Outcomes of LLMs?](https://arxiv.org/abs/2504.12491)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [Accelerating Clinical NLP at Scale with a Hybrid Framework with Reduced GPU Demands: A Case Study in Dementia Identification](https://arxiv.org/abs/2504.12494)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [Beyond Text: Characterizing Domain Expert Needs in Document Research](https://arxiv.org/abs/2504.12495)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents](https://arxiv.org/abs/2504.12516)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Evaluating the Diversity and Quality of LLM Generated Content](https://arxiv.org/abs/2504.12522)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [Memorization vs. Reasoning: Updating LLMs with New Knowledge](https://arxiv.org/abs/2504.12523)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Memorization: A Close Look at Books](https://arxiv.org/abs/2504.12549)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [ELAB: Extensive LLM Alignment Benchmark in Persian Language](https://arxiv.org/abs/2504.12553)
Token length: 1303
Summarized using GPT-3.5-turbo
Append: [CDF-RAG: Causal Dynamic Feedback for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2504.12560)
Token length: 1595
Summarized using GPT-3.5-turbo
Append: [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)
Append: [Identifying and Mitigating the Influence of the Prior Distribution in Large Language Models](https://arxiv.org/abs/2504.12585)
Append: [GeoSense: Evaluating Identification and Application of Geometric Principles in Multimodal Reasoning](https://arxiv.org/abs/2504.12597)
Append: [Towards Characterizing Subjectivity of Individuals through Modeling Value Conflicts and Trade-offs](https://arxiv.org/abs/2504.12633)
Append: [Scaling Instruction-Tuned LLMs to Million-Token Contexts via Hierarchical Synthetic Data Generation](https://arxiv.org/abs/2504.12637)
Append: [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)
Append: [ACoRN: Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2504.12673)
Append: [GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs](https://arxiv.org/abs/2504.12681)
Append: [Data-efficient LLM Fine-tuning for Code Generation](https://arxiv.org/abs/2504.12687)
Append: [Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations](https://arxiv.org/abs/2504.12691)
Append: [KODIS: A Multicultural Dispute Resolution Dialogue Corpus](https://arxiv.org/abs/2504.12723)
Append: [Pandora: A Code-Driven Large Language Model Agent for Unified Reasoning Across Diverse Structured Knowledge](https://arxiv.org/abs/2504.12734)
Append: [Chinese-Vicuna: A Chinese Instruction-following Llama-based Model](https://arxiv.org/abs/2504.12737)
Append: [Out of Sight Out of Mind, Out of Sight Out of Mind: Measuring Bias in Language Models Against Overlooked Marginalized Groups in Regional Contexts](https://arxiv.org/abs/2504.12767)
Append: [Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration](https://arxiv.org/abs/2504.12773)
Append: [Assesing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation](https://arxiv.org/abs/2504.12805)
Append: [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
Append: [Can LLMs reason over extended multilingual contexts? Towards long-context evaluation beyond retrieval and haystacks](https://arxiv.org/abs/2504.12845)
Append: [ViClaim: A Multilingual Multilabel Dataset for Automatic Claim Detection in Videos](https://arxiv.org/abs/2504.12882)
Append: [Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication](https://arxiv.org/abs/2504.12891)
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Append: [Benchmarking Multi-National Value Alignment for Large Language Models](https://arxiv.org/abs/2504.12911)
Append: [MAIN: Mutual Alignment Is Necessary for instruction tuning](https://arxiv.org/abs/2504.12913)
Append: [ConExion: Concept Extraction with Large Language Models](https://arxiv.org/abs/2504.12915)
Append: [Are Retrials All You Need? Enhancing Large Language Model Reasoning Without Verbalized Feedback](https://arxiv.org/abs/2504.12951)
Append: [Estimating Optimal Context Length for Hybrid Retrieval-augmented Multi-document Summarization](https://arxiv.org/abs/2504.12972)
Append: [Sparks of Science: Hypothesis Generation Using Structured Paper Data](https://arxiv.org/abs/2504.12976)
Append: [Accommodate Knowledge Conflicts in Retrieval-augmented LLMs: Towards Reliable Response Generation in the Wild](https://arxiv.org/abs/2504.12982)
Append: [SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation](https://arxiv.org/abs/2504.12996)
Append: [ChatEXAONEPath: An Expert-level Multimodal Large Language Model for Histopathology Using Whole Slide Images](https://arxiv.org/abs/2504.13023)
Append: [Aspect-Based Summarization with Self-Aspect Retrieval Enhanced Generation](https://arxiv.org/abs/2504.13054)
Append: [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
Append: [Retrieval-Augmented Generation with Conflicting Evidence](https://arxiv.org/abs/2504.13079)
Append: [LLMs Meet Finance: Fine-Tuning Foundation Models for the Open FinLLM Leaderboard](https://arxiv.org/abs/2504.13125)
Append: [Energy-Based Reward Models for Robust Language Model Alignment](https://arxiv.org/abs/2504.13134)
Append: [Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo](https://arxiv.org/abs/2504.13139)
Append: [CLIMB: CLustering-based Iterative Data Mixture Bootstrapping for Language Model Pre-training](https://arxiv.org/abs/2504.13161)
Append: [Large Language Model-Based Knowledge Graph System Construction for Sustainable Development Goals: An AI-Based Speculative Design Perspective](https://arxiv.org/abs/2504.12309)
Append: [Specialized text classification: an approach to classifying Open Banking transactions](https://arxiv.org/abs/2504.12319)
Append: [A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment](https://arxiv.org/abs/2504.12408)
Append: [Towards Conversational AI for Human-Machine Collaborative MLOps](https://arxiv.org/abs/2504.12477)
Append: [Knowledge Acquisition on Mass-shooting Events via LLMs for AI-Driven Justice](https://arxiv.org/abs/2504.12545)
Append: [Benchmarking LLM-based Relevance Judgment Methods](https://arxiv.org/abs/2504.12558)
Append: [ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2504.12562)
Append: [Provable Secure Steganography Based on Adaptive Dynamic Sampling](https://arxiv.org/abs/2504.12579)
Append: [Simplifying Graph Transformers](https://arxiv.org/abs/2504.12588)
Append: [VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization](https://arxiv.org/abs/2504.12661)
Append: [WebLists: Extracting Structured Information From Complex Interactive Websites Using Executable LLM Agents](https://arxiv.org/abs/2504.12682)
Append: [Towards Lossless Token Pruning in Late-Interaction Retrieval Models](https://arxiv.org/abs/2504.12778)
Append: [EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting](https://arxiv.org/abs/2504.12867)
Append: [Building Russian Benchmark for Evaluation of Information Retrieval Models](https://arxiv.org/abs/2504.12879)
Append: [A Phenomenological Approach to Analyzing User Queries in IT Systems Using Heidegger's Fundamental Ontology](https://arxiv.org/abs/2504.12977)
Append: [How Large Language Models Are Changing MOOC Essay Answers: A Comparison of Pre- and Post-LLM Responses](https://arxiv.org/abs/2504.13038)
Append: [RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins](https://arxiv.org/abs/2504.13059)
Append: [Tackling Social Bias against the Poor: A Dataset and Taxonomy on Aporophobia](https://arxiv.org/abs/2504.13085)
Append: [Probing and Inducing Combinational Creativity in Vision-Language Models](https://arxiv.org/abs/2504.13120)
Append: [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)
Append: [Antidistillation Sampling](https://arxiv.org/abs/2504.13146)
Append: [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
Append: [Sleep-time Compute: Beyond Inference Scaling at Test-time](https://arxiv.org/abs/2504.13171)
Append: [SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework with MLLMs](https://arxiv.org/abs/2504.13172)
Append: [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000)
Append: [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305)
Append: [Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers](https://arxiv.org/abs/2402.11700)
Append: [Citation-Enhanced Generation for LLM-based Chatbots](https://arxiv.org/abs/2402.16063)
Append: [MemLLM: Finetuning LLMs to Use An Explicit Read-Write Memory](https://arxiv.org/abs/2404.11672)
Append: [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/abs/2405.06691)
Append: [Unipa-GPT: Large Language Models for university-oriented QA in Italian](https://arxiv.org/abs/2407.14246)
Append: [ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities](https://arxiv.org/abs/2408.04682)
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Append: [Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective](https://arxiv.org/abs/2410.10291)
Append: [In-context KV-Cache Eviction for LLMs via Attention-Gate](https://arxiv.org/abs/2410.12876)
Append: [Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities](https://arxiv.org/abs/2410.17385)
Append: [IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs](https://arxiv.org/abs/2411.07466)
Append: [AMPS: ASR with Multimodal Paraphrase Supervision](https://arxiv.org/abs/2411.18368)
Append: [Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Step Arithmetic Reasoning](https://arxiv.org/abs/2412.01113)
Append: [Arabizi vs LLMs: Can the Genie Understand the Language of Aladdin?](https://arxiv.org/abs/2502.20973)
Append: [ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition](https://arxiv.org/abs/2503.10673)
Append: [Multi-Stakeholder Disaster Insights from Social Media Using Large Language Models](https://arxiv.org/abs/2504.00046)
Append: [OnRL-RAG: Real-Time Personalized Mental Health Dialogue System](https://arxiv.org/abs/2504.02894)
Append: [FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion](https://arxiv.org/abs/2504.06562)
Append: [Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare](https://arxiv.org/abs/2504.08260)
Append: [Taxonomy and Analysis of Sensitive User Queries in Generative AI Search](https://arxiv.org/abs/2404.08672)
Append: [ALCM: Autonomous LLM-Augmented Causal Discovery Framework](https://arxiv.org/abs/2405.01744)
Append: [ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs](https://arxiv.org/abs/2409.09586)
Append: [Multi-Field Adaptive Retrieval](https://arxiv.org/abs/2410.20056)
Append: [Multimodal LLMs Can Reason about Aesthetics in Zero-Shot](https://arxiv.org/abs/2501.09012)
Append: [Contextual Agent Security: A Policy for Every Purpose](https://arxiv.org/abs/2501.17070)
Append: [DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/abs/2504.03160)
Append: [SIFT-50M: A Large-Scale Multilingual Dataset for Speech Instruction Fine-Tuning](https://arxiv.org/abs/2504.09081)
Append: [CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography](https://arxiv.org/abs/2504.10090)
Append: [CSPLADE: Learned Sparse Retrieval with Causal Language Models](https://arxiv.org/abs/2504.10816)
append_entries: 141
Finish: 2025-04-18 04:23:56.116842
------------------------------------------------------
Started: 2025-04-18 06:22:49.448041
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 06:22:49.651237
------------------------------------------------------
Started: 2025-04-18 08:20:28.573579
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 08:20:28.802837
------------------------------------------------------
Started: 2025-04-18 10:16:58.114721
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 10:16:58.334868
------------------------------------------------------
Started: 2025-04-18 12:30:48.575312
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 12:30:48.782049
------------------------------------------------------
Started: 2025-04-18 14:14:26.630654
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 14:14:26.886108
------------------------------------------------------
Started: 2025-04-18 16:19:00.392459
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 16:19:00.604683
------------------------------------------------------
Started: 2025-04-18 18:21:10.218530
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 18:21:10.449185
------------------------------------------------------
Started: 2025-04-18 20:16:52.105165
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 20:16:52.304820
------------------------------------------------------
Started: 2025-04-18 22:15:47.891963
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-18 22:15:48.104332
------------------------------------------------------
Started: 2025-04-19 01:12:23.296762
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 01:12:23.505671
------------------------------------------------------
Started: 2025-04-19 02:54:27.494880
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 02:54:27.699064
------------------------------------------------------
Started: 2025-04-19 04:18:13.814489
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 04:18:13.860699
------------------------------------------------------
Started: 2025-04-19 06:20:49.522606
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 06:20:49.571120
------------------------------------------------------
Started: 2025-04-19 08:18:19.457168
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 08:18:19.506663
------------------------------------------------------
Started: 2025-04-19 10:14:57.449370
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 10:14:57.514971
------------------------------------------------------
Started: 2025-04-19 12:28:10.834622
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 12:28:10.896698
------------------------------------------------------
Started: 2025-04-19 14:13:29.774947
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 14:13:29.817857
------------------------------------------------------
Started: 2025-04-19 16:19:00.848640
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 16:19:00.895567
------------------------------------------------------
Started: 2025-04-19 18:19:24.418417
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 18:19:24.467285
------------------------------------------------------
Started: 2025-04-19 20:16:14.713065
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 20:16:14.756591
------------------------------------------------------
Started: 2025-04-19 22:14:08.761221
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-19 22:14:08.808443
------------------------------------------------------
Started: 2025-04-20 01:20:16.180253
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 01:20:16.224170
------------------------------------------------------
Started: 2025-04-20 03:05:48.313731
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 03:05:48.376659
------------------------------------------------------
Started: 2025-04-20 04:17:47.627738
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 04:17:47.679948
------------------------------------------------------
Started: 2025-04-20 06:20:54.713747
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 06:20:54.774109
------------------------------------------------------
Started: 2025-04-20 08:18:10.390530
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 08:18:10.441756
------------------------------------------------------
Started: 2025-04-20 10:15:19.146242
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 10:15:19.205367
------------------------------------------------------
Started: 2025-04-20 12:28:45.757837
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 12:28:45.799343
------------------------------------------------------
Started: 2025-04-20 14:13:54.969232
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 14:13:55.015833
------------------------------------------------------
Started: 2025-04-20 16:17:31.583863
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 16:17:31.649257
------------------------------------------------------
Started: 2025-04-20 18:19:58.800955
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 18:19:58.893111
------------------------------------------------------
Started: 2025-04-20 20:16:04.221792
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 20:16:04.296078
------------------------------------------------------
Started: 2025-04-20 22:14:46.880363
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-20 22:14:46.959585
------------------------------------------------------
Started: 2025-04-21 01:18:44.968624
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 01:18:45.016905
------------------------------------------------------
Started: 2025-04-21 03:07:34.438892
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 03:07:34.487419
------------------------------------------------------
Started: 2025-04-21 04:24:17.719976
Existing_entries: 227
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Benchmarking Large Language Models for Calculus Problem-Solving: A Comparative Analysis](https://arxiv.org/abs/2504.13187)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [BASIR: Budget-Assisted Sectoral Impact Ranking -- A Dataset for Sector Identification and Performance Prediction Using Language Models](https://arxiv.org/abs/2504.13189)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [KFinEval-Pilot: A Comprehensive Benchmark Suite for Korean Financial Language Understanding](https://arxiv.org/abs/2504.13216)
Token length: 1860
Summarized using GPT-3.5-turbo
Append: [Sustainability via LLM Right-sizing](https://arxiv.org/abs/2504.13217)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [DIDS: Domain Impact-aware Data Sampling for Large Language Model Training](https://arxiv.org/abs/2504.13227)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [ImPart: Importance-Aware Delta-Sparsification for Improved Model Compression and Merging in LLMs](https://arxiv.org/abs/2504.13237)
Token length: 1651
Summarized using GPT-3.5-turbo
Append: [CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models](https://arxiv.org/abs/2504.13261)
Token length: 890
Summarized using GPT-3.5-turbo
Append: [Sentiment Analysis on the young people's perception about the mobile Internet costs in Senegal](https://arxiv.org/abs/2504.13284)
Token length: 1139
Summarized using GPT-3.5-turbo
Append: [THOUGHTTERMINATOR: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models](https://arxiv.org/abs/2504.13367)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Secure Multifaceted-RAG for Enterprise: Hybrid Knowledge Retrieval with Security Filtering](https://arxiv.org/abs/2504.13425)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model](https://arxiv.org/abs/2504.13439)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [LLM Sensitivity Evaluation Framework for Clinical Diagnosis](https://arxiv.org/abs/2504.13475)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Prejudge-Before-Think: Enhancing Large Language Models at Test-Time by Process Prejudge Reasoning](https://arxiv.org/abs/2504.13500)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Enhancing Multilingual Sentiment Analysis with Explainability for Sinhala, English, and Code-Mixed Content](https://arxiv.org/abs/2504.13545)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification](https://arxiv.org/abs/2504.13562)
Token length: 1308
Summarized using GPT-3.5-turbo
Append: [Improving Generalization in Intent Detection: GRPO with Reward-Based Curriculum Sampling](https://arxiv.org/abs/2504.13592)
Token length: 1078
Summarized using GPT-3.5-turbo
Append: [Continual Pre-Training is (not) What You Need in Domain Adaption](https://arxiv.org/abs/2504.13603)
Token length: 1840
Summarized using GPT-3.5-turbo
Append: [Long-context Non-factoid Question Answering in Indic Languages](https://arxiv.org/abs/2504.13615)
Token length: 1593
Summarized using GPT-3.5-turbo
Append: [Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models](https://arxiv.org/abs/2504.13626)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [Divergent LLM Adoption and Heterogeneous Convergence Paths in Research Writing](https://arxiv.org/abs/2504.13629)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Remedy: Learning Machine Translation Evaluation from Human Preferences with Reward Modeling](https://arxiv.org/abs/2504.13630)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Simulating Before Planning: Constructing Intrinsic User World Model for User-Tailored Dialogue Policy Planning](https://arxiv.org/abs/2504.13643)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Word Embedding Techniques for Classification of Star Ratings](https://arxiv.org/abs/2504.13653)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Multi-Type Context-Aware Conversational Recommender Systems via Mixture-of-Experts](https://arxiv.org/abs/2504.13655)
Token length: 934
Summarized using GPT-3.5-turbo
Append: [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Deep literature reviews: an application of fine-tuned language models to migration research](https://arxiv.org/abs/2504.13685)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied Territory from Open Source Intelligence](https://arxiv.org/abs/2504.13730)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of Black-box Large Language Models](https://arxiv.org/abs/2504.13775)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/abs/2504.13816)
Token length: 1148
Summarized using GPT-3.5-turbo
Append: [Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2504.13825)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/abs/2504.13828)
Token length: 1914
Summarized using GPT-3.5-turbo
Append: [Science Hierarchography: Hierarchical Organization of Science Literature](https://arxiv.org/abs/2504.13834)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space](https://arxiv.org/abs/2504.13835)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents](https://arxiv.org/abs/2504.13203)
Token length: 1635
Summarized using GPT-3.5-turbo
Append: [Interpersonal Theory of Suicide as a Lens to Examine Suicidal Ideation in Online Spaces](https://arxiv.org/abs/2504.13277)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Acoustic to Articulatory Inversion of Speech; Data Driven Approaches, Challenges, Applications, and Future Scope](https://arxiv.org/abs/2504.13308)
Token length: 1891
Summarized using GPT-3.5-turbo
Append: [Cost-of-Pass: An Economic Framework for Evaluating Language Models](https://arxiv.org/abs/2504.13359)
Token length: 1012
Summarized using GPT-3.5-turbo
Append: [A mean teacher algorithm for unlearning of language models](https://arxiv.org/abs/2504.13388)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [LangCoop: Collaborative Driving with Language](https://arxiv.org/abs/2504.13406)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings](https://arxiv.org/abs/2504.13416)
Token length: 1782
Summarized using GPT-3.5-turbo
Append: [CodeVisionary: An Agent-based Framework for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2504.13472)
Token length: 1658
Summarized using GPT-3.5-turbo
Append: [Integrating Locality-Aware Attention with Transformers for General Geometry PDEs](https://arxiv.org/abs/2504.13480)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Q-FAKER: Query-free Hard Black-box Attack via Controlled Generation](https://arxiv.org/abs/2504.13551)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Exploring the Potential for Large Language Models to Demonstrate Rational Probabilistic Beliefs](https://arxiv.org/abs/2504.13644)
Token length: 601
Summarized using GPT-3.5-turbo
Append: [Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm](https://arxiv.org/abs/2504.13667)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [OpenDeception: Benchmarking and Investigating AI Deceptive Behaviors via Open-ended Interaction Simulation](https://arxiv.org/abs/2504.13707)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Learning to Attribute with Attention](https://arxiv.org/abs/2504.13752)
Append: [Scaling sparse feature circuit finding for in-context learning](https://arxiv.org/abs/2504.13756)
Append: [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
Append: [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
Append: [Only Send What You Need: Learning to Communicate Efficiently in Federated Multilingual Machine Translation](https://arxiv.org/abs/2401.07456)
Append: [A Theory of LLM Sampling: Part Descriptive and Part Prescriptive](https://arxiv.org/abs/2402.11005)
Append: [Where is the answer? Investigating Positional Bias in Language Model Knowledge Extraction](https://arxiv.org/abs/2402.12170)
Append: [Argumentative Large Language Models for Explainable and Contestable Claim Verification](https://arxiv.org/abs/2405.02079)
Append: [Babysit A Language Model From Scratch: Interactive Language Learning by Trials and Demonstrations](https://arxiv.org/abs/2405.13828)
Append: [Is In-Context Learning Sufficient for Instruction Following in LLMs?](https://arxiv.org/abs/2405.19874)
Append: [Adversarial Style Augmentation via Large Language Model for Robust Fake News Detection](https://arxiv.org/abs/2406.11260)
Append: [Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?](https://arxiv.org/abs/2406.12307)
Append: [The Comparative Trap: Pairwise Comparisons Amplifies Biased Preferences of LLM Evaluators](https://arxiv.org/abs/2406.12319)
Append: [Does Refusal Training in LLMs Generalize to the Past Tense?](https://arxiv.org/abs/2407.11969)
Append: [Understanding Epistemic Language with a Language-augmented Bayesian Theory of Mind](https://arxiv.org/abs/2408.12022)
Append: [Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts](https://arxiv.org/abs/2409.11056)
Append: [Reducing the Scope of Language Models](https://arxiv.org/abs/2410.21597)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Are You Doubtful? Oh, It Might Be Difficult Then! Exploring the Use of Model Uncertainty for Question Difficulty Estimation](https://arxiv.org/abs/2412.11831)
Append: [StaICC: Standardized Evaluation for Classification Task in In-context Learning](https://arxiv.org/abs/2501.15708)
Append: [A-MEM: Agentic Memory for LLM Agents](https://arxiv.org/abs/2502.12110)
Append: [Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models](https://arxiv.org/abs/2502.14427)
Append: [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729)
Append: [Adaptive Layer-skipping in Pre-trained LLMs](https://arxiv.org/abs/2503.23798)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [From Token to Line: Enhancing Code Generation with a Long-Term Perspective](https://arxiv.org/abs/2504.07433)
Append: [Can postgraduate translation students identify machine-generated text?](https://arxiv.org/abs/2504.09164)
Append: [C-MTCSD: A Chinese Multi-Turn Conversational Stance Detection Dataset](https://arxiv.org/abs/2504.09958)
Append: [The Mirage of Performance Gains: Why Contrastive Decoding Fails to Address Multimodal Hallucination](https://arxiv.org/abs/2504.10020)
Append: [SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems](https://arxiv.org/abs/2405.19653)
Append: [LLM-Select: Feature Selection with Large Language Models](https://arxiv.org/abs/2407.02694)
Append: [Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization](https://arxiv.org/abs/2407.07880)
Append: [Spin glass model of in-context learning](https://arxiv.org/abs/2408.02288)
Append: [SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding](https://arxiv.org/abs/2408.15545)
Append: [United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections](https://arxiv.org/abs/2409.09045)
Append: [AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents](https://arxiv.org/abs/2410.09024)
Append: [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](https://arxiv.org/abs/2410.21465)
Append: [PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation](https://arxiv.org/abs/2411.16750)
Append: [Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant](https://arxiv.org/abs/2501.17176)
Append: [Project Alexandria: Towards Freeing Scientific Knowledge from Copyright Burdens via LLMs](https://arxiv.org/abs/2502.19413)
Append: [Psycholinguistic Analyses in Software Engineering Text: A Systematic Literature Review](https://arxiv.org/abs/2503.05992)
Append: [DocAgent: A Multi-Agent System for Automated Code Documentation Generation](https://arxiv.org/abs/2504.08725)
Append: [Assessing Judging Bias in Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2504.09946)
Append: [GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents](https://arxiv.org/abs/2504.10458)
append_entries: 93
Finish: 2025-04-21 04:26:21.470194
------------------------------------------------------
Started: 2025-04-21 06:23:11.446219
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 06:23:11.621175
------------------------------------------------------
Started: 2025-04-21 08:21:32.665323
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 08:21:32.824343
------------------------------------------------------
Started: 2025-04-21 10:17:11.123964
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 10:17:11.292386
------------------------------------------------------
Started: 2025-04-21 12:31:41.885468
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 12:31:42.045876
------------------------------------------------------
Started: 2025-04-21 14:14:45.792935
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 14:14:45.954796
------------------------------------------------------
Started: 2025-04-21 16:19:34.585177
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 16:19:34.784087
------------------------------------------------------
Started: 2025-04-21 18:21:23.595244
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 18:21:23.768949
------------------------------------------------------
Started: 2025-04-21 20:17:45.253500
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 20:17:45.413071
------------------------------------------------------
Started: 2025-04-21 22:16:01.866912
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-21 22:16:02.025241
------------------------------------------------------
Started: 2025-04-22 01:15:42.061651
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 01:15:42.240146
------------------------------------------------------
Started: 2025-04-22 03:01:08.089373
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 03:01:08.260772
------------------------------------------------------
Started: 2025-04-22 04:23:33.107276
Existing_entries: 320
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 940
Summarized using GPT-3.5-turbo
Append: [Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Uncovering Conspiratorial Narratives within Arabic Online Content](https://arxiv.org/abs/2504.14037)
Token length: 855
Summarized using GPT-3.5-turbo
Append: [MEQA: A Meta-Evaluation Framework for Question & Answer LLM Benchmarks](https://arxiv.org/abs/2504.14039)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [A Baseline for Self-state Identification and Classification in Mental Health Data: CLPsych 2025 Task](https://arxiv.org/abs/2504.14066)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)
Token length: 1667
Summarized using GPT-3.5-turbo
Append: [PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models](https://arxiv.org/abs/2504.14117)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Self-Correction Makes LLMs Better Parsers](https://arxiv.org/abs/2504.14165)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
Token length: 1166
Summarized using GPT-3.5-turbo
Append: [EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition](https://arxiv.org/abs/2504.14203)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Bias Analysis and Mitigation through Protected Attribute Detection and Regard Classification](https://arxiv.org/abs/2504.14212)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [SimplifyMyText: An LLM-Based System for Inclusive Plain Language Text Simplification](https://arxiv.org/abs/2504.14223)
Token length: 1819
Summarized using GPT-3.5-turbo
Append: [Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale](https://arxiv.org/abs/2504.14225)
Token length: 1213
Summarized using GPT-3.5-turbo
Append: [Probing the Subtle Ideological Manipulation of Large Language Models](https://arxiv.org/abs/2504.14287)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
Token length: 976
Summarized using GPT-3.5-turbo
Append: [Diverse Prompts: Illuminating the Prompt Space of Large Language Models with MAP-Elites](https://arxiv.org/abs/2504.14367)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail Knowledge](https://arxiv.org/abs/2504.14462)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment](https://arxiv.org/abs/2504.14468)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for Multi-Party Dialogue](https://arxiv.org/abs/2504.14482)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation Steering](https://arxiv.org/abs/2504.14492)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Functional Abstraction of Knowledge Recall in Large Language Models](https://arxiv.org/abs/2504.14496)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [Causality for Natural Language Processing](https://arxiv.org/abs/2504.14530)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [BookWorld: From Novels to Interactive Agent Societies for Creative Story Generation](https://arxiv.org/abs/2504.14538)
Token length: 1612
Summarized using GPT-3.5-turbo
Append: [a1: Steep Test-time Scaling Law via Environment Augmented Generation](https://arxiv.org/abs/2504.14597)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Translation Analytics for Freelancers: I. Introduction, Data Preparation, Baseline Evaluations](https://arxiv.org/abs/2504.14619)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [A Hierarchical Framework for Measuring Scientific Paper Innovation via Large Language Models](https://arxiv.org/abs/2504.14620)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Automatic Text Summarization (ATS) for Research Documents in Sorani Kurdish](https://arxiv.org/abs/2504.14630)
Token length: 1753
Summarized using GPT-3.5-turbo
Append: [Harnessing Generative LLMs for Enhanced Financial Event Entity Extraction Performance](https://arxiv.org/abs/2504.14633)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large language models](https://arxiv.org/abs/2504.14690)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding](https://arxiv.org/abs/2504.14692)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch Daily Narratives](https://arxiv.org/abs/2504.14707)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom Production Large Language Model Pipelines](https://arxiv.org/abs/2504.14738)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [Disentangling Linguistic Features with Dimension-Wise Analysis of Vector Embeddings](https://arxiv.org/abs/2504.14766)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions](https://arxiv.org/abs/2504.14772)
Token length: 1764
Summarized using GPT-3.5-turbo
Append: [Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends](https://arxiv.org/abs/2504.14804)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [On Self-improving Token Embeddings](https://arxiv.org/abs/2504.14808)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Transparentize the Internal and External Knowledge Utilization in LLMs with Trustworthy Citation](https://arxiv.org/abs/2504.14856)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Natural Fingerprints of Large Language Models](https://arxiv.org/abs/2504.14871)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2504.14891)
Token length: 1892
Summarized using GPT-3.5-turbo
Append: [CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs](https://arxiv.org/abs/2504.14905)
Token length: 1074
Summarized using GPT-3.5-turbo
Append: [Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in Multiparty Dialogues](https://arxiv.org/abs/2504.14963)
Token length: 497
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs on Chinese Topic Constructions: A Research Proposal Inspired by Tian et al. (2024)](https://arxiv.org/abs/2504.14969)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Efficient Pretraining Length Scaling](https://arxiv.org/abs/2504.14992)
Append: [Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation with LLMs](https://arxiv.org/abs/2504.15013)
Append: [LLMs as Data Annotators: How Close Are We to Human Performance](https://arxiv.org/abs/2504.15022)
Append: [DistilQwen2.5: Industrial Practices of Training Distilled Open Lightweight Language Models](https://arxiv.org/abs/2504.15027)
Append: [RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary Quality-Diversity Search](https://arxiv.org/abs/2504.15047)
Append: [Testing LLMs' Capabilities in Annotating Translations Based on an Error Typology Designed for LSP Translation: First Experiments with ChatGPT](https://arxiv.org/abs/2504.15052)
Append: [Rethinking the Potential of Multimodality in Collaborative Problem Solving Diagnosis with Large Language Models](https://arxiv.org/abs/2504.15093)
Append: [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)
Append: [EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://arxiv.org/abs/2504.15133)
Append: [The Synthetic Imputation Approach: Generating Optimal Synthetic Texts For Underrepresented Categories In Supervised Classification Tasks](https://arxiv.org/abs/2504.15160)
Append: [On true empty category](https://arxiv.org/abs/2504.15168)
Append: [Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus LLM Judges](https://arxiv.org/abs/2504.15205)
Append: [EvalAgent: Discovering Implicit Evaluation Criteria from the Web](https://arxiv.org/abs/2504.15219)
Append: [Fully Bayesian Approaches to Topics over Time](https://arxiv.org/abs/2504.15220)
Append: [Values in the Wild: Discovering and Analyzing Values in Real-World Language Model Interactions](https://arxiv.org/abs/2504.15236)
Append: [MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning](https://arxiv.org/abs/2504.15241)
Append: [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
Append: [Interview AI-ssistant: Designing for Real-Time Human-AI Collaboration in Interview Preparation and Execution](https://arxiv.org/abs/2504.13847)
Append: [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
Append: [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
Append: [Toward Automated Qualitative Analysis: Leveraging Large Language Models for Tutoring Dialogue Evaluation](https://arxiv.org/abs/2504.13882)
Append: [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)
Append: [Kanji Workbook: A Writing-Based Intelligent Tutoring System for Learning Proper Japanese Kanji Writing Technique with Instructor-Emulated Assessment](https://arxiv.org/abs/2504.13888)
Append: [Measuring Mental Health Variables in Computational Research: Toward Validated, Dimensional, and Transdiagnostic Approaches](https://arxiv.org/abs/2504.13890)
Append: [TALLMesh: a simple application for performing Thematic Analysis with Large Language Models](https://arxiv.org/abs/2504.13892)
Append: [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
Append: [Evaluation and Incident Prevention in an Enterprise AI Assistant](https://arxiv.org/abs/2504.13924)
Append: [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
Append: [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
Append: [ToolRL: Reward is All Tool Learning Needs](https://arxiv.org/abs/2504.13958)
Append: [AI Safety Should Prioritize the Future of Work](https://arxiv.org/abs/2504.13959)
Append: [One Jump Is All You Need: Short-Cutting Transformers for Early Exit Prediction with One Jump to Fit All Exit Levels](https://arxiv.org/abs/2504.13984)
Append: [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
Append: [Sentiment Analysis of Airbnb Reviews: Exploring Their Impact on Acceptance Rates and Pricing Across Multiple U.S. Regions](https://arxiv.org/abs/2504.14053)
Append: [Linking forward-pass dynamics in Transformers and real-time human processing](https://arxiv.org/abs/2504.14107)
Append: [System of Agentic AI for the Discovery of Metal-Organic Frameworks](https://arxiv.org/abs/2504.14110)
Append: [Bayesian Principles Improve Prompt Learning In Vision-Language Models](https://arxiv.org/abs/2504.14123)
Append: [Large Language Model Enhanced Particle Swarm Optimization for Hyperparameter Tuning for Deep Learning Models](https://arxiv.org/abs/2504.14126)
Append: [TALES: Text Adventure Learning Environment Suite](https://arxiv.org/abs/2504.14128)
Append: [HF4Rec: Human-Like Feedback-Driven Optimization Framework for Explainable Recommendation](https://arxiv.org/abs/2504.14147)
Append: [Direct Advantage Regression: Aligning LLMs with Online AI Reward](https://arxiv.org/abs/2504.14177)
Append: [The First VoicePrivacy Attacker Challenge](https://arxiv.org/abs/2504.14183)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
Append: [Assessing AI-Generated Questions' Alignment with Cognitive Frameworks in Educational Assessment](https://arxiv.org/abs/2504.14232)
Append: [InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners](https://arxiv.org/abs/2504.14239)
Append: [Towards Explainable Fake Image Detection with Multi-Modal Large Language Models](https://arxiv.org/abs/2504.14245)
Append: [Cross-attention for State-based model RWKV-7](https://arxiv.org/abs/2504.14260)
Append: [A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling](https://arxiv.org/abs/2504.14359)
Append: [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
Append: [Improving RL Exploration for LLM Reasoning through Retrospective Replay](https://arxiv.org/abs/2504.14363)
Append: [Density Measures for Language Generation](https://arxiv.org/abs/2504.14370)
Append: [LoRe: Personalizing LLMs via Low-Rank Reward Modeling](https://arxiv.org/abs/2504.14439)
Append: [Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey](https://arxiv.org/abs/2504.14520)
Append: [Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding](https://arxiv.org/abs/2504.14526)
Append: [HealthGenie: Empowering Users with Healthy Dietary Guidance through Knowledge Graph and Large Language Models](https://arxiv.org/abs/2504.14594)
Append: [Risk Assessment Framework for Code LLMs via Leveraging Internal States](https://arxiv.org/abs/2504.14640)
Append: [LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient Training of Code LLMs](https://arxiv.org/abs/2504.14655)
Append: [PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning Capabilities](https://arxiv.org/abs/2504.14773)
Append: [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
Append: [AlignRAG: An Adaptable Framework for Resolving Misalignments in Retrieval-Aware Reasoning of RAG](https://arxiv.org/abs/2504.14858)
Append: [OTC: Optimal Tool Calls via Reinforcement Learning](https://arxiv.org/abs/2504.14870)
Append: [VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform](https://arxiv.org/abs/2504.14904)
Append: [EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent Dialogue Framework](https://arxiv.org/abs/2504.14928)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation with Large Language Models](https://arxiv.org/abs/2504.15068)
Append: [Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation Analysis](https://arxiv.org/abs/2504.15072)
Append: [KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking](https://arxiv.org/abs/2504.15135)
Append: [CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation](https://arxiv.org/abs/2504.15254)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [An LMM for Efficient Video Understanding via Reinforced Compression of Video Cubes](https://arxiv.org/abs/2504.15270)
Append: [Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs](https://arxiv.org/abs/2504.15280)
Append: [Persian Abstract Meaning Representation: Annotation Guidelines and Gold Standard Dataset](https://arxiv.org/abs/2205.07712)
Append: [GLoRE: Evaluating Logical Reasoning of Large Language Models](https://arxiv.org/abs/2310.09107)
Append: [LongStory: Coherent, Complete and Length Controlled Long story Generation](https://arxiv.org/abs/2311.15208)
Append: [Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models](https://arxiv.org/abs/2403.10258)
Append: [Aligning Language Models with Demonstrated Feedback](https://arxiv.org/abs/2406.00888)
Append: [Inverse Constitutional AI: Compressing Preferences into Principles](https://arxiv.org/abs/2406.06560)
Append: [Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets and Languages for Open Named Entity Recognition](https://arxiv.org/abs/2406.11192)
Append: [Temporal Knowledge Graph Question Answering: A Survey](https://arxiv.org/abs/2406.14191)
Append: [LiveBench: A Challenging, Contamination-Limited LLM Benchmark](https://arxiv.org/abs/2406.19314)
Append: [Training on the Test Task Confounds Evaluation and Emergence](https://arxiv.org/abs/2407.07890)
Append: [IFShip: Interpretable Fine-grained Ship Classification with Domain Knowledge-Enhanced Vision-Language Models](https://arxiv.org/abs/2408.06631)
Append: [DualKanbaFormer: An Efficient Selective Sparse Framework for Multimodal Aspect-based Sentiment Analysis](https://arxiv.org/abs/2408.15379)
Append: [Self-evolving Agents with reflective and memory-augmented abilities](https://arxiv.org/abs/2409.00872)
Append: [Task-Specific Directions: Definition, Exploration, and Utilization in Parameter Efficient Fine-Tuning](https://arxiv.org/abs/2409.01035)
Append: [Seek and Solve Reasoning for Table Question Answering](https://arxiv.org/abs/2409.05286)
Append: [Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval](https://arxiv.org/abs/2410.04585)
Append: [Detecting Training Data of Large Language Models via Expectation Maximization](https://arxiv.org/abs/2410.07582)
Append: [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/abs/2410.09300)
Append: [Adapting Multilingual LLMs to Low-Resource Languages using Continued Pre-training and Synthetic Corpus](https://arxiv.org/abs/2410.14815)
Append: [Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment](https://arxiv.org/abs/2410.16714)
Append: [Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine Translation](https://arxiv.org/abs/2410.20941)
Append: [ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building Large Language Model-Based Conversational AI Agents](https://arxiv.org/abs/2411.00927)
Append: [CHATTER: A Character Attribution Dataset for Narrative Understanding](https://arxiv.org/abs/2411.05227)
Append: [FactLens: Benchmarking Fine-Grained Fact Verification](https://arxiv.org/abs/2411.05980)
Append: [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)
Append: [WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation](https://arxiv.org/abs/2412.01626)
Append: [Unanswerability Evaluation for Retrieval Augmented Generation](https://arxiv.org/abs/2412.12300)
Append: [State Space Models are Strong Text Rerankers](https://arxiv.org/abs/2412.14354)
Append: [LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation](https://arxiv.org/abs/2501.05414)
Append: [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)
Append: [Idiom Detection in Sorani Kurdish Texts](https://arxiv.org/abs/2501.14528)
Append: [Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet](https://arxiv.org/abs/2502.05291)
Append: [BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models](https://arxiv.org/abs/2502.07346)
Append: [SparQLe: Speech Queries to Text Translation Through LLMs](https://arxiv.org/abs/2502.09284)
Append: [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)
Append: [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](https://arxiv.org/abs/2502.14866)
Append: [Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA Dataset with Answer Plausibility Scores](https://arxiv.org/abs/2502.16358)
Append: [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/abs/2502.18036)
Append: [Semantic Wave Functions: Exploring Meaning in Large Language Models Through Quantum Formalism](https://arxiv.org/abs/2503.10664)
Append: [Halving transcription time: A fast, user-friendly and GDPR-compliant workflow to create AI-assisted transcripts for content analysis](https://arxiv.org/abs/2503.13031)
Append: [A Language Anchor-Guided Method for Robust Noisy Domain Generalization](https://arxiv.org/abs/2503.17211)
Append: [LLM Agents That Act Like Us: Accurate Human Behavior Simulation with Real-World Data](https://arxiv.org/abs/2503.20749)
Append: [ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging](https://arxiv.org/abs/2503.21088)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data Selection](https://arxiv.org/abs/2504.00695)
Append: [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)
Append: [Extending the SAREF4ENER Ontology with Flexibility Based on FlexOffers](https://arxiv.org/abs/2504.03595)
Append: [NAACL2025 Tutorial: Adaptation of Large Language Models](https://arxiv.org/abs/2504.03931)
Append: [Following the Whispers of Values: Unraveling Neural Mechanisms Behind Value-Oriented Behaviors in LLMs](https://arxiv.org/abs/2504.04994)
Append: [Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games](https://arxiv.org/abs/2504.06868)
Append: [AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation](https://arxiv.org/abs/2504.07532)
Append: [UXAgent: A System for Simulating Usability Testing of Web Design with LLM Agents](https://arxiv.org/abs/2504.09407)
Append: [Forecasting from Clinical Textual Time Series: Adaptations of the Encoder and Decoder Language Model Families](https://arxiv.org/abs/2504.10340)
Append: [Characterizing Knowledge Manipulation in a Russian Wikipedia Fork](https://arxiv.org/abs/2504.10663)
Append: [Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge](https://arxiv.org/abs/2312.05693)
Append: [Embedding Ontologies via Incorporating Extensional and Intensional Knowledge](https://arxiv.org/abs/2402.01677)
Append: [SLMRec: Distilling Large Language Models into Small for Sequential Recommendation](https://arxiv.org/abs/2405.17890)
Append: [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)
Append: [Jailbreaking as a Reward Misspecification Problem](https://arxiv.org/abs/2406.14393)
Append: [OpenHands: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/abs/2407.16741)
Append: [MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders](https://arxiv.org/abs/2409.06635)
Append: [DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models](https://arxiv.org/abs/2410.09344)
Append: [Context-Parametric Inversion: Why Instruction Finetuning Can Worsen Context Reliance](https://arxiv.org/abs/2410.10796)
Append: [Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment](https://arxiv.org/abs/2410.14148)
Append: [Aioli: A Unified Optimization Framework for Language Model Data Mixing](https://arxiv.org/abs/2411.05735)
Append: [Unraveling Arithmetic in Large Language Models: The Role of Algebraic Structures](https://arxiv.org/abs/2411.16260)
Append: [Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems](https://arxiv.org/abs/2503.21074)
Append: [Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R) Strategy for Better Expert Parallelism Design](https://arxiv.org/abs/2504.01337)
Append: [Self-Resource Allocation in Multi-Agent LLM Systems](https://arxiv.org/abs/2504.02051)
Append: [Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood Modeling](https://arxiv.org/abs/2504.05216)
Append: [Fine-tuning a Large Language Model for Automating Computational Fluid Dynamics Simulations](https://arxiv.org/abs/2504.09602)
Append: [EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety](https://arxiv.org/abs/2504.09689)
append_entries: 192
Finish: 2025-04-22 04:25:33.740083
------------------------------------------------------
Started: 2025-04-22 06:23:11.701646
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 06:23:12.073259
------------------------------------------------------
Started: 2025-04-22 08:21:26.118165
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 08:21:26.477918
------------------------------------------------------
Started: 2025-04-22 10:17:12.338506
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 10:17:12.670682
------------------------------------------------------
Started: 2025-04-22 12:31:55.759357
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 12:31:56.094493
------------------------------------------------------
Started: 2025-04-22 14:16:20.423052
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 14:16:20.782122
------------------------------------------------------
Started: 2025-04-22 16:19:48.170928
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 16:19:48.517521
------------------------------------------------------
Started: 2025-04-22 18:21:57.037809
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 18:21:57.372659
------------------------------------------------------
Started: 2025-04-22 20:17:54.394184
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 20:17:54.717448
------------------------------------------------------
Started: 2025-04-22 22:15:20.731987
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-22 22:15:21.064344
------------------------------------------------------
Started: 2025-04-23 01:16:01.524302
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 01:16:01.926845
------------------------------------------------------
Started: 2025-04-23 03:01:55.044807
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 03:01:55.364725
------------------------------------------------------
Started: 2025-04-23 04:23:47.884309
Existing_entries: 512
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Exploring Compositional Generalization (in ReCOGS_pos) by Transformers using Restricted Access Sequence Processing (RASP)](https://arxiv.org/abs/2504.15349)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Tell Me What You Know About Sexism: Expert-LLM Interaction Strategies and Co-Created Definitions for Zero-Shot Sexism Detection](https://arxiv.org/abs/2504.15392)
Token length: 777
Summarized using GPT-3.5-turbo
Append: [Trillion 7B Technical Report](https://arxiv.org/abs/2504.15431)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Feeding LLM Annotations to BERT Classifiers at Your Own Risk](https://arxiv.org/abs/2504.15432)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)
Token length: 770
Summarized using GPT-3.5-turbo
Append: [Speculative Sampling via Exponential Races](https://arxiv.org/abs/2504.15475)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation](https://arxiv.org/abs/2504.15509)
Token length: 1874
Summarized using GPT-3.5-turbo
Append: [The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks](https://arxiv.org/abs/2504.15521)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [IPBench: Benchmarking the Knowledge of Large Language Models in Intellectual Property](https://arxiv.org/abs/2504.15524)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Compass-V2 Technical Report](https://arxiv.org/abs/2504.15527)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length](https://arxiv.org/abs/2504.15544)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [LLM-based Semantic Augmentation for Harmful Content Detection](https://arxiv.org/abs/2504.15548)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [Exploring Next Token Prediction in Theory of Mind (ToM) Tasks: Comparative Experiments with GPT-2 and LLaMA-2 AI Models](https://arxiv.org/abs/2504.15604)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [Exploiting Contextual Knowledge in LLMs through V-usable Information based Layer Enhancement](https://arxiv.org/abs/2504.15630)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Cost-Effective Text Clustering with Large Language Models](https://arxiv.org/abs/2504.15640)
Token length: 744
Summarized using GPT-3.5-turbo
Append: [Computational Typology](https://arxiv.org/abs/2504.15642)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [FinTextSim: Enhancing Financial Text Analysis with BERTopic](https://arxiv.org/abs/2504.15683)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [Subject islands do not reduce to construction-specific discourse function](https://arxiv.org/abs/2504.15688)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Tina: Tiny Reasoning Models via LoRA](https://arxiv.org/abs/2504.15777)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Automated Creativity Evaluation for Large Language Models: A Reference-Based Approach](https://arxiv.org/abs/2504.15784)
Token length: 1800
Summarized using GPT-3.5-turbo
Append: [A closer look at how large language models trust humans: patterns and biases](https://arxiv.org/abs/2504.15801)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)
Token length: 1828
Summarized using GPT-3.5-turbo
Append: [Exploring Cognitive and Aesthetic Causality for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2504.15848)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity](https://arxiv.org/abs/2504.15941)
Token length: 1635
Summarized using GPT-3.5-turbo
Append: [W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models](https://arxiv.org/abs/2504.15983)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Few-shot Hate Speech Detection Based on the MindSpore Framework](https://arxiv.org/abs/2504.15987)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
Token length: 682
Summarized using GPT-3.5-turbo
Append: [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
Token length: 1441
Summarized using GPT-3.5-turbo
Append: [Certified Mitigation of Worst-Case LLM Copyright Infringement](https://arxiv.org/abs/2504.16046)
Token length: 1838
Summarized using GPT-3.5-turbo
Append: [LongMamba: Enhancing Mamba's Long Context Capabilities via Training-Free Receptive Field Enlargement](https://arxiv.org/abs/2504.16053)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [Honey, I Shrunk the Language Model: Impact of Knowledge Distillation Methods on Performance and Explainability](https://arxiv.org/abs/2504.16056)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)
Token length: 1692
Summarized using GPT-3.5-turbo
Append: [A Python Tool for Reconstructing Full News Text from GDELT](https://arxiv.org/abs/2504.16063)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation](https://arxiv.org/abs/2504.16073)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Med-CoDE: Medical Critique based Disagreement Evaluation Framework](https://arxiv.org/abs/2504.15330)
Token length: 1585
Summarized using GPT-3.5-turbo
Append: [LongPerceptualThoughts: Distilling System-2 Reasoning for System-1 Perception](https://arxiv.org/abs/2504.15362)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Towards Understanding Camera Motions in Any Video](https://arxiv.org/abs/2504.15376)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs](https://arxiv.org/abs/2504.15415)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [Real-Time Sentiment Insights from X Using VADER, DistilBERT, and Web-Scraped Data](https://arxiv.org/abs/2504.15448)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [Learning Adaptive Parallel Reasoning with Language Models](https://arxiv.org/abs/2504.15466)
Token length: 1846
Summarized using GPT-3.5-turbo
Append: [CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting](https://arxiv.org/abs/2504.15485)
Token length: 1949
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [VeriCoder: Enhancing LLM-Based RTL Code Generation through Functional Correctness Validation](https://arxiv.org/abs/2504.15659)
Append: [TrustGeoGen: Scalable and Formal-Verified Data Engine for Trustworthy Multi-modal Geometric Problem Solving](https://arxiv.org/abs/2504.15780)
Append: [How Private is Your Attention? Bridging Privacy with In-Context Learning](https://arxiv.org/abs/2504.16000)
Append: [Survey of Video Diffusion Models: Foundations, Implementations, and Applications](https://arxiv.org/abs/2504.16081)
Append: [Aggregating Soft Labels from Crowd Annotations Improves Uncertainty Estimation Under Distribution Shift](https://arxiv.org/abs/2212.09409)
Append: [On the Low-Rank Parametrization of Reward Models for Controlled Language Generation](https://arxiv.org/abs/2407.04615)
Append: [Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation](https://arxiv.org/abs/2408.06276)
Append: [Open-World Evaluation for Retrieving Diverse Perspectives](https://arxiv.org/abs/2409.18110)
Append: [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://arxiv.org/abs/2410.02355)
Append: [SWITCH: Studying with Teacher for Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2410.19503)
Append: [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)
Append: [Falcon: Faster and Parallel Inference of Large Language Models through Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree](https://arxiv.org/abs/2412.12639)
Append: [Fine-tuning Whisper on Low-Resource Languages for Real-World Applications](https://arxiv.org/abs/2412.15726)
Append: [Fearful Falcons and Angry Llamas: Emotion Category Annotations of Arguments by Humans and LLMs](https://arxiv.org/abs/2412.15993)
Append: [Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks](https://arxiv.org/abs/2502.13053)
Append: [Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review](https://arxiv.org/abs/2503.04797)
Append: [Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks](https://arxiv.org/abs/2503.09572)
Append: [Key, Value, Compress: A Systematic Exploration of KV Cache Compression Techniques](https://arxiv.org/abs/2503.11816)
Append: [FUSE : A Ridge and Random Forest-Based Metric for Evaluating MT in Indigenous Languages](https://arxiv.org/abs/2504.00021)
Append: [Investigating and Scaling up Code-Switching for Multilingual Language Model Pre-Training](https://arxiv.org/abs/2504.01801)
Append: [VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation](https://arxiv.org/abs/2504.04060)
Append: [Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance](https://arxiv.org/abs/2504.07989)
Append: [Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the Evaluation Protocol](https://arxiv.org/abs/2504.10284)
Append: [Certifying Knowledge Comprehension in LLMs](https://arxiv.org/abs/2402.15929)
Append: [Optimizing RLHF Training for Large Language Models with Stage Fusion](https://arxiv.org/abs/2409.13221)
Append: [Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct](https://arxiv.org/abs/2410.02064)
Append: [A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement](https://arxiv.org/abs/2410.13828)
Append: [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669)
Append: [Towards Unifying Evaluation of Counterfactual Explanations: Leveraging Large Language Models for Human-Centric Assessments](https://arxiv.org/abs/2410.21131)
Append: [Codenames as a Benchmark for Large Language Models](https://arxiv.org/abs/2412.11373)
Append: [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/abs/2502.19676)
Append: [AgentA/B: Automated and Scalable Web A/BTesting with Interactive LLM Agents](https://arxiv.org/abs/2504.09723)
append_entries: 81
Finish: 2025-04-23 04:25:49.528895
------------------------------------------------------
Started: 2025-04-23 06:23:05.510321
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 06:23:05.708699
------------------------------------------------------
Started: 2025-04-23 08:21:12.383997
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 08:21:12.583563
------------------------------------------------------
Started: 2025-04-23 10:18:03.041403
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 10:18:03.218827
------------------------------------------------------
Started: 2025-04-23 12:32:26.453256
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 12:32:26.631945
------------------------------------------------------
Started: 2025-04-23 14:16:58.304240
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 14:16:58.503814
------------------------------------------------------
Started: 2025-04-23 16:20:48.513946
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 16:20:48.687699
------------------------------------------------------
Started: 2025-04-23 18:23:17.886460
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 18:23:18.072062
------------------------------------------------------
Started: 2025-04-23 20:18:04.155370
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 20:18:04.350490
------------------------------------------------------
Started: 2025-04-23 22:15:40.121407
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-23 22:15:40.325700
------------------------------------------------------
Started: 2025-04-24 01:16:03.671826
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 01:16:03.917041
------------------------------------------------------
Started: 2025-04-24 03:03:43.859659
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 03:03:44.034993
------------------------------------------------------
Started: 2025-04-24 04:23:41.148106
Existing_entries: 593
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 912
Summarized using GPT-3.5-turbo
Append: [FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking](https://arxiv.org/abs/2504.16188)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy](https://arxiv.org/abs/2504.16271)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation](https://arxiv.org/abs/2504.16286)
Token length: 806
Summarized using GPT-3.5-turbo
Append: [Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives](https://arxiv.org/abs/2504.16312)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Transformer-Based Extraction of Statutory Definitions from the U.S. Code](https://arxiv.org/abs/2504.16353)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions](https://arxiv.org/abs/2504.16358)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [SplitReason: Learning To Offload Reasoning](https://arxiv.org/abs/2504.16379)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Out-of-the-Box Conditional Text Embeddings from Large Language Models](https://arxiv.org/abs/2504.16411)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study](https://arxiv.org/abs/2504.16414)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark](https://arxiv.org/abs/2504.16427)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records](https://arxiv.org/abs/2504.16448)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning](https://arxiv.org/abs/2504.16460)
Token length: 1674
Summarized using GPT-3.5-turbo
Append: [QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining](https://arxiv.org/abs/2504.16511)
Token length: 1338
Summarized using GPT-3.5-turbo
Append: [Transformers for Complex Query Answering over Knowledge Hypergraphs](https://arxiv.org/abs/2504.16537)
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression](https://arxiv.org/abs/2504.16574)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study](https://arxiv.org/abs/2504.16601)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories](https://arxiv.org/abs/2504.16604)
Token length: 734
Summarized using GPT-3.5-turbo
Append: [TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval](https://arxiv.org/abs/2504.16627)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics](https://arxiv.org/abs/2504.16677)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations](https://arxiv.org/abs/2504.16754)
Token length: 1521
Summarized using GPT-3.5-turbo
Append: [How Effective are Generative Large Language Models in Performing Requirements Classification?](https://arxiv.org/abs/2504.16768)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [Evaluation Framework for AI Systems in "the Wild"](https://arxiv.org/abs/2504.16778)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores](https://arxiv.org/abs/2504.16786)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Credible plan-driven RAG method for Multi-hop Question Answering](https://arxiv.org/abs/2504.16787)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention](https://arxiv.org/abs/2504.16795)
Token length: 748
Summarized using GPT-3.5-turbo
Append: [LLM-assisted Graph-RAG Information Extraction from IFC Data](https://arxiv.org/abs/2504.16813)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning](https://arxiv.org/abs/2504.16832)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Monte Carlo Planning with Large Language Model for Text-Based Game Agents](https://arxiv.org/abs/2504.16855)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification](https://arxiv.org/abs/2504.16856)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Planning with Diffusion Models for Target-Oriented Dialogue Systems](https://arxiv.org/abs/2504.16858)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models know who did what to whom?](https://arxiv.org/abs/2504.16884)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text](https://arxiv.org/abs/2504.16913)
Token length: 1637
Summarized using GPT-3.5-turbo
Append: [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [IberBench: LLM Evaluation on Iberian Languages](https://arxiv.org/abs/2504.16921)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Cooperative Speech, Semantic Competence, and AI](https://arxiv.org/abs/2504.16092)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing](https://arxiv.org/abs/2504.16112)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval](https://arxiv.org/abs/2504.16121)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends](https://arxiv.org/abs/2504.16134)
Token length: 1921
Summarized using GPT-3.5-turbo
Append: [Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design](https://arxiv.org/abs/2504.16204)
Token length: 757
Summarized using GPT-3.5-turbo
Append: [Using Phonemes in cascaded S2S translation pipeline](https://arxiv.org/abs/2504.16234)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents](https://arxiv.org/abs/2504.16264)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [SignX: The Foundation Model for Sign Recognition](https://arxiv.org/abs/2504.16315)
Token length: 687
Summarized using GPT-3.5-turbo
Append: [MAGIC: Near-Optimal Data Attribution for Deep Learning](https://arxiv.org/abs/2504.16430)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data](https://arxiv.org/abs/2504.16628)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
Token length: 1514
Summarized using GPT-3.5-turbo
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset](https://arxiv.org/abs/2504.16891)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion](https://arxiv.org/abs/2111.15473)
Append: [Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge](https://arxiv.org/abs/2307.08813)
Append: [A dataset and benchmark for hospital course summarization with adapted large language models](https://arxiv.org/abs/2403.05720)
Append: [NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens](https://arxiv.org/abs/2403.12766)
Append: [Large Language Model Sentinel: LLM Agent for Adversarial Purification](https://arxiv.org/abs/2405.20770)
Append: [Synthetic Lyrics Detection Across Languages and Genres](https://arxiv.org/abs/2406.15231)
Append: [SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy](https://arxiv.org/abs/2407.03004)
Append: [ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation](https://arxiv.org/abs/2407.12022)
Append: [Lawma: The Power of Specialization for Legal Annotation](https://arxiv.org/abs/2407.16615)
Append: [Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models](https://arxiv.org/abs/2407.17914)
Append: [The advantages of context specific language models: the case of the Erasmian Language Model](https://arxiv.org/abs/2408.06931)
Append: [lamss: when large language models meet self-skepticism](https://arxiv.org/abs/2409.06601)
Append: [ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems](https://arxiv.org/abs/2410.19572)
Append: [MEG: Medical Knowledge-Augmented Large Language Models for Question Answering](https://arxiv.org/abs/2411.03883)
Append: [Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?](https://arxiv.org/abs/2411.05000)
Append: [Sufficient Context: A New Lens on Retrieval Augmented Generation Systems](https://arxiv.org/abs/2411.06037)
Append: [7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/abs/2412.06845)
Append: [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)
Append: [Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?](https://arxiv.org/abs/2502.04718)
Append: [Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization](https://arxiv.org/abs/2502.13108)
Append: [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](https://arxiv.org/abs/2503.01840)
Append: [Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations](https://arxiv.org/abs/2503.14477)
Append: [Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models](https://arxiv.org/abs/2503.16419)
Append: [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](https://arxiv.org/abs/2503.20533)
Append: [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)
Append: [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2404.04545)
Append: [Multimodal Situational Safety](https://arxiv.org/abs/2410.06172)
Append: [Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers](https://arxiv.org/abs/2410.22663)
Append: [Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning](https://arxiv.org/abs/2411.18203)
Append: [StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist](https://arxiv.org/abs/2412.06412)
Append: [MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants](https://arxiv.org/abs/2412.12661)
Append: [Lorecast: Layout-Aware Performance and Power Forecasting from Natural Language](https://arxiv.org/abs/2503.11662)
Append: [Dynamic hashtag recommendation in social media with trend shift detection and adaptation](https://arxiv.org/abs/2504.00044)
Append: [Analyzing 16,193 LLM Papers for Fun and Profits](https://arxiv.org/abs/2504.08619)
append_entries: 83
Finish: 2025-04-24 04:25:54.807250
------------------------------------------------------
Started: 2025-04-24 06:23:36.982518
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 06:23:37.215010
------------------------------------------------------
Started: 2025-04-24 08:21:31.629261
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 08:21:31.818649
------------------------------------------------------
Started: 2025-04-24 10:17:44.427132
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 10:17:44.644702
------------------------------------------------------
Started: 2025-04-24 12:33:41.557085
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 12:33:41.775548
------------------------------------------------------
Started: 2025-04-24 14:15:17.148893
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 14:15:17.336330
------------------------------------------------------
Started: 2025-04-24 16:20:00.365624
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 16:20:00.579474
------------------------------------------------------
Started: 2025-04-24 18:21:39.126113
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 18:21:39.315127
------------------------------------------------------
Started: 2025-04-24 20:18:03.965115
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 20:18:04.153653
------------------------------------------------------
Started: 2025-04-24 22:15:20.157299
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-24 22:15:20.375474
------------------------------------------------------
Started: 2025-04-25 01:16:38.936972
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 01:16:39.121732
------------------------------------------------------
Started: 2025-04-25 03:03:50.691430
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 03:03:50.878160
------------------------------------------------------
Started: 2025-04-25 04:21:30.354646
Existing_entries: 676
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1362
Summarized using GPT-3.5-turbo
Append: [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)
Token length: 1375
Summarized using GPT-3.5-turbo
Append: [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)
Token length: 1408
Summarized using GPT-3.5-turbo
Append: [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)
Token length: 1636
Summarized using GPT-3.5-turbo
Append: [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)
Token length: 1883
Summarized using GPT-3.5-turbo
Append: [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)
Token length: 1101
Summarized using GPT-3.5-turbo
Append: [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)
Token length: 992
Summarized using GPT-3.5-turbo
Append: [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)
Token length: 1584
Summarized using GPT-3.5-turbo
Append: [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)
Token length: 1656
Summarized using GPT-3.5-turbo
Append: [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)
Token length: 836
Summarized using GPT-3.5-turbo
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Token length: 1053
Summarized using GPT-3.5-turbo
Append: [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)
Token length: 1702
Summarized using GPT-3.5-turbo
Append: [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions](https://arxiv.org/abs/2504.16939)
Token length: 1890
Summarized using GPT-3.5-turbo
Append: [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
Token length: 874
Summarized using GPT-3.5-turbo
Append: [SCALAR: A Part-of-speech Tagger for Identifiers](https://arxiv.org/abs/2504.17038)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation](https://arxiv.org/abs/2504.17365)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [HMI: Hierarchical Knowledge Management for Efficient Multi-Tenant Inference in Pretrained Language Models](https://arxiv.org/abs/2504.17449)
Token length: 1960
Summarized using GPT-3.5-turbo
Append: [CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization](https://arxiv.org/abs/2406.07494)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [OPT-Tree: Speculative Decoding with Adaptive Draft Tree Structure](https://arxiv.org/abs/2406.17276)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse](https://arxiv.org/abs/2409.11242)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Lab-AI: Using Retrieval Augmentation to Enhance Language Models for Personalized Lab Test Interpretation in Clinical Medicine](https://arxiv.org/abs/2409.18986)
Token length: 1539
Summarized using GPT-3.5-turbo
Append: [Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?](https://arxiv.org/abs/2409.19151)
Token length: 1504
Summarized using GPT-3.5-turbo
Append: [TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking](https://arxiv.org/abs/2410.01952)
Append: [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703)
Append: [Post-hoc Study of Climate Microtargeting on Social Media Ads with LLMs: Thematic Insights and Fairness Evaluation](https://arxiv.org/abs/2410.05401)
Append: [Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies](https://arxiv.org/abs/2410.19878)
Append: [Estimating the Influence of Sequentially Correlated Literary Properties in Textual Classification: A Data-Centric Hypothesis-Testing Approach](https://arxiv.org/abs/2411.04950)
Append: [jina-clip-v2: Multilingual Multimodal Embeddings for Text and Images](https://arxiv.org/abs/2412.08802)
Append: [Context-Aware Neural Gradient Mapping for Fine-Grained Instruction Processing](https://arxiv.org/abs/2501.14936)
Append: [Multilingual State Space Models for Structured Question Answering in Indic Languages](https://arxiv.org/abs/2502.01673)
Append: [Probabilistic Subspace Manifolds for Contextual Inference in Large Language Models](https://arxiv.org/abs/2502.05346)
Append: [Towards Reasoning Ability of Small Language Models](https://arxiv.org/abs/2502.11569)
Append: [PSCon: Product Search Through Conversations](https://arxiv.org/abs/2502.13881)
Append: [Automatically Evaluating the Paper Reviewing Capability of Large Language Models](https://arxiv.org/abs/2502.17086)
Append: [SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection](https://arxiv.org/abs/2503.07269)
Append: [HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks](https://arxiv.org/abs/2503.10894)
Append: [Shared Global and Local Geometry of Language Model Embeddings](https://arxiv.org/abs/2503.21073)
Append: [Cognitive Memory in Large Language Models](https://arxiv.org/abs/2504.02441)
Append: [Not All Data Are Unlearned Equally](https://arxiv.org/abs/2504.05058)
Append: [Multilingual MFA: Forced Alignment on Low-Resource Related Languages](https://arxiv.org/abs/2504.07315)
Append: [Transferable text data distillation by trajectory matching](https://arxiv.org/abs/2504.09818)
Append: [ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation](https://arxiv.org/abs/2406.14088)
Append: [Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF](https://arxiv.org/abs/2410.04612)
Append: [CallNavi, A Challenge and Empirical Study on LLM Function Calling and Routing](https://arxiv.org/abs/2501.05255)
Append: [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
Append: [Keyframe-oriented Vision Token Pruning: Enhancing Efficiency of Large Vision Language Models on Long-Form Video Processing](https://arxiv.org/abs/2503.10742)
Append: [Looking beyond the next token](https://arxiv.org/abs/2504.11336)
Append: [Teaching Large Language Models to Reason through Learning and Forgetting](https://arxiv.org/abs/2504.11364)
append_entries: 75
Finish: 2025-04-25 04:23:26.404476
------------------------------------------------------
Started: 2025-04-25 06:22:52.147618
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 06:22:52.370816
------------------------------------------------------
Started: 2025-04-25 08:21:37.045192
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 08:21:37.296065
------------------------------------------------------
Started: 2025-04-25 10:17:06.515090
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 10:17:06.699985
------------------------------------------------------
Started: 2025-04-25 12:32:33.596439
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 12:32:33.777022
------------------------------------------------------
Started: 2025-04-25 14:15:10.371478
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 14:15:10.584149
------------------------------------------------------
Started: 2025-04-25 16:19:37.968500
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 16:19:38.156762
------------------------------------------------------
Started: 2025-04-25 18:21:48.634558
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 18:21:48.842632
------------------------------------------------------
Started: 2025-04-25 20:17:46.391295
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 20:17:46.637887
------------------------------------------------------
Started: 2025-04-25 22:15:39.484283
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-25 22:15:39.724851
------------------------------------------------------
Started: 2025-04-26 01:13:49.773938
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 01:13:49.988037
------------------------------------------------------
Started: 2025-04-26 02:57:28.791726
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 02:57:29.025375
------------------------------------------------------
Started: 2025-04-26 04:18:23.245536
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 04:18:23.302458
------------------------------------------------------
Started: 2025-04-26 06:20:46.878442
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 06:20:46.961783
------------------------------------------------------
Started: 2025-04-26 08:18:43.599009
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 08:18:43.670760
------------------------------------------------------
Started: 2025-04-26 10:15:17.019775
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 10:15:17.091715
------------------------------------------------------
Started: 2025-04-26 12:28:37.289104
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 12:28:37.359790
------------------------------------------------------
Started: 2025-04-26 14:13:16.409050
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 14:13:16.482023
------------------------------------------------------
Started: 2025-04-26 16:18:26.092728
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 16:18:26.166364
------------------------------------------------------
Started: 2025-04-26 18:19:44.152571
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 18:19:44.240748
------------------------------------------------------
Started: 2025-04-26 20:15:43.516778
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 20:15:43.577322
------------------------------------------------------
Started: 2025-04-26 22:14:03.104258
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-26 22:14:03.192620
------------------------------------------------------
Started: 2025-04-27 01:20:37.743123
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 01:20:37.796413
------------------------------------------------------
Started: 2025-04-27 03:06:29.498566
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 03:06:29.552011
------------------------------------------------------
Started: 2025-04-27 04:18:17.123855
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 04:18:17.182030
------------------------------------------------------
Started: 2025-04-27 06:20:58.862543
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 06:20:58.920000
------------------------------------------------------
Started: 2025-04-27 08:18:50.720727
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 08:18:50.817021
------------------------------------------------------
Started: 2025-04-27 10:16:12.208862
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 10:16:12.277405
------------------------------------------------------
Started: 2025-04-27 12:28:55.734421
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 12:28:55.816515
------------------------------------------------------
Started: 2025-04-27 14:13:24.908627
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 14:13:24.965676
------------------------------------------------------
Started: 2025-04-27 16:18:11.271664
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 16:18:11.329220
------------------------------------------------------
Started: 2025-04-27 18:19:52.750001
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 18:19:52.802594
------------------------------------------------------
Started: 2025-04-27 20:16:35.406210
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 20:16:35.458989
------------------------------------------------------
Started: 2025-04-27 22:14:45.710258
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-27 22:14:45.793573
------------------------------------------------------
Started: 2025-04-28 01:18:28.296301
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 01:18:28.377376
------------------------------------------------------
Started: 2025-04-28 03:08:03.661980
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 03:08:03.718214
------------------------------------------------------
Started: 2025-04-28 04:24:01.274055
Existing_entries: 751
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/abs/2504.17974)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Improving LLM Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
Token length: 1318
Summarized using GPT-3.5-turbo
Append: [Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation](https://arxiv.org/abs/2504.18012)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Exploring Personality-Aware Interactions in Salesperson Dialogue Agents](https://arxiv.org/abs/2504.18058)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization](https://arxiv.org/abs/2504.18080)
Token length: 1326
Summarized using GPT-3.5-turbo
Append: [Random-Set Large Language Models](https://arxiv.org/abs/2504.18085)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation](https://arxiv.org/abs/2504.18104)
Token length: 928
Summarized using GPT-3.5-turbo
Append: [Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering](https://arxiv.org/abs/2504.18106)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Temporal Entailment Pretraining for Clinical Language Models over EHR Data](https://arxiv.org/abs/2504.18128)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)](https://arxiv.org/abs/2504.18142)
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Aligning Language Models for Icelandic Legal Text Summarization](https://arxiv.org/abs/2504.18180)
Token length: 718
Summarized using GPT-3.5-turbo
Append: [Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish](https://arxiv.org/abs/2504.18221)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Efficient Single-Pass Training for Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [MAGI: Multi-Agent Guided Interview for Psychiatric Assessment](https://arxiv.org/abs/2504.18260)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation](https://arxiv.org/abs/2504.18269)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)
Token length: 1282
Summarized using GPT-3.5-turbo
Append: [Pushing the boundary on Natural Language Inference](https://arxiv.org/abs/2504.18376)
Token length: 829
Summarized using GPT-3.5-turbo
Append: [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412)
Token length: 900
Summarized using GPT-3.5-turbo
Append: [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
Token length: 1075
Summarized using GPT-3.5-turbo
Append: [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions](https://arxiv.org/abs/2504.18474)
Token length: 1182
Summarized using GPT-3.5-turbo
Append: [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)
Token length: 1516
Summarized using GPT-3.5-turbo
Append: [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
Token length: 1913
Summarized using GPT-3.5-turbo
Append: [Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval](https://arxiv.org/abs/2504.17884)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Token Sequence Compression for Efficient Multimodal Computing](https://arxiv.org/abs/2504.17892)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [CAMU: Context Augmentation for Meme Understanding](https://arxiv.org/abs/2504.17902)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning](https://arxiv.org/abs/2504.17950)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [SMARTFinRAG: Interactive Modularized Financial RAG Benchmark](https://arxiv.org/abs/2504.18024)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture](https://arxiv.org/abs/2504.18099)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections](https://arxiv.org/abs/2504.18333)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [Kimi-Audio Technical Report](https://arxiv.org/abs/2504.18425)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [PRobELM: Plausibility Ranking Evaluation for Language Models](https://arxiv.org/abs/2404.03818)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arxiv.org/abs/2405.19325)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction](https://arxiv.org/abs/2406.10432)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Multilingual Large Language Models and Curse of Multilinguality](https://arxiv.org/abs/2406.10602)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings](https://arxiv.org/abs/2408.16073)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Your Weak LLM is Secretly a Strong Teacher for Alignment](https://arxiv.org/abs/2409.08813)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning](https://arxiv.org/abs/2409.12059)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"](https://arxiv.org/abs/2410.03727)
Append: [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
Append: [ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data](https://arxiv.org/abs/2412.11704)
Append: [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)
Append: [EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2502.12486)
Append: [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)
Append: [LRAGE: Legal Retrieval Augmented Generation Evaluation Tool](https://arxiv.org/abs/2504.01840)
Append: [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810)
Append: [Can Reasoning LLMs Enhance Clinical Document Classification?](https://arxiv.org/abs/2504.08040)
Append: [Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio](https://arxiv.org/abs/1711.08058)
Append: [M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models](https://arxiv.org/abs/2405.15638)
Append: [Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition](https://arxiv.org/abs/2406.02566)
Append: [GOFA: A Generative One-For-All Model for Joint Graph Language Modeling](https://arxiv.org/abs/2407.09709)
Append: [Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs](https://arxiv.org/abs/2408.06621)
Append: [Adaptive Uncertainty Quantification for Generative AI](https://arxiv.org/abs/2408.08990)
Append: [MIND: Math Informed syNthetic Dialogues for Pretraining LLMs](https://arxiv.org/abs/2410.12881)
Append: [Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification](https://arxiv.org/abs/2411.01841)
Append: [Repurposing the scientific literature with vision-language models](https://arxiv.org/abs/2502.19546)
Append: [Spatial Audio Processing with Large Language Model on Wearable Devices](https://arxiv.org/abs/2504.08907)
append_entries: 68
Finish: 2025-04-28 04:25:53.968059
------------------------------------------------------
Started: 2025-04-28 06:30:42.394218
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 06:30:42.588139
------------------------------------------------------
Started: 2025-04-28 08:57:44.055561
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 08:57:44.246486
------------------------------------------------------
Started: 2025-04-28 11:03:55.719388
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 11:03:55.907081
------------------------------------------------------
Started: 2025-04-28 12:33:56.928650
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 12:33:57.121104
------------------------------------------------------
Started: 2025-04-28 14:17:38.964210
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 14:17:39.207577
------------------------------------------------------
Started: 2025-04-28 16:19:30.632004
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 16:19:30.821051
------------------------------------------------------
Started: 2025-04-28 18:23:05.315985
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 18:23:05.536692
------------------------------------------------------
Started: 2025-04-28 20:17:28.413307
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 20:17:28.603960
------------------------------------------------------
Started: 2025-04-28 22:15:51.625514
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-28 22:15:51.814415
------------------------------------------------------
Started: 2025-04-29 01:16:35.727114
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 01:16:35.922910
------------------------------------------------------
Started: 2025-04-29 03:04:03.381857
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 03:04:03.574013
------------------------------------------------------
Started: 2025-04-29 04:22:36.669946
Existing_entries: 819
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 751
Summarized using GPT-3.5-turbo
Append: [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Can Third-parties Read Our Emotions?](https://arxiv.org/abs/2504.18673)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation](https://arxiv.org/abs/2504.18805)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks](https://arxiv.org/abs/2504.18838)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning](https://arxiv.org/abs/2504.18839)
Token length: 1104
Summarized using GPT-3.5-turbo
Append: [When2Call: When (not) to Call Tools](https://arxiv.org/abs/2504.18851)
Token length: 1615
Summarized using GPT-3.5-turbo
Append: [Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation](https://arxiv.org/abs/2504.18857)
Token length: 1706
Summarized using GPT-3.5-turbo
Append: [Latent Adversarial Training Improves the Representation of Refusal](https://arxiv.org/abs/2504.18872)
Token length: 701
Summarized using GPT-3.5-turbo
Append: [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/abs/2504.18884)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)
Token length: 1904
Summarized using GPT-3.5-turbo
Append: [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://arxiv.org/abs/2504.18992)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)
Token length: 817
Summarized using GPT-3.5-turbo
Append: [KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation](https://arxiv.org/abs/2504.19024)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)
Token length: 1732
Summarized using GPT-3.5-turbo
Append: [ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics](https://arxiv.org/abs/2504.19066)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Sample-Efficient Language Model for Hinglish Conversational AI](https://arxiv.org/abs/2504.19070)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
Token length: 1726
Summarized using GPT-3.5-turbo
Append: [Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19101)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
Token length: 1620
Summarized using GPT-3.5-turbo
Append: [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [WuNeng: Hybrid State with Attention](https://arxiv.org/abs/2504.19191)
Token length: 777
Summarized using GPT-3.5-turbo
Append: [Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora](https://arxiv.org/abs/2504.19209)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [AndroidGen: Building an Android Language Agent under Data Scarcity](https://arxiv.org/abs/2504.19298)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
Token length: 1391
Summarized using GPT-3.5-turbo
Append: [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [Context Selection and Rewriting for Video-based EducationalQuestion Generation](https://arxiv.org/abs/2504.19406)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models](https://arxiv.org/abs/2504.19436)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks](https://arxiv.org/abs/2504.19445)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [Conflicts in Texts: Data, Implications and Challenges](https://arxiv.org/abs/2504.19472)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
Token length: 662
Summarized using GPT-3.5-turbo
Append: [Arabic Metaphor Sentiment Classification Using Semantic Information](https://arxiv.org/abs/2504.19590)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Coreference Resolution for Vietnamese Narrative Texts](https://arxiv.org/abs/2504.19606)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks](https://arxiv.org/abs/2504.19645)
Append: [Multimodal Conditioned Diffusive Time Series Forecasting](https://arxiv.org/abs/2504.19669)
Append: [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)
Append: [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)
Append: [LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding](https://arxiv.org/abs/2504.19734)
Append: [Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs](https://arxiv.org/abs/2504.19759)
Append: [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)
Append: [To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels](https://arxiv.org/abs/2504.19850)
Append: [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)
Append: [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](https://arxiv.org/abs/2504.19867)
Append: [GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets](https://arxiv.org/abs/2504.19898)
Append: [Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking](https://arxiv.org/abs/2504.19940)
Append: [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)
Append: [Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom](https://arxiv.org/abs/2504.20000)
Append: [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)
Append: [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)
Append: [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)
Append: [Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines](https://arxiv.org/abs/2504.18596)
Append: [Generative Product Recommendations for Implicit Superlative Queries](https://arxiv.org/abs/2504.18748)
Append: [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919)
Append: [LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings](https://arxiv.org/abs/2504.18988)
Append: [Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions](https://arxiv.org/abs/2504.19056)
Append: [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
Append: [Hierarchical Attention Generates Better Proofs](https://arxiv.org/abs/2504.19188)
Append: [Anyprefer: An Agentic Framework for Preference Data Synthesis](https://arxiv.org/abs/2504.19276)
Append: [Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks](https://arxiv.org/abs/2504.19444)
Append: [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
Append: [Improving Reasoning Performance in Large Language Models via Representation Engineering](https://arxiv.org/abs/2504.19483)
Append: [Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2504.19500)
Append: [FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation](https://arxiv.org/abs/2504.19519)
Append: [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
Append: [Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge](https://arxiv.org/abs/2504.19730)
Append: [Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19754)
Append: [A Bayesian approach to modeling topic-metadata relationships](https://arxiv.org/abs/2104.02496)
Append: [Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information](https://arxiv.org/abs/2110.08420)
Append: [Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach](https://arxiv.org/abs/2112.06876)
Append: [Generative Meta-Learning for Zero-Shot Relation Triplet Extraction](https://arxiv.org/abs/2305.01920)
Append: [Benchmarking large language models for biomedical natural language processing applications and recommendations](https://arxiv.org/abs/2305.16326)
Append: [Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)
Append: [Large language models for newspaper sentiment analysis during COVID-19: The Guardian](https://arxiv.org/abs/2405.13056)
Append: [Fake News Detection: It's All in the Data!](https://arxiv.org/abs/2407.02122)
Append: [Pula: Training Large Language Models for Setswana](https://arxiv.org/abs/2408.02239)
Append: [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://arxiv.org/abs/2408.05906)
Append: [W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering](https://arxiv.org/abs/2408.08444)
Append: [Data Processing for the OpenGPT-X Model Family](https://arxiv.org/abs/2410.08800)
Append: [Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling](https://arxiv.org/abs/2410.11325)
Append: [Open Domain Question Answering with Conflicting Contexts](https://arxiv.org/abs/2410.12311)
Append: [From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization](https://arxiv.org/abs/2410.13961)
Append: [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030)
Append: [An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese](https://arxiv.org/abs/2411.17270)
Append: [Investigating Length Issues in Document-level Machine Translation](https://arxiv.org/abs/2412.17592)
Append: [LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context](https://arxiv.org/abs/2412.17596)
Append: [Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora](https://arxiv.org/abs/2502.00090)
Append: [InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context](https://arxiv.org/abs/2502.12257)
Append: [Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13019)
Append: [LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning](https://arxiv.org/abs/2502.14644)
Append: [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://arxiv.org/abs/2502.15147)
Append: [Protecting multimodal large language models against misleading visualizations](https://arxiv.org/abs/2502.20503)
Append: [Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models](https://arxiv.org/abs/2503.10617)
Append: [Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish](https://arxiv.org/abs/2504.09714)
Append: [The Semantic Scholar Open Data Platform](https://arxiv.org/abs/2301.10140)
Append: [NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models](https://arxiv.org/abs/2303.10430)
Append: [Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans](https://arxiv.org/abs/2307.12369)
Append: [Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation](https://arxiv.org/abs/2403.19103)
Append: [Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/abs/2403.20331)
Append: [GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation](https://arxiv.org/abs/2409.03140)
Append: [AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents](https://arxiv.org/abs/2409.09013)
Append: [Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization](https://arxiv.org/abs/2410.08847)
Append: [CREAM: Consistency Regularized Self-Rewarding Language Models](https://arxiv.org/abs/2410.12735)
Append: [Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models](https://arxiv.org/abs/2410.18252)
Append: [A Guide to Misinformation Detection Data and Evaluation](https://arxiv.org/abs/2411.05060)
Append: [An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training](https://arxiv.org/abs/2501.15579)
Append: [Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering](https://arxiv.org/abs/2502.09573)
Append: [Exploring LLM-based Student Simulation for Metacognitive Cultivation](https://arxiv.org/abs/2502.11678)
Append: [NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence](https://arxiv.org/abs/2502.20601)
Append: [Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking](https://arxiv.org/abs/2504.03947)
Append: [Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use](https://arxiv.org/abs/2504.04736)
Append: [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089)
Append: [Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection](https://arxiv.org/abs/2504.17834)
append_entries: 129
Finish: 2025-04-29 04:24:34.291533
------------------------------------------------------
Started: 2025-04-29 06:24:30.171459
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 06:24:30.486505
------------------------------------------------------
Started: 2025-04-29 08:22:47.034261
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 08:22:47.357448
------------------------------------------------------
Started: 2025-04-29 10:18:27.720219
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 10:18:28.043674
------------------------------------------------------
Started: 2025-04-29 12:34:40.885194
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 12:34:41.210261
------------------------------------------------------
Started: 2025-04-29 14:16:39.217356
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 14:16:39.544472
------------------------------------------------------
Started: 2025-04-29 16:20:02.492784
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 16:20:02.808175
------------------------------------------------------
Started: 2025-04-29 18:22:13.455317
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 18:22:13.839323
------------------------------------------------------
Started: 2025-04-29 20:18:07.124266
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 20:18:07.444309
------------------------------------------------------
Started: 2025-04-29 22:15:17.365462
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-29 22:15:17.718552
------------------------------------------------------
Started: 2025-04-30 01:16:44.203366
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 01:16:44.552175
------------------------------------------------------
Started: 2025-04-30 03:03:54.454656
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 03:03:54.786597
------------------------------------------------------
Started: 2025-04-30 04:23:40.425225
Existing_entries: 948
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1831
Summarized using GPT-3.5-turbo
Append: [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)
Token length: 1097
Summarized using GPT-3.5-turbo
Append: [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)
Token length: 419
Summarized using GPT-3.5-turbo
Append: [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)
Token length: 653
Summarized using GPT-3.5-turbo
Append: [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)
Token length: 1121
Summarized using GPT-3.5-turbo
Append: [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)
Token length: 1032
Summarized using GPT-3.5-turbo
Append: [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)
Token length: 801
Summarized using GPT-3.5-turbo
Append: [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)
Token length: 967
Summarized using GPT-3.5-turbo
Append: [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)
Token length: 1352
Summarized using GPT-3.5-turbo
Append: [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)
Token length: 827
Summarized using GPT-3.5-turbo
Append: [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)
Token length: 493
Summarized using GPT-3.5-turbo
Append: [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)
Token length: 855
Summarized using GPT-3.5-turbo
Append: [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)
Token length: 1465
Summarized using GPT-3.5-turbo
Append: [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)
Token length: 833
Summarized using GPT-3.5-turbo
Append: [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence](https://arxiv.org/abs/2504.20059)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
Token length: 1895
Summarized using GPT-3.5-turbo
Append: [AI Awareness](https://arxiv.org/abs/2504.20084)
Token length: 1091
Summarized using GPT-3.5-turbo
Append: [MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?](https://arxiv.org/abs/2504.20094)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies](https://arxiv.org/abs/2504.20117)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains](https://arxiv.org/abs/2504.20199)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [mrCAD: Multimodal Refinement of Computer-aided Designs](https://arxiv.org/abs/2504.20294)
Token length: 1530
Summarized using GPT-3.5-turbo
Append: [Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding](https://arxiv.org/abs/2504.20456)
Token length: 1884
Summarized using GPT-3.5-turbo
Append: [Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User](https://arxiv.org/abs/2504.20458)
Append: [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
Append: [ReasonIR: Training Retrievers for Reasoning Tasks](https://arxiv.org/abs/2504.20595)
Append: [X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2504.20859)
Append: [The Leaderboard Illusion](https://arxiv.org/abs/2504.20879)
Append: [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
Append: [Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition](https://arxiv.org/abs/2504.20938)
Append: [Semantic Consistency for Assuring Reliability of Large Language Models](https://arxiv.org/abs/2308.09138)
Append: [LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models](https://arxiv.org/abs/2310.03903)
Append: [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education](https://arxiv.org/abs/2310.12059)
Append: [Agentic AI: The Era of Semantic Decoding](https://arxiv.org/abs/2403.14562)
Append: [A Practical Analysis of Human Alignment with *PO](https://arxiv.org/abs/2407.15229)
Append: [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://arxiv.org/abs/2408.00137)
Append: [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://arxiv.org/abs/2409.07394)
Append: [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://arxiv.org/abs/2410.02102)
Append: [Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context](https://arxiv.org/abs/2410.07103)
Append: [MDCure: A Scalable Pipeline for Multi-Document Instruction-Following](https://arxiv.org/abs/2410.23463)
Append: [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175)
Append: [Benchmarking LLMs' Judgments with No Gold Standard](https://arxiv.org/abs/2411.07127)
Append: [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)
Append: [A Bayesian Optimization Approach to Machine Translation Reranking](https://arxiv.org/abs/2411.09694)
Append: [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)
Append: [An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation](https://arxiv.org/abs/2502.12836)
Append: [MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation](https://arxiv.org/abs/2502.17163)
Append: [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/abs/2503.15358)
Append: [CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation](https://arxiv.org/abs/2503.19878)
Append: [Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users](https://arxiv.org/abs/2504.00799)
Append: [LLM-based Automated Grading with Human-in-the-Loop](https://arxiv.org/abs/2504.05239)
Append: [Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation](https://arxiv.org/abs/2504.07072)
Append: [Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2303.01903)
Append: [Pose-Based Sign Language Appearance Transfer](https://arxiv.org/abs/2410.13675)
Append: [DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators](https://arxiv.org/abs/2412.02467)
Append: [SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering](https://arxiv.org/abs/2412.06832)
Append: [Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models](https://arxiv.org/abs/2501.12433)
Append: [From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors](https://arxiv.org/abs/2501.18045)
Append: [REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations](https://arxiv.org/abs/2502.03629)
Append: [The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation](https://arxiv.org/abs/2503.04606)
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Append: [LocAgent: Graph-Guided LLM Agents for Code Localization](https://arxiv.org/abs/2503.09089)
append_entries: 88
Finish: 2025-04-30 04:25:44.527424
------------------------------------------------------
Started: 2025-04-30 06:23:15.361193
Existing_entries: 1036
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 06:23:15.618642
------------------------------------------------------
Started: 2025-04-30 08:21:50.001071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 08:21:50.237415
------------------------------------------------------
Started: 2025-04-30 10:17:34.865107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 10:17:35.137225
------------------------------------------------------
Started: 2025-04-30 12:32:19.416250
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 12:32:19.691571
------------------------------------------------------
Started: 2025-04-30 14:15:51.805005
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 14:15:52.072578
------------------------------------------------------
Started: 2025-04-30 16:20:13.711852
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 16:20:13.952481
------------------------------------------------------
Started: 2025-04-30 18:22:26.118946
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 18:22:26.394410
------------------------------------------------------
Started: 2025-04-30 20:17:52.505808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 20:17:52.744465
------------------------------------------------------
Started: 2025-04-30 22:15:17.634986
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-04-30 22:15:17.869587
------------------------------------------------------
Started: 2025-05-01 01:23:25.222117
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 01:23:25.488610
------------------------------------------------------
Started: 2025-05-01 03:14:18.921097
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 03:14:19.188706
------------------------------------------------------
Started: 2025-05-01 04:25:19.936055
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)
Token length: 664
Summarized using GPT-3.5-turbo
Append: [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)
Token length: 1174
Summarized using GPT-3.5-turbo
Append: [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Param$\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)
Token length: 1896
Summarized using GPT-3.5-turbo
Append: [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)
Token length: 967
Summarized using GPT-3.5-turbo
Append: [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)
Token length: 1767
Summarized using GPT-3.5-turbo
Append: [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)
Token length: 1718
Summarized using GPT-3.5-turbo
Append: [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)
Token length: 1922
Summarized using GPT-3.5-turbo
Append: [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)
Token length: 866
Summarized using GPT-3.5-turbo
Append: [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)
Token length: 811
Summarized using GPT-3.5-turbo
Append: [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)
Token length: 834
Summarized using GPT-3.5-turbo
Append: [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)
Token length: 924
Summarized using GPT-3.5-turbo
Append: [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)
Token length: 778
Summarized using GPT-3.5-turbo
Append: [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)
Token length: 807
Summarized using GPT-3.5-turbo
Append: [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)
Token length: 867
Summarized using GPT-3.5-turbo
Append: [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval](https://arxiv.org/abs/2504.21015)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage](https://arxiv.org/abs/2504.21035)
Append: [Multimodal Large Language Models for Medicine: A Comprehensive Survey](https://arxiv.org/abs/2504.21051)
Append: [Phi-4-reasoning Technical Report](https://arxiv.org/abs/2504.21318)
Append: [Who Gets the Callback? Generative AI and Gender Bias](https://arxiv.org/abs/2504.21400)
Append: [SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding](https://arxiv.org/abs/2504.21435)
Append: [Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models](https://arxiv.org/abs/2504.21559)
Append: [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
Append: [AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
Append: [LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics](https://arxiv.org/abs/2504.21716)
Append: [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
Append: [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
Append: [LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection](https://arxiv.org/abs/2310.18964)
Append: [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)
Append: [Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs](https://arxiv.org/abs/2404.19442)
Append: [Emergence of a High-Dimensional Abstraction Phase in Language Transformers](https://arxiv.org/abs/2405.15471)
Append: [Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models](https://arxiv.org/abs/2410.07825)
Append: [Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent](https://arxiv.org/abs/2410.16658)
Append: [KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities](https://arxiv.org/abs/2501.00571)
Append: [Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification](https://arxiv.org/abs/2502.11258)
Append: [Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice](https://arxiv.org/abs/2503.04785)
Append: [JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System](https://arxiv.org/abs/2503.14258)
Append: [Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/abs/2503.21934)
Append: [VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge](https://arxiv.org/abs/2504.10342)
Append: [Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding](https://arxiv.org/abs/2301.11564)
Append: [SignLLM: Sign Language Production Large Language Models](https://arxiv.org/abs/2405.10718)
Append: [HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes](https://arxiv.org/abs/2408.05794)
Append: [Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance](https://arxiv.org/abs/2409.15545)
Append: [Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction](https://arxiv.org/abs/2409.15551)
Append: [Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models](https://arxiv.org/abs/2409.16920)
Append: [Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling](https://arxiv.org/abs/2409.16937)
Append: [Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations](https://arxiv.org/abs/2409.17899)
Append: [How to Construct Random Unitaries](https://arxiv.org/abs/2410.10116)
Append: [Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion](https://arxiv.org/abs/2411.08165)
Append: [All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages](https://arxiv.org/abs/2411.16508)
Append: [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/abs/2412.12119)
Append: [OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis](https://arxiv.org/abs/2412.19723)
Append: [Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews](https://arxiv.org/abs/2502.05439)
Append: [Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training](https://arxiv.org/abs/2502.12734)
Append: [SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations](https://arxiv.org/abs/2502.16949)
Append: [Learning Code-Edit Embedding to Model Student Debugging Behavior](https://arxiv.org/abs/2502.19407)
Append: [AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language](https://arxiv.org/abs/2503.01453)
Append: [Urban Computing in the Era of Large Language Models](https://arxiv.org/abs/2504.02009)
append_entries: 91
Finish: 2025-05-01 04:27:19.501001
------------------------------------------------------
Started: 2025-05-01 06:24:07.644520
Existing_entries: 1091
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 06:24:07.915664
------------------------------------------------------
Started: 2025-05-01 08:21:25.162194
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 08:21:25.453475
------------------------------------------------------
Started: 2025-05-01 10:17:37.752923
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 10:17:37.987974
------------------------------------------------------
Started: 2025-05-01 12:31:33.382774
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 12:31:33.655939
------------------------------------------------------
Started: 2025-05-01 14:14:59.420908
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 14:14:59.676124
------------------------------------------------------
Started: 2025-05-01 16:20:18.825099
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 16:20:19.095232
------------------------------------------------------
Started: 2025-05-01 18:22:26.998973
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 18:22:27.231121
------------------------------------------------------
Started: 2025-05-01 20:16:40.740301
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 20:16:41.003808
------------------------------------------------------
Started: 2025-05-01 22:15:22.977906
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-01 22:15:23.214372
------------------------------------------------------
Started: 2025-05-02 01:17:00.594983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 01:17:00.874513
------------------------------------------------------
Started: 2025-05-02 03:05:38.010694
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 03:05:38.274249
------------------------------------------------------
Started: 2025-05-02 04:22:47.078995
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/abs/2505.00001)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [Symbol grounding in computational systems: A paradox of intentions](https://arxiv.org/abs/2505.00002)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs](https://arxiv.org/abs/2505.00003)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [LangVAE and LangSpace: Building and Probing for Language Model VAEs](https://arxiv.org/abs/2505.00004)
Token length: 979
Summarized using GPT-3.5-turbo
Append: [Toward a digital twin of U.S. Congress](https://arxiv.org/abs/2505.00006)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination](https://arxiv.org/abs/2505.00008)
Token length: 1571
Summarized using GPT-3.5-turbo
Append: [Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation](https://arxiv.org/abs/2505.00009)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)
Token length: 595
Summarized using GPT-3.5-turbo
Append: [The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?](https://arxiv.org/abs/2505.00012)
Token length: 1577
Summarized using GPT-3.5-turbo
Append: [Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa](https://arxiv.org/abs/2505.00013)
Token length: 1404
Summarized using GPT-3.5-turbo
Append: [Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and M\"obius Strips](https://arxiv.org/abs/2505.00014)
Token length: 1884
Summarized using GPT-3.5-turbo
Append: [Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation](https://arxiv.org/abs/2505.00015)
Token length: 1389
Summarized using GPT-3.5-turbo
Append: [Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning](https://arxiv.org/abs/2505.00016)
Token length: 574
Summarized using GPT-3.5-turbo
Append: [ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation](https://arxiv.org/abs/2505.00017)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [An Empirical Study on Prompt Compression for Large Language Models](https://arxiv.org/abs/2505.00019)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [Beyond Public Access in LLM Pre-Training Data](https://arxiv.org/abs/2505.00020)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss](https://arxiv.org/abs/2505.00021)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [CORG: Generating Answers from Complex, Interrelated Contexts](https://arxiv.org/abs/2505.00023)
Token length: 1370
Summarized using GPT-3.5-turbo
Append: [Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://arxiv.org/abs/2505.00024)
Token length: 1728
Summarized using GPT-3.5-turbo
Append: [A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1](https://arxiv.org/abs/2505.00025)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [Theory of Mind in Large Language Models: Assessment and Enhancement](https://arxiv.org/abs/2505.00026)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts](https://arxiv.org/abs/2505.00027)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation](https://arxiv.org/abs/2505.00028)
Token length: 1544
Summarized using GPT-3.5-turbo
Append: [Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting](https://arxiv.org/abs/2505.00029)
Token length: 636
Summarized using GPT-3.5-turbo
Append: [Can Language Models Represent the Past without Anachronism?](https://arxiv.org/abs/2505.00030)
Token length: 1610
Summarized using GPT-3.5-turbo
Append: [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://arxiv.org/abs/2505.00031)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2505.00032)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models](https://arxiv.org/abs/2505.00033)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Improving Phishing Email Detection Performance of Small Large Language Models](https://arxiv.org/abs/2505.00034)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics](https://arxiv.org/abs/2505.00035)
Token length: 1810
Summarized using GPT-3.5-turbo
Append: [A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies](https://arxiv.org/abs/2505.00036)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: [HyPerAlign: Hypotheses-driven Personalized Alignment](https://arxiv.org/abs/2505.00038)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Base Models Beat Aligned Models at Randomness and Creativity](https://arxiv.org/abs/2505.00047)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting](https://arxiv.org/abs/2505.00050)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity](https://arxiv.org/abs/2505.00056)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [A Report on the llms evaluating the high school questions](https://arxiv.org/abs/2505.00057)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition](https://arxiv.org/abs/2505.00059)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5](https://arxiv.org/abs/2505.00060)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems](https://arxiv.org/abs/2505.00061)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [ConSens: Assessing context grounding in open-book question answering](https://arxiv.org/abs/2505.00065)
Token length: 1073
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese](https://arxiv.org/abs/2505.00114)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs](https://arxiv.org/abs/2505.00127)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models](https://arxiv.org/abs/2505.00147)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports](https://arxiv.org/abs/2505.00191)
Token length: 828
Summarized using GPT-3.5-turbo
Append: [Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring](https://arxiv.org/abs/2505.00261)
Token length: 998
Summarized using GPT-3.5-turbo
Append: [Consistency in Language Models: Current Landscape, Challenges, and Future Directions](https://arxiv.org/abs/2505.00268)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation](https://arxiv.org/abs/2505.00339)
Append: [KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis](https://arxiv.org/abs/2505.00367)
Append: [CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass](https://arxiv.org/abs/2505.00389)
Append: [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)
Append: [Computational Identification of Regulatory Statements in EU Legislation](https://arxiv.org/abs/2505.00479)
Append: [HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection](https://arxiv.org/abs/2505.00506)
Append: [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)
Append: [Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models](https://arxiv.org/abs/2505.00557)
Append: [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
Append: [Block Circulant Adapter for Large Language Models](https://arxiv.org/abs/2505.00582)
Append: [FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation](https://arxiv.org/abs/2505.00624)
Append: [The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)](https://arxiv.org/abs/2505.00626)
Append: [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/abs/2505.00654)
Append: [On the generalization of language models from in-context learning and finetuning: a controlled study](https://arxiv.org/abs/2505.00661)
Append: [DeepCritic: Deliberate Critique with Large Language Models](https://arxiv.org/abs/2505.00662)
Append: [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
Append: [Steering Large Language Models with Register Analysis for Arbitrary Style Transfer](https://arxiv.org/abs/2505.00679)
Append: [Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications](https://arxiv.org/abs/2505.00049)
Append: [Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques](https://arxiv.org/abs/2505.00105)
Append: [Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models](https://arxiv.org/abs/2505.00150)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
Append: [EnronQA: Towards Personalized RAG over Private Documents](https://arxiv.org/abs/2505.00263)
Append: [Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing](https://arxiv.org/abs/2505.00315)
Append: [T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation](https://arxiv.org/abs/2505.00337)
Append: [R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training](https://arxiv.org/abs/2505.00358)
Append: [Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training](https://arxiv.org/abs/2505.00422)
Append: [Investigating Task Arithmetic for Zero-Shot Information Retrieval](https://arxiv.org/abs/2505.00649)
Append: [T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT](https://arxiv.org/abs/2505.00703)
Append: [EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532)
Append: [LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning](https://arxiv.org/abs/2401.15371)
Append: ["Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments](https://arxiv.org/abs/2402.08498)
Append: [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532)
Append: [(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts](https://arxiv.org/abs/2405.11804)
Append: [Automated Review Generation Method Based on Large Language Models](https://arxiv.org/abs/2407.20906)
Append: [Challenges and Future Directions of Data-Centric AI Alignment](https://arxiv.org/abs/2410.01957)
Append: [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://arxiv.org/abs/2410.20774)
Append: [Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis](https://arxiv.org/abs/2412.05862)
Append: [A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://arxiv.org/abs/2501.13947)
Append: [HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models](https://arxiv.org/abs/2502.05945)
Append: [Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?](https://arxiv.org/abs/2502.07963)
Append: [UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation](https://arxiv.org/abs/2502.20984)
Append: [Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](https://arxiv.org/abs/2503.23895)
Append: [Opioid Named Entity Recognition (ONER-2025) from Reddit](https://arxiv.org/abs/2504.00027)
Append: [LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem](https://arxiv.org/abs/2403.00108)
Append: [Large Language Model Agent as a Mechanical Designer](https://arxiv.org/abs/2404.17525)
Append: [Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers](https://arxiv.org/abs/2405.04620)
Append: [TaeBench: Improving Quality of Toxic Adversarial Examples](https://arxiv.org/abs/2410.05573)
Append: [Bridging Personalization and Control in Scientific Personalized Search](https://arxiv.org/abs/2411.02790)
Append: [Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner](https://arxiv.org/abs/2412.18086)
Append: [Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking](https://arxiv.org/abs/2501.18265)
Append: [Efficient Reinforcement Finetuning via Adaptive Curriculum Learning](https://arxiv.org/abs/2504.05520)
append_entries: 101
Finish: 2025-05-02 04:24:50.973466
------------------------------------------------------
Started: 2025-05-02 06:22:58.565597
Existing_entries: 1101
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 06:22:58.831768
------------------------------------------------------
Started: 2025-05-02 08:21:03.015241
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 08:21:03.309858
------------------------------------------------------
Started: 2025-05-02 10:17:17.122492
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 10:17:17.382094
------------------------------------------------------
Started: 2025-05-02 12:31:57.572343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 12:31:57.835419
------------------------------------------------------
Started: 2025-05-02 14:15:36.330689
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 14:15:36.613397
------------------------------------------------------
Started: 2025-05-02 16:20:00.940575
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 16:20:01.202289
------------------------------------------------------
Started: 2025-05-02 18:21:56.113792
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 18:21:56.364424
------------------------------------------------------
Started: 2025-05-02 20:17:57.421515
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 20:17:57.684845
------------------------------------------------------
Started: 2025-05-02 22:15:24.006114
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-02 22:15:24.290948
------------------------------------------------------
Started: 2025-05-03 01:15:13.934457
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 01:15:14.192303
------------------------------------------------------
Started: 2025-05-03 03:01:18.688971
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 03:01:18.959935
------------------------------------------------------
Started: 2025-05-03 04:18:37.330220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 04:18:37.392968
------------------------------------------------------
Started: 2025-05-03 06:20:53.828651
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 06:20:53.905038
------------------------------------------------------
Started: 2025-05-03 08:18:55.653907
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 08:18:55.739166
------------------------------------------------------
Started: 2025-05-03 10:15:17.902925
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 10:15:17.958709
------------------------------------------------------
Started: 2025-05-03 12:29:35.037106
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 12:29:35.119349
------------------------------------------------------
Started: 2025-05-03 14:14:12.630838
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 14:14:12.715767
------------------------------------------------------
Started: 2025-05-03 16:18:19.315741
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 16:18:19.396727
------------------------------------------------------
Started: 2025-05-03 18:20:16.425313
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 18:20:16.490246
------------------------------------------------------
Started: 2025-05-03 20:16:21.203195
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 20:16:21.263042
------------------------------------------------------
Started: 2025-05-03 22:14:21.941006
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-03 22:14:22.013179
------------------------------------------------------
Started: 2025-05-04 01:24:07.894825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 01:24:07.950782
------------------------------------------------------
Started: 2025-05-04 03:14:23.995746
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 03:14:24.054440
------------------------------------------------------
Started: 2025-05-04 04:19:51.392192
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 04:19:51.452550
------------------------------------------------------
Started: 2025-05-04 06:22:05.880652
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 06:22:05.939796
------------------------------------------------------
Started: 2025-05-04 08:19:11.811528
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 08:19:11.872311
------------------------------------------------------
Started: 2025-05-04 10:15:51.226512
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 10:15:51.288132
------------------------------------------------------
Started: 2025-05-04 12:29:52.291566
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 12:29:52.404019
------------------------------------------------------
Started: 2025-05-04 14:13:39.705622
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 14:13:39.767489
------------------------------------------------------
Started: 2025-05-04 16:18:20.734342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 16:18:20.799536
------------------------------------------------------
Started: 2025-05-04 18:20:17.390516
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 18:20:17.446402
------------------------------------------------------
Started: 2025-05-04 20:16:38.507668
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 20:16:38.584513
------------------------------------------------------
Started: 2025-05-04 22:14:35.751245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-04 22:14:35.829778
------------------------------------------------------
Started: 2025-05-05 01:20:10.431233
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 01:20:10.513727
------------------------------------------------------
Started: 2025-05-05 03:10:31.585894
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 03:10:31.645818
------------------------------------------------------
Started: 2025-05-05 04:24:16.722852
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1440
Summarized using GPT-3.5-turbo
Append: [FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models](https://arxiv.org/abs/2505.00725)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Reasoning Capabilities and Invariability of Large Language Models](https://arxiv.org/abs/2505.00776)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing](https://arxiv.org/abs/2505.00931)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949)
Token length: 1907
Summarized using GPT-3.5-turbo
Append: [A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts](https://arxiv.org/abs/2505.00977)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
Token length: 846
Summarized using GPT-3.5-turbo
Append: [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
Token length: 1615
Summarized using GPT-3.5-turbo
Append: [VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language](https://arxiv.org/abs/2505.00989)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Token-free Models for Sarcasm Detection](https://arxiv.org/abs/2505.01006)
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark](https://arxiv.org/abs/2505.01015)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?](https://arxiv.org/abs/2505.01035)
Token length: 1435
Summarized using GPT-3.5-turbo
Append: [Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs](https://arxiv.org/abs/2505.01068)
Token length: 1327
Summarized using GPT-3.5-turbo
Append: [MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning](https://arxiv.org/abs/2505.01110)
Token length: 775
Summarized using GPT-3.5-turbo
Append: [On the Limitations of Steering in Language Model Alignment](https://arxiv.org/abs/2505.01162)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods](https://arxiv.org/abs/2505.01198)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models](https://arxiv.org/abs/2505.01238)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [PREMISE: Matching-based Prediction for Accurate Review Recommendation](https://arxiv.org/abs/2505.01255)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Anti-adversarial Learning: Desensitizing Prompts for Large Language Models](https://arxiv.org/abs/2505.01273)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types](https://arxiv.org/abs/2505.01311)
Token length: 823
Summarized using GPT-3.5-turbo
Append: [A Transformer-based Neural Architecture Search Method](https://arxiv.org/abs/2505.01314)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System](https://arxiv.org/abs/2505.01315)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References](https://arxiv.org/abs/2505.01325)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Multi-Modal Language Models as Text-to-Image Model Evaluators](https://arxiv.org/abs/2505.00759)
Token length: 928
Summarized using GPT-3.5-turbo
Append: [A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i](https://arxiv.org/abs/2505.00808)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation](https://arxiv.org/abs/2505.00831)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [NeMo-Inspector: A Visualization Tool for LLM Generation Analysis](https://arxiv.org/abs/2505.00903)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [Attack and defense techniques in large language models: A survey and new perspectives](https://arxiv.org/abs/2505.00976)
Token length: 875
Summarized using GPT-3.5-turbo
Append: [Towards the Resistance of Neural Network Watermarking to Fine-tuning](https://arxiv.org/abs/2505.01007)
Token length: 1965
Summarized using GPT-3.5-turbo
Append: [Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages](https://arxiv.org/abs/2505.01096)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii](https://arxiv.org/abs/2505.01372)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark](https://arxiv.org/abs/2402.14359)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?](https://arxiv.org/abs/2404.18624)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [REFFLY: Melody-Constrained Lyrics Editing Model](https://arxiv.org/abs/2409.00292)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Does Self-Attention Need Separate Weights in Transformers?](https://arxiv.org/abs/2412.00359)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [When Every Token Counts: Optimal Segmentation for Low-Resource Language Models](https://arxiv.org/abs/2412.06926)
Token length: 1803
Summarized using GPT-3.5-turbo
Append: [AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework](https://arxiv.org/abs/2412.10422)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [ICLR: In-Context Learning of Representations](https://arxiv.org/abs/2501.00070)
Token length: 1475
Summarized using GPT-3.5-turbo
Append: [Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention](https://arxiv.org/abs/2501.06382)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [TableMaster: A Recipe to Advance Table Understanding with Language Models](https://arxiv.org/abs/2501.19378)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing](https://arxiv.org/abs/2502.01976)
Token length: 1152
Summarized using GPT-3.5-turbo
Append: [Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models](https://arxiv.org/abs/2504.13068)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning](https://arxiv.org/abs/2311.09830)
Token length: 1249
Summarized using GPT-3.5-turbo
Append: [FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning](https://arxiv.org/abs/2402.18789)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables](https://arxiv.org/abs/2403.04577)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [MoDeGPT: Modular Decomposition for Large Language Model Compression](https://arxiv.org/abs/2408.09632)
Token length: 1710
Summarized using GPT-3.5-turbo
Append: [Competition Dynamics Shape Algorithmic Phases of In-Context Learning](https://arxiv.org/abs/2412.01003)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment](https://arxiv.org/abs/2412.07446)
Append: [A Rate-Distortion Framework for Summarization](https://arxiv.org/abs/2501.13100)
Append: [Activation Steering in Neural Theorem Provers](https://arxiv.org/abs/2502.15507)
append_entries: 52
Finish: 2025-05-05 04:26:15.688257
------------------------------------------------------
Started: 2025-05-05 06:24:33.721452
Existing_entries: 1052
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 06:24:33.936195
------------------------------------------------------
Started: 2025-05-05 08:23:01.354074
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 08:23:01.521425
------------------------------------------------------
Started: 2025-05-05 10:18:15.542958
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 10:18:15.735466
------------------------------------------------------
Started: 2025-05-05 12:33:08.883418
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 12:33:09.086435
------------------------------------------------------
Started: 2025-05-05 14:16:43.864983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 14:16:44.062886
------------------------------------------------------
Started: 2025-05-05 16:21:10.141364
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 16:21:10.304225
------------------------------------------------------
Started: 2025-05-05 18:19:23.508124
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 18:19:23.705744
------------------------------------------------------
Started: 2025-05-05 20:18:03.120230
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 20:18:03.292202
------------------------------------------------------
Started: 2025-05-05 22:15:56.872759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-05 22:15:57.034904
------------------------------------------------------
Started: 2025-05-06 01:17:38.706983
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 01:17:38.877576
------------------------------------------------------
Started: 2025-05-06 03:06:12.461245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 03:06:12.638391
------------------------------------------------------
Started: 2025-05-06 04:24:00.678404
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1642
Summarized using GPT-3.5-turbo
Append: [Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation](https://arxiv.org/abs/2505.01456)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling](https://arxiv.org/abs/2505.01459)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [SymPlanner: Deliberate Planning in Language Models with Symbolic Representation](https://arxiv.org/abs/2505.01479)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [On the effectiveness of Large Language Models in the mechanical design domain](https://arxiv.org/abs/2505.01559)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains](https://arxiv.org/abs/2505.01560)
Token length: 1481
Summarized using GPT-3.5-turbo
Append: [PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents](https://arxiv.org/abs/2505.01592)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Always Tell Me The Odds: Fine-grained Conditional Probability Estimation](https://arxiv.org/abs/2505.01595)
Token length: 1665
Summarized using GPT-3.5-turbo
Append: [A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency](https://arxiv.org/abs/2505.01658)
Token length: 1735
Summarized using GPT-3.5-turbo
Append: [High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers](https://arxiv.org/abs/2505.01693)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models](https://arxiv.org/abs/2505.01761)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments](https://arxiv.org/abs/2505.01794)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis](https://arxiv.org/abs/2505.01800)
Token length: 1668
Summarized using GPT-3.5-turbo
Append: [$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge](https://arxiv.org/abs/2505.01812)
Token length: 787
Summarized using GPT-3.5-turbo
Append: [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Positional Attention for Efficient BERT-Based Named Entity Recognition](https://arxiv.org/abs/2505.01868)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Humans can learn to detect AI-generated texts, or at least learn when they can't](https://arxiv.org/abs/2505.01877)
Token length: 847
Summarized using GPT-3.5-turbo
Append: [Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams](https://arxiv.org/abs/2505.01883)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation](https://arxiv.org/abs/2505.01900)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview](https://arxiv.org/abs/2505.01967)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load](https://arxiv.org/abs/2505.01980)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [An overview of artificial intelligence in computer-assisted language learning](https://arxiv.org/abs/2505.02032)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction](https://arxiv.org/abs/2505.02072)
Token length: 1029
Summarized using GPT-3.5-turbo
Append: [LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning](https://arxiv.org/abs/2505.02078)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications](https://arxiv.org/abs/2505.02091)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study](https://arxiv.org/abs/2505.02142)
Token length: 1860
Summarized using GPT-3.5-turbo
Append: [QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach](https://arxiv.org/abs/2505.02146)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents](https://arxiv.org/abs/2505.02156)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use](https://arxiv.org/abs/2505.02164)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking](https://arxiv.org/abs/2505.02171)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Token length: 1594
Summarized using GPT-3.5-turbo
Append: [Measuring Hong Kong Massive Multi-Task Language Understanding](https://arxiv.org/abs/2505.02177)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation](https://arxiv.org/abs/2505.02235)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models](https://arxiv.org/abs/2505.02252)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Parameter-Efficient Transformer Embeddings](https://arxiv.org/abs/2505.02266)
Token length: 847
Summarized using GPT-3.5-turbo
Append: [Demystifying optimized prompts in language models](https://arxiv.org/abs/2505.02273)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition](https://arxiv.org/abs/2505.02304)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering](https://arxiv.org/abs/2505.02311)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning](https://arxiv.org/abs/2505.02363)
Token length: 1954
Summarized using GPT-3.5-turbo
Append: [JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings](https://arxiv.org/abs/2505.02366)
Token length: 1892
Summarized using GPT-3.5-turbo
Append: [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Bielik 11B v2 Technical Report](https://arxiv.org/abs/2505.02410)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs](https://arxiv.org/abs/2505.02456)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda](https://arxiv.org/abs/2505.02463)
Token length: 477
Summarized using GPT-3.5-turbo
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Token length: 1356
Summarized using GPT-3.5-turbo
Append: [EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning](https://arxiv.org/abs/2505.02579)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [Ensemble Kalman filter for uncertainty in human language comprehension](https://arxiv.org/abs/2505.02590)
Token length: 1075
Summarized using GPT-3.5-turbo
Append: [Automatic Proficiency Assessment in L2 English Learners](https://arxiv.org/abs/2505.02615)
Token length: 942
Summarized using GPT-3.5-turbo
Append: [LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis](https://arxiv.org/abs/2505.02625)
Append: [Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset](https://arxiv.org/abs/2505.02656)
Append: [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/abs/2505.02666)
Append: [Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models](https://arxiv.org/abs/2505.02686)
Append: [fastabx: A library for efficient computation of ABX discriminability](https://arxiv.org/abs/2505.02692)
Append: [Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models](https://arxiv.org/abs/2505.02763)
Append: [ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations](https://arxiv.org/abs/2505.02819)
Append: [Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations](https://arxiv.org/abs/2505.01433)
Append: [AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine](https://arxiv.org/abs/2505.01435)
Append: [CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code](https://arxiv.org/abs/2505.01485)
Append: [Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation](https://arxiv.org/abs/2505.01636)
Append: [Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm](https://arxiv.org/abs/2505.01706)
Append: [Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias](https://arxiv.org/abs/2505.01754)
Append: [Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos](https://arxiv.org/abs/2505.01790)
Append: [Explainability by design: an experimental analysis of the legal coding process](https://arxiv.org/abs/2505.01944)
Append: [A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2505.01958)
Append: [Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data](https://arxiv.org/abs/2505.02130)
Append: [Exploring new Approaches for Information Retrieval through Natural Language Processing](https://arxiv.org/abs/2505.02199)
Append: [DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units](https://arxiv.org/abs/2505.02206)
Append: [Interpretable Emergent Language Using Inter-Agent Transformers](https://arxiv.org/abs/2505.02215)
Append: [Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques](https://arxiv.org/abs/2505.02309)
Append: [Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL](https://arxiv.org/abs/2505.02391)
Append: [Incentivizing Inclusive Contributions in Model Sharing Markets](https://arxiv.org/abs/2505.02462)
Append: [Bielik v3 Small: Technical Report](https://arxiv.org/abs/2505.02550)
Append: [Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning](https://arxiv.org/abs/2505.02639)
Append: [Predicting Movie Hits Before They Happen with LLMs](https://arxiv.org/abs/2505.02693)
Append: [Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play](https://arxiv.org/abs/2505.02707)
Append: [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
Append: [Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing](https://arxiv.org/abs/2505.02811)
Append: [AutoLibra: Agent Metric Induction from Open-Ended Feedback](https://arxiv.org/abs/2505.02820)
Append: [AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation](https://arxiv.org/abs/2505.02830)
Append: [R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning](https://arxiv.org/abs/2505.02835)
Append: [Transformadores: Fundamentos teoricos y Aplicaciones](https://arxiv.org/abs/2302.09327)
Append: [SMUTF: Schema Matching Using Generative Tags and Hybrid Features](https://arxiv.org/abs/2402.01685)
Append: [DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation](https://arxiv.org/abs/2403.01954)
Append: [ParaICL: Towards Parallel In-Context Learning](https://arxiv.org/abs/2404.00570)
Append: [Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind](https://arxiv.org/abs/2404.04748)
Append: [From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences](https://arxiv.org/abs/2405.05572)
Append: [Large Language Models as Carriers of Hidden Messages](https://arxiv.org/abs/2406.02481)
Append: [LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models](https://arxiv.org/abs/2407.12772)
Append: [A Logical Fallacy-Informed Framework for Argument Generation](https://arxiv.org/abs/2408.03618)
Append: [LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library](https://arxiv.org/abs/2408.06150)
Append: [Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence](https://arxiv.org/abs/2409.09413)
Append: [ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions](https://arxiv.org/abs/2410.14567)
Append: [LLMs for Extremely Low-Resource Finno-Ugric Languages](https://arxiv.org/abs/2410.18902)
Append: [Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction](https://arxiv.org/abs/2412.04454)
Append: [AD-LLM: Benchmarking Large Language Models for Anomaly Detection](https://arxiv.org/abs/2412.11142)
Append: [ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis](https://arxiv.org/abs/2501.00062)
Append: [LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models](https://arxiv.org/abs/2501.00874)
Append: [Towards the Anonymization of the Language Modeling](https://arxiv.org/abs/2501.02407)
Append: [How do Humans and Language Models Reason About Creativity? A Comparative Analysis](https://arxiv.org/abs/2502.03253)
Append: [SpeechT: Findings of the First Mentorship in Speech Translation](https://arxiv.org/abs/2502.12050)
Append: [Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing](https://arxiv.org/abs/2502.15666)
Append: [Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data](https://arxiv.org/abs/2502.16892)
Append: [Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs](https://arxiv.org/abs/2502.17424)
Append: [Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs](https://arxiv.org/abs/2502.21239)
Append: [A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications](https://arxiv.org/abs/2503.17003)
Append: [A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?](https://arxiv.org/abs/2503.24235)
Append: [Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2504.03302)
Append: [APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay](https://arxiv.org/abs/2504.03601)
Append: [Better Estimation of the KL Divergence Between Language Models](https://arxiv.org/abs/2504.10637)
Append: [MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection](https://arxiv.org/abs/2309.15670)
Append: [Impact of Noisy Supervision in Foundation Model Learning](https://arxiv.org/abs/2403.06869)
Append: [Tailored Design of Audio-Visual Speech Recognition Models using Branchformers](https://arxiv.org/abs/2407.06606)
Append: [Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant](https://arxiv.org/abs/2501.17176)
Append: [Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization](https://arxiv.org/abs/2502.04667)
Append: [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/abs/2502.20490)
Append: [The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats](https://arxiv.org/abs/2503.02650)
Append: [Dysarthria Normalization via Local Lie Group Transformations for Robust ASR](https://arxiv.org/abs/2504.12279)
append_entries: 118
Finish: 2025-05-06 04:26:11.843811
------------------------------------------------------
Started: 2025-05-06 06:23:59.634339
Existing_entries: 1118
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 06:23:59.940227
------------------------------------------------------
Started: 2025-05-06 08:22:21.601435
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 08:22:21.904152
------------------------------------------------------
Started: 2025-05-06 10:17:45.917359
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 10:17:46.216210
------------------------------------------------------
Started: 2025-05-06 12:35:05.056216
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 12:35:05.356908
------------------------------------------------------
Started: 2025-05-06 14:16:47.151709
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 14:16:47.449708
------------------------------------------------------
Started: 2025-05-06 16:19:03.156137
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 16:19:03.484089
------------------------------------------------------
Started: 2025-05-06 18:22:47.924209
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 18:22:48.258386
------------------------------------------------------
Started: 2025-05-06 20:18:16.547581
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 20:18:16.881116
------------------------------------------------------
Started: 2025-05-06 22:15:32.302078
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-06 22:15:32.596858
------------------------------------------------------
Started: 2025-05-07 01:18:02.364414
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 01:18:02.694941
------------------------------------------------------
Started: 2025-05-07 03:07:36.694136
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 03:07:36.996131
------------------------------------------------------
Started: 2025-05-07 04:21:39.613376
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
Token length: 1956
Summarized using GPT-3.5-turbo
Append: [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)
Token length: 725
Summarized using GPT-3.5-turbo
Append: [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)
Token length: 1470
Summarized using GPT-3.5-turbo
Append: [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
Token length: 1245
Summarized using GPT-3.5-turbo
Append: [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)
Token length: 656
Summarized using GPT-3.5-turbo
Append: [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)
Token length: 1314
Summarized using GPT-3.5-turbo
Append: [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)
Token length: 944
Summarized using GPT-3.5-turbo
Append: [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)
Token length: 1218
Summarized using GPT-3.5-turbo
Append: [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)
Token length: 921
Summarized using GPT-3.5-turbo
Append: [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)
Token length: 1383
Summarized using GPT-3.5-turbo
Append: [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [{\Psi}-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)
Token length: 763
Summarized using GPT-3.5-turbo
Append: [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)
Token length: 1283
Summarized using GPT-3.5-turbo
Append: [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)
Token length: 996
Summarized using GPT-3.5-turbo
Append: [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)
Token length: 849
Summarized using GPT-3.5-turbo
Append: [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)
Token length: 929
Summarized using GPT-3.5-turbo
Append: [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)
Token length: 1784
Summarized using GPT-3.5-turbo
Append: [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration](https://arxiv.org/abs/2505.02848)
Token length: 779
Summarized using GPT-3.5-turbo
Append: [When Your Own Output Becomes Your Training Data: Noise-to-Meaning Loops and a Formal RSI Trigger](https://arxiv.org/abs/2505.02888)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [The Art of Repair: Optimizing Iterative Program Repair with Instruction-Tuned Models](https://arxiv.org/abs/2505.02931)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Iterative Resolution of Prompt Ambiguities Using a Progressive Cutting-Search Approach](https://arxiv.org/abs/2505.02952)
Token length: 692
Summarized using GPT-3.5-turbo
Append: [Radio: Rate-Distortion Optimization for Large Language Model Compression](https://arxiv.org/abs/2505.03031)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [BLAB: Brutally Long Audio Bench](https://arxiv.org/abs/2505.03054)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Token length: 974
Summarized using GPT-3.5-turbo
Append: [Elevating Semantic Exploration: A Novel Approach Utilizing Distributed Repositories](https://arxiv.org/abs/2505.03443)
Token length: 1760
Summarized using GPT-3.5-turbo
Append: [BadLingual: A Novel Lingual-Backdoor Attack against Large Language Models](https://arxiv.org/abs/2505.03501)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [Rational Retrieval Acts: Leveraging Pragmatic Reasoning to Improve Sparse Retrieval](https://arxiv.org/abs/2505.03676)
Token length: 941
Summarized using GPT-3.5-turbo
Append: [Incoherent Probability Judgments in Large Language Models](https://arxiv.org/abs/2401.16646)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors](https://arxiv.org/abs/2406.14498)
Token length: 1484
Summarized using GPT-3.5-turbo
Append: [CFBench: A Comprehensive Constraints-Following Benchmark for LLMs](https://arxiv.org/abs/2408.01122)
Token length: 1787
Summarized using GPT-3.5-turbo
Append: [LLM-3D Print: Large Language Models To Monitor and Control 3D Printing](https://arxiv.org/abs/2408.14307)
Append: [Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution](https://arxiv.org/abs/2410.00153)
Append: [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://arxiv.org/abs/2410.09580)
Append: [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)
Append: [LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment](https://arxiv.org/abs/2412.18135)
Append: [Self-reflecting Large Language Models: A Hegelian Dialectical Approach](https://arxiv.org/abs/2501.14917)
Append: [Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech](https://arxiv.org/abs/2501.15858)
Append: [Predicting potentially abusive clauses in Chilean terms of services with natural language processing](https://arxiv.org/abs/2502.00865)
Append: [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)
Append: [English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports](https://arxiv.org/abs/2502.14338)
Append: [BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187)
Append: [CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models](https://arxiv.org/abs/2503.10707)
Append: [Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models](https://arxiv.org/abs/2503.13551)
Append: [CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement](https://arxiv.org/abs/2503.17279)
Append: [HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment](https://arxiv.org/abs/2503.18991)
Append: [Clean & Clear: Feasibility of Safe LLM Clinical Guidance](https://arxiv.org/abs/2503.20953)
Append: [SEAL: Steerable Reasoning Calibration of Large Language Models for Free](https://arxiv.org/abs/2504.07986)
Append: [FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback](https://arxiv.org/abs/2404.05046)
Append: [OAC: Output-adaptive Calibration for Accurate Post-training Quantization](https://arxiv.org/abs/2405.15025)
Append: [Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice](https://arxiv.org/abs/2405.19313)
Append: [AudioBench: A Universal Benchmark for Audio Large Language Models](https://arxiv.org/abs/2406.16020)
Append: [Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate](https://arxiv.org/abs/2410.22086)
Append: [MATATA: Weakly Supervised End-to-End MAthematical Tool-Augmented Reasoning for Tabular Applications](https://arxiv.org/abs/2411.18915)
Append: [Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization](https://arxiv.org/abs/2412.17739)
Append: [Music for All: Representational Bias and Cross-Cultural Adaptability of Music Generation Models](https://arxiv.org/abs/2502.07328)
Append: [BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modelling](https://arxiv.org/abs/2503.02445)
Append: [LiteWebAgent: The Open-Source Suite for VLM-Based Web-Agent Applications](https://arxiv.org/abs/2503.02950)
Append: [UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction](https://arxiv.org/abs/2503.15661)
Append: [The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation](https://arxiv.org/abs/2504.11739)
append_entries: 78
Finish: 2025-05-07 04:23:43.413339
------------------------------------------------------
Started: 2025-05-07 06:23:49.623941
Existing_entries: 1078
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 06:23:49.867404
------------------------------------------------------
Started: 2025-05-07 08:23:01.933170
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 08:23:02.182829
------------------------------------------------------
Started: 2025-05-07 10:18:03.800220
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 10:18:04.018132
------------------------------------------------------
Started: 2025-05-07 12:35:11.670684
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 12:35:11.904580
------------------------------------------------------
Started: 2025-05-07 14:17:11.945548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 14:17:12.164682
------------------------------------------------------
Started: 2025-05-07 16:21:15.650555
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 16:21:15.918321
------------------------------------------------------
Started: 2025-05-07 18:23:24.636034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 18:23:24.898600
------------------------------------------------------
Started: 2025-05-07 20:18:26.448800
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 20:18:26.669037
------------------------------------------------------
Started: 2025-05-07 22:16:09.694027
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-07 22:16:09.912166
------------------------------------------------------
Started: 2025-05-08 01:18:41.723176
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 01:18:41.945235
------------------------------------------------------
Started: 2025-05-08 03:11:21.572356
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 03:11:21.804024
------------------------------------------------------
Started: 2025-05-08 04:25:11.789348
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)
Token length: 923
Summarized using GPT-3.5-turbo
Append: [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)
Token length: 1111
Summarized using GPT-3.5-turbo
Append: [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)
Token length: 1600
Summarized using GPT-3.5-turbo
Append: [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)
Token length: 1428
Summarized using GPT-3.5-turbo
Append: [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)
Token length: 1288
Summarized using GPT-3.5-turbo
Append: [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)
Token length: 1894
Summarized using GPT-3.5-turbo
Append: [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)
Token length: 652
Summarized using GPT-3.5-turbo
Append: [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)
Token length: 895
Summarized using GPT-3.5-turbo
Append: [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)
Token length: 1955
Summarized using GPT-3.5-turbo
Append: [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)
Token length: 1905
Summarized using GPT-3.5-turbo
Append: [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)
Token length: 1542
Summarized using GPT-3.5-turbo
Append: [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)
Token length: 1696
Summarized using GPT-3.5-turbo
Append: [When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator](https://arxiv.org/abs/2505.03786)
Token length: 1684
Summarized using GPT-3.5-turbo
Append: [Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling](https://arxiv.org/abs/2505.03799)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free](https://arxiv.org/abs/2505.03810)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs](https://arxiv.org/abs/2505.03814)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective](https://arxiv.org/abs/2505.03828)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete](https://arxiv.org/abs/2505.03961)
Token length: 959
Summarized using GPT-3.5-turbo
Append: [Quiet Feature Learning in Algorithmic Tasks](https://arxiv.org/abs/2505.03997)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [LLAMAPIE: Proactive In-Ear Conversation Assistants](https://arxiv.org/abs/2505.04066)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts](https://arxiv.org/abs/2505.04171)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning](https://arxiv.org/abs/2505.04192)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration](https://arxiv.org/abs/2505.04457)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving](https://arxiv.org/abs/2505.04528)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [Playing repeated games with Large Language Models](https://arxiv.org/abs/2305.16867)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)
Token length: 1007
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are Struggle to Cope with Unreasonability in Math Problems](https://arxiv.org/abs/2403.19346)
Token length: 1865
Summarized using GPT-3.5-turbo
Append: [Re-ReST: Reflection-Reinforced Self-Training for Language Agents](https://arxiv.org/abs/2406.01495)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA](https://arxiv.org/abs/2406.02044)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://arxiv.org/abs/2406.18501)
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning](https://arxiv.org/abs/2408.13184)
Token length: 1719
Summarized using GPT-3.5-turbo
Append: [Advancements and limitations of LLMs in replicating human color-word associations](https://arxiv.org/abs/2411.02116)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution](https://arxiv.org/abs/2501.05040)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Estimating LLM Uncertainty with Logits](https://arxiv.org/abs/2502.00290)
Token length: 1687
Summarized using GPT-3.5-turbo
Append: [Liger: Linearizing Large Language Models to Gated Recurrent Structures](https://arxiv.org/abs/2503.01496)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation](https://arxiv.org/abs/2503.03186)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/abs/2503.11280)
Token length: 745
Summarized using GPT-3.5-turbo
Append: [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/abs/2503.21813)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance](https://arxiv.org/abs/2311.18681)
Append: [Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners](https://arxiv.org/abs/2410.02131)
Append: [Vision-Language Models Create Cross-Modal Task Representations](https://arxiv.org/abs/2410.22330)
Append: [LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation](https://arxiv.org/abs/2411.04997)
Append: [Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation](https://arxiv.org/abs/2411.05261)
Append: [Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT](https://arxiv.org/abs/2411.10246)
Append: [SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild](https://arxiv.org/abs/2503.18892)
append_entries: 56
Finish: 2025-05-08 04:27:25.060534
------------------------------------------------------
Started: 2025-05-08 06:24:50.458891
Existing_entries: 1056
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 06:24:50.651650
------------------------------------------------------
Started: 2025-05-08 08:21:53.195102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 08:21:53.365599
------------------------------------------------------
Started: 2025-05-08 10:18:17.035680
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 10:18:17.232308
------------------------------------------------------
Started: 2025-05-08 12:33:06.354143
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 12:33:06.529342
------------------------------------------------------
Started: 2025-05-08 14:15:00.054341
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 14:15:00.260603
------------------------------------------------------
Started: 2025-05-08 16:21:01.117397
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 16:21:01.305577
------------------------------------------------------
Started: 2025-05-08 18:22:58.306924
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 18:22:58.483787
------------------------------------------------------
Started: 2025-05-08 20:18:32.451493
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 20:18:32.642931
------------------------------------------------------
Started: 2025-05-08 22:15:42.386952
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-08 22:15:42.592826
------------------------------------------------------
Started: 2025-05-09 01:17:54.167988
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 01:17:54.356910
------------------------------------------------------
Started: 2025-05-09 03:07:40.451032
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 03:07:40.674312
------------------------------------------------------
Started: 2025-05-09 04:30:56.020834
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks](https://arxiv.org/abs/2505.04628)
Token length: 1524
Summarized using GPT-3.5-turbo
Append: [Adaptive Token Boundaries: Integrating Human Chunking Mechanisms into Multimodal LLMs](https://arxiv.org/abs/2505.04637)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Language translation, and change of accent for speech-to-speech task using diffusion model](https://arxiv.org/abs/2505.04639)
Token length: 872
Summarized using GPT-3.5-turbo
Append: [A Comparative Benchmark of a Moroccan Darija Toxicity Detection Model (Typica.ai) and Major LLM-Based Moderation APIs (OpenAI, Mistral, Anthropic)](https://arxiv.org/abs/2505.04640)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Rethinking Multimodal Sentiment Analysis: A High-Accuracy, Simplified Fusion Architecture](https://arxiv.org/abs/2505.04642)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Prediction-powered estimators for finite population statistics in highly imbalanced textual data: Public hate crime estimation](https://arxiv.org/abs/2505.04643)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [ChatGPT for automated grading of short answer questions in mechanical ventilation](https://arxiv.org/abs/2505.04645)
Token length: 1494
Summarized using GPT-3.5-turbo
Append: [FRAME: Feedback-Refined Agent Methodology for Enhancing Medical Research Insights](https://arxiv.org/abs/2505.04649)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions](https://arxiv.org/abs/2505.04651)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [Advancing Conversational Diagnostic AI with Multimodal Reasoning](https://arxiv.org/abs/2505.04653)
Token length: 854
Summarized using GPT-3.5-turbo
Append: [A Comparative Analysis of Ethical and Safety Gaps in LLMs using Relative Danger Coefficient](https://arxiv.org/abs/2505.04654)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Integration of Large Language Models and Traditional Deep Learning for Social Determinants of Health Prediction](https://arxiv.org/abs/2505.04655)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [AI-Generated Fall Data: Assessing LLMs and Diffusion Model for Wearable Fall Detection](https://arxiv.org/abs/2505.04660)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Personalized Risks and Regulatory Strategies of Large Language Models in Digital Advertising](https://arxiv.org/abs/2505.04665)
Token length: 1945
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Large Language Models and Evaluating Retrieval Methods for Improved Question Answering on Building Codes](https://arxiv.org/abs/2505.04666)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Reward-SQL: Boosting Text-to-SQL via Stepwise Reasoning and Process-Supervised Rewards](https://arxiv.org/abs/2505.04671)
Token length: 1697
Summarized using GPT-3.5-turbo
Append: [REVEAL: Multi-turn Evaluation of Image-Input Harms for Vision LLM](https://arxiv.org/abs/2505.04673)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Advanced Deep Learning Approaches for Automated Recognition of Cuneiform Symbols](https://arxiv.org/abs/2505.04678)
Token length: 1781
Summarized using GPT-3.5-turbo
Append: [SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding](https://arxiv.org/abs/2505.04723)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Flower Across Time and Media: Sentiment Analysis of Tang Song Poetry and Visual Correspondence](https://arxiv.org/abs/2505.04785)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [Osiris: A Lightweight Open-Source Hallucination Detection System](https://arxiv.org/abs/2505.04844)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards](https://arxiv.org/abs/2505.04847)
Token length: 1661
Summarized using GPT-3.5-turbo
Append: [An Open-Source Dual-Loss Embedding Model for Semantic Retrieval in Higher Education](https://arxiv.org/abs/2505.04916)
Token length: 1161
Summarized using GPT-3.5-turbo
Append: [Chain-of-Thought Tokens are Computer Program Variables](https://arxiv.org/abs/2505.04955)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Rethinking the Relationship between the Power Law and Hierarchical Structures](https://arxiv.org/abs/2505.04984)
Token length: 1533
Summarized using GPT-3.5-turbo
Append: [Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes](https://arxiv.org/abs/2505.04993)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Rethinking Invariance in In-context Learning](https://arxiv.org/abs/2505.04994)
Token length: 1714
Summarized using GPT-3.5-turbo
Append: [The Pitfalls of Growing Group Complexity: LLMs and Social Choice-Based Aggregation for Group Recommendations](https://arxiv.org/abs/2505.05016)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization](https://arxiv.org/abs/2505.05017)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness](https://arxiv.org/abs/2505.05026)
Token length: 731
Summarized using GPT-3.5-turbo
Append: [Image-Text Relation Prediction for Multilingual Tweets](https://arxiv.org/abs/2505.05040)
Token length: 788
Summarized using GPT-3.5-turbo
Append: [Teochew-Wild: The First In-the-wild Teochew Dataset with Orthographic Annotations](https://arxiv.org/abs/2505.05056)
Token length: 1084
Summarized using GPT-3.5-turbo
Append: [Performance Evaluation of Large Language Models in Bangla Consumer Health Query Summarization](https://arxiv.org/abs/2505.05070)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction](https://arxiv.org/abs/2505.05084)
Token length: 1108
Summarized using GPT-3.5-turbo
Append: [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2505.05111)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [A Benchmark Dataset and a Framework for Urdu Multimodal Named Entity Recognition](https://arxiv.org/abs/2505.05148)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [QualBench: Benchmarking Chinese LLMs with Localized Professional Qualifications for Vertical Domain Evaluation](https://arxiv.org/abs/2505.05225)
Token length: 1704
Summarized using GPT-3.5-turbo
Append: [T-T: Table Transformer for Tagging-based Aspect Sentiment Triplet Extraction](https://arxiv.org/abs/2505.05271)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design](https://arxiv.org/abs/2505.05298)
Token length: 1467
Summarized using GPT-3.5-turbo
Append: [ICon: In-Context Contribution for Automatic Data Selection](https://arxiv.org/abs/2505.05327)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [Frame In, Frame Out: Do LLMs Generate More Biased News Headlines than Humans?](https://arxiv.org/abs/2505.05406)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Crosslingual Reasoning through Test-Time Scaling](https://arxiv.org/abs/2505.05408)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Reasoning Models Don't Always Say What They Think](https://arxiv.org/abs/2505.05410)
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [TransProQA: an LLM-based literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/abs/2505.05423)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [Ultra-FineWeb: Efficient Data Filtering and Verification for High-Quality LLM Training Data](https://arxiv.org/abs/2505.05427)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [clem:todd: A Framework for the Systematic Benchmarking of LLM-Based Task-Oriented Dialogue System Realisations](https://arxiv.org/abs/2505.05445)
Token length: 897
Summarized using GPT-3.5-turbo
Append: [UKElectionNarratives: A Dataset of Misleading Narratives Surrounding Recent UK General Elections](https://arxiv.org/abs/2505.05459)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging](https://arxiv.org/abs/2505.05464)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [ComPO: Preference Alignment via Comparison Oracles](https://arxiv.org/abs/2505.05465)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [From Dialect Gaps to Identity Maps: Tackling Variability in Speaker Verification](https://arxiv.org/abs/2505.04629)
Append: [Towards Artificial Intelligence Research Assistant for Expert-Involved Learning](https://arxiv.org/abs/2505.04638)
Append: [When Bad Data Leads to Good Models](https://arxiv.org/abs/2505.04741)
Append: [Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs](https://arxiv.org/abs/2505.04806)
Append: [HiPerRAG: High-Performance Retrieval Augmented Generation for Scientific Insights](https://arxiv.org/abs/2505.04846)
Append: [CRAFT: Cultural Russian-Oriented Dataset Adaptation for Focused Text-to-Image Generation](https://arxiv.org/abs/2505.04851)
Append: [ConCISE: Confidence-guided Compression in Step-by-step Efficient Reasoning](https://arxiv.org/abs/2505.04881)
Append: [SpatialPrompting: Keyframe-driven Zero-Shot Spatial Reasoning with Off-the-Shelf Multimodal Large Language Models](https://arxiv.org/abs/2505.04911)
Append: [Enigme: Generative Text Puzzles for Evaluating Reasoning in Language Models](https://arxiv.org/abs/2505.04914)
Append: [Perception, Reason, Think, and Plan: A Survey on Large Multimodal Reasoning Models](https://arxiv.org/abs/2505.04921)
Append: [T2VTextBench: A Human Evaluation Benchmark for Textual Control in Video Generation Models](https://arxiv.org/abs/2505.04946)
Append: [Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations](https://arxiv.org/abs/2505.04948)
Append: [General Transform: A Unified Framework for Adaptive Transform to Enhance Representations](https://arxiv.org/abs/2505.04969)
Append: [CodeMixBench: Evaluating Large Language Models on Code Generation with Code-Mixed Prompts](https://arxiv.org/abs/2505.05063)
Append: [X-Driver: Explainable Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2505.05098)
Append: [Understanding In-context Learning of Addition via Activation Subspaces](https://arxiv.org/abs/2505.05145)
Append: [Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks](https://arxiv.org/abs/2505.05190)
Append: [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315)
Append: [TokLIP: Marry Visual Tokens to CLIP for Multimodal Comprehension and Generation](https://arxiv.org/abs/2505.05422)
Append: [Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding](https://arxiv.org/abs/2505.05446)
Append: [StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant](https://arxiv.org/abs/2505.05467)
Append: [Position: AI Evaluation Should Learn from How We Test Humans](https://arxiv.org/abs/2306.10512)
Append: [DEGAP: Dual Event-Guided Adaptive Prefixes for Templated-Based Event Argument Extraction with Slot Querying](https://arxiv.org/abs/2405.13325)
Append: [Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon](https://arxiv.org/abs/2406.17746)
Append: [Scaling Synthetic Data Creation with 1,000,000,000 Personas](https://arxiv.org/abs/2406.20094)
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Append: [To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning](https://arxiv.org/abs/2409.12183)
Append: [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/abs/2410.04055)
Append: [WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines](https://arxiv.org/abs/2410.12705)
Append: [E2E-AFG: An End-to-End Model with Adaptive Filtering for Retrieval-Augmented Generation](https://arxiv.org/abs/2411.00437)
Append: [Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models](https://arxiv.org/abs/2411.04996)
Append: [ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning](https://arxiv.org/abs/2501.01031)
Append: [Communicating Activations Between Language Model Agents](https://arxiv.org/abs/2501.14082)
Append: [Safety Evaluation of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2502.11137)
Append: [Drift: Decoding-time Personalized Alignments with Implicit User Preferences](https://arxiv.org/abs/2502.14289)
Append: [Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework](https://arxiv.org/abs/2503.05505)
Append: [Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges](https://arxiv.org/abs/2503.08292)
Append: [Atyaephyra at SemEval-2025 Task 4: Low-Rank Negative Preference Optimization](https://arxiv.org/abs/2503.13690)
Append: [Benchmarking Open-Source Large Language Models on Healthcare Text Classification Tasks](https://arxiv.org/abs/2503.15169)
Append: [Combating Confirmation Bias: A Unified Pseudo-Labeling Framework for Entity Alignment](https://arxiv.org/abs/2307.02075)
Append: [HORAE: A Domain-Agnostic Language for Automated Service Regulation](https://arxiv.org/abs/2406.06600)
Append: [Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding](https://arxiv.org/abs/2409.03757)
Append: [Quantifying Risk Propensities of Large Language Models: Ethical Focus and Bias Detection through Role-Play](https://arxiv.org/abs/2411.08884)
Append: [Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems](https://arxiv.org/abs/2502.18635)
Append: [Re-evaluating Open-ended Evaluation of Large Language Models](https://arxiv.org/abs/2502.20170)
Append: [TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining](https://arxiv.org/abs/2504.02107)
Append: [Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets](https://arxiv.org/abs/2504.19981)
append_entries: 96
Finish: 2025-05-09 04:36:36.837588
------------------------------------------------------
Started: 2025-05-09 06:24:29.845477
Existing_entries: 1096
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2404.04545)
append_entries: 1
Finish: 2025-05-09 06:24:39.843162
------------------------------------------------------
Started: 2025-05-09 08:21:41.261413
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 08:21:41.534147
------------------------------------------------------
Started: 2025-05-09 10:17:26.988457
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 10:17:27.239872
------------------------------------------------------
Started: 2025-05-09 12:32:30.636984
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 12:32:30.888378
------------------------------------------------------
Started: 2025-05-09 14:16:10.248188
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 14:16:10.497113
------------------------------------------------------
Started: 2025-05-09 16:20:05.494920
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 16:20:05.834049
------------------------------------------------------
Started: 2025-05-09 18:22:28.070092
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 18:22:28.352192
------------------------------------------------------
Started: 2025-05-09 20:17:58.394100
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 20:17:58.730561
------------------------------------------------------
Started: 2025-05-09 22:15:19.357945
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-09 22:15:19.637458
------------------------------------------------------
Started: 2025-05-10 01:15:15.764777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 01:15:16.048224
------------------------------------------------------
Started: 2025-05-10 03:01:31.343550
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 03:01:31.690190
------------------------------------------------------
Started: 2025-05-10 04:18:20.956078
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 04:18:21.014676
------------------------------------------------------
Started: 2025-05-10 06:20:29.195808
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 06:20:29.287884
------------------------------------------------------
Started: 2025-05-10 08:19:00.888975
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 08:19:00.955350
------------------------------------------------------
Started: 2025-05-10 10:15:51.112993
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 10:15:51.172193
------------------------------------------------------
Started: 2025-05-10 12:29:21.996055
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 12:29:22.090206
------------------------------------------------------
Started: 2025-05-10 14:13:36.155397
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 14:13:36.265321
------------------------------------------------------
Started: 2025-05-10 16:18:20.922105
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 16:18:20.986543
------------------------------------------------------
Started: 2025-05-10 18:19:50.477117
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 18:19:50.552951
------------------------------------------------------
Started: 2025-05-10 20:16:11.045855
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 20:16:11.106975
------------------------------------------------------
Started: 2025-05-10 22:14:06.690930
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-10 22:14:06.823538
------------------------------------------------------
Started: 2025-05-11 01:22:45.037232
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 01:22:45.097800
------------------------------------------------------
Started: 2025-05-11 03:12:35.847149
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 03:12:35.963953
------------------------------------------------------
Started: 2025-05-11 04:19:17.590073
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 04:19:17.741930
------------------------------------------------------
Started: 2025-05-11 06:21:36.442033
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 06:21:36.551911
------------------------------------------------------
Started: 2025-05-11 08:19:26.124793
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 08:19:26.194978
------------------------------------------------------
Started: 2025-05-11 10:15:21.828016
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 10:15:21.895291
------------------------------------------------------
Started: 2025-05-11 12:29:50.747601
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 12:29:50.874298
------------------------------------------------------
Started: 2025-05-11 14:14:05.915839
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 14:14:06.026804
------------------------------------------------------
Started: 2025-05-11 16:18:10.414755
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 16:18:10.479890
------------------------------------------------------
Started: 2025-05-11 18:20:18.644728
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 18:20:18.706304
------------------------------------------------------
Started: 2025-05-11 20:16:23.343317
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 20:16:23.401726
------------------------------------------------------
Started: 2025-05-11 22:14:52.226102
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-11 22:14:52.308179
------------------------------------------------------
Started: 2025-05-12 01:20:55.561761
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 01:20:55.681904
------------------------------------------------------
Started: 2025-05-12 03:11:58.255368
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 03:11:58.318219
------------------------------------------------------
Started: 2025-05-12 04:25:06.023151
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/abs/2505.05583)
Token length: 692
Summarized using GPT-3.5-turbo
Append: [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/abs/2505.05648)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/abs/2505.05687)
Token length: 1416
Summarized using GPT-3.5-turbo
Append: [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/abs/2505.05704)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/abs/2505.05714)
Token length: 1592
Summarized using GPT-3.5-turbo
Append: [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)
Token length: 1881
Summarized using GPT-3.5-turbo
Append: [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/abs/2505.05772)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)
Token length: 1354
Summarized using GPT-3.5-turbo
Append: [Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI](https://arxiv.org/abs/2505.05864)
Token length: 776
Summarized using GPT-3.5-turbo
Append: [Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2](https://arxiv.org/abs/2505.05946)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [Summarisation of German Judgments in conjunction with a Class-based Evaluation](https://arxiv.org/abs/2505.05947)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [NeoQA: Evidence-based Question Answering with Generated News Events](https://arxiv.org/abs/2505.05949)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models](https://arxiv.org/abs/2505.05970)
Token length: 1689
Summarized using GPT-3.5-turbo
Append: [An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition](https://arxiv.org/abs/2505.05973)
Token length: 882
Summarized using GPT-3.5-turbo
Append: [Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models](https://arxiv.org/abs/2505.06004)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective](https://arxiv.org/abs/2505.06010)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation](https://arxiv.org/abs/2505.06027)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/abs/2505.06046)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax](https://arxiv.org/abs/2505.06062)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/abs/2505.06110)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies](https://arxiv.org/abs/2505.06145)
Token length: 984
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 840
Summarized using GPT-3.5-turbo
Append: [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)
Token length: 1915
Summarized using GPT-3.5-turbo
Append: [Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework](https://arxiv.org/abs/2505.06151)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Token length: 1392
Summarized using GPT-3.5-turbo
Append: [Adaptive Stress Testing Black-Box LLM Planners](https://arxiv.org/abs/2505.05665)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Prompted Meta-Learning for Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2505.05684)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [Multimodal Integrated Knowledge Transfer to Large Language Models through Preference Optimization with Biomedical Applications](https://arxiv.org/abs/2505.05736)
Token length: 1576
Summarized using GPT-3.5-turbo
Append: [Harnessing LLMs Explanations to Boost Surrogate Models in Tabular Data Classification](https://arxiv.org/abs/2505.05744)
Token length: 1094
Summarized using GPT-3.5-turbo
Append: [BMMDetect: A Multimodal Deep Learning Framework for Comprehensive Biomedical Misconduct Detection](https://arxiv.org/abs/2505.05763)
Token length: 953
Summarized using GPT-3.5-turbo
Append: [An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers](https://arxiv.org/abs/2505.05828)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Evolutionary ecology of words](https://arxiv.org/abs/2505.05863)
Token length: 986
Summarized using GPT-3.5-turbo
Append: [Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification](https://arxiv.org/abs/2505.06032)
Token length: 1926
Summarized using GPT-3.5-turbo
Append: [Differentiating Emigration from Return Migration of Scholars Using Name-Based Nationality Detection Models](https://arxiv.org/abs/2505.06107)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [From Millions of Tweets to Actionable Insights: Leveraging LLMs for User Profiling](https://arxiv.org/abs/2505.06184)
Token length: 1000
Summarized using GPT-3.5-turbo
Append: [Neuro-Symbolic Concepts](https://arxiv.org/abs/2505.06191)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [PART: Pre-trained Authorship Representation Transformer](https://arxiv.org/abs/2209.15373)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [Talking Heads: Understanding Inter-layer Communication in Transformer Language Models](https://arxiv.org/abs/2406.09519)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?](https://arxiv.org/abs/2407.11963)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget](https://arxiv.org/abs/2408.00103)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits](https://arxiv.org/abs/2410.18234)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2411.11053)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes](https://arxiv.org/abs/2501.12106)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models](https://arxiv.org/abs/2501.14851)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought](https://arxiv.org/abs/2501.16154)
Token length: 1116
Summarized using GPT-3.5-turbo
Append: [Phonetic accommodation and inhibition in a dynamic neural field model](https://arxiv.org/abs/2502.01210)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs](https://arxiv.org/abs/2502.04134)
Token length: 1143
Summarized using GPT-3.5-turbo
Append: [k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids](https://arxiv.org/abs/2502.09667)
Append: [Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](https://arxiv.org/abs/2502.20364)
Append: [ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach](https://arxiv.org/abs/2503.17460)
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Append: [Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security](https://arxiv.org/abs/2406.09831)
Append: [Rec-R1: Bridging Generative Large Language Models and User-Centric Recommendation Systems via Reinforcement Learning](https://arxiv.org/abs/2503.24289)
append_entries: 55
Finish: 2025-05-12 04:27:05.983439
------------------------------------------------------
Started: 2025-05-12 06:25:09.140121
Existing_entries: 1055
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 06:25:09.327037
------------------------------------------------------
Started: 2025-05-12 08:23:08.138885
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 08:23:08.403509
------------------------------------------------------
Started: 2025-05-12 10:18:49.206756
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 10:18:49.430031
------------------------------------------------------
Started: 2025-05-12 12:34:24.594379
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 12:34:24.815123
------------------------------------------------------
Started: 2025-05-12 14:17:23.442187
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 14:17:23.709236
------------------------------------------------------
Started: 2025-05-12 16:21:13.967442
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 16:21:14.155998
------------------------------------------------------
Started: 2025-05-12 18:22:49.634977
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 18:22:49.845269
------------------------------------------------------
Started: 2025-05-12 20:18:31.152441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 20:18:31.345055
------------------------------------------------------
Started: 2025-05-12 22:15:59.147892
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-12 22:15:59.364834
------------------------------------------------------
Started: 2025-05-13 01:19:19.194204
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 01:19:19.410942
------------------------------------------------------
Started: 2025-05-13 03:09:51.376951
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 03:09:51.583310
------------------------------------------------------
Started: 2025-05-13 04:25:03.357222
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1611
Summarized using GPT-3.5-turbo
Append: [ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents](https://arxiv.org/abs/2505.06416)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [Is your multimodal large language model a good science tutor?](https://arxiv.org/abs/2505.06418)
Token length: 593
Summarized using GPT-3.5-turbo
Append: [xGen-small Technical Report](https://arxiv.org/abs/2505.06496)
Token length: 1373
Summarized using GPT-3.5-turbo
Append: [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback](https://arxiv.org/abs/2505.06548)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/abs/2505.06552)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
Token length: 800
Summarized using GPT-3.5-turbo
Append: [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/abs/2505.06591)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation](https://arxiv.org/abs/2505.06594)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation](https://arxiv.org/abs/2505.06599)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Using External knowledge to Enhanced PLM for Semantic Matching](https://arxiv.org/abs/2505.06605)
Token length: 1753
Summarized using GPT-3.5-turbo
Append: [Boosting Neural Language Inference via Cascaded Interactive Reasoning](https://arxiv.org/abs/2505.06607)
Token length: 948
Summarized using GPT-3.5-turbo
Append: [The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification](https://arxiv.org/abs/2505.06624)
Token length: 1755
Summarized using GPT-3.5-turbo
Append: [Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis](https://arxiv.org/abs/2505.06630)
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models](https://arxiv.org/abs/2505.06633)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models](https://arxiv.org/abs/2505.06660)
Token length: 1090
Summarized using GPT-3.5-turbo
Append: [Enhancing BERTopic with Intermediate Layer Representations](https://arxiv.org/abs/2505.06696)
Token length: 1861
Summarized using GPT-3.5-turbo
Append: [From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback](https://arxiv.org/abs/2505.06698)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK](https://arxiv.org/abs/2505.06782)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting](https://arxiv.org/abs/2505.06862)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method](https://arxiv.org/abs/2505.06889)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation](https://arxiv.org/abs/2505.06904)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [The Distracting Effect: Understanding Irrelevant Passages in RAG](https://arxiv.org/abs/2505.06914)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire](https://arxiv.org/abs/2505.06974)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
Token length: 1605
Summarized using GPT-3.5-turbo
Append: [HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling](https://arxiv.org/abs/2505.07157)
Token length: 1922
Summarized using GPT-3.5-turbo
Append: [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](https://arxiv.org/abs/2505.07161)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification](https://arxiv.org/abs/2505.07162)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs](https://arxiv.org/abs/2505.07184)
Token length: 945
Summarized using GPT-3.5-turbo
Append: [On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud](https://arxiv.org/abs/2505.07202)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030](https://arxiv.org/abs/2505.07205)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.07233)
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/abs/2505.07247)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [No Query, No Access](https://arxiv.org/abs/2505.07258)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [On the Robustness of Reward Models for Language Model Alignment](https://arxiv.org/abs/2505.07271)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/abs/2505.07289)
Token length: 1478
Summarized using GPT-3.5-turbo
Append: [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/abs/2505.07313)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines](https://arxiv.org/abs/2505.07345)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles](https://arxiv.org/abs/2505.07409)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](https://arxiv.org/abs/2505.07416)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights](https://arxiv.org/abs/2505.07430)
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge](https://arxiv.org/abs/2505.07440)
Token length: 913
Summarized using GPT-3.5-turbo
Append: [Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions](https://arxiv.org/abs/2505.07495)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution](https://arxiv.org/abs/2505.07512)
Token length: 1640
Summarized using GPT-3.5-turbo
Append: [SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion](https://arxiv.org/abs/2505.07528)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models](https://arxiv.org/abs/2505.07591)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596)
Token length: 1688
Summarized using GPT-3.5-turbo
Append: [Characterizing the Investigative Methods of Fictional Detectives with Large Language Models](https://arxiv.org/abs/2505.07601)
Append: [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)
Append: [Concept-Level Explainability for Auditing & Steering LLM Responses](https://arxiv.org/abs/2505.07610)
Append: [Chronocept: Instilling a Sense of Time in Machines](https://arxiv.org/abs/2505.07637)
Append: [JobHop: A Large-Scale Dataset of Career Trajectories](https://arxiv.org/abs/2505.07653)
Append: [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/abs/2505.07659)
Append: [Benchmarking Retrieval-Augmented Generation for Chemistry](https://arxiv.org/abs/2505.07671)
Append: [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/abs/2505.07672)
Append: [Codifying Character Logic in Role-Playing](https://arxiv.org/abs/2505.07705)
Append: [Spoken Language Understanding on Unseen Tasks With In-Context Learning](https://arxiv.org/abs/2505.07731)
Append: [Must Read: A Systematic Survey of Computational Persuasion](https://arxiv.org/abs/2505.07775)
Append: [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/abs/2505.07784)
Append: [Learning from Peers in Reasoning Models](https://arxiv.org/abs/2505.07787)
Append: [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)
Append: [A Comparative Analysis of Static Word Embeddings for Hungarian](https://arxiv.org/abs/2505.07809)
Append: [Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction](https://arxiv.org/abs/2505.06297)
Append: [AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity](https://arxiv.org/abs/2505.06313)
Append: [Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution](https://arxiv.org/abs/2505.06320)
Append: [Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations](https://arxiv.org/abs/2505.06653)
Append: [Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation](https://arxiv.org/abs/2505.06803)
Append: [Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge](https://arxiv.org/abs/2505.06814)
Append: [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843)
Append: [Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration](https://arxiv.org/abs/2505.06898)
Append: [A digital perspective on the role of a stemma in material-philological transmission studies](https://arxiv.org/abs/2505.06938)
Append: [Web Page Classification using LLMs for Crawling Support](https://arxiv.org/abs/2505.06972)
Append: [Towards the Three-Phase Dynamics of Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
Append: [LLM-Augmented Chemical Synthesis and Design Decision Programs](https://arxiv.org/abs/2505.07027)
Append: [Reassessing Large Language Model Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2505.07155)
Append: [Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition](https://arxiv.org/abs/2505.07166)
Append: [One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models](https://arxiv.org/abs/2505.07167)
Append: [Securing Genomic Data Against Inference Attacks in Federated Learning Environments](https://arxiv.org/abs/2505.07188)
Append: [Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge](https://arxiv.org/abs/2505.07365)
Append: [A Survey on Collaborative Mechanisms Between Large and Small Language Models](https://arxiv.org/abs/2505.07460)
Append: [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/abs/2505.07558)
Append: [Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images](https://arxiv.org/abs/2505.07704)
Append: [Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding](https://arxiv.org/abs/2505.07768)
Append: [Clickbait Detection via Large Language Models](https://arxiv.org/abs/2306.09597)
Append: [Towards Understanding Sycophancy in Language Models](https://arxiv.org/abs/2310.13548)
Append: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)
Append: [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/abs/2405.06691)
Append: [A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis](https://arxiv.org/abs/2406.15163)
Append: [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://arxiv.org/abs/2406.17692)
Append: [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)
Append: [MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge](https://arxiv.org/abs/2408.05497)
Append: [Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2408.09701)
Append: [Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy](https://arxiv.org/abs/2409.13746)
Append: [Endless Jailbreaks with Bijection Learning](https://arxiv.org/abs/2410.01294)
Append: [Isolated Causal Effects of Natural Language](https://arxiv.org/abs/2410.14812)
Append: [Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions](https://arxiv.org/abs/2410.18966)
Append: [Evaluating Creative Short Story Generation in Humans and Large Language Models](https://arxiv.org/abs/2411.02316)
Append: [The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models](https://arxiv.org/abs/2411.03700)
Append: [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)
Append: [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](https://arxiv.org/abs/2411.15100)
Append: [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/abs/2412.05342)
Append: [Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning](https://arxiv.org/abs/2412.08587)
Append: [Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain](https://arxiv.org/abs/2412.20309)
Append: [MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments](https://arxiv.org/abs/2501.01652)
Append: [Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective](https://arxiv.org/abs/2501.11110)
Append: [Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models](https://arxiv.org/abs/2501.13428)
Append: [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/abs/2502.01568)
Append: [VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/abs/2502.12084)
Append: [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)
Append: [SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking](https://arxiv.org/abs/2503.00955)
Append: [HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition](https://arxiv.org/abs/2503.01217)
Append: [Can (A)I Change Your Mind?](https://arxiv.org/abs/2503.01844)
Append: [NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT](https://arxiv.org/abs/2503.01921)
Append: [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/abs/2503.03122)
Append: [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/abs/2503.10354)
Append: [From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs](https://arxiv.org/abs/2504.13471)
Append: [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)
Append: [ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs](https://arxiv.org/abs/2504.16394)
Append: [Semantic and Expressive Variation in Image Captions Across Languages](https://arxiv.org/abs/2310.14356)
Append: [Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems](https://arxiv.org/abs/2311.11796)
Append: [AIOS: LLM Agent Operating System](https://arxiv.org/abs/2403.16971)
Append: [EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization](https://arxiv.org/abs/2405.15189)
Append: [MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning](https://arxiv.org/abs/2406.06620)
Append: [CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models](https://arxiv.org/abs/2408.14419)
Append: [Self-Data Distillation for Recovering Quality in Pruned Large Language Models](https://arxiv.org/abs/2410.09982)
Append: [Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning](https://arxiv.org/abs/2410.13439)
Append: [Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-based Prompt Injection Attacks via the Fine-Tuning Interface](https://arxiv.org/abs/2501.09798)
Append: [Training and Evaluating with Human Label Variation: An Empirical Study](https://arxiv.org/abs/2502.01891)
Append: [Integrating Expert Knowledge into Logical Programs via LLMs](https://arxiv.org/abs/2502.12275)
Append: [A Statistical Case Against Empirical Human-AI Alignment](https://arxiv.org/abs/2502.14581)
Append: [SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models](https://arxiv.org/abs/2502.14908)
Append: [I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?](https://arxiv.org/abs/2503.08980)
Append: [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382)
Append: [Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines](https://arxiv.org/abs/2504.07840)
append_entries: 136
Finish: 2025-05-13 04:27:34.565166
------------------------------------------------------
Started: 2025-05-13 06:24:19.088746
Existing_entries: 1136
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 06:24:19.440681
------------------------------------------------------
Started: 2025-05-13 08:22:56.297543
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 08:22:56.697694
------------------------------------------------------
Started: 2025-05-13 10:18:16.790316
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 10:18:17.111899
------------------------------------------------------
Started: 2025-05-13 12:34:53.868107
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 12:34:54.181889
------------------------------------------------------
Started: 2025-05-13 14:17:17.905696
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 14:17:18.309672
------------------------------------------------------
Started: 2025-05-13 16:21:12.955734
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 16:21:13.273154
------------------------------------------------------
Started: 2025-05-13 18:23:32.109942
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 18:23:32.452073
------------------------------------------------------
Started: 2025-05-13 20:18:24.456400
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 20:18:24.768735
------------------------------------------------------
Started: 2025-05-13 22:15:57.896985
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-13 22:15:58.261927
------------------------------------------------------
Started: 2025-05-14 01:18:02.163443
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 01:18:02.519141
------------------------------------------------------
Started: 2025-05-14 03:08:00.942377
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 03:08:01.312006
------------------------------------------------------
Started: 2025-05-14 04:26:11.661705
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 818
Summarized using GPT-3.5-turbo
Append: [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/abs/2505.07831)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/abs/2505.07850)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/abs/2505.07852)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/abs/2505.07853)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/abs/2505.07856)
Token length: 1758
Summarized using GPT-3.5-turbo
Append: [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/abs/2505.07857)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Scaling Laws for Speculative Decoding](https://arxiv.org/abs/2505.07858)
Token length: 1060
Summarized using GPT-3.5-turbo
Append: [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
Token length: 1064
Summarized using GPT-3.5-turbo
Append: [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)
Token length: 681
Summarized using GPT-3.5-turbo
Append: [Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition](https://arxiv.org/abs/2505.07862)
Token length: 1723
Summarized using GPT-3.5-turbo
Append: [QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](https://arxiv.org/abs/2505.07863)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/abs/2505.07870)
Token length: 1968
Summarized using GPT-3.5-turbo
Append: [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/abs/2505.07871)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [The Sound of Populism: Distinct Linguistic Features Across Populist Variants](https://arxiv.org/abs/2505.07874)
Token length: 1341
Summarized using GPT-3.5-turbo
Append: [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/abs/2505.07883)
Token length: 1836
Summarized using GPT-3.5-turbo
Append: [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/abs/2505.07884)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/abs/2505.07886)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/abs/2505.07888)
Token length: 1844
Summarized using GPT-3.5-turbo
Append: [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/abs/2505.07899)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/abs/2505.07903)
Token length: 1632
Summarized using GPT-3.5-turbo
Append: [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/abs/2505.07920)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration](https://arxiv.org/abs/2505.07980)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Large Language Models and Arabic Content: A Review](https://arxiv.org/abs/2505.08004)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/abs/2505.08037)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)
Token length: 758
Summarized using GPT-3.5-turbo
Append: [HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method](https://arxiv.org/abs/2505.08058)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/abs/2505.08106)
Token length: 1302
Summarized using GPT-3.5-turbo
Append: [Putting It All into Context: Simplifying Agents with LCLMs](https://arxiv.org/abs/2505.08120)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/abs/2505.08130)
Token length: 1953
Summarized using GPT-3.5-turbo
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/abs/2505.08168)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/abs/2505.08261)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow](https://arxiv.org/abs/2505.08303)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [On the Geometry of Semantics in Next-token Prediction](https://arxiv.org/abs/2505.08348)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Towards Contamination Resistant Benchmarks](https://arxiv.org/abs/2505.08389)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/abs/2505.08392)
Token length: 1627
Summarized using GPT-3.5-turbo
Append: [TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers](https://arxiv.org/abs/2505.08402)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)
Token length: 1060
Summarized using GPT-3.5-turbo
Append: [A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court](https://arxiv.org/abs/2505.08439)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/abs/2505.08450)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
Append: [Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions](https://arxiv.org/abs/2505.08464)
Append: [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)
Append: [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)
Append: [Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding](https://arxiv.org/abs/2505.08504)
Append: [Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation](https://arxiv.org/abs/2505.08546)
Append: [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)
Append: [Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models](https://arxiv.org/abs/2505.08590)
Append: [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](https://arxiv.org/abs/2505.08600)
Append: [Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing](https://arxiv.org/abs/2505.08651)
Append: [Revealing economic facts: LLMs know more than they say](https://arxiv.org/abs/2505.08662)
Append: [Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08690)
Append: [NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context](https://arxiv.org/abs/2505.08734)
Append: [Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies](https://arxiv.org/abs/2505.08739)
Append: [AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models](https://arxiv.org/abs/2505.08750)
Append: [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/abs/2505.08751)
Append: [HealthBench: Evaluating Large Language Models Towards Improved Human Health](https://arxiv.org/abs/2505.08775)
Append: [Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding](https://arxiv.org/abs/2505.07864)
Append: [CellVerse: Do Large Language Models Really Understand Cell Biology?](https://arxiv.org/abs/2505.07865)
Append: [Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach](https://arxiv.org/abs/2505.07902)
Append: [A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny](https://arxiv.org/abs/2505.07908)
Append: [SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts](https://arxiv.org/abs/2505.07912)
Append: [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
Append: [Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders](https://arxiv.org/abs/2505.08080)
Append: [Large Language Models for Computer-Aided Design: A Survey](https://arxiv.org/abs/2505.08137)
Append: [A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem](https://arxiv.org/abs/2505.08148)
Append: [Not that Groove: Zero-Shot Symbolic Music Editing](https://arxiv.org/abs/2505.08203)
Append: [Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency](https://arxiv.org/abs/2505.08445)
Append: [Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models](https://arxiv.org/abs/2505.08622)
Append: [TRAIL: Trace Reasoning and Agentic Issue Localization](https://arxiv.org/abs/2505.08638)
Append: [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
Append: [Memorization-Compression Cycles Improve Generalization](https://arxiv.org/abs/2505.08727)
Append: [CodePDE: An Inference Framework for LLM-driven PDE Solver Generation](https://arxiv.org/abs/2505.08783)
Append: [Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions](https://arxiv.org/abs/2408.06787)
Append: [Studying the Effects of Collaboration in Interactive Theme Discovery Systems](https://arxiv.org/abs/2408.09030)
Append: [From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks](https://arxiv.org/abs/2409.04168)
Append: [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/abs/2410.06205)
Append: [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/abs/2410.07002)
Append: [No Preference Left Behind: Group Distributional Preference Optimization](https://arxiv.org/abs/2412.20299)
Append: [FutureVision: A methodology for the investigation of future cognition](https://arxiv.org/abs/2502.01597)
Append: [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/abs/2502.04066)
Append: [Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data](https://arxiv.org/abs/2502.18679)
Append: [Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents](https://arxiv.org/abs/2503.04830)
Append: [CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning](https://arxiv.org/abs/2503.13517)
Append: [Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes](https://arxiv.org/abs/2503.24027)
Append: [Why do LLMs attend to the first token?](https://arxiv.org/abs/2504.02732)
Append: [AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening](https://arxiv.org/abs/2504.02870)
Append: [Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models](https://arxiv.org/abs/2504.04717)
Append: [DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning](https://arxiv.org/abs/2504.07128)
Append: [LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation](https://arxiv.org/abs/2504.16408)
Append: [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)
Append: [MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation](https://arxiv.org/abs/2410.13757)
Append: [2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining](https://arxiv.org/abs/2501.00958)
Append: [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
Append: [Vision-Language Models Do Not Understand Negation](https://arxiv.org/abs/2501.09425)
Append: [Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?](https://arxiv.org/abs/2501.15857)
Append: [Adaptive Integrated Layered Attention (AILA)](https://arxiv.org/abs/2503.22742)
Append: [Efficient Adaptation For Remote Sensing Visual Grounding](https://arxiv.org/abs/2503.23083)
Append: [Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs](https://arxiv.org/abs/2504.13989)
Append: [Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction](https://arxiv.org/abs/2504.14361)
append_entries: 109
Finish: 2025-05-14 04:28:19.156852
------------------------------------------------------
Started: 2025-05-14 06:24:10.695034
Existing_entries: 1109
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 06:24:10.983477
------------------------------------------------------
Started: 2025-05-14 08:22:14.629183
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 08:22:14.905203
------------------------------------------------------
Started: 2025-05-14 10:17:47.481171
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 10:17:47.781346
------------------------------------------------------
Started: 2025-05-14 12:33:37.080492
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 12:33:37.355053
------------------------------------------------------
Started: 2025-05-14 14:16:51.916410
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 14:16:52.262249
------------------------------------------------------
Started: 2025-05-14 16:20:32.820366
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 16:20:33.180875
------------------------------------------------------
Started: 2025-05-14 18:21:00.601168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 18:21:00.889546
------------------------------------------------------
Started: 2025-05-14 20:15:25.088274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 20:15:25.384824
------------------------------------------------------
Started: 2025-05-14 22:13:34.172426
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-14 22:13:34.461730
------------------------------------------------------
Started: 2025-05-15 01:16:13.662011
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 01:16:14.030464
------------------------------------------------------
Started: 2025-05-15 03:07:33.869065
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 03:07:34.175994
------------------------------------------------------
Started: 2025-05-15 04:23:42.465084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)
Token length: 1271
Summarized using GPT-3.5-turbo
Append: [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)
Token length: 1005
Summarized using GPT-3.5-turbo
Append: [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)
Token length: 852
Summarized using GPT-3.5-turbo
Append: [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)
Token length: 1804
Summarized using GPT-3.5-turbo
Append: [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)
Token length: 1830
Summarized using GPT-3.5-turbo
Append: [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures](https://arxiv.org/abs/2505.08795)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits](https://arxiv.org/abs/2505.08823)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries](https://arxiv.org/abs/2505.08842)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora](https://arxiv.org/abs/2505.08905)
Token length: 721
Summarized using GPT-3.5-turbo
Append: [Behind Maya: Building a Multilingual Vision Language Model](https://arxiv.org/abs/2505.08910)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers](https://arxiv.org/abs/2505.08941)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training](https://arxiv.org/abs/2505.08971)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Automated Meta Prompt Engineering for Alignment with the Theory of Mind](https://arxiv.org/abs/2505.09024)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification](https://arxiv.org/abs/2505.09031)
Token length: 793
Summarized using GPT-3.5-turbo
Append: [Ornithologist: Towards Trustworthy "Reasoning" about Central Bank Communications](https://arxiv.org/abs/2505.09083)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases](https://arxiv.org/abs/2505.09246)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios](https://arxiv.org/abs/2505.09436)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors](https://arxiv.org/abs/2505.09610)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?](https://arxiv.org/abs/2505.09614)
Token length: 828
Summarized using GPT-3.5-turbo
Append: [LLM-based NLG Evaluation: Current Status and Challenges](https://arxiv.org/abs/2402.01383)
Token length: 1447
Summarized using GPT-3.5-turbo
Append: [Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model](https://arxiv.org/abs/2404.03080)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering](https://arxiv.org/abs/2410.04526)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs](https://arxiv.org/abs/2411.09116)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples](https://arxiv.org/abs/2502.09650)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [PropNet: a White-Box and Human-Like Network for Sentence Representation](https://arxiv.org/abs/2502.10725)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference](https://arxiv.org/abs/2503.10652)
Token length: 1093
Summarized using GPT-3.5-turbo
Append: [Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark](https://arxiv.org/abs/2503.17599)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](https://arxiv.org/abs/2503.21696)
Token length: 991
Summarized using GPT-3.5-turbo
Append: [Is analogy enough to draw novel adjective-noun inferences?](https://arxiv.org/abs/2503.24293)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks](https://arxiv.org/abs/2411.03343)
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [FAS: Fast ANN-SNN Conversion for Spiking Large Language Models](https://arxiv.org/abs/2502.04405)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [InductionBench: LLMs Fail in the Simplest Complexity Class](https://arxiv.org/abs/2502.15823)
Token length: 1280
Summarized using GPT-3.5-turbo
Append: [An Analytical Emotion Framework of Rumour Threads on Social Media](https://arxiv.org/abs/2502.16560)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering](https://arxiv.org/abs/2503.11197)
append_entries: 46
Finish: 2025-05-15 04:25:30.181142
------------------------------------------------------
Started: 2025-05-15 06:24:21.829988
Existing_entries: 1046
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 06:24:21.987811
------------------------------------------------------
Started: 2025-05-15 08:22:29.976329
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 08:22:30.158590
------------------------------------------------------
Started: 2025-05-15 10:18:15.633083
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 10:18:15.809404
------------------------------------------------------
Started: 2025-05-15 12:33:45.423237
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 12:33:45.579638
------------------------------------------------------
Started: 2025-05-15 14:17:05.928336
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 14:17:06.101991
------------------------------------------------------
Started: 2025-05-15 16:20:49.317551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 16:20:49.475292
------------------------------------------------------
Started: 2025-05-15 18:22:53.030341
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 18:22:53.214931
------------------------------------------------------
Started: 2025-05-15 20:18:17.266258
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 20:18:17.436020
------------------------------------------------------
Started: 2025-05-15 22:15:13.908506
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-15 22:15:14.073473
------------------------------------------------------
Started: 2025-05-16 01:19:05.234892
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 01:19:05.386928
------------------------------------------------------
Started: 2025-05-16 03:10:25.697402
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 03:10:25.902579
------------------------------------------------------
Started: 2025-05-16 04:25:39.740599
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 841
Summarized using GPT-3.5-turbo
Append: [Next Word Suggestion using Graph Neural Network](https://arxiv.org/abs/2505.09649)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
Token length: 1406
Summarized using GPT-3.5-turbo
Append: [System Prompt Optimization with Meta-Learning](https://arxiv.org/abs/2505.09666)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [VeriFact: Enhancing Long-Form Factuality Evaluation with Refined Fact Extraction and Reference Facts](https://arxiv.org/abs/2505.09701)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)
Token length: 1964
Summarized using GPT-3.5-turbo
Append: [Achieving Tokenizer Flexibility in Language Models through Heuristic Adaptation and Supertoken Learning](https://arxiv.org/abs/2505.09738)
Token length: 1967
Summarized using GPT-3.5-turbo
Append: [Automated Detection of Clinical Entities in Lung and Breast Cancer Reports Using NLP Techniques](https://arxiv.org/abs/2505.09794)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Exploring the generalization of LLM truth directions on conversational formats](https://arxiv.org/abs/2505.09807)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Know Conflict? Investigating Parametric vs. Non-Parametric Knowledge of LLMs for Conflict Forecasting](https://arxiv.org/abs/2505.09852)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Crossing Borders Without Crossing Boundaries: How Sociolinguistic Awareness Can Optimize User Engagement with Localized Spanish AI Models Across Hispanophone Countries](https://arxiv.org/abs/2505.09902)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Personalizing Large Language Models using Retrieval Augmented Generation and Knowledge Graph](https://arxiv.org/abs/2505.09945)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs](https://arxiv.org/abs/2505.10013)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance Multi-Document QA Capability](https://arxiv.org/abs/2505.10063)
Token length: 1558
Summarized using GPT-3.5-turbo
Append: [Dark LLMs: The Growing Threat of Unaligned AI Models](https://arxiv.org/abs/2505.10066)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [XRAG: Cross-lingual Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10089)
Token length: 919
Summarized using GPT-3.5-turbo
Append: [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [GE-Chat: A Graph Enhanced RAG Framework for Evidential Response Generation of LLMs](https://arxiv.org/abs/2505.10143)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with Synthetic Data for LLM Reasoning](https://arxiv.org/abs/2505.10182)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [The CoT Encyclopedia: Analyzing, Predicting, and Controlling how a Reasoning Model will Think](https://arxiv.org/abs/2505.10185)
Token length: 1537
Summarized using GPT-3.5-turbo
Append: [VQ-Logits: Compressing the Output Bottleneck of Large Language Models via Vector Quantized Logits](https://arxiv.org/abs/2505.10202)
Token length: 1156
Summarized using GPT-3.5-turbo
Append: [RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable Reward](https://arxiv.org/abs/2505.10218)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [Comparing LLM Text Annotation Skills: A Study on Human Rights Violations in Social Media Data](https://arxiv.org/abs/2505.10260)
Token length: 587
Summarized using GPT-3.5-turbo
Append: [The Evolving Landscape of Generative Large Language Models and Traditional Natural Language Processing in Medicine](https://arxiv.org/abs/2505.10261)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [From Questions to Clinical Recommendations: Large Language Models Driving Evidence-Based Clinical Decision Making](https://arxiv.org/abs/2505.10282)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [Coherent Language Reconstruction from Brain Recordings with Flexible Multi-Modal Input Stimuli](https://arxiv.org/abs/2505.10356)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)
Token length: 1501
Summarized using GPT-3.5-turbo
Append: [Rethinking Repetition Problems of LLMs in Code Generation](https://arxiv.org/abs/2505.10402)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [Are LLM-generated plain language summaries truly understandable? A large-scale crowdsourced evaluation](https://arxiv.org/abs/2505.10409)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [Hierarchical Document Refinement for Long-context Retrieval-augmented Generation](https://arxiv.org/abs/2505.10413)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning](https://arxiv.org/abs/2505.10493)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [Can You Really Trust Code Copilots? Evaluating Large Language Models from a Code Security Perspective](https://arxiv.org/abs/2505.10494)
Token length: 1829
Summarized using GPT-3.5-turbo
Append: [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [Multi-Token Prediction Needs Registers](https://arxiv.org/abs/2505.10518)
Token length: 1643
Summarized using GPT-3.5-turbo
Append: [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
Token length: 1961
Summarized using GPT-3.5-turbo
Append: [Tales of the 2025 Los Angeles Fire: Hotwash for Public Health Concerns in Reddit via LLM-Enhanced Topic Modeling](https://arxiv.org/abs/2505.09665)
Token length: 1130
Summarized using GPT-3.5-turbo
Append: [A Survey on Large Language Models in Multimodal Recommender Systems](https://arxiv.org/abs/2505.09777)
Token length: 1847
Summarized using GPT-3.5-turbo
Append: [Predictability Shapes Adaptation: An Evolutionary Perspective on Modes of Learning in Transformers](https://arxiv.org/abs/2505.09855)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Comparing Exploration-Exploitation Strategies of LLMs and Humans: Insights from Standard Multi-armed Bandit Tasks](https://arxiv.org/abs/2505.09901)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization](https://arxiv.org/abs/2505.09921)
Token length: 1886
Summarized using GPT-3.5-turbo
Append: [Advanced Crash Causation Analysis for Freeway Safety: A Large Language Model Approach to Identifying Key Contributing Factors](https://arxiv.org/abs/2505.09949)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI](https://arxiv.org/abs/2505.10093)
Append: [Learning Virtual Machine Scheduling in Cloud Computing through Language Agents](https://arxiv.org/abs/2505.10117)
Append: [Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering](https://arxiv.org/abs/2505.10118)
Append: [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
Append: [On the Interplay of Human-AI Alignment,Fairness, and Performance Trade-offs in Medical Imaging](https://arxiv.org/abs/2505.10231)
Append: [StoryReasoning Dataset: Using Chain-of-Thought for Scene Understanding and Grounded Story Generation](https://arxiv.org/abs/2505.10292)
Append: [Superposition Yields Robust Neural Scaling](https://arxiv.org/abs/2505.10465)
Append: [Parallel Scaling Law for Language Models](https://arxiv.org/abs/2505.10475)
Append: [RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs](https://arxiv.org/abs/2505.10495)
Append: [MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models](https://arxiv.org/abs/2505.10526)
Append: [Towards a Deeper Understanding of Reasoning Capabilities in Large Language Models](https://arxiv.org/abs/2505.10543)
Append: [MathCoder-VL: Bridging Vision and Code for Enhanced Multimodal Mathematical Reasoning](https://arxiv.org/abs/2505.10557)
Append: [Temporal Scaling Law for Large Language Models](https://arxiv.org/abs/2404.17785)
Append: [The Mosaic Memory of Large Language Models](https://arxiv.org/abs/2405.15523)
Append: [Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization](https://arxiv.org/abs/2405.17067)
Append: [RoBERTa-BiLSTM: A Context-Aware Hybrid Model for Sentiment Analysis](https://arxiv.org/abs/2406.00367)
Append: [PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling](https://arxiv.org/abs/2406.02069)
Append: [uDistil-Whisper: Label-Free Data Filtering for Knowledge Distillation in Low-Data Regimes](https://arxiv.org/abs/2407.01257)
Append: [Conversational Query Reformulation with the Guidance of Retrieved Documents](https://arxiv.org/abs/2407.12363)
Append: [PersLLM: A Personified Training Approach for Large Language Models](https://arxiv.org/abs/2407.12393)
Append: [Beyond Next Token Prediction: Patch-Level Training for Large Language Models](https://arxiv.org/abs/2407.12665)
Append: [Compensate Quantization Errors+: Quantized Models Are Inquisitive Learners](https://arxiv.org/abs/2407.15508)
Append: [Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time](https://arxiv.org/abs/2409.13338)
Append: [MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder](https://arxiv.org/abs/2409.14074)
Append: [TopoLM: brain-like spatio-functional organization in a topographic language model](https://arxiv.org/abs/2410.11516)
Append: [How Does Knowledge Selection Help Retrieval Augmented Generation?](https://arxiv.org/abs/2410.13258)
Append: [ChronoFact: Timeline-based Temporal Fact Verification](https://arxiv.org/abs/2410.14964)
Append: [SceneGenAgent: Precise Industrial Scene Generation with Coding Agent](https://arxiv.org/abs/2410.21909)
Append: [Phase Diagram of Vision Large Language Models Inference: A Perspective from Interaction across Image and Instruction](https://arxiv.org/abs/2411.00646)
Append: [Disentangling Memory and Reasoning Ability in Large Language Models](https://arxiv.org/abs/2411.13504)
Append: [KBAlign: Efficient Self Adaptation on Specific Knowledge Bases](https://arxiv.org/abs/2411.14790)
Append: [Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477)
Append: [Not All Adapters Matter: Selective Adapter Freezing for Memory-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2412.03587)
Append: [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/abs/2501.00777)
Append: [Benchmarking Generative AI for Scoring Medical Student Interviews in Objective Structured Clinical Examinations (OSCEs)](https://arxiv.org/abs/2501.13957)
Append: [TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and Compression in LLMs](https://arxiv.org/abs/2501.15674)
Append: [ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning](https://arxiv.org/abs/2502.04689)
Append: [Harnessing Multiple Large Language Models: A Survey on LLM Ensemble](https://arxiv.org/abs/2502.18036)
Append: [KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue Corpus](https://arxiv.org/abs/2503.06899)
Append: [Concise Reasoning via Reinforcement Learning](https://arxiv.org/abs/2504.05185)
Append: [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/abs/2504.07440)
Append: [CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives](https://arxiv.org/abs/2504.10823)
Append: [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)
Append: [Pose Priors from Language Models](https://arxiv.org/abs/2405.03689)
Append: [Latent Action Pretraining from Videos](https://arxiv.org/abs/2410.11758)
Append: [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)
Append: [Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective](https://arxiv.org/abs/2504.19458)
append_entries: 96
Finish: 2025-05-16 04:28:00.376203
------------------------------------------------------
Started: 2025-05-16 06:24:50.159901
Existing_entries: 1096
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 06:24:50.403957
------------------------------------------------------
Started: 2025-05-16 08:22:04.392653
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 08:22:04.648402
------------------------------------------------------
Started: 2025-05-16 10:17:38.361279
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 10:17:38.613920
------------------------------------------------------
Started: 2025-05-16 12:33:50.559468
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 12:33:50.840442
------------------------------------------------------
Started: 2025-05-16 14:16:01.577993
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 14:16:01.823176
------------------------------------------------------
Started: 2025-05-16 16:20:17.493465
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 16:20:17.738159
------------------------------------------------------
Started: 2025-05-16 18:22:27.241999
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 18:22:27.530125
------------------------------------------------------
Started: 2025-05-16 20:17:56.667442
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 20:17:56.984374
------------------------------------------------------
Started: 2025-05-16 22:15:23.444669
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-16 22:15:23.691005
------------------------------------------------------
Started: 2025-05-17 01:17:14.205421
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 01:17:14.518592
------------------------------------------------------
Started: 2025-05-17 03:05:25.955719
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 03:05:26.211248
------------------------------------------------------
Started: 2025-05-17 04:19:23.134244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 04:19:23.214468
------------------------------------------------------
Started: 2025-05-17 06:21:23.492830
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 06:21:23.560662
------------------------------------------------------
Started: 2025-05-17 08:19:27.543713
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 08:19:27.627887
------------------------------------------------------
Started: 2025-05-17 10:16:34.255954
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 10:16:34.318065
------------------------------------------------------
Started: 2025-05-17 12:29:58.284037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 12:29:58.347130
------------------------------------------------------
Started: 2025-05-17 14:13:39.650637
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 14:13:39.720559
------------------------------------------------------
Started: 2025-05-17 16:18:42.753874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 16:18:42.813312
------------------------------------------------------
Started: 2025-05-17 18:20:45.399746
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 18:20:45.490750
------------------------------------------------------
Started: 2025-05-17 20:16:43.439064
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 20:16:43.504094
------------------------------------------------------
Started: 2025-05-17 22:14:06.814953
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-17 22:14:06.893712
------------------------------------------------------
Started: 2025-05-18 01:23:51.398096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 01:23:51.496391
------------------------------------------------------
Started: 2025-05-18 03:15:15.477630
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 03:15:15.544305
------------------------------------------------------
Started: 2025-05-18 04:21:52.888062
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 04:21:52.953720
------------------------------------------------------
Started: 2025-05-18 06:21:58.213143
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 06:21:58.275032
------------------------------------------------------
Started: 2025-05-18 08:19:06.838699
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 08:19:06.930797
------------------------------------------------------
Started: 2025-05-18 10:16:00.678084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 10:16:00.772593
------------------------------------------------------
Started: 2025-05-18 12:30:22.155034
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 12:30:22.219013
------------------------------------------------------
Started: 2025-05-18 14:13:42.887998
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 14:13:42.947718
------------------------------------------------------
Started: 2025-05-18 16:18:03.880707
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 16:18:03.959610
------------------------------------------------------
Started: 2025-05-18 18:20:41.159262
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 18:20:41.220329
------------------------------------------------------
Started: 2025-05-18 20:17:29.277522
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 20:17:29.338751
------------------------------------------------------
Started: 2025-05-18 22:14:39.650678
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-18 22:14:39.709852
------------------------------------------------------
Started: 2025-05-19 01:22:19.392978
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 01:22:19.454334
------------------------------------------------------
Started: 2025-05-19 03:15:52.418636
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 03:15:52.478690
------------------------------------------------------
Started: 2025-05-19 04:29:32.914640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/abs/2505.10643)
Token length: 1319
Summarized using GPT-3.5-turbo
Append: [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
Token length: 1532
Summarized using GPT-3.5-turbo
Append: [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/abs/2505.10717)
Token length: 1003
Summarized using GPT-3.5-turbo
Append: [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/abs/2505.10718)
Token length: 1286
Summarized using GPT-3.5-turbo
Append: [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.10740)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [Ranked Voting based Self-Consistency of Large Language Models](https://arxiv.org/abs/2505.10772)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [A Systematic Analysis of Base Model Choice for Reward Modeling](https://arxiv.org/abs/2505.10775)
Token length: 891
Summarized using GPT-3.5-turbo
Append: [Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10792)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets](https://arxiv.org/abs/2505.10798)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances](https://arxiv.org/abs/2505.10829)
Token length: 1575
Summarized using GPT-3.5-turbo
Append: [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs](https://arxiv.org/abs/2505.10836)
Token length: 755
Summarized using GPT-3.5-turbo
Append: [Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?](https://arxiv.org/abs/2505.10862)
Token length: 1353
Summarized using GPT-3.5-turbo
Append: [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](https://arxiv.org/abs/2505.10870)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents](https://arxiv.org/abs/2505.10936)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations](https://arxiv.org/abs/2505.10937)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Accurate KV Cache Quantization with Outlier Tokens Tracing](https://arxiv.org/abs/2505.10938)
Token length: 1367
Summarized using GPT-3.5-turbo
Append: [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/abs/2505.10939)
Token length: 1357
Summarized using GPT-3.5-turbo
Append: [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs](https://arxiv.org/abs/2505.10948)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio](https://arxiv.org/abs/2505.10975)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs](https://arxiv.org/abs/2505.11008)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/abs/2505.11010)
Token length: 896
Summarized using GPT-3.5-turbo
Append: [StRuCom: A Novel Dataset of Structured Code Comments in Russian](https://arxiv.org/abs/2505.11026)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/abs/2505.11031)
Token length: 727
Summarized using GPT-3.5-turbo
Append: [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/abs/2505.11051)
Token length: 1626
Summarized using GPT-3.5-turbo
Append: [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [Towards Better Evaluation for Generated Patent Claims](https://arxiv.org/abs/2505.11095)
Token length: 1793
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140)
Token length: 1623
Summarized using GPT-3.5-turbo
Append: [SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization](https://arxiv.org/abs/2505.11166)
Token length: 864
Summarized using GPT-3.5-turbo
Append: [Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline](https://arxiv.org/abs/2505.11177)
Token length: 1698
Summarized using GPT-3.5-turbo
Append: [NoPE: The Counting Power of Transformers with No Positional Encodings](https://arxiv.org/abs/2505.11199)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/abs/2505.11271)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Temporal fine-tuning for early risk detection](https://arxiv.org/abs/2505.11280)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)
Token length: 1371
Summarized using GPT-3.5-turbo
Append: [XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision](https://arxiv.org/abs/2505.11336)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/abs/2505.11341)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors](https://arxiv.org/abs/2505.11352)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)
Token length: 1608
Summarized using GPT-3.5-turbo
Append: [A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography](https://arxiv.org/abs/2505.11379)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://arxiv.org/abs/2505.11413)
Token length: 1725
Summarized using GPT-3.5-turbo
Append: [Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model](https://arxiv.org/abs/2505.11421)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/abs/2505.11423)
Token length: 1483
Summarized using GPT-3.5-turbo
Append: [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/abs/2505.11462)
Append: [No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies](https://arxiv.org/abs/2505.11470)
Append: [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475)
Append: [Improving Assembly Code Performance with Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.11480)
Append: [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
Append: [Modeling cognitive processes of natural reading with transformer-based Language Models](https://arxiv.org/abs/2505.11485)
Append: [Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models](https://arxiv.org/abs/2505.10583)
Append: [Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports](https://arxiv.org/abs/2505.10586)
Append: [Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation](https://arxiv.org/abs/2505.10588)
Append: [Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment](https://arxiv.org/abs/2505.10597)
Append: [UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech](https://arxiv.org/abs/2505.10599)
Append: [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
Append: [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)
Append: [LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs](https://arxiv.org/abs/2505.10838)
Append: [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
Append: [MatTools: Benchmarking Large Language Models for Materials Science Tools](https://arxiv.org/abs/2505.10852)
Append: [REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?](https://arxiv.org/abs/2505.10872)
Append: [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
Append: [$\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection](https://arxiv.org/abs/2505.11079)
Append: [MPMA: Preference Manipulation Attack Against Model Context Protocol](https://arxiv.org/abs/2505.11154)
Append: [Maximizing Asynchronicity in Event-based Neural Networks](https://arxiv.org/abs/2505.11165)
Append: [CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback](https://arxiv.org/abs/2505.11178)
Append: [On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms](https://arxiv.org/abs/2505.11183)
Append: [Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese](https://arxiv.org/abs/2505.11200)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks](https://arxiv.org/abs/2505.11314)
Append: [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
Append: [EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2505.11405)
Append: [Large Language Model Use Impact Locus of Control](https://arxiv.org/abs/2505.11406)
Append: [Visual Planning: Let's Think Only with Images](https://arxiv.org/abs/2505.11409)
Append: [Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator](https://arxiv.org/abs/2305.15099)
Append: [Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?](https://arxiv.org/abs/2311.07564)
Append: [COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models](https://arxiv.org/abs/2402.14889)
Append: [ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images](https://arxiv.org/abs/2404.10652)
Append: [Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](https://arxiv.org/abs/2405.00715)
Append: [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/abs/2406.06326)
Append: [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://arxiv.org/abs/2408.06518)
Append: [Towards understanding evolution of science through language model series](https://arxiv.org/abs/2409.09636)
Append: [Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach](https://arxiv.org/abs/2409.20204)
Append: [Training of Scaffolded Language Models with Language Supervision: A Survey](https://arxiv.org/abs/2410.16392)
Append: [ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework](https://arxiv.org/abs/2410.19453)
Append: [How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP](https://arxiv.org/abs/2411.05527)
Append: [UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction](https://arxiv.org/abs/2411.07019)
Append: [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://arxiv.org/abs/2411.08135)
Append: [When to Speak, When to Abstain: Contrastive Decoding with Abstention](https://arxiv.org/abs/2412.12527)
Append: [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context](https://arxiv.org/abs/2412.12632)
Append: [XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation](https://arxiv.org/abs/2412.15529)
Append: [Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines](https://arxiv.org/abs/2501.00745)
Append: [LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena](https://arxiv.org/abs/2501.03266)
Append: [TreeKV: Smooth Key-Value Cache Compression with Tree Structures](https://arxiv.org/abs/2501.04987)
Append: [Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling](https://arxiv.org/abs/2501.10316)
Append: [Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine](https://arxiv.org/abs/2501.11885)
Append: [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/abs/2501.13977)
Append: [Do we really have to filter out random noise in pre-training data for language models?](https://arxiv.org/abs/2502.06604)
Append: [Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging](https://arxiv.org/abs/2502.06876)
Append: [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490)
Append: [Hallucination, Monofacts, and Miscalibration: An Empirical Investigation](https://arxiv.org/abs/2502.08666)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [Can Your Uncertainty Scores Detect Hallucinated Entity?](https://arxiv.org/abs/2502.11948)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing](https://arxiv.org/abs/2502.15208)
Append: [Call for Rigor in Reporting Quality of Instruction Tuning Data](https://arxiv.org/abs/2503.04807)
Append: [TigerLLM -- A Family of Bangla Large Language Models](https://arxiv.org/abs/2503.10995)
Append: [KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse](https://arxiv.org/abs/2503.16525)
Append: [Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2503.16529)
Append: [Mixture of Routers](https://arxiv.org/abs/2503.23362)
Append: [Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?](https://arxiv.org/abs/2504.01698)
Append: [Parameterized Synthetic Text Generation with SimpleStories](https://arxiv.org/abs/2504.09184)
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Append: [Customizing Visual-Language Foundation Models for Multi-modal Anomaly Detection and Reasoning](https://arxiv.org/abs/2403.11083)
Append: [Linear Attention Sequence Parallelism](https://arxiv.org/abs/2404.02882)
Append: [Intervention-Aware Forecasting: Breaking Historical Limits from a System Perspective](https://arxiv.org/abs/2405.13522)
Append: [Item-Language Model for Conversational Recommendation](https://arxiv.org/abs/2406.02844)
Append: [Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers](https://arxiv.org/abs/2406.11624)
Append: [Strategic Collusion of LLM Agents: Market Division in Multi-Commodity Competitions](https://arxiv.org/abs/2410.00031)
Append: [TestAgent: A Framework for Domain-Adaptive Evaluation of LLMs via Dynamic Benchmark Construction and Exploratory Interaction](https://arxiv.org/abs/2410.11507)
Append: [DiSCo: LLM Knowledge Distillation for Efficient Sparse Retrieval in Conversational Search](https://arxiv.org/abs/2410.14609)
Append: [MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection](https://arxiv.org/abs/2410.14731)
Append: [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](https://arxiv.org/abs/2411.02335)
Append: [Evaluating Vision-Language Models as Evaluators in Path Planning](https://arxiv.org/abs/2411.18711)
Append: [Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities](https://arxiv.org/abs/2501.02406)
Append: [Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods](https://arxiv.org/abs/2502.01384)
Append: [Shuttle Between the Instructions and the Parameters of Large Language Models](https://arxiv.org/abs/2502.02315)
Append: [MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?](https://arxiv.org/abs/2502.09933)
Append: [FOReCAst: The Future Outcome Reasoning and Confidence Assessment Benchmark](https://arxiv.org/abs/2502.19676)
Append: [Structured Preference Optimization for Vision-Language Long-Horizon Task Planning](https://arxiv.org/abs/2502.20742)
Append: [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)
Append: [Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations](https://arxiv.org/abs/2504.13955)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
append_entries: 140
Finish: 2025-05-19 04:32:19.711995
------------------------------------------------------
Started: 2025-05-19 06:25:43.413949
Existing_entries: 1140
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 06:25:43.746012
------------------------------------------------------
Started: 2025-05-19 08:24:40.082193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 08:24:40.418120
------------------------------------------------------
Started: 2025-05-19 10:18:42.642578
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 10:18:42.972156
------------------------------------------------------
Started: 2025-05-19 12:35:18.764670
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 12:35:19.088869
------------------------------------------------------
Started: 2025-05-19 14:16:53.787825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 14:16:54.128196
------------------------------------------------------
Started: 2025-05-19 16:20:44.459170
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 16:20:44.833076
------------------------------------------------------
Started: 2025-05-19 18:23:30.942168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 18:23:31.278604
------------------------------------------------------
Started: 2025-05-19 20:18:25.401477
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 20:18:25.784530
------------------------------------------------------
Started: 2025-05-19 22:16:03.787886
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-19 22:16:04.129114
------------------------------------------------------
Started: 2025-05-20 01:20:08.980858
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 01:20:09.369961
------------------------------------------------------
Started: 2025-05-20 03:10:13.816305
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 03:10:14.185486
------------------------------------------------------
Started: 2025-05-20 04:24:54.654973
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1759
Summarized using GPT-3.5-turbo
Append: [A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism](https://arxiv.org/abs/2505.11533)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification](https://arxiv.org/abs/2505.11550)
Token length: 1722
Summarized using GPT-3.5-turbo
Append: [Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models](https://arxiv.org/abs/2505.11604)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models](https://arxiv.org/abs/2505.11613)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations](https://arxiv.org/abs/2505.11615)
Token length: 733
Summarized using GPT-3.5-turbo
Append: [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/abs/2505.11628)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2](https://arxiv.org/abs/2505.11643)
Token length: 1783
Summarized using GPT-3.5-turbo
Append: [Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks](https://arxiv.org/abs/2505.11665)
Token length: 1495
Summarized using GPT-3.5-turbo
Append: [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation](https://arxiv.org/abs/2505.11683)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions](https://arxiv.org/abs/2505.11690)
Token length: 706
Summarized using GPT-3.5-turbo
Append: [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/abs/2505.11733)
Token length: 1969
Summarized using GPT-3.5-turbo
Append: [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)
Token length: 983
Summarized using GPT-3.5-turbo
Append: [Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/abs/2505.11746)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation](https://arxiv.org/abs/2505.11754)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
Token length: 1657
Summarized using GPT-3.5-turbo
Append: [BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2505.11811)
Token length: 1793
Summarized using GPT-3.5-turbo
Append: [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
Token length: 1679
Summarized using GPT-3.5-turbo
Append: [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks](https://arxiv.org/abs/2505.11829)
Token length: 1717
Summarized using GPT-3.5-turbo
Append: [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855)
Token length: 794
Summarized using GPT-3.5-turbo
Append: [NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization](https://arxiv.org/abs/2505.11876)
Token length: 1451
Summarized using GPT-3.5-turbo
Append: [AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation](https://arxiv.org/abs/2505.11887)
Token length: 1584
Summarized using GPT-3.5-turbo
Append: [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
Token length: 1488
Summarized using GPT-3.5-turbo
Append: [RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving](https://arxiv.org/abs/2505.11893)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data](https://arxiv.org/abs/2505.11900)
Token length: 1294
Summarized using GPT-3.5-turbo
Append: [ELITE: Embedding-Less retrieval with Iterative Text Exploration](https://arxiv.org/abs/2505.11908)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning](https://arxiv.org/abs/2505.11922)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts](https://arxiv.org/abs/2505.11924)
Token length: 993
Summarized using GPT-3.5-turbo
Append: [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932)
Token length: 1669
Summarized using GPT-3.5-turbo
Append: [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
Token length: 938
Summarized using GPT-3.5-turbo
Append: [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English](https://arxiv.org/abs/2505.11959)
Token length: 930
Summarized using GPT-3.5-turbo
Append: [CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation](https://arxiv.org/abs/2505.11965)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
Token length: 1852
Summarized using GPT-3.5-turbo
Append: [Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation](https://arxiv.org/abs/2505.11995)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method](https://arxiv.org/abs/2505.12028)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/abs/2505.12043)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [ABoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)
Token length: 702
Summarized using GPT-3.5-turbo
Append: [GenderBench: Evaluation Suite for Gender Biases in LLMs](https://arxiv.org/abs/2505.12054)
Token length: 1513
Summarized using GPT-3.5-turbo
Append: [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060)
Token length: 1875
Summarized using GPT-3.5-turbo
Append: [Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach](https://arxiv.org/abs/2505.12071)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/abs/2505.12075)
Append: [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
Append: [Personalized Author Obfuscation with Large Language Models](https://arxiv.org/abs/2505.12090)
Append: [Improving Fairness in LLMs Through Testing-Time Adversaries](https://arxiv.org/abs/2505.12100)
Append: [A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings](https://arxiv.org/abs/2505.12116)
Append: [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
Append: [Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse](https://arxiv.org/abs/2505.12160)
Append: [Truth Neurons](https://arxiv.org/abs/2505.12182)
Append: [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)
Append: [Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled](https://arxiv.org/abs/2505.12196)
Append: [How Reliable is Multilingual LLM-as-a-Judge?](https://arxiv.org/abs/2505.12201)
Append: [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
Append: [GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment](https://arxiv.org/abs/2505.12215)
Append: [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
Append: [Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers](https://arxiv.org/abs/2505.12218)
Append: [Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training](https://arxiv.org/abs/2505.12236)
Append: [PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs](https://arxiv.org/abs/2505.12238)
Append: [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)
Append: [Not All Documents Are What You Need for Extracting Instruction Tuning Data](https://arxiv.org/abs/2505.12250)
Append: [Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches](https://arxiv.org/abs/2505.12259)
Append: [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)
Append: [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
Append: [LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark](https://arxiv.org/abs/2505.12273)
Append: [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)
Append: [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
Append: [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)
Append: [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306)
Append: [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)
Append: [LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning](https://arxiv.org/abs/2505.12328)
Append: [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
Append: [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)
Append: [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
Append: [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)
Append: [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
Append: [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)
Append: [The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT](https://arxiv.org/abs/2505.12405)
Append: [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
Append: [PSC: Extending Context Window of Large Language Models via Phase Shift Calibration](https://arxiv.org/abs/2505.12423)
Append: [Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games](https://arxiv.org/abs/2505.12439)
Append: [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
Append: [Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations](https://arxiv.org/abs/2505.12454)
Append: [What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)
Append: [Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering](https://arxiv.org/abs/2505.12476)
Append: [KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation](https://arxiv.org/abs/2505.12495)
Append: [LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection](https://arxiv.org/abs/2505.12507)
Append: [DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design](https://arxiv.org/abs/2505.12511)
Append: [ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents](https://arxiv.org/abs/2505.12531)
Append: [Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE](https://arxiv.org/abs/2505.12533)
Append: [Disambiguation in Conversational Question Answering in the Era of LLM: A Survey](https://arxiv.org/abs/2505.12543)
Append: [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/abs/2505.12545)
Append: [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
Append: [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)
Append: [Enriching Patent Claim Generation with European Patent Dataset](https://arxiv.org/abs/2505.12568)
Append: [Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)
Append: [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)
Append: [CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling](https://arxiv.org/abs/2505.12587)
Append: [PromptPrism: A Linguistically-Inspired Taxonomy for Prompts](https://arxiv.org/abs/2505.12592)
Append: [AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection](https://arxiv.org/abs/2505.12594)
Append: [Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.12616)
Append: [Think Before You Attribute: Improving the Performance of LLMs Attribution Systems](https://arxiv.org/abs/2505.12621)
Append: [R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model](https://arxiv.org/abs/2505.12625)
Append: [Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing](https://arxiv.org/abs/2505.12636)
Append: [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/abs/2505.12654)
Append: [Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/abs/2505.12662)
Append: [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
Append: [ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](https://arxiv.org/abs/2505.12717)
Append: [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)
Append: [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)
Append: [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
Append: [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2505.12768)
Append: [A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/abs/2505.12781)
Append: [EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs](https://arxiv.org/abs/2505.12792)
Append: [Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2505.12808)
Append: [PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs](https://arxiv.org/abs/2505.12814)
Append: [SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models](https://arxiv.org/abs/2505.12821)
Append: [Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering](https://arxiv.org/abs/2505.12831)
Append: [FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models](https://arxiv.org/abs/2505.12835)
Append: [The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting](https://arxiv.org/abs/2505.12837)
Append: [Re-identification of De-identified Documents with Autoregressive Infilling](https://arxiv.org/abs/2505.12859)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation](https://arxiv.org/abs/2505.12888)
Append: [On the Thinking-Language Modeling Gap in Large Language Models](https://arxiv.org/abs/2505.12896)
Append: [PyFCG: Fluid Construction Grammar in Python](https://arxiv.org/abs/2505.12920)
Append: [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)
Append: [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
Append: [Neural Morphological Tagging for Nguni Languages](https://arxiv.org/abs/2505.12949)
Append: [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)
Append: [MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition](https://arxiv.org/abs/2505.12964)
Append: [Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down](https://arxiv.org/abs/2505.12969)
Append: [A Structured Literature Review on Traditional Approaches in Current Natural Language Processing](https://arxiv.org/abs/2505.12970)
Append: [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973)
Append: [An Empirical Study of Many-to-Many Summarization with Large Language Models](https://arxiv.org/abs/2505.12983)
Append: [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996)
Append: [EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code](https://arxiv.org/abs/2505.13004)
Append: [Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://arxiv.org/abs/2505.13006)
Append: [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)
Append: [topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation](https://arxiv.org/abs/2505.13034)
Append: [KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025](https://arxiv.org/abs/2505.13036)
Append: [SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation](https://arxiv.org/abs/2505.13053)
Append: [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
Append: [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
Append: [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
Append: [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)
Append: [Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning](https://arxiv.org/abs/2505.13115)
Append: [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136)
Append: [Understanding Cross-Lingual Inconsistency in Large Language Models](https://arxiv.org/abs/2505.13141)
Append: [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/abs/2505.13147)
Append: [Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice](https://arxiv.org/abs/2505.13156)
Append: [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157)
Append: [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
Append: [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
Append: [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
Append: [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)
Append: [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)
Append: [Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry](https://arxiv.org/abs/2505.13210)
Append: [SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science](https://arxiv.org/abs/2505.13220)
Append: [JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models](https://arxiv.org/abs/2505.13244)
Append: [Stronger Together: Unleashing the Social Impact of Hate Speech Research](https://arxiv.org/abs/2505.13251)
Append: [Natural Language Planning via Coding and Inference Scaling](https://arxiv.org/abs/2505.13252)
Append: [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)
Append: [WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?](https://arxiv.org/abs/2505.13257)
Append: [Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability](https://arxiv.org/abs/2505.13258)
Append: [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
Append: [Representation of perceived prosodic similarity of conversational feedback](https://arxiv.org/abs/2505.13268)
Append: [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)
Append: [$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
Append: [RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.13307)
Append: [GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)
Append: [Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges](https://arxiv.org/abs/2505.13328)
Append: [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
Append: [J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization](https://arxiv.org/abs/2505.13346)
Append: [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)
Append: [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/abs/2505.13353)
Append: [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)
Append: [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)
Append: [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
Append: [MR. Judge: Multimodal Reasoner as a Judge](https://arxiv.org/abs/2505.13403)
Append: [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/abs/2505.13404)
Append: [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417)
Append: [Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness](https://arxiv.org/abs/2505.13418)
Append: [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/abs/2505.13434)
Append: [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)
Append: [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)
Append: [TARGET: Benchmarking Table Retrieval for Generative Tasks](https://arxiv.org/abs/2505.11545)
Append: [ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems](https://arxiv.org/abs/2505.11572)
Append: [Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
Append: [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://arxiv.org/abs/2505.11611)
Append: [Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions](https://arxiv.org/abs/2505.11614)
Append: [EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents](https://arxiv.org/abs/2505.11717)
Append: [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
Append: [Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
Append: [Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders](https://arxiv.org/abs/2505.11756)
Append: [Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors](https://arxiv.org/abs/2505.11770)
Append: [VenusX: Unlocking Fine-Grained Functional Understanding of Proteins](https://arxiv.org/abs/2505.11812)
Append: [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842)
Append: [Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity](https://arxiv.org/abs/2505.11861)
Append: [J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge](https://arxiv.org/abs/2505.11875)
Append: [Introduction to Analytical Software Engineering Design Paradigm](https://arxiv.org/abs/2505.11979)
Append: [AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research](https://arxiv.org/abs/2505.12039)
Append: [Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation](https://arxiv.org/abs/2505.12058)
Append: [Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents](https://arxiv.org/abs/2505.12065)
Append: [LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs](https://arxiv.org/abs/2505.12135)
Append: [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
Append: [Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering](https://arxiv.org/abs/2505.12189)
Append: [Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling](https://arxiv.org/abs/2505.12225)
Append: [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
Append: [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
Append: [Efficient RL Training for Reasoning Models via Length-Aware Optimization](https://arxiv.org/abs/2505.12284)
Append: [Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge](https://arxiv.org/abs/2505.12301)
Append: [LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?](https://arxiv.org/abs/2505.12307)
Append: [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
Append: [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
Append: [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/abs/2505.12371)
Append: [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
Append: [UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection](https://arxiv.org/abs/2505.12457)
Append: [mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model](https://arxiv.org/abs/2505.12565)
Append: [Enhancing Latent Computation in Transformers with Latent Tokens](https://arxiv.org/abs/2505.12629)
Append: [Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents](https://arxiv.org/abs/2505.12632)
Append: [Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities](https://arxiv.org/abs/2505.12680)
Append: [Bullying the Machine: How Personas Increase LLM Vulnerability](https://arxiv.org/abs/2505.12692)
Append: [Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization](https://arxiv.org/abs/2505.12763)
Append: [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
Append: [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871)
Append: [Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/abs/2505.12886)
Append: [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
Append: [AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models](https://arxiv.org/abs/2505.12900)
Append: [Leveraging LLM Inconsistency to Boost Pass@k Performance](https://arxiv.org/abs/2505.12938)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset](https://arxiv.org/abs/2505.13028)
Append: [MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix](https://arxiv.org/abs/2505.13032)
Append: [LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs](https://arxiv.org/abs/2505.13098)
Append: [FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference](https://arxiv.org/abs/2505.13109)
Append: [Zero-Shot Iterative Formalization and Planning in Partially Observable Environments](https://arxiv.org/abs/2505.13126)
Append: [Efficient Generation of Parameterised Quantum Circuits from Large Texts](https://arxiv.org/abs/2505.13208)
Append: [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
Append: [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
Append: [Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space](https://arxiv.org/abs/2505.13308)
Append: [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition](https://arxiv.org/abs/2505.13380)
Append: [IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar](https://arxiv.org/abs/2505.13393)
Append: [A Minimum Description Length Approach to Regularization in Neural Networks](https://arxiv.org/abs/2505.13398)
Append: [CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process](https://arxiv.org/abs/2505.13408)
Append: [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430)
Append: [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
Append: [Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.13445)
Append: [Large Linguistic Models: Investigating LLMs' metalinguistic abilities](https://arxiv.org/abs/2305.00948)
Append: [Physics of Language Models: Part 1, Learning Hierarchical Language Structures](https://arxiv.org/abs/2305.13673)
Append: [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://arxiv.org/abs/2310.10378)
Append: [Automatically generating Riddles aiding Concept Attainment](https://arxiv.org/abs/2310.18290)
Append: [Streaming Sequence Transduction through Dynamic Compression](https://arxiv.org/abs/2402.01172)
Append: [Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528)
Append: [FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning](https://arxiv.org/abs/2402.12692)
Append: [Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance](https://arxiv.org/abs/2402.12819)
Append: [From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets](https://arxiv.org/abs/2404.17874)
Append: [Sparse Matrix in Large Language Model Fine-tuning](https://arxiv.org/abs/2405.15525)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation](https://arxiv.org/abs/2406.10785)
Append: [Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging](https://arxiv.org/abs/2406.16330)
Append: [Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models](https://arxiv.org/abs/2406.17513)
Append: [DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising](https://arxiv.org/abs/2407.00248)
Append: [A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding](https://arxiv.org/abs/2407.01976)
Append: [ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks](https://arxiv.org/abs/2407.18525)
Append: [SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning](https://arxiv.org/abs/2408.05517)
Append: [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](https://arxiv.org/abs/2408.08696)
Append: [Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions](https://arxiv.org/abs/2408.08780)
Append: [LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction](https://arxiv.org/abs/2408.12249)
Append: [What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices](https://arxiv.org/abs/2409.01893)
Append: [Learning Efficient Recursive Numeral Systems via Reinforcement Learning](https://arxiv.org/abs/2409.07170)
Append: [PACE: Abstractions for Communicating Efficiently](https://arxiv.org/abs/2409.20120)
Append: [SSR: Alignment-Aware Modality Connector for Speech Language Models](https://arxiv.org/abs/2410.00168)
Append: [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707)
Append: [Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions](https://arxiv.org/abs/2410.06577)
Append: [MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses](https://arxiv.org/abs/2410.07076)
Append: [Enhancing LLM Evaluations: The Garbling Trick](https://arxiv.org/abs/2411.01533)
Append: [VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs](https://arxiv.org/abs/2411.11266)
Append: [Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework](https://arxiv.org/abs/2411.16707)
Append: [Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths](https://arxiv.org/abs/2412.02466)
Append: [Intention Knowledge Graph Construction for User Intention Relation Modeling](https://arxiv.org/abs/2412.11500)
Append: [DateLogicQA: Benchmarking Temporal Biases in Large Language Models](https://arxiv.org/abs/2412.13377)
Append: [Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set](https://arxiv.org/abs/2412.14872)
Append: [Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration](https://arxiv.org/abs/2412.17061)
Append: [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506)
Append: [Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios](https://arxiv.org/abs/2501.11269)
Append: [Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning](https://arxiv.org/abs/2501.11292)
Append: [Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges](https://arxiv.org/abs/2501.11496)
Append: [AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding](https://arxiv.org/abs/2501.12162)
Append: [Option-ID Based Elimination For Multiple Choice Questions](https://arxiv.org/abs/2501.15175)
Append: [How Linguistics Learned to Stop Worrying and Love the Language Models](https://arxiv.org/abs/2501.17047)
Append: [Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models](https://arxiv.org/abs/2501.18280)
Append: [Vision-centric Token Compression in Large Language Model](https://arxiv.org/abs/2502.00791)
Append: [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/abs/2502.01179)
Append: [Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637)
Append: [Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models](https://arxiv.org/abs/2502.02444)
Append: [Reformulation for Pretraining Data Augmentation](https://arxiv.org/abs/2502.04235)
Append: [ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data](https://arxiv.org/abs/2502.05567)
Append: [Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization](https://arxiv.org/abs/2502.05605)
Append: [Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement](https://arxiv.org/abs/2502.06207)
Append: [Who Taught You That? Tracing Teachers in Model Distillation](https://arxiv.org/abs/2502.06659)
Append: [Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues](https://arxiv.org/abs/2502.09120)
Append: [RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation](https://arxiv.org/abs/2502.10996)
Append: [Beyond Pairwise: Global Zero-shot Temporal Graph Generation](https://arxiv.org/abs/2502.11114)
Append: [Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures](https://arxiv.org/abs/2502.11150)
Append: [The Mirage of Model Editing: Revisiting Evaluation in the Wild](https://arxiv.org/abs/2502.11177)
Append: [From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs](https://arxiv.org/abs/2502.11380)
Append: [Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs](https://arxiv.org/abs/2502.11525)
Append: [FaMTEB: Massive Text Embedding Benchmark in Persian Language](https://arxiv.org/abs/2502.11571)
Append: [To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models](https://arxiv.org/abs/2502.12202)
Append: [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)
Append: [Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora](https://arxiv.org/abs/2502.13691)
Append: [Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach](https://arxiv.org/abs/2502.14285)
Append: [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)
Append: [FANformer: Improving Large Language Models Through Effective Periodicity Modeling](https://arxiv.org/abs/2502.21309)
Append: [MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings](https://arxiv.org/abs/2503.03008)
Append: [Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions](https://arxiv.org/abs/2503.03261)
Append: [SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios](https://arxiv.org/abs/2503.06218)
Append: [PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs](https://arxiv.org/abs/2503.09543)
Append: [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)
Append: [UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality](https://arxiv.org/abs/2503.10669)
Append: [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)
Append: [Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models](https://arxiv.org/abs/2503.14411)
Append: [Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](https://arxiv.org/abs/2503.21380)
Append: [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729)
Append: [ImF: Implicit Fingerprint for Large Language Models](https://arxiv.org/abs/2503.21805)
Append: [Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](https://arxiv.org/abs/2503.22388)
Append: [RARE: Retrieval-Augmented Reasoning Modeling](https://arxiv.org/abs/2503.23513)
Append: [FISH-Tuning: Enhancing PEFT Methods with Fisher Information](https://arxiv.org/abs/2504.04050)
Append: [Leveraging Robust Optimization for LLM Alignment under Distribution Shifts](https://arxiv.org/abs/2504.05831)
Append: [LSR-MCTS: Alleviating Long Range Dependency in Code Generation](https://arxiv.org/abs/2504.07433)
Append: [Large Language Models Could Be Rote Learners](https://arxiv.org/abs/2504.08300)
Append: [DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation](https://arxiv.org/abs/2504.10198)
Append: [Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events](https://arxiv.org/abs/2504.12052)
Append: [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)
Append: [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)
Append: [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)
Append: [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)
Append: [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)
Append: [OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents](https://arxiv.org/abs/2504.16918)
Append: [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)
Append: [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)
Append: [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)
Append: [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)
Append: [UniversalRAG: Retrieval-Augmented Generation over Corpora of Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)
Append: [Computational Reasoning of Large Language Models](https://arxiv.org/abs/2504.20771)
Append: [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)
Append: [PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent](https://arxiv.org/abs/2309.12555)
Append: [BAT: Learning to Reason about Spatial Sounds with Large Language Models](https://arxiv.org/abs/2402.01591)
Append: [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/abs/2403.08946)
Append: [Controlled Training Data Generation with Diffusion Models](https://arxiv.org/abs/2403.15309)
Append: [Efficient Indirect LLM Jailbreak via Multimodal-LLM Jailbreak](https://arxiv.org/abs/2405.20015)
Append: [$S^3$ -- Semantic Signal Separation](https://arxiv.org/abs/2406.09556)
Append: [Watermarking Language Models with Error Correcting Codes](https://arxiv.org/abs/2406.10281)
Append: [Task Facet Learning: A Structured Approach to Prompt Optimization](https://arxiv.org/abs/2406.10504)
Append: [Gradient descent with generalized Newton's method](https://arxiv.org/abs/2407.02772)
Append: [EfficientQAT: Efficient Quantization-Aware Training for Large Language Models](https://arxiv.org/abs/2407.11062)
Append: ["Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation](https://arxiv.org/abs/2409.02718)
Append: [Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection](https://arxiv.org/abs/2410.02647)
Append: [Inference and Verbalization Functions During In-Context Learning](https://arxiv.org/abs/2410.09349)
Append: [Bias Similarity Across Large Language Models](https://arxiv.org/abs/2410.12010)
Append: [BrainECHO: Semantic Brain Signal Decoding through Vector-Quantized Spectrogram Reconstruction for Whisper-Enhanced Text Generation](https://arxiv.org/abs/2410.14971)
Append: [LLMScan: Causal Scan for LLM Misbehavior Detection](https://arxiv.org/abs/2410.16638)
Append: [Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation](https://arxiv.org/abs/2410.17462)
Append: [VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939)
Append: [Training-Free Bayesianization for Low-Rank Adapters of Large Language Models](https://arxiv.org/abs/2412.05723)
Append: [MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization](https://arxiv.org/abs/2412.06141)
Append: [Superhuman performance of a large language model on the reasoning tasks of a physician](https://arxiv.org/abs/2412.10849)
Append: [Feedback-Driven Vision-Language Alignment with Minimal Human Supervision](https://arxiv.org/abs/2501.04568)
Append: [Disentangling Length Bias In Preference Learning Via Response-Conditioned Modeling](https://arxiv.org/abs/2502.00814)
Append: [Explaining Context Length Scaling and Bounds for Language Models](https://arxiv.org/abs/2502.01481)
Append: [Leveraging the true depth of LLMs](https://arxiv.org/abs/2502.02790)
Append: [Exploring the Potential of Encoder-free Architectures in 3D LMMs](https://arxiv.org/abs/2502.09620)
Append: [Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning](https://arxiv.org/abs/2502.11799)
Append: [ARS: Automatic Routing Solver with Large Language Models](https://arxiv.org/abs/2502.15359)
Append: [The Hidden Strength of Disagreement: Unraveling the Consensus-Diversity Tradeoff in Adaptive Multi-Agent Systems](https://arxiv.org/abs/2502.16565)
Append: [Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore](https://arxiv.org/abs/2502.20034)
Append: [A Pilot Empirical Study on When and How to Use Knowledge Graphs as Retrieval Augmented Generation](https://arxiv.org/abs/2502.20854)
Append: [MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment](https://arxiv.org/abs/2503.01711)
Append: [Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding](https://arxiv.org/abs/2503.13139)
Append: [DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation](https://arxiv.org/abs/2503.18225)
Append: [MaintainCoder: Maintainable Code Generation Under Dynamic Requirements](https://arxiv.org/abs/2503.24260)
Append: [Effectively Controlling Reasoning Models through Thinking Intervention](https://arxiv.org/abs/2503.24370)
Append: [Prot42: a Novel Family of Protein Language Models for Target-aware Protein Binder Generation](https://arxiv.org/abs/2504.04453)
Append: [Signatures of human-like processing in Transformer forward passes](https://arxiv.org/abs/2504.14107)
Append: [AlignRAG: Leveraging Critique Learning for Evidence-Sensitive Retrieval-Augmented Reasoning](https://arxiv.org/abs/2504.14858)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Process Reward Models That Think](https://arxiv.org/abs/2504.16828)
Append: [Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks](https://arxiv.org/abs/2505.00234)
append_entries: 395
Finish: 2025-05-20 04:27:00.270035
------------------------------------------------------
Started: 2025-05-20 06:26:04.397935
Existing_entries: 1395
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant](https://arxiv.org/abs/2409.11055)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models](https://arxiv.org/abs/2505.00979)
Token length: 1645
Summarized using GPT-3.5-turbo
Append: [RM-R1: Reward Modeling as Reasoning](https://arxiv.org/abs/2505.02387)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [RICo: Refined In-Context Contribution for Automatic Instruction-Tuning Data Selection](https://arxiv.org/abs/2505.05327)
Token length: 1942
Summarized using GPT-3.5-turbo
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
append_entries: 5
Finish: 2025-05-20 06:26:19.666697
------------------------------------------------------
Started: 2025-05-20 08:23:02.950582
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 08:23:03.790412
------------------------------------------------------
Started: 2025-05-20 10:18:44.050375
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 10:18:44.903192
------------------------------------------------------
Started: 2025-05-20 12:35:16.406592
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 12:35:17.219973
------------------------------------------------------
Started: 2025-05-20 14:16:46.580116
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 14:16:47.490675
------------------------------------------------------
Started: 2025-05-20 16:20:48.673500
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 16:20:49.482040
------------------------------------------------------
Started: 2025-05-20 18:23:29.680254
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 18:23:30.534037
------------------------------------------------------
Started: 2025-05-20 20:18:39.656533
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 20:18:40.506488
------------------------------------------------------
Started: 2025-05-20 22:15:27.733243
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-20 22:15:28.534784
------------------------------------------------------
Started: 2025-05-21 01:19:46.369245
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 01:19:47.210846
------------------------------------------------------
Started: 2025-05-21 03:09:49.567441
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 03:09:50.512521
------------------------------------------------------
Started: 2025-05-21 04:24:16.225321
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)
Token length: 1103
Summarized using GPT-3.5-turbo
Append: [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)
Token length: 1724
Summarized using GPT-3.5-turbo
Append: [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)
Token length: 1026
Summarized using GPT-3.5-turbo
Append: [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)
Token length: 956
Summarized using GPT-3.5-turbo
Append: [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)
Token length: 730
Summarized using GPT-3.5-turbo
Append: [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)
Token length: 900
Summarized using GPT-3.5-turbo
Append: [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)
Token length: 963
Summarized using GPT-3.5-turbo
Append: [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)
Token length: 1933
Summarized using GPT-3.5-turbo
Append: [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)
Token length: 1843
Summarized using GPT-3.5-turbo
Append: [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)
Token length: 1525
Summarized using GPT-3.5-turbo
Append: [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)
Token length: 1529
Summarized using GPT-3.5-turbo
Append: [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)
Token length: 1814
Summarized using GPT-3.5-turbo
Append: [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)
Token length: 1580
Summarized using GPT-3.5-turbo
Append: [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
Token length: 811
Summarized using GPT-3.5-turbo
Append: [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
Token length: 1559
Summarized using GPT-3.5-turbo
Append: [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
Token length: 1158
Summarized using GPT-3.5-turbo
Append: [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
Token length: 1304
Summarized using GPT-3.5-turbo
Append: [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
Append: [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)
Append: [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)
Append: [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)
Append: [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)
Append: [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
Append: [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
Append: [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)
Append: [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)
Append: [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)
Append: [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)
Append: [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)
Append: [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)
Append: [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)
Append: [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)
Append: [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)
Append: [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)
Append: [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)
Append: [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)
Append: [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)
Append: [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)
Append: [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)
Append: [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)
Append: [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)
Append: [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)
Append: ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)
Append: [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)
Append: [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)
Append: [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)
Append: [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)
Append: [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)
Append: [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)
Append: [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)
Append: [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
Append: [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
Append: [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)
Append: [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)
Append: [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)
Append: [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)
Append: [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
Append: [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)
Append: [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)
Append: [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)
Append: [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)
Append: [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)
Append: [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)
Append: [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)
Append: [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)
Append: [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)
Append: [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)
Append: [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
Append: [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)
Append: [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)
Append: [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)
Append: [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)
Append: [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)
Append: [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)
Append: [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)
Append: [Void in Language Models](https://arxiv.org/abs/2505.14467)
Append: [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)
Append: [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
Append: [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)
Append: [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)
Append: [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
Append: [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)
Append: [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)
Append: [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)
Append: [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)
Append: [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)
Append: [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)
Append: [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
Append: [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)
Append: [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)
Append: [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
Append: [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)
Append: [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)
Append: [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
Append: [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)
Append: [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)
Append: [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)
Append: [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)
Append: [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)
Append: [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)
Append: [MedEIR: A Specialized Medical Embedding Model for Enhanced Information Retrieval](https://arxiv.org/abs/2505.13482)
Append: [Evaluating Large Language Models for Real-World Engineering Tasks](https://arxiv.org/abs/2505.13484)
Append: [Contrastive Cross-Course Knowledge Tracing via Concept Graph Guided Knowledge Transfer](https://arxiv.org/abs/2505.13489)
Append: [Can AI Freelancers Compete? Benchmarking Earnings, Reliability, and Task Success at Scale](https://arxiv.org/abs/2505.13511)
Append: [LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades](https://arxiv.org/abs/2505.13515)
Append: [BARREL: Boundary-Aware Reasoning for Factual and Reliable LRMs](https://arxiv.org/abs/2505.13529)
Append: [AdAEM: An Adaptively and Automated Extensible Measurement of LLMs' Value Difference](https://arxiv.org/abs/2505.13531)
Append: [InterFeat: An Automated Pipeline for Finding Interesting Hypotheses in Structured Biomedical Data](https://arxiv.org/abs/2505.13534)
Append: [RAR: Setting Knowledge Tripwires for Retrieval Augmented Rejection](https://arxiv.org/abs/2505.13581)
Append: [Guided Search Strategies in Non-Serializable Environments with Applications to Software Engineering Agents](https://arxiv.org/abs/2505.13652)
Append: [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
Append: [Power Lines: Scaling Laws for Weight Decay and Batch Size in LLM Pre-training](https://arxiv.org/abs/2505.13738)
Append: [LLM-Based Compact Reranking with Document Features for Scientific Retrieval](https://arxiv.org/abs/2505.13757)
Append: [Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations](https://arxiv.org/abs/2505.13763)
Append: [Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques](https://arxiv.org/abs/2505.13766)
Append: [Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical Pitfalls in Causal Inference](https://arxiv.org/abs/2505.13770)
Append: [Structured Agent Distillation for Large Language Model](https://arxiv.org/abs/2505.13820)
Append: [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
Append: [PandaGuard: Systematic Evaluation of LLM Safety in the Era of Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
Append: [InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models](https://arxiv.org/abs/2505.13878)
Append: [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
Append: [Efficient Agent Training for Computer Use](https://arxiv.org/abs/2505.13909)
Append: [MLZero: A Multi-Agent System for End-to-end Machine Learning Automation](https://arxiv.org/abs/2505.13941)
Append: [Beyond Text: Unveiling Privacy Vulnerabilities in Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13957)
Append: [ProMind-LLM: Proactive Mental Health Care via Causal Reasoning with Sensor Data](https://arxiv.org/abs/2505.14038)
Append: [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2505.14071)
Append: [s3: You Don't Need That Much Data to Train a Search Agent via RL](https://arxiv.org/abs/2505.14146)
Append: [Safety Subspaces are Not Distinct: A Fine-Tuning Case Study](https://arxiv.org/abs/2505.14185)
Append: [Reinforcement Learning vs. Distillation: Understanding Accuracy and Capability in LLM Reasoning](https://arxiv.org/abs/2505.14216)
Append: [AAPO: Enhance the Reasoning Capabilities of LLMs with Advantage Momentum](https://arxiv.org/abs/2505.14264)
Append: [SafetyNet: Detecting Harmful Outputs in LLMs by Modeling and Monitoring Deceptive Behaviors](https://arxiv.org/abs/2505.14300)
Append: [Scaling Law for Quantization-Aware Training](https://arxiv.org/abs/2505.14302)
Append: [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
Append: [FMSD-TTS: Few-shot Multi-Speaker Multi-Dialect Text-to-Speech Synthesis for \"U-Tsang, Amdo and Kham Speech Dataset Generation](https://arxiv.org/abs/2505.14351)
Append: [PersonaTAB: Predicting Personality Traits using Textual, Acoustic, and Behavioral Cues in Fully-Duplex Speech Dialogs](https://arxiv.org/abs/2505.14356)
Append: [Is Your Prompt Safe? Investigating Prompt Injection Attacks Against Open-Source LLMs](https://arxiv.org/abs/2505.14368)
Append: [Causal Cartographer: From Mapping to Reasoning Over Counterfactual Worlds](https://arxiv.org/abs/2505.14396)
Append: [OmniGenBench: A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](https://arxiv.org/abs/2505.14402)
Append: [Pairwise Evaluation of Accent Similarity in Speech Synthesis](https://arxiv.org/abs/2505.14410)
Append: [PRL: Prompts from Reinforcement Learning](https://arxiv.org/abs/2505.14412)
Append: [Rank-K: Test-Time Reasoning for Listwise Reranking](https://arxiv.org/abs/2505.14432)
Append: [S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models](https://arxiv.org/abs/2505.14438)
Append: [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
Append: [RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding](https://arxiv.org/abs/2505.14462)
Append: [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
Append: [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
Append: [Reasoning Models Better Express Their Confidence](https://arxiv.org/abs/2505.14489)
Append: [Teaching Audio-Aware Large Language Models What Does Not Hear: Mitigating Hallucinations through Synthesized Negative Samples](https://arxiv.org/abs/2505.14518)
Append: [Agent Context Protocols Enhance Collective Inference](https://arxiv.org/abs/2505.14569)
Append: [SATBench: Benchmarking LLMs' Logical Reasoning via Automated Puzzle Generation from SAT Formulas](https://arxiv.org/abs/2505.14615)
Append: [Enhancing Learned Knowledge in LoRA Adapters Through Efficient Contrastive Decoding on Ascend NPUs](https://arxiv.org/abs/2505.14620)
Append: [TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning](https://arxiv.org/abs/2505.14625)
Append: [Debating for Better Reasoning: An Unsupervised Multimodal Approach](https://arxiv.org/abs/2505.14627)
Append: [KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models](https://arxiv.org/abs/2505.14629)
Append: [Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference](https://arxiv.org/abs/2505.14638)
Append: [Beyond Words: Multimodal LLM Knows When to Speak](https://arxiv.org/abs/2505.14654)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [ContextAgent: Context-Aware Proactive LLM Agents with Open-World Sensory Perceptions](https://arxiv.org/abs/2505.14668)
Append: [NExT-Search: Rebuilding User Feedback Ecosystem for Generative AI Search](https://arxiv.org/abs/2505.14680)
Append: [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
Append: [Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy](https://arxiv.org/abs/2312.10097)
Append: [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)
Append: [PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games](https://arxiv.org/abs/2404.17662)
Append: [Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs](https://arxiv.org/abs/2407.01082)
Append: [PersonaGym: Evaluating Persona Agents and LLMs](https://arxiv.org/abs/2407.18416)
Append: [Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework](https://arxiv.org/abs/2409.00054)
Append: [RoMath: A Mathematical Reasoning Benchmark in Romanian](https://arxiv.org/abs/2409.11074)
Append: [Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing](https://arxiv.org/abs/2409.11726)
Append: [Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review](https://arxiv.org/abs/2410.03663)
Append: [SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition](https://arxiv.org/abs/2410.10624)
Append: [RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals](https://arxiv.org/abs/2410.11348)
Append: [Interpreting token compositionality in LLMs: A robustness analysis](https://arxiv.org/abs/2410.12924)
Append: [The Mystery of the Pathological Path-star Task for Language Models](https://arxiv.org/abs/2410.13779)
Append: [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)
Append: [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/abs/2410.15522)
Append: [Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics](https://arxiv.org/abs/2410.21272)
Append: [Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models](https://arxiv.org/abs/2411.02448)
Append: [Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training](https://arxiv.org/abs/2411.14318)
Append: [Can LLMs be Good Graph Judge for Knowledge Graph Construction?](https://arxiv.org/abs/2411.17388)
Append: [A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension](https://arxiv.org/abs/2412.06245)
Append: [A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges](https://arxiv.org/abs/2412.11936)
Append: [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161)
Append: [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)
Append: [SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs](https://arxiv.org/abs/2412.16783)
Append: [Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts](https://arxiv.org/abs/2501.02009)
Append: [ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting](https://arxiv.org/abs/2501.06582)
Append: [TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time](https://arxiv.org/abs/2501.07482)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection](https://arxiv.org/abs/2501.15451)
Append: [People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text](https://arxiv.org/abs/2501.15654)
Append: [Improving LLM Unlearning Robustness via Random Perturbations](https://arxiv.org/abs/2501.19202)
Append: [LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information](https://arxiv.org/abs/2502.02095)
Append: [Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs](https://arxiv.org/abs/2502.02362)
Append: [A comparison of translation performance between DeepL and Supertext](https://arxiv.org/abs/2502.02577)
Append: [Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation](https://arxiv.org/abs/2502.02789)
Append: [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)
Append: [CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment](https://arxiv.org/abs/2502.11066)
Append: [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/abs/2502.11100)
Append: [Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment](https://arxiv.org/abs/2502.11733)
Append: [FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2502.11811)
Append: [EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models](https://arxiv.org/abs/2502.11916)
Append: [R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](https://arxiv.org/abs/2502.12767)
Append: [Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection](https://arxiv.org/abs/2502.13061)
Append: [TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation](https://arxiv.org/abs/2502.13442)
Append: [DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation](https://arxiv.org/abs/2502.14037)
Append: [Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction](https://arxiv.org/abs/2502.14171)
Append: [Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare](https://arxiv.org/abs/2502.16051)
Append: [SQLong: Enhanced NL2SQL for Longer Contexts with LLMs](https://arxiv.org/abs/2502.16747)
Append: [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/abs/2502.16894)
Append: [Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs](https://arxiv.org/abs/2502.16901)
Append: [Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models](https://arxiv.org/abs/2502.19982)
Append: [Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing](https://arxiv.org/abs/2502.20592)
Append: [CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation](https://arxiv.org/abs/2502.21074)
Append: [Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey](https://arxiv.org/abs/2503.01513)
Append: [MCiteBench: A Multimodal Benchmark for Generating Text with Citations](https://arxiv.org/abs/2503.02589)
Append: [Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms](https://arxiv.org/abs/2503.04372)
Append: [Cost-Optimal Grouped-Query Attention for Long-Context Modeling](https://arxiv.org/abs/2503.09579)
Append: [RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs](https://arxiv.org/abs/2503.10657)
Append: [MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection](https://arxiv.org/abs/2503.18132)
Append: [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)
Append: [Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models](https://arxiv.org/abs/2504.08399)
Append: [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)
Append: [Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)
Append: [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)
Append: [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)
Append: [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)
Append: [HyPerAlign: Interpretable Personalized LLM Alignment via Hypothesis Generation](https://arxiv.org/abs/2505.00038)
Append: [A Survey on Large Language Model based Human-Agent Systems](https://arxiv.org/abs/2505.00753)
Append: [Adaptive Thinking via Mode Policy Optimization for Social Language Agents](https://arxiv.org/abs/2505.02156)
Append: [From Theft to Bomb-Making: The Ripple Effect of Unlearning in Defending Against Jailbreak Attacks](https://arxiv.org/abs/2407.02855)
Append: [Frozen Large Language Models Can Perceive Paralinguistic Aspects of Speech](https://arxiv.org/abs/2410.01162)
Append: [IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models](https://arxiv.org/abs/2410.02429)
Append: [Evaluating the Correctness of Inference Patterns Used by LLMs for Judgment](https://arxiv.org/abs/2410.09083)
Append: [Large Continual Instruction Assistant](https://arxiv.org/abs/2410.10868)
Append: [Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study](https://arxiv.org/abs/2410.17980)
Append: [ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models](https://arxiv.org/abs/2412.04756)
Append: [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559)
Append: [MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents](https://arxiv.org/abs/2501.08828)
Append: [InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model](https://arxiv.org/abs/2501.12368)
Append: [Fairshare Data Pricing via Data Valuation for Large Language Models](https://arxiv.org/abs/2502.00198)
Append: [From Words to Collisions: LLM-Guided Evaluation and Adversarial Generation of Safety-Critical Driving Scenarios](https://arxiv.org/abs/2502.02145)
Append: [Training Language Models to Reason Efficiently](https://arxiv.org/abs/2502.04463)
Append: [VLMs as GeoGuessr Masters: Exceptional Performance, Hidden Biases, and Privacy Risks](https://arxiv.org/abs/2502.11163)
Append: [EquiBench: Benchmarking Large Language Models' Understanding of Program Semantics via Equivalence Checking](https://arxiv.org/abs/2502.12466)
Append: [DeepResonance: Enhancing Multimodal Music Understanding via Music-centric Multi-way Instruction Tuning](https://arxiv.org/abs/2502.12623)
Append: [Does Acceleration Cause Hidden Instability in Vision Language Models? Uncovering Instance-Level Divergence Through a Large-Scale Empirical Study](https://arxiv.org/abs/2503.06794)
Append: [VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models](https://arxiv.org/abs/2503.07575)
Append: [Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More](https://arxiv.org/abs/2503.10542)
Append: [CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2503.14232)
Append: [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476)
Append: [Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions](https://arxiv.org/abs/2503.16505)
Append: [Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding](https://arxiv.org/abs/2504.01281)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [VideoVista-CulturalLingo: 360$^\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension](https://arxiv.org/abs/2504.17821)
Append: [CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation](https://arxiv.org/abs/2504.21751)
append_entries: 291
Finish: 2025-05-21 04:26:10.770767
------------------------------------------------------
Started: 2025-05-21 06:25:14.275548
Existing_entries: 1291
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 988
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 1175
Summarized using GPT-3.5-turbo
Append: [Technical Report: Quantifying and Analyzing the Generalization Power of a DNN](https://arxiv.org/abs/2505.06993)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models](https://arxiv.org/abs/2505.07558)
append_entries: 3
Finish: 2025-05-21 06:25:21.275814
------------------------------------------------------
Started: 2025-05-21 08:22:12.489475
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 08:22:13.116071
------------------------------------------------------
Started: 2025-05-21 10:18:11.354685
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 10:18:11.974912
------------------------------------------------------
Started: 2025-05-21 12:34:07.703990
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 12:34:08.326567
------------------------------------------------------
Started: 2025-05-21 14:17:26.799262
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 14:17:27.506976
------------------------------------------------------
Started: 2025-05-21 16:21:12.123448
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 16:21:12.759748
------------------------------------------------------
Started: 2025-05-21 18:23:37.410084
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 18:23:38.036958
------------------------------------------------------
Started: 2025-05-21 20:18:20.404359
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 20:18:21.088953
------------------------------------------------------
Started: 2025-05-21 22:15:39.845177
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-21 22:15:40.493513
------------------------------------------------------
Started: 2025-05-22 01:18:51.220002
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 01:18:51.929571
------------------------------------------------------
Started: 2025-05-22 03:09:58.222323
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 03:09:58.868096
------------------------------------------------------
Started: 2025-05-22 04:25:00.150692
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 758
Summarized using GPT-3.5-turbo
Append: [Addressing the Challenges of Planning Language Generation](https://arxiv.org/abs/2505.14763)
Token length: 696
Summarized using GPT-3.5-turbo
Append: [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/abs/2505.14824)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/abs/2505.14845)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)
Token length: 536
Summarized using GPT-3.5-turbo
Append: [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/abs/2505.14852)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/abs/2505.14871)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Incorporating Token Usage into Prompting Strategy Evaluation](https://arxiv.org/abs/2505.14880)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters](https://arxiv.org/abs/2505.14886)
Token length: 1236
Summarized using GPT-3.5-turbo
Append: [In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties](https://arxiv.org/abs/2505.14887)
Token length: 1295
Summarized using GPT-3.5-turbo
Append: [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/abs/2505.14892)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [Concept Incongruence: An Exploration of Time and Death in Role Playing](https://arxiv.org/abs/2505.14905)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain](https://arxiv.org/abs/2505.14906)
Token length: 1644
Summarized using GPT-3.5-turbo
Append: [ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories](https://arxiv.org/abs/2505.14917)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/abs/2505.14918)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/abs/2505.14925)
Token length: 1382
Summarized using GPT-3.5-turbo
Append: [MedBrowseComp: Benchmarking Medical Deep Research and Computer Use](https://arxiv.org/abs/2505.14963)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies](https://arxiv.org/abs/2505.14972)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [CRAFT: Training-Free Cascaded Retrieval for Tabular QA](https://arxiv.org/abs/2505.14984)
Token length: 1780
Summarized using GPT-3.5-turbo
Append: [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](https://arxiv.org/abs/2505.14992)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/abs/2505.14996)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems](https://arxiv.org/abs/2505.15000)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/abs/2505.15024)
Token length: 909
Summarized using GPT-3.5-turbo
Append: [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)
Token length: 726
Summarized using GPT-3.5-turbo
Append: [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective](https://arxiv.org/abs/2505.15045)
Token length: 1523
Summarized using GPT-3.5-turbo
Append: [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)
Token length: 1745
Summarized using GPT-3.5-turbo
Append: [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/abs/2505.15050)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)
Token length: 1080
Summarized using GPT-3.5-turbo
Append: [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/abs/2505.15055)
Token length: 1873
Summarized using GPT-3.5-turbo
Append: [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
Token length: 1312
Summarized using GPT-3.5-turbo
Append: [UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking](https://arxiv.org/abs/2505.15063)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)
Token length: 1017
Summarized using GPT-3.5-turbo
Append: [In-Domain African Languages Translation Using LLMs and Multi-armed Bandits](https://arxiv.org/abs/2505.15069)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Understand Internet Buzzwords Through User-Generated Content](https://arxiv.org/abs/2505.15071)
Token length: 1498
Summarized using GPT-3.5-turbo
Append: [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
Token length: 1027
Summarized using GPT-3.5-turbo
Append: [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora](https://arxiv.org/abs/2505.15087)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2505.15090)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models](https://arxiv.org/abs/2505.15094)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)
Token length: 1458
Summarized using GPT-3.5-turbo
Append: [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
Append: [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
Append: [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)
Append: [RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals](https://arxiv.org/abs/2505.15110)
Append: [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/abs/2505.15117)
Append: [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154)
Append: [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)
Append: [EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association](https://arxiv.org/abs/2505.15196)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs](https://arxiv.org/abs/2505.15210)
Append: [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
Append: [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)
Append: [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/abs/2505.15245)
Append: [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/abs/2505.15249)
Append: [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/abs/2505.15255)
Append: [When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners](https://arxiv.org/abs/2505.15257)
Append: [AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection](https://arxiv.org/abs/2505.15261)
Append: [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/abs/2505.15277)
Append: [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/abs/2505.15282)
Append: [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/abs/2505.15291)
Append: [Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites](https://arxiv.org/abs/2505.15297)
Append: [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/abs/2505.15299)
Append: [Emotional Supporters often Use Multiple Strategies in a Single Turn](https://arxiv.org/abs/2505.15316)
Append: [Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack](https://arxiv.org/abs/2505.15323)
Append: [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/abs/2505.15333)
Append: [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
Append: [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/abs/2505.15347)
Append: [The Super Emotion Dataset](https://arxiv.org/abs/2505.15348)
Append: [Revealing Language Model Trajectories via Kullback-Leibler Divergence](https://arxiv.org/abs/2505.15353)
Append: [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/abs/2505.15355)
Append: [NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging](https://arxiv.org/abs/2505.15356)
Append: [X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System](https://arxiv.org/abs/2505.15372)
Append: [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)
Append: [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)
Append: [An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations](https://arxiv.org/abs/2505.15392)
Append: [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404)
Append: [Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches](https://arxiv.org/abs/2505.15422)
Append: [Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models](https://arxiv.org/abs/2505.15424)
Append: [NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish](https://arxiv.org/abs/2505.15426)
Append: [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/abs/2505.15427)
Append: [Likelihood Variance as Text Importance for Resampling Texts to Map Language Models](https://arxiv.org/abs/2505.15428)
Append: [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431)
Append: [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/abs/2505.15442)
Append: [AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs](https://arxiv.org/abs/2505.15443)
Append: [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/abs/2505.15444)
Append: [Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment](https://arxiv.org/abs/2505.15456)
Append: [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/abs/2505.15467)
Append: [CoLA: Collaborative Low-Rank Adaptation](https://arxiv.org/abs/2505.15471)
Append: [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/abs/2505.15472)
Append: [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2505.15475)
Append: [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
Append: [Collaborative Problem-Solving in an Optimization Game](https://arxiv.org/abs/2505.15490)
Append: [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/abs/2505.15501)
Append: [Multilingual Test-Time Scaling via Initial Thought Transfer](https://arxiv.org/abs/2505.15508)
Append: [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/abs/2505.15524)
Append: [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/abs/2505.15553)
Append: [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/abs/2505.15554)
Append: [A Survey on Multilingual Mental Disorders Detection from Social Media Data](https://arxiv.org/abs/2505.15556)
Append: [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561)
Append: [Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis](https://arxiv.org/abs/2505.15563)
Append: [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/abs/2505.15607)
Append: [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612)
Append: [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/abs/2505.15623)
Append: [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/abs/2505.15633)
Append: [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
Append: [Word Level Timestamp Generation for Automatic Speech Recognition and Translation](https://arxiv.org/abs/2505.15646)
Append: [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656)
Append: [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
Append: [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674)
Append: [The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect](https://arxiv.org/abs/2505.15682)
Append: [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683)
Append: [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/abs/2505.15684)
Append: [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
Append: [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/abs/2505.15695)
Append: [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/abs/2505.15696)
Append: ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
Append: [LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing](https://arxiv.org/abs/2505.15702)
Append: [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710)
Append: [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/abs/2505.15712)
Append: [Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling](https://arxiv.org/abs/2505.15715)
Append: [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/abs/2505.15722)
Append: [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/abs/2505.15727)
Append: [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)
Append: [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/abs/2505.15769)
Append: [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774)
Append: [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776)
Append: [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)
Append: [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781)
Append: [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/abs/2505.15792)
Append: [Reverse Engineering Human Preferences with Reinforcement Learning](https://arxiv.org/abs/2505.15795)
Append: [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
Append: [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805)
Append: [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/abs/2505.15807)
Append: [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/abs/2505.15810)
Append: [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)
Append: [Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers](https://arxiv.org/abs/2505.14692)
Append: [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/abs/2505.14699)
Append: [QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding](https://arxiv.org/abs/2505.14723)
Append: [MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models](https://arxiv.org/abs/2505.14728)
Append: [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/abs/2505.14826)
Append: [Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs](https://arxiv.org/abs/2505.14899)
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
Append: [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
Append: [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
Append: [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/abs/2505.15068)
Append: [An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc](https://arxiv.org/abs/2505.15070)
Append: [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
Append: [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
Append: [ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://arxiv.org/abs/2505.15158)
Append: [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
Append: [BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems](https://arxiv.org/abs/2505.15216)
Append: [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
Append: [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/abs/2505.15276)
Append: [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
Append: [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
Append: [AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals](https://arxiv.org/abs/2505.15365)
Append: [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
Append: [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/abs/2505.15400)
Append: [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/abs/2505.15410)
Append: [Set-LLM: A Permutation-Invariant LLM](https://arxiv.org/abs/2505.15433)
Append: [A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach](https://arxiv.org/abs/2505.15466)
Append: [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
Append: [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.15510)
Append: [Explainable embeddings with Distance Explainer](https://arxiv.org/abs/2505.15516)
Append: [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
Append: [MIRB: Mathematical Information Retrieval Benchmark](https://arxiv.org/abs/2505.15585)
Append: [Mechanistic Insights into Grokking from the Embedding Layer](https://arxiv.org/abs/2505.15624)
Append: [Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information](https://arxiv.org/abs/2505.15667)
Append: [HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases](https://arxiv.org/abs/2505.15701)
Append: [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738)
Append: [Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications](https://arxiv.org/abs/2505.15741)
Append: [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753)
Append: [MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling](https://arxiv.org/abs/2505.15772)
Append: [ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality](https://arxiv.org/abs/2505.15773)
Append: [Large Language Models as Computable Approximations to Solomonoff Induction](https://arxiv.org/abs/2505.15784)
Append: [Predicting generalization performance with correctness discriminators](https://arxiv.org/abs/2311.09422)
Append: [A Framework for Real-time Safeguarding the Text Generation of Large Language Model](https://arxiv.org/abs/2404.19048)
Append: [MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset](https://arxiv.org/abs/2406.02106)
Append: [Exploring the Robustness of Language Models for Tabular Question Answering via Attention Analysis](https://arxiv.org/abs/2406.12719)
Append: [Helpful assistant or fruitful facilitator? Investigating how personas affect language model behavior](https://arxiv.org/abs/2407.02099)
Append: [A Comprehensive Evaluation of Large Language Models on Temporal Event Forecasting](https://arxiv.org/abs/2407.11638)
Append: [dMel: Speech Tokenization made Simple](https://arxiv.org/abs/2407.15835)
Append: [Fine-tuning Large Language Models for Entity Matching](https://arxiv.org/abs/2409.08185)
Append: [A Closer Look at Machine Unlearning for Large Language Models](https://arxiv.org/abs/2410.08109)
Append: [Meta-Chunking: Learning Text Segmentation and Semantic Completion via Logical Perception](https://arxiv.org/abs/2410.12788)
Append: [Retrospective Learning from Interactions](https://arxiv.org/abs/2410.13852)
Append: [GATEAU: Selecting Influential Samples for Long Context Alignment](https://arxiv.org/abs/2410.15633)
Append: [Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models](https://arxiv.org/abs/2410.16168)
Append: [Robust and Minimally Invasive Watermarking for EaaS](https://arxiv.org/abs/2410.17552)
Append: [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/abs/2410.21728)
Append: [Untangling Hate Speech Definitions: A Semantic Componential Analysis Across Cultures and Domains](https://arxiv.org/abs/2411.07417)
Append: [FastDraft: How to Train Your Draft](https://arxiv.org/abs/2411.11055)
Append: [Dial-In LLM: Human-Aligned LLM-in-the-loop Intent Clustering for Customer Service Dialogues](https://arxiv.org/abs/2412.09049)
Append: [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/abs/2412.10138)
Append: [DARWIN 1.5: Large Language Models as Materials Science Adapted Learners](https://arxiv.org/abs/2412.11970)
Append: [Exploring Cross-lingual Latent Transplantation: Mutual Opportunities and Open Challenges](https://arxiv.org/abs/2412.12686)
Append: [MRAG: A Modular Retrieval Framework for Time-Sensitive Question Answering](https://arxiv.org/abs/2412.15540)
Append: [Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models](https://arxiv.org/abs/2412.17034)
Append: [How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond](https://arxiv.org/abs/2501.05714)
Append: [Analyzing the Effect of Linguistic Similarity on Cross-Lingual Transfer: Tasks and Experimental Setups Matter](https://arxiv.org/abs/2501.14491)
Append: [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2502.00299)
Append: [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)
Append: [Lifelong Knowledge Editing requires Better Regularization](https://arxiv.org/abs/2502.01636)
Append: [BARE: Leveraging Base Language Models for Few-Shot Synthetic Data Generation](https://arxiv.org/abs/2502.01697)
Append: [Can LLMs Maintain Fundamental Abilities under KV Cache Compression?](https://arxiv.org/abs/2502.01941)
Append: [Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization](https://arxiv.org/abs/2502.04295)
Append: [An Analysis for Reasoning Bias of Language Models with Small Initialization](https://arxiv.org/abs/2502.04375)
Append: [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/abs/2502.04964)
Append: [CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction](https://arxiv.org/abs/2502.07316)
Append: [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/abs/2502.11441)
Append: [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/abs/2502.11471)
Append: [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/abs/2502.12562)
Append: [Linguistic Generalizations are not Rules: Impacts on Evaluation of LMs](https://arxiv.org/abs/2502.13195)
Append: [FineEdit: Unlock Instruction-Based Text Editing for LLMs](https://arxiv.org/abs/2502.13358)
Append: [Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval](https://arxiv.org/abs/2502.13369)
Append: [UniKnow: A Unified Framework for Reliable Language Model Behavior across Parametric and External Knowledge](https://arxiv.org/abs/2502.13648)
Append: [Rapid Word Learning Through Meta In-Context Learning](https://arxiv.org/abs/2502.14791)
Append: [Sparsity May Be All You Need: Sparse Random Parameter Adaptation](https://arxiv.org/abs/2502.15975)
Append: [Finding the Sweet Spot: Preference Data Construction for Scaling Preference Optimization](https://arxiv.org/abs/2502.16825)
Append: [Spontaneous Giving and Calculated Greed in Language Models](https://arxiv.org/abs/2502.17720)
Append: [Stay Focused: Problem Drift in Multi-Agent Debate](https://arxiv.org/abs/2502.19559)
Append: [Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs](https://arxiv.org/abs/2502.19721)
Append: [Adaptively profiling models with task elicitation](https://arxiv.org/abs/2503.01986)
Append: [Scaling Laws for Many-Shot In-Context Learning with Self-Generated Annotations](https://arxiv.org/abs/2503.03062)
Append: [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/abs/2503.03122)
Append: [DB-Explore: Automated Database Exploration and Instruction Synthesis for Text-to-SQL](https://arxiv.org/abs/2503.04959)
Append: [Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching](https://arxiv.org/abs/2503.05179)
Append: [Large Language Models Post-training: Surveying Techniques from Alignment to Reasoning](https://arxiv.org/abs/2503.06072)
Append: [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)
Append: [Adaptive Group Policy Optimization: Towards Stable Training and Token-Efficient Reasoning](https://arxiv.org/abs/2503.15952)
Append: [AfroXLMR-Social: Adapting Pre-trained Language Models for African Languages Social Media Text](https://arxiv.org/abs/2503.18247)
Append: [A Multilingual, Culture-First Approach to Addressing Misgendering in LLM Applications](https://arxiv.org/abs/2503.20302)
Append: [Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation](https://arxiv.org/abs/2503.24245)
Append: [GLiNER-BioMed: A Suite of Efficient Models for Open Biomedical Named Entity Recognition](https://arxiv.org/abs/2504.00676)
Append: [Think When You Need: Self-Adaptive Chain-of-Thought Learning](https://arxiv.org/abs/2504.03234)
Append: [Thinking Out Loud: Do Reasoning Models Know When They're Right?](https://arxiv.org/abs/2504.06564)
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Append: [Instruction-Tuning Data Synthesis from Scratch via Web Reconstruction](https://arxiv.org/abs/2504.15573)
Append: [Improving Language Model Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)
Append: [Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Append: [Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models](https://arxiv.org/abs/2505.01731)
Append: [Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs](https://arxiv.org/abs/2505.02009)
Append: [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)
Append: [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)
Append: [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)
Append: [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)
Append: [SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network](https://arxiv.org/abs/2310.06488)
Append: [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264)
Append: [DPO Meets PPO: Reinforced Token Optimization for RLHF](https://arxiv.org/abs/2404.18922)
Append: [Inverse Design of Metal-Organic Frameworks Using Quantum Natural Language Processing](https://arxiv.org/abs/2405.11783)
Append: [SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Model](https://arxiv.org/abs/2406.12030)
Append: [Parameter-Efficient Fine-Tuning via Circular Convolution](https://arxiv.org/abs/2407.19342)
Append: [An In-Depth Investigation of Data Collection in LLM App Ecosystems](https://arxiv.org/abs/2408.13247)
Append: [NESTFUL: A Benchmark for Evaluating LLMs on Nested Sequences of API Calls](https://arxiv.org/abs/2409.03797)
Append: [Quantifying Feature Space Universality Across Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2410.06981)
Append: [Parameter Efficient Fine-tuning via Explained Variance Adaptation](https://arxiv.org/abs/2410.07170)
Append: [How to Construct Random Unitaries](https://arxiv.org/abs/2410.10116)
Append: [WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning](https://arxiv.org/abs/2501.16344)
Append: [PixelWorld: Towards Perceiving Everything as Pixels](https://arxiv.org/abs/2501.19339)
Append: [How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large Language Models with Kernel Divergence](https://arxiv.org/abs/2502.00678)
Append: [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](https://arxiv.org/abs/2502.01068)
Append: [The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles](https://arxiv.org/abs/2502.01081)
Append: [Neurons Speak in Ranges: Breaking Free from Discrete Neuronal Attribution](https://arxiv.org/abs/2502.06809)
Append: [MoHAVE: Mixture of Hierarchical Audio-Visual Experts for Robust Speech Recognition](https://arxiv.org/abs/2502.10447)
Append: [Probing Semantic Routing in Large Mixture-of-Expert Models](https://arxiv.org/abs/2502.10928)
Append: [Automated Visualization Code Synthesis via Multi-Path Reasoning and Feedback-Driven Optimization](https://arxiv.org/abs/2502.11140)
Append: [GiFT: Gibbs Fine-Tuning for Code Generation](https://arxiv.org/abs/2502.11466)
Append: [Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation](https://arxiv.org/abs/2502.14846)
Append: [Intermediate Languages Matter: Formal Choice Drives Neurosymbolic LLM Reasoning](https://arxiv.org/abs/2502.17216)
Append: [Large Language Models are Powerful Electronic Health Record Encoders](https://arxiv.org/abs/2502.17403)
Append: [ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction](https://arxiv.org/abs/2502.18744)
Append: [SQLCritic: Correcting Text-to-SQL Generation via Clause-wise Critic](https://arxiv.org/abs/2503.07996)
Append: [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Design and Implementation of an FPGA-Based Hardware Accelerator for Transformer](https://arxiv.org/abs/2503.16731)
Append: [Plain Transformers Can be Powerful Graph Learners](https://arxiv.org/abs/2504.12588)
Append: [ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification](https://arxiv.org/abs/2504.20930)
Append: [Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization](https://arxiv.org/abs/2504.21659)
Append: [SWE-smith: Scaling Data for Software Engineering Agents](https://arxiv.org/abs/2504.21798)
Append: [Scalable Chain of Thoughts via Elastic Reasoning](https://arxiv.org/abs/2505.05315)
append_entries: 288
Finish: 2025-05-22 04:26:48.503834
------------------------------------------------------
Started: 2025-05-22 06:24:39.848926
Existing_entries: 1288
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Large Language Models Are More Persuasive Than Incentivized Human Persuaders](https://arxiv.org/abs/2505.09662)
Token length: 1547
Summarized using GPT-3.5-turbo
Append: [Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models](https://arxiv.org/abs/2505.10446)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)
append_entries: 3
Finish: 2025-05-22 06:24:47.496843
------------------------------------------------------
Started: 2025-05-22 08:25:06.511999
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 08:25:07.187998
------------------------------------------------------
Started: 2025-05-22 10:18:16.315037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 10:18:17.025357
------------------------------------------------------
Started: 2025-05-22 12:34:57.470534
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 12:34:58.070158
------------------------------------------------------
Started: 2025-05-22 14:16:38.128724
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 14:16:38.741083
------------------------------------------------------
Started: 2025-05-22 16:20:57.760111
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 16:20:58.471178
------------------------------------------------------
Started: 2025-05-22 18:23:07.590818
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 18:23:08.208239
------------------------------------------------------
Started: 2025-05-22 20:18:25.348077
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 20:18:25.981756
------------------------------------------------------
Started: 2025-05-22 22:16:05.593090
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-22 22:16:06.192508
------------------------------------------------------
Started: 2025-05-23 01:19:00.778553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 01:19:01.476595
------------------------------------------------------
Started: 2025-05-23 03:09:21.731424
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 03:09:22.381982
------------------------------------------------------
Started: 2025-05-23 04:30:47.266131
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/abs/2505.15922)
Token length: 1512
Summarized using GPT-3.5-turbo
Append: [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
Token length: 1721
Summarized using GPT-3.5-turbo
Append: [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/abs/2505.15960)
Token length: 1021
Summarized using GPT-3.5-turbo
Append: [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
Token length: 846
Summarized using GPT-3.5-turbo
Append: [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/abs/2505.15993)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
Token length: 1743
Summarized using GPT-3.5-turbo
Append: [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
Token length: 1274
Summarized using GPT-3.5-turbo
Append: [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/abs/2505.16036)
Token length: 936
Summarized using GPT-3.5-turbo
Append: [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/abs/2505.16061)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
Token length: 1492
Summarized using GPT-3.5-turbo
Append: [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
Token length: 1569
Summarized using GPT-3.5-turbo
Append: [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/abs/2505.16102)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/abs/2505.16107)
Token length: 812
Summarized using GPT-3.5-turbo
Append: [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/abs/2505.16118)
Token length: 1591
Summarized using GPT-3.5-turbo
Append: [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/abs/2505.16125)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/abs/2505.16128)
Token length: 932
Summarized using GPT-3.5-turbo
Append: [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/abs/2505.16129)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
Token length: 1579
Summarized using GPT-3.5-turbo
Append: [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
Token length: 1038
Summarized using GPT-3.5-turbo
Append: [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/abs/2505.16162)
Token length: 1044
Summarized using GPT-3.5-turbo
Append: [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/abs/2505.16164)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
Token length: 1399
Summarized using GPT-3.5-turbo
Append: [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/abs/2505.16178)
Token length: 1180
Summarized using GPT-3.5-turbo
Append: [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/abs/2505.16188)
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/abs/2505.16189)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
Token length: 952
Summarized using GPT-3.5-turbo
Append: [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/abs/2505.16212)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
Token length: 1267
Summarized using GPT-3.5-turbo
Append: [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
Token length: 1789
Summarized using GPT-3.5-turbo
Append: [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/abs/2505.16277)
Append: [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
Append: [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293)
Append: [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
Append: [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/abs/2505.16303)
Append: [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
Append: [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
Append: [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
Append: [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/abs/2505.16348)
Append: [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/abs/2505.16349)
Append: [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
Append: [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2505.16385)
Append: [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
Append: [On the reliability of feature attribution methods for speech classification](https://arxiv.org/abs/2505.16406)
Append: [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
Append: [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
Append: [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
Append: [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/abs/2505.16418)
Append: [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
Append: [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
Append: [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
Append: [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
Append: [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
Append: [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
Append: [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
Append: [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
Append: [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
Append: [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
Append: [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
Append: [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/abs/2505.16526)
Append: [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/abs/2505.16566)
Append: [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570)
Append: [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
Append: [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
Append: [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/abs/2505.16591)
Append: [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
Append: [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/abs/2505.16610)
Append: [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
Append: [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
Append: [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
Append: [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
Append: [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
Append: [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
Append: [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/abs/2505.16703)
Append: [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
Append: [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
Append: [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/abs/2505.16774)
Append: [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.16782)
Append: [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
Append: [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/abs/2505.16800)
Append: [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/abs/2505.16806)
Append: [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
Append: [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
Append: [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
Append: [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/abs/2505.16838)
Append: [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/abs/2505.16847)
Append: [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
Append: [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/abs/2505.16868)
Append: [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869)
Append: [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
Append: [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
Append: [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
Append: [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
Append: [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/abs/2505.16931)
Append: [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934)
Append: [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/abs/2505.16956)
Append: [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
Append: [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/abs/2505.16972)
Append: [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/abs/2505.16973)
Append: [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
Append: [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
Append: [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
Append: [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/abs/2505.16995)
Append: [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
Append: [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
Append: [InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872)
Append: [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
Append: [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
Append: [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
Append: [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
Append: [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
Append: [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
Append: [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
Append: [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
Append: [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
Append: [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
Append: [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
Append: [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
Append: [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
Append: [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
Append: [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
Append: [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
Append: [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
Append: [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
Append: [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
Append: [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
Append: [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
Append: [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
Append: [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
Append: [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/abs/2505.16220)
Append: [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
Append: [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
Append: [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
Append: [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
Append: [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
Append: [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
Append: [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
Append: [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)
Append: [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
Append: [MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries](https://arxiv.org/abs/2505.16631)
Append: [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
Append: [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
Append: [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
Append: [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
Append: [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
Append: [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
Append: [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
Append: [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888)
Append: [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
Append: [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
Append: [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
Append: [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/abs/2505.16944)
Append: [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/abs/2505.16964)
Append: [Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](https://arxiv.org/abs/2505.16967)
Append: [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
Append: [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
Append: [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16984)
Append: [$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning](https://arxiv.org/abs/2505.16994)
Append: [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/abs/2505.16997)
Append: [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015)
Append: [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
Append: [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/abs/2505.17022)
Append: [Language Models are Universal Embedders](https://arxiv.org/abs/2310.08232)
Append: [Large Language Models are Miscalibrated In-Context Learners](https://arxiv.org/abs/2312.13772)
Append: [EntGPT: Entity Linking with Generative Large Language Models](https://arxiv.org/abs/2402.06738)
Append: [Red-Teaming for Inducing Societal Bias in Large Language Models](https://arxiv.org/abs/2405.04756)
Append: [BlockPruner: Fine-grained Pruning for Large Language Models](https://arxiv.org/abs/2406.10594)
Append: [Determination of language families using deep learning](https://arxiv.org/abs/2409.02393)
Append: [MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding Benchmark](https://arxiv.org/abs/2409.02813)
Append: [Normal forms in Virus Machines](https://arxiv.org/abs/2409.03327)
Append: [LangSAMP: Language-Script Aware Multilingual Pretraining](https://arxiv.org/abs/2409.18199)
Append: [GLEE: A Unified Framework and Benchmark for Language-based Economic Environments](https://arxiv.org/abs/2410.05254)
Append: [Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering](https://arxiv.org/abs/2410.08085)
Append: [Keys to Robust Edits: from Theoretical Insights to Practical Advances](https://arxiv.org/abs/2410.09338)
Append: [A Unified Approach to Routing and Cascading for LLMs](https://arxiv.org/abs/2410.10347)
Append: [Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of Architectural Inductive Biases on Hallucination](https://arxiv.org/abs/2410.17477)
Append: [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
Append: [AAAR-1.0: Assessing AI's Potential to Assist Research](https://arxiv.org/abs/2410.22394)
Append: [Graph-based Confidence Calibration for Large Language Models](https://arxiv.org/abs/2411.02454)
Append: [Prompt-Guided Internal States for Hallucination Detection of Large Language Models](https://arxiv.org/abs/2411.04847)
Append: [Evaluating Automated Radiology Report Quality through Fine-Grained Phrasal Grounding of Clinical Findings](https://arxiv.org/abs/2412.01031)
Append: [Evaluating LLM-based Approaches to Legal Citation Prediction: Domain-specific Pre-training, Fine-tuning, or RAG? A Benchmark and an Australian Law Case Study](https://arxiv.org/abs/2412.06272)
Append: [My Words Imply Your Opinion: Reader Agent-based Propagation Enhancement for Personalized Implicit Emotion Analysis](https://arxiv.org/abs/2412.07367)
Append: [LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework](https://arxiv.org/abs/2412.12459)
Append: [DocFusion: A Unified Framework for Document Parsing Tasks](https://arxiv.org/abs/2412.12505)
Append: [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
Append: [BenCzechMark : A Czech-centric Multitask and Multimetric Benchmark for Large Language Models with Duel Scoring Mechanism](https://arxiv.org/abs/2412.17933)
Append: [Diverse Preference Optimization](https://arxiv.org/abs/2501.18101)
Append: [ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration](https://arxiv.org/abs/2502.00675)
Append: [FIRE: Flexible Integration of Data Quality Ratings for Effective Pre-Training](https://arxiv.org/abs/2502.00761)
Append: [Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data](https://arxiv.org/abs/2502.04380)
Append: [Is a Peeled Apple Still Red? Evaluating LLMs' Ability for Conceptual Combination with Property Type](https://arxiv.org/abs/2502.06086)
Append: [LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs](https://arxiv.org/abs/2502.06139)
Append: [C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation](https://arxiv.org/abs/2502.06205)
Append: [No Need for Explanations: LLMs can implicitly learn from mistakes in-context](https://arxiv.org/abs/2502.08550)
Append: [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/abs/2502.09604)
Append: [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
Append: [GRIFFIN: Effective Token Alignment for Faster Speculative Decoding](https://arxiv.org/abs/2502.11018)
Append: [SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL](https://arxiv.org/abs/2502.11438)
Append: [M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2502.11824)
Append: [Whose story is it? Personalizing story generation by inferring author styles](https://arxiv.org/abs/2502.13028)
Append: [Transferring Textual Preferences to Vision-Language Understanding through Model Merging](https://arxiv.org/abs/2502.13487)
Append: [CoT-ICL Lab: A Synthetic Framework for Studying Chain-of-Thought Learning from In-Context Demonstrations](https://arxiv.org/abs/2502.15132)
Append: [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](https://arxiv.org/abs/2502.16002)
Append: [FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks](https://arxiv.org/abs/2502.17775)
Append: [Towards Better Understanding of Program-of-Thought Reasoning in Cross-Lingual and Multilingual Environments](https://arxiv.org/abs/2502.17956)
Append: [Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents](https://arxiv.org/abs/2502.20073)
Append: [HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization](https://arxiv.org/abs/2503.04598)
Append: [ReviewAgents: Bridging the Gap Between Human and AI-Generated Paper Reviews](https://arxiv.org/abs/2503.08506)
Append: [Uncertainty Distillation: Teaching Language Models to Express Semantic Confidence](https://arxiv.org/abs/2503.14749)
Append: [From 1,000,000 Users to Every User: Scaling Up Personalized Preference for User-level Alignment](https://arxiv.org/abs/2503.15463)
Append: [Through the LLM Looking Glass: A Socratic Probing of Donkeys, Elephants, and Markets](https://arxiv.org/abs/2503.16674)
Append: [Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning](https://arxiv.org/abs/2503.16965)
Append: [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Append: [DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](https://arxiv.org/abs/2503.19498)
Append: [Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching](https://arxiv.org/abs/2503.20083)
Append: [TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling](https://arxiv.org/abs/2504.07053)
Append: [Improving Multilingual Capabilities with Cultural and Local Knowledge in Large Language Models While Enhancing Native Performance](https://arxiv.org/abs/2504.09753)
Append: [Hallucination Detection in LLMs with Topological Divergence on Attention Graphs](https://arxiv.org/abs/2504.10063)
Append: [Robust and Fine-Grained Detection of AI Generated Texts](https://arxiv.org/abs/2504.11952)
Append: [SMARTe: Slot-based Method for Accountable Relational Triple extraction](https://arxiv.org/abs/2504.12816)
Append: [Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators](https://arxiv.org/abs/2504.15253)
Append: [TTRL: Test-Time Reinforcement Learning](https://arxiv.org/abs/2504.16084)
Append: [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)
Append: [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)
Append: [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Append: [Say It Another Way: Auditing LLMs with a User-Grounded Automated Paraphrasing Framework](https://arxiv.org/abs/2505.03563)
Append: [LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering](https://arxiv.org/abs/2505.05423)
Append: [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)
Append: [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)
Append: [LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization](https://arxiv.org/abs/2305.04971)
Append: [CodeMind: Evaluating Large Language Models for Code Reasoning](https://arxiv.org/abs/2402.09664)
Append: [FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering](https://arxiv.org/abs/2405.13873)
Append: [How Well Can a Long Sequence Model Model Long Sequences? Comparing Architechtural Inductive Biases on Long-Context Abilities](https://arxiv.org/abs/2407.08112)
Append: [More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding](https://arxiv.org/abs/2408.15966)
Append: [Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for Exploration and Exploitation in Recommender Systems](https://arxiv.org/abs/2411.13865)
Append: [Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts](https://arxiv.org/abs/2412.04614)
Append: [Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian](https://arxiv.org/abs/2501.11264)
Append: [To Code or not to Code? Adaptive Tool Integration for Math Language Models via Expectation-Maximization](https://arxiv.org/abs/2502.00691)
Append: [Slamming: Training a Speech Language Model on One GPU in a Day](https://arxiv.org/abs/2502.15814)
Append: [Similarity-Distance-Magnitude Universal Verification](https://arxiv.org/abs/2502.20167)
Append: [Retrieval-Augmented Perception: High-Resolution Image Perception Meets Visual RAG](https://arxiv.org/abs/2503.01222)
Append: [Steer LLM Latents for Hallucination Detection](https://arxiv.org/abs/2503.01917)
Append: [Transformers for molecular property prediction: Domain adaptation efficiently improves performance](https://arxiv.org/abs/2503.03360)
Append: [Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts](https://arxiv.org/abs/2503.05066)
Append: [MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?](https://arxiv.org/abs/2503.09499)
Append: [Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models](https://arxiv.org/abs/2503.20576)
append_entries: 271
Finish: 2025-05-23 04:35:33.824028
------------------------------------------------------
Started: 2025-05-23 06:24:29.000999
Existing_entries: 1271
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1125
Summarized using GPT-3.5-turbo
Append: [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
append_entries: 4
Finish: 2025-05-23 06:24:37.080464
------------------------------------------------------
Started: 2025-05-23 08:22:14.266211
Existing_entries: 1004
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 08:22:14.846135
------------------------------------------------------
Started: 2025-05-23 10:17:46.897739
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 10:17:47.478582
------------------------------------------------------
Started: 2025-05-23 12:33:10.922271
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 12:33:11.590726
------------------------------------------------------
Started: 2025-05-23 14:15:59.605118
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 14:16:00.225100
------------------------------------------------------
Started: 2025-05-23 16:20:00.026904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 16:20:00.669140
------------------------------------------------------
Started: 2025-05-23 18:21:27.355076
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 18:21:27.938311
------------------------------------------------------
Started: 2025-05-23 20:18:20.074644
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 20:18:20.702629
------------------------------------------------------
Started: 2025-05-23 22:15:46.373433
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-23 22:15:46.946569
------------------------------------------------------
Started: 2025-05-24 01:16:09.705148
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 01:16:10.289553
------------------------------------------------------
Started: 2025-05-24 03:03:51.247139
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 03:03:51.916913
------------------------------------------------------
Started: 2025-05-24 04:18:59.578741
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 04:18:59.643109
------------------------------------------------------
Started: 2025-05-24 06:21:09.447247
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 06:21:09.524598
------------------------------------------------------
Started: 2025-05-24 08:19:02.227821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 08:19:02.345400
------------------------------------------------------
Started: 2025-05-24 10:15:39.260693
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 10:15:39.320388
------------------------------------------------------
Started: 2025-05-24 12:30:09.592412
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 12:30:09.657835
------------------------------------------------------
Started: 2025-05-24 14:13:47.071589
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 14:13:47.133103
------------------------------------------------------
Started: 2025-05-24 16:18:12.897787
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 16:18:12.950882
------------------------------------------------------
Started: 2025-05-24 18:20:31.315128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 18:20:31.369411
------------------------------------------------------
Started: 2025-05-24 20:16:41.143313
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 20:16:41.196972
------------------------------------------------------
Started: 2025-05-24 22:14:41.105882
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-24 22:14:41.164455
------------------------------------------------------
Started: 2025-05-25 01:25:47.440750
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 01:25:47.521681
------------------------------------------------------
Started: 2025-05-25 03:17:43.175539
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 03:17:43.244043
------------------------------------------------------
Started: 2025-05-25 04:23:13.399152
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 04:23:13.474157
------------------------------------------------------
Started: 2025-05-25 06:21:29.464935
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 06:21:29.535799
------------------------------------------------------
Started: 2025-05-25 08:19:15.733380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 08:19:15.789991
------------------------------------------------------
Started: 2025-05-25 10:16:04.479884
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 10:16:04.539247
------------------------------------------------------
Started: 2025-05-25 12:30:25.520204
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 12:30:25.591784
------------------------------------------------------
Started: 2025-05-25 14:13:56.524611
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 14:13:56.599616
------------------------------------------------------
Started: 2025-05-25 16:18:30.668141
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 16:18:30.719751
------------------------------------------------------
Started: 2025-05-25 18:20:29.639196
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 18:20:29.694466
------------------------------------------------------
Started: 2025-05-25 20:16:44.848153
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 20:16:44.963651
------------------------------------------------------
Started: 2025-05-25 22:15:01.068796
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-25 22:15:01.127998
------------------------------------------------------
Started: 2025-05-26 01:21:06.135810
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 01:21:06.218666
------------------------------------------------------
Started: 2025-05-26 03:14:34.396552
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 03:14:34.457337
------------------------------------------------------
Started: 2025-05-26 04:26:27.806251
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)
Token length: 1595
Summarized using GPT-3.5-turbo
Append: [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)
Token length: 1506
Summarized using GPT-3.5-turbo
Append: [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)
Token length: 1803
Summarized using GPT-3.5-turbo
Append: [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\'esum\'e Evaluations](https://arxiv.org/abs/2505.17049)
Token length: 1715
Summarized using GPT-3.5-turbo
Append: [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)
Token length: 904
Summarized using GPT-3.5-turbo
Append: [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)
Token length: 1757
Summarized using GPT-3.5-turbo
Append: [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)
Token length: 1210
Summarized using GPT-3.5-turbo
Append: [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)
Token length: 948
Summarized using GPT-3.5-turbo
Append: [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)
Token length: 1758
Summarized using GPT-3.5-turbo
Append: [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)
Token length: 1590
Summarized using GPT-3.5-turbo
Append: [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)
Token length: 878
Summarized using GPT-3.5-turbo
Append: [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/abs/2505.17071)
Token length: 1253
Summarized using GPT-3.5-turbo
Append: [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)
Token length: 1487
Summarized using GPT-3.5-turbo
Append: [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)
Token length: 1006
Summarized using GPT-3.5-turbo
Append: [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)
Token length: 1650
Summarized using GPT-3.5-turbo
Append: [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)
Token length: 820
Summarized using GPT-3.5-turbo
Append: [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)
Token length: 1284
Summarized using GPT-3.5-turbo
Append: [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/abs/2505.17089)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/abs/2505.17095)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/abs/2505.17099)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/abs/2505.17102)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/abs/2505.17110)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek](https://arxiv.org/abs/2505.17112)
Token length: 1570
Summarized using GPT-3.5-turbo
Append: [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
Append: [Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data](https://arxiv.org/abs/2505.17116)
Append: [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)
Append: [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/abs/2505.17118)
Append: [Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models](https://arxiv.org/abs/2505.17119)
Append: [Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training](https://arxiv.org/abs/2505.17120)
Append: [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/abs/2505.17121)
Append: [Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?](https://arxiv.org/abs/2505.17122)
Append: [MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation](https://arxiv.org/abs/2505.17123)
Append: [Conformal Language Model Reasoning with Coherent Factuality](https://arxiv.org/abs/2505.17126)
Append: [Relative Bias: A Comparative Framework for Quantifying Bias in LLMs](https://arxiv.org/abs/2505.17131)
Append: [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)
Append: [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)
Append: [Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations](https://arxiv.org/abs/2505.17136)
Append: [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)
Append: [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
Append: [Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs](https://arxiv.org/abs/2505.17140)
Append: [MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models](https://arxiv.org/abs/2505.17144)
Append: [Large Language Models for Predictive Analysis: How Far Are They?](https://arxiv.org/abs/2505.17149)
Append: [Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions](https://arxiv.org/abs/2505.17151)
Append: [Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN](https://arxiv.org/abs/2505.17153)
Append: [PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG](https://arxiv.org/abs/2505.17156)
Append: [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160)
Append: [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/abs/2505.17167)
Append: [Next Token Perception Score: Analytical Assessment of your LLM Perception Skills](https://arxiv.org/abs/2505.17169)
Append: [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/abs/2505.17206)
Append: [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/abs/2505.17217)
Append: [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/abs/2505.17222)
Append: [ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects](https://arxiv.org/abs/2505.17231)
Append: [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2505.17238)
Append: [ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models](https://arxiv.org/abs/2505.17244)
Append: [ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models](https://arxiv.org/abs/2505.17250)
Append: [The Rise of Parameter Specialization for Knowledge Storage in Large Language Models](https://arxiv.org/abs/2505.17260)
Append: [CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports](https://arxiv.org/abs/2505.17265)
Append: [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/abs/2505.17266)
Append: [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/abs/2505.17267)
Append: [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/abs/2505.17281)
Append: [SELF: Self-Extend the Context Length With Logistic Growth Function](https://arxiv.org/abs/2505.17296)
Append: [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)
Append: [Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2](https://arxiv.org/abs/2505.17320)
Append: [From Compression to Expansion: A Layerwise Analysis of In-Context Learning](https://arxiv.org/abs/2505.17322)
Append: [GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints](https://arxiv.org/abs/2505.17327)
Append: [SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332)
Append: [Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking](https://arxiv.org/abs/2505.17345)
Append: [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)
Append: [AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing](https://arxiv.org/abs/2505.17380)
Append: [WiNGPT-3.0 Technical Report](https://arxiv.org/abs/2505.17387)
Append: [Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting](https://arxiv.org/abs/2505.17390)
Append: [Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation](https://arxiv.org/abs/2505.17391)
Append: [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](https://arxiv.org/abs/2505.17399)
Append: [Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?](https://arxiv.org/abs/2505.17407)
Append: [Conversations: Love Them, Hate Them, Steer Them](https://arxiv.org/abs/2505.17413)
Append: [DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](https://arxiv.org/abs/2505.17420)
Append: [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/abs/2505.17427)
Append: [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)
Append: [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/abs/2505.17446)
Append: [LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization](https://arxiv.org/abs/2505.17447)
Append: [Towards Evaluating Proactive Risk Awareness of Multimodal Language Models](https://arxiv.org/abs/2505.17455)
Append: [Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.17464)
Append: [A Position Paper on the Automatic Generation of Machine Learning Leaderboards](https://arxiv.org/abs/2505.17465)
Append: [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://arxiv.org/abs/2505.17470)
Append: [FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain](https://arxiv.org/abs/2505.17471)
Append: [MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning](https://arxiv.org/abs/2505.17481)
Append: [keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection](https://arxiv.org/abs/2505.17485)
Append: [Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models](https://arxiv.org/abs/2505.17496)
Append: [CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents](https://arxiv.org/abs/2505.17503)
Append: [L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](https://arxiv.org/abs/2505.17505)
Append: [Large Language Models Do Multi-Label Classification Differently](https://arxiv.org/abs/2505.17510)
Append: [Multimodal Conversation Structure Understanding](https://arxiv.org/abs/2505.17536)
Append: [How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception](https://arxiv.org/abs/2505.17537)
Append: [Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition](https://arxiv.org/abs/2505.17538)
Append: [Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection](https://arxiv.org/abs/2505.17558)
Append: [PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models](https://arxiv.org/abs/2505.17565)
Append: [Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation](https://arxiv.org/abs/2505.17571)
Append: [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
Append: [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/abs/2505.17612)
Append: [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/abs/2505.17616)
Append: [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/abs/2505.17625)
Append: [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/abs/2505.17630)
Append: [Stereotype Detection in Natural Language Processing](https://arxiv.org/abs/2505.17642)
Append: [Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks](https://arxiv.org/abs/2505.17643)
Append: [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)
Append: [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
Append: [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)
Append: [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)
Append: [MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis](https://arxiv.org/abs/2505.17671)
Append: [Tuning Language Models for Robust Prediction of Diverse User Behaviors](https://arxiv.org/abs/2505.17682)
Append: [ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction](https://arxiv.org/abs/2505.17691)
Append: [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/abs/2505.17697)
Append: [SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus](https://arxiv.org/abs/2505.17704)
Append: [Understanding How Value Neurons Shape the Generation of Specified Values in LLMs](https://arxiv.org/abs/2505.17712)
Append: [The Pilot Corpus of the English Semantic Sketches](https://arxiv.org/abs/2505.17733)
Append: [Fast Quiet-STaR: Thinking Without Thought Tokens](https://arxiv.org/abs/2505.17746)
Append: [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)
Append: [Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.17762)
Append: [The Real Barrier to LLM Agent Usability is Agentic ROI](https://arxiv.org/abs/2505.17767)
Append: [EXECUTE: A Multilingual Benchmark for LLM Token Understanding](https://arxiv.org/abs/2505.17784)
Append: [Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion](https://arxiv.org/abs/2505.17793)
Append: [DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors](https://arxiv.org/abs/2505.17795)
Append: [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning](https://arxiv.org/abs/2505.17813)
Append: [Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong](https://arxiv.org/abs/2505.17816)
Append: [Not All Tokens Are What You Need In Thinking](https://arxiv.org/abs/2505.17827)
Append: [Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning](https://arxiv.org/abs/2505.17829)
Append: [Emerging categories in scientific explanations](https://arxiv.org/abs/2505.17832)
Append: [Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus](https://arxiv.org/abs/2505.17833)
Append: [Explaining Sources of Uncertainty in Automated Fact-Checking](https://arxiv.org/abs/2505.17855)
Append: [Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods](https://arxiv.org/abs/2505.17870)
Append: [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)
Append: [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/abs/2505.17894)
Append: [Language models can learn implicit multi-hop reasoning, but only if they have lots of training data](https://arxiv.org/abs/2505.17923)
Append: [Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models](https://arxiv.org/abs/2505.17950)
Append: [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952)
Append: [Counting Cycles with Deepseek](https://arxiv.org/abs/2505.17964)
Append: [AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web](https://arxiv.org/abs/2505.17978)
Append: [TRACE for Tracking the Emergence of Semantic Representations in Transformers](https://arxiv.org/abs/2505.17998)
Append: [Training with Pseudo-Code for Instruction Following](https://arxiv.org/abs/2505.18011)
Append: [Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition](https://arxiv.org/abs/2505.18040)
Append: [MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving](https://arxiv.org/abs/2505.18056)
Append: [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/abs/2505.18071)
Append: [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)
Append: [Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL](https://arxiv.org/abs/2505.18098)
Append: [ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework](https://arxiv.org/abs/2505.18105)
Append: [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/abs/2505.18110)
Append: [UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification](https://arxiv.org/abs/2505.18122)
Append: [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
Append: [Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection](https://arxiv.org/abs/2505.18136)
Append: [Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find](https://arxiv.org/abs/2505.18148)
Append: [First Finish Search: Efficient Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2505.18149)
Append: [Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs](https://arxiv.org/abs/2505.18152)
Append: [The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas](https://arxiv.org/abs/2505.18154)
Append: [Generalizing Large Language Model Usability Across Resource-Constrained](https://arxiv.org/abs/2505.17040)
Append: [Exploring EFL Secondary Students' AI-generated Text Editing While Composition Writing](https://arxiv.org/abs/2505.17041)
Append: [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)
Append: [GSDFuse: Capturing Cognitive Inconsistencies from Multi-Dimensional Weak Signals in Social Media Steganalysis](https://arxiv.org/abs/2505.17085)
Append: [From Weak Labels to Strong Results: Utilizing 5,000 Hours of Noisy Classroom Transcripts with Minimal Accurate Data](https://arxiv.org/abs/2505.17088)
Append: [Voicing Personas: Rewriting Persona Descriptions into Style Prompts for Controllable Text-to-Speech](https://arxiv.org/abs/2505.17093)
Append: [CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention](https://arxiv.org/abs/2505.17097)
Append: [Mitigating Cyber Risk in the Age of Open-Weight LLMs: Policy Gaps and Technical Realities](https://arxiv.org/abs/2505.17109)
Append: [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
Append: [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
Append: [OCR-Reasoning Benchmark: Unveiling the True Capabilities of MLLMs in Complex Text-Rich Image Reasoning](https://arxiv.org/abs/2505.17163)
Append: [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)
Append: [CHAOS: Chart Analysis with Outlier Samples](https://arxiv.org/abs/2505.17235)
Append: [Zebra-Llama: Towards Extremely Efficient Hybrid Models](https://arxiv.org/abs/2505.17272)
Append: [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
Append: [Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning](https://arxiv.org/abs/2505.17315)
Append: [Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models](https://arxiv.org/abs/2505.17316)
Append: [FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding](https://arxiv.org/abs/2505.17330)
Append: [ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training](https://arxiv.org/abs/2505.17331)
Append: [DEL-ToM: Inference-Time Scaling for Theory-of-Mind Reasoning via Dynamic Epistemic Logic](https://arxiv.org/abs/2505.17348)
Append: [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
Append: [Value-Guided Search for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.17373)
Append: [Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts](https://arxiv.org/abs/2505.17374)
Append: [LLM-based Generative Error Correction for Rare Words with Synthetic Data and Phonetic Context](https://arxiv.org/abs/2505.17410)
Append: [Speechless: Speech Instruction Training Without Speech for Low Resource Languages](https://arxiv.org/abs/2505.17417)
Append: [Debiasing CLIP: Interpreting and Correcting Bias in Attention Heads](https://arxiv.org/abs/2505.17425)
Append: [Self-Training Large Language Models with Confident Reasoning](https://arxiv.org/abs/2505.17454)
Append: [Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies](https://arxiv.org/abs/2505.17461)
Append: [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
Append: [From Reasoning to Generalization: Knowledge-Augmented LLMs for ARC Benchmark](https://arxiv.org/abs/2505.17482)
Append: [PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate](https://arxiv.org/abs/2505.17492)
Append: [ProxySPEX: Inference-Efficient Interpretability via Sparse Feature Interactions in LLMs](https://arxiv.org/abs/2505.17495)
Append: [On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning](https://arxiv.org/abs/2505.17508)
Append: [Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs](https://arxiv.org/abs/2505.17512)
Append: [What You Read Isn't What You Hear: Linguistic Sensitivity in Deepfake Speech Detection](https://arxiv.org/abs/2505.17513)
Append: [Chain-of-Lure: A Synthetic Narrative-Driven Approach to Compromise Large Language Models](https://arxiv.org/abs/2505.17519)
Append: [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
Append: [CoMoE: Contrastive Representation for Mixture-of-Experts in Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.17553)
Append: [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
Append: [One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs](https://arxiv.org/abs/2505.17598)
Append: [Controlled Agentic Planning & Reasoning for Mechanism Synthesis](https://arxiv.org/abs/2505.17607)
Append: [MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask Multimodal Generation](https://arxiv.org/abs/2505.17613)
Append: [Large language model as user daily behavior data generator: balancing population diversity and individual personality](https://arxiv.org/abs/2505.17615)
Append: [Surfacing Semantic Orthogonality Across Model Safety Benchmarks: A Multi-Dimensional Analysis](https://arxiv.org/abs/2505.17636)
Append: [HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning](https://arxiv.org/abs/2505.17645)
Append: [COUNTDOWN: Contextually Sparse Activation Filtering Out Unnecessary Weights in Down Projection](https://arxiv.org/abs/2505.17701)
Append: [PPO-BR: Dual-Signal Entropy-Reward Adaptation for Trust Region Policy Optimization](https://arxiv.org/abs/2505.17714)
Append: [PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions](https://arxiv.org/abs/2505.17818)
Append: [Trinity-RFT: A General-Purpose and Unified Framework for Reinforcement Fine-Tuning of Large Language Models](https://arxiv.org/abs/2505.17826)
Append: [T2I-Eval-R1: Reinforcement Learning-Driven Reasoning for Interpretable Text-to-Image Evaluation](https://arxiv.org/abs/2505.17897)
Append: [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
Append: [Understanding Gated Neurons in Transformers from Their Input-Output Functionality](https://arxiv.org/abs/2505.17936)
Append: [Are Large Language Models Reliable AI Scientists? Assessing Reverse-Engineering of Black-Box Systems](https://arxiv.org/abs/2505.17968)
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
Append: [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
Append: [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
Append: [Data Mixing Can Induce Phase Transitions in Knowledge Acquisition](https://arxiv.org/abs/2505.18091)
Append: [How Can I Publish My LLM Benchmark Without Giving the True Answers Away?](https://arxiv.org/abs/2505.18102)
Append: [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
Append: [ProgRM: Build Better GUI Agents with Progress Rewards](https://arxiv.org/abs/2505.18121)
Append: [TabSTAR: A Foundation Tabular Model With Semantically Target-Aware Representations](https://arxiv.org/abs/2505.18125)
Append: [Reward Model Overoptimisation in Iterated RLHF](https://arxiv.org/abs/2505.18126)
Append: [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
Append: [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
Append: [Gaming Tool Preferences in Agentic LLMs](https://arxiv.org/abs/2505.18135)
Append: [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources](https://arxiv.org/abs/2310.07147)
Append: [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)
Append: [Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy](https://arxiv.org/abs/2405.03111)
Append: [ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios](https://arxiv.org/abs/2405.10808)
Append: [Mitigate Position Bias in Large Language Models via Scaling a Single Dimension](https://arxiv.org/abs/2406.02536)
Append: [ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods](https://arxiv.org/abs/2406.15968)
Append: [Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks](https://arxiv.org/abs/2407.00869)
Append: [Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation](https://arxiv.org/abs/2407.01796)
Append: [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)
Append: [Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models](https://arxiv.org/abs/2407.21077)
Append: [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/abs/2408.10615)
Append: [Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models](https://arxiv.org/abs/2409.10999)
Append: [Task Arithmetic for Language Expansion in Speech Translation](https://arxiv.org/abs/2409.11274)
Append: [From Lists to Emojis: How Format Bias Affects Model Alignment](https://arxiv.org/abs/2409.11704)
Append: [Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning](https://arxiv.org/abs/2409.12887)
Append: [Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models](https://arxiv.org/abs/2409.14364)
Append: [Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization](https://arxiv.org/abs/2409.17673)
Append: [KCIF: Knowledge-Conditioned Instruction Following](https://arxiv.org/abs/2410.12972)
Append: [TrendFact: A Benchmark for Explainable Hotspot Perception in Fact-Checking with Natural Language Explanation](https://arxiv.org/abs/2410.15135)
Append: [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/abs/2410.15639)
Append: [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)
Append: [Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning](https://arxiv.org/abs/2410.20926)
Append: [LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines](https://arxiv.org/abs/2411.16365)
Append: [Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning](https://arxiv.org/abs/2411.19557)
Append: [MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines](https://arxiv.org/abs/2412.02271)
Append: [UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models](https://arxiv.org/abs/2412.11803)
Append: [Boosting Long-Context Management via Query-Guided Activation Refilling](https://arxiv.org/abs/2412.12486)
Append: [TrustRAG: Enhancing Robustness and Trustworthiness in Retrieval-Augmented Generation](https://arxiv.org/abs/2501.00879)
Append: [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/abs/2501.04686)
Append: [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694)
Append: [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346)
Append: [TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection](https://arxiv.org/abs/2501.11960)
Append: [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/abs/2501.16975)
Append: [CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality](https://arxiv.org/abs/2502.08923)
Append: [Beyond One-Size-Fits-All Pruning via Evolutionary Metric Search for Large Language Models](https://arxiv.org/abs/2502.10735)
Append: [1bit-Merging: Dynamic Quantized Merging for Large Language Models](https://arxiv.org/abs/2502.10743)
Append: [LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging](https://arxiv.org/abs/2502.10749)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs](https://arxiv.org/abs/2502.11228)
Append: [System Message Generation for User Preferences using Open-Source Models](https://arxiv.org/abs/2502.11330)
Append: [Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI](https://arxiv.org/abs/2502.11614)
Append: [Personality Editing for Language Models through Relevant Knowledge Editing](https://arxiv.org/abs/2502.11789)
Append: [PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery](https://arxiv.org/abs/2502.12594)
Append: [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)
Append: [Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests](https://arxiv.org/abs/2502.14359)
Append: [ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis](https://arxiv.org/abs/2502.14614)
Append: [Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs](https://arxiv.org/abs/2502.14645)
Append: [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/abs/2502.15594)
Append: [Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation](https://arxiv.org/abs/2502.16529)
Append: [PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance](https://arxiv.org/abs/2502.17041)
Append: [Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews](https://arxiv.org/abs/2502.17086)
Append: [Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective](https://arxiv.org/abs/2502.17262)
Append: [Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review](https://arxiv.org/abs/2502.19614)
Append: [Do Retrieval-Augmented Language Models Adapt to Varying User Needs?](https://arxiv.org/abs/2502.19779)
Append: [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/abs/2503.02003)
Append: [Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models](https://arxiv.org/abs/2503.02623)
Append: [SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection](https://arxiv.org/abs/2503.03303)
Append: [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/abs/2503.04556)
Append: [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)
Append: [MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation](https://arxiv.org/abs/2503.17900)
Append: [A Retrieval-Based Approach to Medical Procedure Matching in Romanian](https://arxiv.org/abs/2503.20556)
Append: [Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging](https://arxiv.org/abs/2503.20641)
Append: [Cognitive Debiasing Large Language Models for Decision-Making](https://arxiv.org/abs/2504.04141)
Append: [SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog](https://arxiv.org/abs/2504.07199)
Append: [ConceptCarve: Dynamic Realization of Evidence](https://arxiv.org/abs/2504.07228)
Append: [Playpen: An Environment for Exploring Learning Through Conversational Interaction](https://arxiv.org/abs/2504.08590)
Append: [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456)
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Append: [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)
Append: [Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [Explaining Black-box Model Predictions via Two-level Nested Feature Attributions with Consistency Property](https://arxiv.org/abs/2405.14522)
Append: [Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity](https://arxiv.org/abs/2406.14479)
Append: [Fundamental Limitations on Subquadratic Alternatives to Transformers](https://arxiv.org/abs/2410.04271)
Append: [MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool Calling](https://arxiv.org/abs/2410.13610)
Append: [Mastering Board Games by External and Internal Planning with Language Models](https://arxiv.org/abs/2412.12119)
Append: [FBQuant: FeedBack Quantization for Large Language Models](https://arxiv.org/abs/2501.16385)
Append: [Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/abs/2501.17116)
Append: [WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training](https://arxiv.org/abs/2501.18511)
Append: [Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models](https://arxiv.org/abs/2501.18533)
Append: [SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling](https://arxiv.org/abs/2501.19306)
Append: [GRADIEND: Monosemantic Feature Learning within Neural Networks Applied to Gender Debiasing of Transformer Models](https://arxiv.org/abs/2502.01406)
Append: [Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs](https://arxiv.org/abs/2502.01926)
Append: [Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Reasoning](https://arxiv.org/abs/2502.10440)
Append: [Table-Critic: A Multi-Agent Framework for Collaborative Criticism and Refinement in Table Reasoning](https://arxiv.org/abs/2502.11799)
Append: [Provably Correct Automata Embeddings for Optimal Automata-Conditioned Reinforcement Learning](https://arxiv.org/abs/2503.05042)
Append: [StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization](https://arxiv.org/abs/2504.05804)
Append: [Hogwild! Inference: Parallel LLM Generation via Concurrent Attention](https://arxiv.org/abs/2504.06261)
Append: [The Quantum LLM: Modeling Semantic Spaces with Quantum Principles](https://arxiv.org/abs/2504.13202)
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Append: [SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning](https://arxiv.org/abs/2505.11274)
Append: [Phare: A Safety Probe for Large Language Models](https://arxiv.org/abs/2505.11365)
Append: [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
Append: [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
append_entries: 338
Finish: 2025-05-26 04:28:03.495728
------------------------------------------------------
Started: 2025-05-26 06:26:06.199943
Existing_entries: 1338
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)
append_entries: 5
Finish: 2025-05-26 06:26:19.018598
------------------------------------------------------
Started: 2025-05-26 08:35:39.721530
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1430
Summarized using GPT-3.5-turbo
Append: [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)
append_entries: 1
Finish: 2025-05-26 08:35:43.786190
------------------------------------------------------
Started: 2025-05-26 10:21:02.873200
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 10:21:03.613807
------------------------------------------------------
Started: 2025-05-26 12:33:12.007667
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 12:33:12.736202
------------------------------------------------------
Started: 2025-05-26 14:16:38.766128
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 14:16:39.494597
------------------------------------------------------
Started: 2025-05-26 16:19:50.194213
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 16:19:50.893869
------------------------------------------------------
Started: 2025-05-26 18:21:52.577873
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 18:21:53.320226
------------------------------------------------------
Started: 2025-05-26 20:18:08.211632
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 20:18:08.899947
------------------------------------------------------
Started: 2025-05-26 22:15:15.323000
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-26 22:15:16.097041
------------------------------------------------------
Started: 2025-05-27 01:18:19.299410
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 01:18:19.995835
------------------------------------------------------
Started: 2025-05-27 03:09:03.308809
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 03:09:04.073008
------------------------------------------------------
Started: 2025-05-27 04:26:38.070620
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1122
Summarized using GPT-3.5-turbo
Append: [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)
Token length: 1115
Summarized using GPT-3.5-turbo
Append: [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
Token length: 1396
Summarized using GPT-3.5-turbo
Append: [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
Token length: 1752
Summarized using GPT-3.5-turbo
Append: [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)
Token length: 1417
Summarized using GPT-3.5-turbo
Append: [Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards](https://arxiv.org/abs/2505.18298)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)
Token length: 1154
Summarized using GPT-3.5-turbo
Append: [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Model Editing with Graph-Based External Memory](https://arxiv.org/abs/2505.18343)
Token length: 1452
Summarized using GPT-3.5-turbo
Append: [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
Token length: 1444
Summarized using GPT-3.5-turbo
Append: [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [RaDeR: Reasoning-aware Dense Retrieval Models](https://arxiv.org/abs/2505.18405)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [BRIT: Bidirectional Retrieval over Unified Image-Text Graph](https://arxiv.org/abs/2505.18450)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [MedScore: Factuality Evaluation of Free-Form Medical Answers](https://arxiv.org/abs/2505.18452)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Hybrid Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.18454)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [Measuring South Asian Biases in Large Language Models](https://arxiv.org/abs/2505.18466)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models](https://arxiv.org/abs/2505.18497)
Token length: 1897
Summarized using GPT-3.5-turbo
Append: [How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation](https://arxiv.org/abs/2505.18522)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [metaTextGrad: Automatically optimizing language model optimizers](https://arxiv.org/abs/2505.18524)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)
Token length: 1197
Summarized using GPT-3.5-turbo
Append: [Business as \textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs](https://arxiv.org/abs/2505.18542)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Composable Cross-prompt Essay Scoring by Merging Models](https://arxiv.org/abs/2505.18548)
Token length: 933
Summarized using GPT-3.5-turbo
Append: [MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors](https://arxiv.org/abs/2505.18549)
Token length: 1298
Summarized using GPT-3.5-turbo
Append: [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555)
Token length: 1566
Summarized using GPT-3.5-turbo
Append: [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)
Token length: 1047
Summarized using GPT-3.5-turbo
Append: [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)
Token length: 1491
Summarized using GPT-3.5-turbo
Append: [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations](https://arxiv.org/abs/2505.18609)
Token length: 1931
Summarized using GPT-3.5-turbo
Append: [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610)
Token length: 989
Summarized using GPT-3.5-turbo
Append: [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)
Token length: 980
Summarized using GPT-3.5-turbo
Append: [Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models](https://arxiv.org/abs/2505.18638)
Token length: 1486
Summarized using GPT-3.5-turbo
Append: [Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster](https://arxiv.org/abs/2505.18642)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)
Append: [Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change](https://arxiv.org/abs/2505.18653)
Append: [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)
Append: [Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models](https://arxiv.org/abs/2505.18673)
Append: [Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts](https://arxiv.org/abs/2505.18677)
Append: [TULUN: Transparent and Adaptable Low-resource Machine Translation](https://arxiv.org/abs/2505.18683)
Append: [From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation](https://arxiv.org/abs/2505.18685)
Append: [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
Append: [Benchmarking and Rethinking Knowledge Editing for Large Language Models](https://arxiv.org/abs/2505.18690)
Append: [Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task](https://arxiv.org/abs/2505.18703)
Append: [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)
Append: [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)
Append: [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)
Append: [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges](https://arxiv.org/abs/2505.18744)
Append: [Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning](https://arxiv.org/abs/2505.18752)
Append: [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
Append: [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
Append: [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)
Append: [Disentangling Knowledge Representations for Large Language Model Editing](https://arxiv.org/abs/2505.18774)
Append: [A generalised editor calculus (Short Paper)](https://arxiv.org/abs/2505.18778)
Append: [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
Append: [Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation](https://arxiv.org/abs/2505.18842)
Append: [Multi-Party Conversational Agents: A Survey](https://arxiv.org/abs/2505.18845)
Append: [Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation](https://arxiv.org/abs/2505.18853)
Append: [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)
Append: [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)
Append: [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)
Append: [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)
Append: [StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos](https://arxiv.org/abs/2505.18903)
Append: [Building a Functional Machine Translation Corpus for Kpelle](https://arxiv.org/abs/2505.18905)
Append: [Federated Retrieval-Augmented Generation: A Systematic Mapping Study](https://arxiv.org/abs/2505.18906)
Append: [SCRum-9: Multilingual Stance Classification over Rumours on Social Media](https://arxiv.org/abs/2505.18916)
Append: [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)
Append: [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems](https://arxiv.org/abs/2505.18943)
Append: [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)
Append: [BnMMLU: Measuring Massive Multitask Language Understanding in Bengali](https://arxiv.org/abs/2505.18951)
Append: [Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?](https://arxiv.org/abs/2505.18953)
Append: [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)
Append: [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)
Append: [Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE](https://arxiv.org/abs/2505.18971)
Append: [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)
Append: [AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2505.18978)
Append: [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)
Append: [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization](https://arxiv.org/abs/2505.19000)
Append: [CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language](https://arxiv.org/abs/2505.19018)
Append: [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)
Append: [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
Append: [UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models](https://arxiv.org/abs/2505.19060)
Append: [Towards Harmonized Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2505.19073)
Append: [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)
Append: [ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning](https://arxiv.org/abs/2505.19100)
Append: [WHISTRESS: Enriching Transcriptions with Sentence Stress Detection](https://arxiv.org/abs/2505.19103)
Append: [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)
Append: [Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering](https://arxiv.org/abs/2505.19112)
Append: [Controlling Language Confusion in Multilingual LLMs](https://arxiv.org/abs/2505.19116)
Append: [Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models](https://arxiv.org/abs/2505.19121)
Append: [MMATH: A Multilingual Benchmark for Mathematical Reasoning](https://arxiv.org/abs/2505.19126)
Append: [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)
Append: [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
Append: [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)
Append: [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
Append: [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)
Append: [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)
Append: [Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection](https://arxiv.org/abs/2505.19191)
Append: [DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](https://arxiv.org/abs/2505.19201)
Append: [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)
Append: [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)
Append: [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)
Append: [The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training](https://arxiv.org/abs/2505.19217)
Append: [Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator](https://arxiv.org/abs/2505.19236)
Append: [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
Append: [PATS: Process-Level Adaptive Thinking Mode Switching](https://arxiv.org/abs/2505.19250)
Append: [Unveiling Dual Quality in Product Reviews: An NLP-Based Approach](https://arxiv.org/abs/2505.19254)
Append: [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)
Append: [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)
Append: [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
Append: [SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking](https://arxiv.org/abs/2505.19300)
Append: [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
Append: [GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance](https://arxiv.org/abs/2505.19354)
Append: [Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement](https://arxiv.org/abs/2505.19355)
Append: [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360)
Append: [Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality](https://arxiv.org/abs/2505.19376)
Append: [GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor](https://arxiv.org/abs/2505.19384)
Append: [gec-metrics: A Unified Library for Grammatical Error Correction Evaluation](https://arxiv.org/abs/2505.19388)
Append: [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)
Append: [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)
Append: [Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering](https://arxiv.org/abs/2505.19410)
Append: [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
Append: [Frictional Agent Alignment Framework: Slow Down and Don't Break Things](https://arxiv.org/abs/2505.19428)
Append: [Rhapsody: A Dataset for Highlight Detection in Podcasts](https://arxiv.org/abs/2505.19429)
Append: [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
Append: [Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection](https://arxiv.org/abs/2505.19435)
Append: [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)
Append: [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
Append: [Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks](https://arxiv.org/abs/2505.19472)
Append: [Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection](https://arxiv.org/abs/2505.19475)
Append: [CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis](https://arxiv.org/abs/2505.19484)
Append: [Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents](https://arxiv.org/abs/2505.19494)
Append: [LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](https://arxiv.org/abs/2505.19510)
Append: [Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models](https://arxiv.org/abs/2505.19511)
Append: [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
Append: [Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework](https://arxiv.org/abs/2505.19515)
Append: [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
Append: [Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation](https://arxiv.org/abs/2505.19529)
Append: [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)
Append: [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)
Append: [Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents](https://arxiv.org/abs/2505.19549)
Append: [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)
Append: [TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](https://arxiv.org/abs/2505.19586)
Append: [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)
Append: [Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study](https://arxiv.org/abs/2505.19598)
Append: [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)
Append: [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)
Append: [Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically](https://arxiv.org/abs/2505.19606)
Append: [HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices](https://arxiv.org/abs/2505.19628)
Append: [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630)
Append: [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)
Append: [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)
Append: [Interleaved Reasoning for Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.19640)
Append: [Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation](https://arxiv.org/abs/2505.19647)
Append: [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)
Append: [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)
Append: [Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models](https://arxiv.org/abs/2505.19670)
Append: [Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations](https://arxiv.org/abs/2505.19674)
Append: [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
Append: [Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs](https://arxiv.org/abs/2505.19678)
Append: [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)
Append: [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)
Append: [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)
Append: [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
Append: [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)
Append: [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)
Append: [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
Append: [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
Append: [Efficient Reasoning via Chain of Unconscious Thought](https://arxiv.org/abs/2505.19756)
Append: [SGM: A Framework for Building Specification-Guided Moderation Filters](https://arxiv.org/abs/2505.19766)
Append: [T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search](https://arxiv.org/abs/2505.19768)
Append: [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)
Append: [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)
Append: [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)
Append: [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800)
Append: [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)
Append: [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)
Append: [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)
Append: [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)
Append: [Improving Multilingual Math Reasoning for African Languages](https://arxiv.org/abs/2505.19848)
Append: [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)
Append: [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)
Append: [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)
Append: [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
Append: [ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs](https://arxiv.org/abs/2505.19937)
Append: [MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2505.19959)
Append: [CP-Router: An Uncertainty-Aware Router Between LLM and LRM](https://arxiv.org/abs/2505.19970)
Append: [Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language](https://arxiv.org/abs/2505.19971)
Append: [DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset](https://arxiv.org/abs/2505.19978)
Append: [How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation](https://arxiv.org/abs/2505.19987)
Append: [Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition](https://arxiv.org/abs/2505.20006)
Append: [WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback](https://arxiv.org/abs/2505.20013)
Append: [Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation](https://arxiv.org/abs/2505.20014)
Append: [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
Append: [TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation](https://arxiv.org/abs/2505.20016)
Append: [Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking](https://arxiv.org/abs/2505.20023)
Append: [Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs](https://arxiv.org/abs/2505.20045)
Append: [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)
Append: [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)
Append: [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
Append: [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
Append: [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)
Append: [S2LPP: Small-to-Large Prompt Prediction across LLMs](https://arxiv.org/abs/2505.20097)
Append: [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
Append: [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/abs/2505.20101)
Append: [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)
Append: [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
Append: [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)
Append: [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)
Append: [Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers](https://arxiv.org/abs/2505.20128)
Append: [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)
Append: [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)
Append: [UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.20154)
Append: [Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs](https://arxiv.org/abs/2505.20155)
Append: [Exploring Generative Error Correction for Dysarthric Speech Recognition](https://arxiv.org/abs/2505.20163)
Append: [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)
Append: ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)
Append: [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)
Append: [Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning](https://arxiv.org/abs/2505.20195)
Append: [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199)
Append: [Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations](https://arxiv.org/abs/2505.20201)
Append: [How to Improve the Robustness of Closed-Source Models on NLI](https://arxiv.org/abs/2505.20209)
Append: [Dependency Parsing is More Parameter-Efficient with Normalization](https://arxiv.org/abs/2505.20215)
Append: [FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](https://arxiv.org/abs/2505.20225)
Append: [Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue](https://arxiv.org/abs/2505.20231)
Append: [Efficient Speech Translation through Model Compression and Knowledge Distillation](https://arxiv.org/abs/2505.20237)
Append: [It's High Time: A Survey of Temporal Information Retrieval and Question Answering](https://arxiv.org/abs/2505.20243)
Append: [KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing](https://arxiv.org/abs/2505.20245)
Append: [WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2505.20249)
Append: [ARM: Adaptive Reasoning Model](https://arxiv.org/abs/2505.20258)
Append: [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)
Append: [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)
Append: [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)
Append: [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)
Append: [MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability](https://arxiv.org/abs/2505.20285)
Append: [Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery](https://arxiv.org/abs/2505.20293)
Append: [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)
Append: [Reasoning LLMs are Wandering Solution Explorers](https://arxiv.org/abs/2505.20296)
Append: [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding](https://arxiv.org/abs/2505.20298)
Append: [Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization](https://arxiv.org/abs/2309.03824)
Append: [Improving Resnet-9 Generalization Trained on Small Datasets](https://arxiv.org/abs/2309.03965)
Append: [GQKVA: Efficient Pre-training of Transformers by Grouping Queries, Keys, and Values](https://arxiv.org/abs/2311.03426)
Append: [News Without Borders: Domain Adaptation of Multilingual Sentence Embeddings for Cross-lingual News Recommendation](https://arxiv.org/abs/2406.12634)
Append: [Accelerating the Low-Rank Decomposed Models](https://arxiv.org/abs/2407.20266)
Append: [Walk&Retrieve: Simple Yet Effective Zero-shot Retrieval-Augmented Generation via Knowledge Graph Walks](https://arxiv.org/abs/2505.16849)
Append: [Towards medical AI misalignment: a preliminary study](https://arxiv.org/abs/2505.18212)
Append: [Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs](https://arxiv.org/abs/2505.18221)
Append: [ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning](https://arxiv.org/abs/2505.18232)
Append: [Will Large Language Models Transform Clinical Prediction?](https://arxiv.org/abs/2505.18246)
Append: [Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control](https://arxiv.org/abs/2505.18279)
Append: [Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?](https://arxiv.org/abs/2505.18350)
Append: [Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems](https://arxiv.org/abs/2505.18366)
Append: [LatentLLM: Attention-Aware Joint Tensor Compression](https://arxiv.org/abs/2505.18413)
Append: [$\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts](https://arxiv.org/abs/2505.18451)
Append: [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
Append: [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)
Append: [Pedagogy-R1: Pedagogically-Aligned Reasoning Model with Balanced Educational Benchmark](https://arxiv.org/abs/2505.18467)
Append: [Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications](https://arxiv.org/abs/2505.18488)
Append: [Knowledge Grafting of Large Language Models](https://arxiv.org/abs/2505.18502)
Append: [AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking](https://arxiv.org/abs/2505.18512)
Append: [B-score: Detecting biases in large language models using response history](https://arxiv.org/abs/2505.18545)
Append: [Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs](https://arxiv.org/abs/2505.18573)
Append: [RvLLM: LLM Runtime Verification with Domain Knowledge](https://arxiv.org/abs/2505.18585)
Append: [Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving](https://arxiv.org/abs/2505.18644)
Append: [SEW: Self-Evolving Agentic Workflows for Automated Code Generation](https://arxiv.org/abs/2505.18646)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
Append: [$PD^3F$: A Pluggable and Dynamic DoS-Defense Framework Against Resource Consumption Attacks Targeting Large Language Models](https://arxiv.org/abs/2505.18680)
Append: [Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer](https://arxiv.org/abs/2505.18713)
Append: [Evaluating the Usefulness of Non-Diagnostic Speech Data for Developing Parkinson's Disease Classifiers](https://arxiv.org/abs/2505.18722)
Append: [From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?](https://arxiv.org/abs/2505.18789)
Append: [AdaCtrl: Towards Adaptive and Controllable Reasoning via Difficulty-Aware Budgeting](https://arxiv.org/abs/2505.18822)
Append: [On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization](https://arxiv.org/abs/2505.18830)
Append: [Signal, Image, or Symbolic: Exploring the Best Input Representation for Electrocardiogram-Language Models Through a Unified Framework](https://arxiv.org/abs/2505.18847)
Append: [Inference Compute-Optimal Video Vision Language Models](https://arxiv.org/abs/2505.18855)
Append: [Meta-aware Learning in text-to-SQL Large Language Model](https://arxiv.org/abs/2505.18929)
Append: [Can Large Language Models Infer Causal Relationships from Real-World Text?](https://arxiv.org/abs/2505.18931)
Append: [REACT: Representation Extraction And Controllable Tuning to Overcome Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2505.18933)
Append: [Language Models Surface the Unwritten Code of Science and Society](https://arxiv.org/abs/2505.18942)
Append: [STRICT: Stress Test of Rendering Images Containing Text](https://arxiv.org/abs/2505.18985)
Append: [Co-AttenDWG: Co-Attentive Dimension-Wise Gating and Expert Fusion for Multi-Modal Offensive Content Detection](https://arxiv.org/abs/2505.19010)
Append: [SQUiD: Synthesizing Relational Databases from Unstructured Text](https://arxiv.org/abs/2505.19025)
Append: [Speech-IFEval: Evaluating Instruction-Following and Quantifying Catastrophic Forgetting in Speech-Aware Language Models](https://arxiv.org/abs/2505.19037)
Append: [Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs](https://arxiv.org/abs/2505.19075)
Append: [Sparse-to-Dense: A Free Lunch for Lossless Acceleration of Video Understanding in LLMs](https://arxiv.org/abs/2505.19155)
Append: [GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling](https://arxiv.org/abs/2505.19234)
Append: [Next Token Prediction Is a Dead End for Creativity](https://arxiv.org/abs/2505.19277)
Append: [Towards Reliable Large Audio Language Model](https://arxiv.org/abs/2505.19294)
Append: [ODIN: A NL2SQL Recommender to Handle Schema Ambiguity](https://arxiv.org/abs/2505.19302)
Append: [Architectures of Error: A Philosophical Inquiry into AI and Human Code Generation](https://arxiv.org/abs/2505.19353)
Append: [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
Append: [Task Memory Engine: Spatial Memory for Robust Multi-Step LLM Agents](https://arxiv.org/abs/2505.19436)
Append: [Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI](https://arxiv.org/abs/2505.19443)
Append: [BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs](https://arxiv.org/abs/2505.19457)
Append: [DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation](https://arxiv.org/abs/2505.19504)
Append: [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
Append: [Automated Text-to-Table for Reasoning-Intensive Table QA: Pipeline Design and Benchmarking Insights](https://arxiv.org/abs/2505.19563)
Append: [Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing](https://arxiv.org/abs/2505.19578)
Append: [Learning to Reason without External Rewards](https://arxiv.org/abs/2505.19590)
Append: [Preference Optimization by Estimating the Ratio of the Data Distribution](https://arxiv.org/abs/2505.19601)
Append: [Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models](https://arxiv.org/abs/2505.19621)
Append: [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
Append: [Large Language Models for Planning: A Comprehensive and Systematic Survey](https://arxiv.org/abs/2505.19683)
Append: [Discrete Markov Bridge](https://arxiv.org/abs/2505.19752)
Append: [CIDRe: A Reference-Free Multi-Aspect Criterion for Code Comment Quality Measurement](https://arxiv.org/abs/2505.19757)
Append: [Understanding the Performance Gap in Preference Learning: A Dichotomy of RLHF and DPO](https://arxiv.org/abs/2505.19770)
Append: [HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation](https://arxiv.org/abs/2505.19866)
Append: [ESLM: Risk-Averse Selective Language Modeling for Efficient Pretraining](https://arxiv.org/abs/2505.19893)
Append: [Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program](https://arxiv.org/abs/2505.19896)
Append: [ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic Scientific Workflows](https://arxiv.org/abs/2505.19897)
Append: [Can Visual Encoder Learn to See Arrows?](https://arxiv.org/abs/2505.19944)
Append: [An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning](https://arxiv.org/abs/2505.19954)
Append: [MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research](https://arxiv.org/abs/2505.19955)
Append: [DCG-SQL: Enhancing In-Context Learning for Text-to-SQL with Deep Contextual Schema Link Graph](https://arxiv.org/abs/2505.19956)
Append: [The Limits of Preference Data for Post-Training](https://arxiv.org/abs/2505.19964)
Append: [Embracing Imperfection: Simulating Students with Diverse Cognitive Levels Using LLM-based Agents](https://arxiv.org/abs/2505.19997)
Append: [Multi-modal brain encoding models for multi-modal stimuli](https://arxiv.org/abs/2505.20027)
Append: [REARANK: Reasoning Re-ranking Agent via Reinforcement Learning](https://arxiv.org/abs/2505.20046)
Append: [MVP: Multi-source Voice Pathology detection](https://arxiv.org/abs/2505.20050)
Append: [Multimodal LLM-Guided Semantic Correction in Text-to-Image Diffusion](https://arxiv.org/abs/2505.20053)
Append: [SAEs Are Good for Steering -- If You Select the Right Features](https://arxiv.org/abs/2505.20063)
Append: [Safety Through Reasoning: An Empirical Study of Reasoning Guardrail Models](https://arxiv.org/abs/2505.20087)
Append: [SCIRGC: Multi-Granularity Citation Recommendation and Citation Sentence Preference Alignment](https://arxiv.org/abs/2505.20103)
Append: [StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs](https://arxiv.org/abs/2505.20139)
Append: [Hard Negative Contrastive Learning for Fine-Grained Geometric Understanding in Large Multimodal Models](https://arxiv.org/abs/2505.20152)
Append: [Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning](https://arxiv.org/abs/2505.20161)
Append: [From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data](https://arxiv.org/abs/2505.20166)
Append: [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
Append: [Learning Extrapolative Sequence Transformations from Markov Chains](https://arxiv.org/abs/2505.20251)
Append: [Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs](https://arxiv.org/abs/2505.20254)
Append: [Lifelong Safety Alignment for Language Models](https://arxiv.org/abs/2505.20259)
Append: [The Coverage Principle: A Framework for Understanding Compositional Generalization](https://arxiv.org/abs/2505.20278)
Append: [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/abs/2505.20279)
Append: [Visualized Text-to-Image Retrieval](https://arxiv.org/abs/2505.20291)
Append: [DiSA: Diffusion Step Annealing in Autoregressive Image Generation](https://arxiv.org/abs/2505.20297)
Append: [AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection](https://arxiv.org/abs/2112.11479)
Append: [ADEPT: A DEbiasing PrompT Framework](https://arxiv.org/abs/2211.05414)
Append: [The More Similar, the Better? Associations between Latent Semantic Similarity and Emotional Experiences Differ across Conversation Contexts](https://arxiv.org/abs/2309.12646)
Append: [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models](https://arxiv.org/abs/2310.13312)
Append: [Unearthing Large Scale Domain-Specific Knowledge from Public Corpora](https://arxiv.org/abs/2401.14624)
Append: [Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation](https://arxiv.org/abs/2402.13211)
Append: [A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion](https://arxiv.org/abs/2402.13405)
Append: [MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models](https://arxiv.org/abs/2402.13606)
Append: [Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency](https://arxiv.org/abs/2402.15481)
Append: [MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263)
Append: [Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation](https://arxiv.org/abs/2405.13984)
Append: [UniICL: An Efficient Unified Framework Unifying Compression, Selection, and Generation](https://arxiv.org/abs/2405.17062)
Append: [Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets](https://arxiv.org/abs/2406.05348)
Append: [Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation](https://arxiv.org/abs/2406.11632)
Append: [USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations](https://arxiv.org/abs/2406.16833)
Append: [Can Large Language Models Generate High-quality Patent Claims?](https://arxiv.org/abs/2406.19465)
Append: [The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations](https://arxiv.org/abs/2407.19299)
Append: [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](https://arxiv.org/abs/2408.08696)
Append: [CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts](https://arxiv.org/abs/2408.09070)
Append: [Language Models Benefit from Preparation with Elicited Knowledge](https://arxiv.org/abs/2409.01345)
Append: [The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language](https://arxiv.org/abs/2409.08103)
Append: [Identifying Knowledge Editing Types in Large Language Models](https://arxiv.org/abs/2409.19663)
Append: [QAEncoder: Towards Aligned Representation Learning in Question Answering System](https://arxiv.org/abs/2409.20434)
Append: [Do Vision-Language Models Really Understand Visual Language?](https://arxiv.org/abs/2410.00193)
Append: [In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement](https://arxiv.org/abs/2410.03124)
Append: [Lens: Rethinking Multilingual Enhancement for Large Language Models](https://arxiv.org/abs/2410.04407)
Append: [PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs](https://arxiv.org/abs/2410.06704)
Append: [Stuffed Mamba: Oversized States Lead to the Inability to Forget](https://arxiv.org/abs/2410.07145)
Append: [LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](https://arxiv.org/abs/2410.10700)
Append: [Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up](https://arxiv.org/abs/2410.12323)
Append: [Conformity in Large Language Models](https://arxiv.org/abs/2410.12428)
Append: [SynapticRAG: Enhancing Temporal Memory Retrieval in Large Language Models through Synaptic Mechanisms](https://arxiv.org/abs/2410.13553)
Append: [RESTOR: Knowledge Recovery in Machine Unlearning](https://arxiv.org/abs/2411.00204)
Append: [Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models](https://arxiv.org/abs/2411.02083)
Append: [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/abs/2411.02391)
Append: [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://arxiv.org/abs/2411.02937)
Append: [Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries](https://arxiv.org/abs/2411.07237)
Append: [SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs](https://arxiv.org/abs/2411.07965)
Append: [On the Compatibility of Generative AI and Generative Linguistics](https://arxiv.org/abs/2411.10533)
Append: [Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?](https://arxiv.org/abs/2411.15821)
Append: [Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning](https://arxiv.org/abs/2411.17679)
Append: [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)
Append: [Patent-CR: A Dataset for Patent Claim Revision](https://arxiv.org/abs/2412.02549)
Append: [Interpretable Company Similarity with Sparse Autoencoders](https://arxiv.org/abs/2412.02605)
Append: [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://arxiv.org/abs/2412.07282)
Append: [On the Limit of Language Models as Planning Formalizers](https://arxiv.org/abs/2412.09879)
Append: [MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset](https://arxiv.org/abs/2412.10105)
Append: [Rethinking Chain-of-Thought from the Perspective of Self-Training](https://arxiv.org/abs/2412.10827)
Append: [Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models](https://arxiv.org/abs/2412.11041)
Append: [Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models](https://arxiv.org/abs/2412.11333)
Append: [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context for Multi-Hop QA](https://arxiv.org/abs/2412.12632)
Append: [Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models](https://arxiv.org/abs/2412.13328)
Append: [EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents](https://arxiv.org/abs/2412.13549)
Append: [Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](https://arxiv.org/abs/2412.13879)
Append: [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/abs/2412.14689)
Append: [DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs](https://arxiv.org/abs/2412.14838)
Append: [ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty](https://arxiv.org/abs/2412.20251)
Append: [Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation](https://arxiv.org/abs/2501.02979)
Append: [OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis](https://arxiv.org/abs/2501.04561)
Append: [A partition cover approach to tokenization](https://arxiv.org/abs/2501.06246)
Append: [Language Fusion for Parameter-Efficient Cross-lingual Transfer](https://arxiv.org/abs/2501.06892)
Append: [Domain Adaptation of Foundation LLMs for e-Commerce](https://arxiv.org/abs/2501.09706)
Append: [iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use](https://arxiv.org/abs/2501.09766)
Append: [Each Graph is a New Language: Graph Learning with LLMs](https://arxiv.org/abs/2501.11478)
Append: [NExtLong: Toward Effective Long-Context Training without Long Documents](https://arxiv.org/abs/2501.12766)
Append: [Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages](https://arxiv.org/abs/2501.13836)
Append: [Token Sampling Uncertainty Does Not Explain Homogeneity Bias in Large Language Models](https://arxiv.org/abs/2501.19337)
Append: [A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment](https://arxiv.org/abs/2502.00136)
Append: [UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models](https://arxiv.org/abs/2502.00334)
Append: [A statistically consistent measure of semantic uncertainty using Language Models](https://arxiv.org/abs/2502.00507)
Append: [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/abs/2502.04066)
Append: [JingFang: An Expert-Level Large Language Model for Traditional Chinese Medicine Clinical Consultation and Syndrome Differentiation-Based Treatment](https://arxiv.org/abs/2502.04345)
Append: [DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2502.04394)
Append: [Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection](https://arxiv.org/abs/2502.04528)
Append: [Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering](https://arxiv.org/abs/2502.07340)
Append: [What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations](https://arxiv.org/abs/2502.08279)
Append: [SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence](https://arxiv.org/abs/2502.08767)
Append: [A Survey of LLM-based Agents in Medicine: How far are we from Baymax?](https://arxiv.org/abs/2502.11211)
Append: [HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning](https://arxiv.org/abs/2502.11393)
Append: [Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study](https://arxiv.org/abs/2502.11514)
Append: [Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?](https://arxiv.org/abs/2502.11598)
Append: [ReviewEval: An Evaluation Framework for AI-Generated Reviews](https://arxiv.org/abs/2502.11736)
Append: [Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning](https://arxiv.org/abs/2502.11962)
Append: [How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines](https://arxiv.org/abs/2502.12051)
Append: [TokenSkip: Controllable Chain-of-Thought Compression in LLMs](https://arxiv.org/abs/2502.12067)
Append: [Evaluating Step-by-step Reasoning Traces: A Survey](https://arxiv.org/abs/2502.12289)
Append: [A Cognitive Writing Perspective for Constrained Long-Form Text Generation](https://arxiv.org/abs/2502.12568)
Append: [Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization](https://arxiv.org/abs/2502.12672)
Append: [Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements](https://arxiv.org/abs/2502.12904)
Append: [Conditioning LLMs to Generate Code-Switched Text](https://arxiv.org/abs/2502.12924)
Append: [Natural Language Generation from Visual Events: Challenges and Future Directions](https://arxiv.org/abs/2502.13034)
Append: [Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors](https://arxiv.org/abs/2502.13311)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?](https://arxiv.org/abs/2502.14924)
Append: [Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models](https://arxiv.org/abs/2502.15094)
Append: [Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning](https://arxiv.org/abs/2502.15361)
Append: [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/abs/2502.16514)
Append: [CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter](https://arxiv.org/abs/2502.16880)
Append: [Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch](https://arxiv.org/abs/2502.17173)
Append: [Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning](https://arxiv.org/abs/2502.18001)
Append: [Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs](https://arxiv.org/abs/2502.18791)
Append: [Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization](https://arxiv.org/abs/2502.19127)
Append: [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/abs/2502.19148)
Append: [R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning](https://arxiv.org/abs/2502.19735)
Append: [GeoEdit: Geometric Knowledge Editing for Large Language Models](https://arxiv.org/abs/2502.19953)
Append: [PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues](https://arxiv.org/abs/2502.21017)
Append: [Detecting LLM-Generated Korean Text through Linguistic Feature Analysis](https://arxiv.org/abs/2503.00032)
Append: [Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models](https://arxiv.org/abs/2503.01763)
Append: [SteerConf: Steering LLMs for Confidence Elicitation](https://arxiv.org/abs/2503.02863)
Append: [LINGOLY-TOO: Disentangling Memorisation from Knowledge with Linguistic Templatisation and Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)
Append: [Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions](https://arxiv.org/abs/2503.03862)
Append: [DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models](https://arxiv.org/abs/2503.04240)
Append: [One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](https://arxiv.org/abs/2503.04856)
Append: [InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2503.06692)
Append: [MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System](https://arxiv.org/abs/2503.09600)
Append: [MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation](https://arxiv.org/abs/2503.10497)
Append: [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/abs/2503.10688)
Append: [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)
Append: [RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning](https://arxiv.org/abs/2503.12759)
Append: [Optimizing Decomposition for Optimal Claim Verification](https://arxiv.org/abs/2503.15354)
Append: [Prompting is Not All You Need! Evaluating LLM Agent Simulation Methodologies with Real-World Online Customer Behavior Data](https://arxiv.org/abs/2503.20749)
Append: [GTR: Graph-Table-RAG for Cross-Table Question Answering](https://arxiv.org/abs/2504.01346)
Append: [FISH-Tuning: Enhancing PEFT Methods with Fisher Information](https://arxiv.org/abs/2504.04050)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [NoveltyBench: Evaluating Language Models for Humanlike Diversity](https://arxiv.org/abs/2504.05228)
Append: [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/abs/2504.07440)
Append: [MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations](https://arxiv.org/abs/2504.07830)
Append: [PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims](https://arxiv.org/abs/2504.09866)
Append: [TextArena](https://arxiv.org/abs/2504.11442)
Append: [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)
Append: [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)
Append: [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)
Append: [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)
Append: [Position: Enough of Scaling LLMs! Lets Focus on Downscaling](https://arxiv.org/abs/2505.00985)
Append: [Intra-Layer Recurrence in Transformers for Language Modeling](https://arxiv.org/abs/2505.01855)
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Append: [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)
Append: [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)
Append: [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)
Append: [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)
Append: [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [Talk to Your Slides: Language-Driven Agents for Efficient Slide Editing](https://arxiv.org/abs/2505.11604)
Append: [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
Append: [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
Append: [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
Append: [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
Append: [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
Append: [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)
Append: [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)
Append: [Towards End-to-End Training of Automatic Speech Recognition for Nigerian Pidgin](https://arxiv.org/abs/2010.11123)
Append: [GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models](https://arxiv.org/abs/2402.03299)
Append: [JailbreakRadar: Comprehensive Assessment of Jailbreak Attacks Against LLMs](https://arxiv.org/abs/2402.05668)
Append: [Query Performance Prediction using Relevance Judgments Generated by Large Language Models](https://arxiv.org/abs/2404.01012)
Append: [Model Extrapolation Expedites Alignment](https://arxiv.org/abs/2404.16792)
Append: [Constructing a BPE Tokenization DFA](https://arxiv.org/abs/2405.07671)
Append: [AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments](https://arxiv.org/abs/2405.07960)
Append: [SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2405.14917)
Append: [Explaining the role of Intrinsic Dimensionality in Adversarial Training](https://arxiv.org/abs/2405.17130)
Append: [Little Data, Big Impact: Privacy-Aware Visual Language Models via Minimal Tuning](https://arxiv.org/abs/2405.17423)
Append: [Parrot: Multilingual Visual Instruction Tuning](https://arxiv.org/abs/2406.02539)
Append: [Algorithmic Language Models with Neurally Compiled Libraries](https://arxiv.org/abs/2407.04899)
Append: [MAPLE: Enhancing Review Generation with Multi-Aspect Prompt LEarning in Explainable Recommendation](https://arxiv.org/abs/2408.09865)
Append: [ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework](https://arxiv.org/abs/2409.10289)
Append: [Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis](https://arxiv.org/abs/2410.02167)
Append: [Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System](https://arxiv.org/abs/2410.09403)
Append: [AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents](https://arxiv.org/abs/2410.13825)
Append: [LLMScan: Causal Scan for LLM Misbehavior Detection](https://arxiv.org/abs/2410.16638)
Append: [Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers](https://arxiv.org/abs/2410.22663)
Append: [P$^2$ Law: Scaling Law for Post-Training After Model Pruning](https://arxiv.org/abs/2411.10272)
Append: [FuseGPT: Learnable Layers Fusion of Generative Pre-trained Transformers](https://arxiv.org/abs/2411.14507)
Append: [BPP-Search: Enhancing Tree of Thought Reasoning for Mathematical Modeling Problem Solving](https://arxiv.org/abs/2411.17404)
Append: [Demonstration Selection for In-Context Learning via Reinforcement Learning](https://arxiv.org/abs/2412.03966)
Append: [ProcessBench: Identifying Process Errors in Mathematical Reasoning](https://arxiv.org/abs/2412.06559)
Append: [Visual Program Distillation with Template-Based Augmentation](https://arxiv.org/abs/2412.08564)
Append: [GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration](https://arxiv.org/abs/2412.16216)
Append: [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639)
Append: [Divide-Then-Aggregate: An Efficient Tool Learning Method via Parallel Tool Invocation](https://arxiv.org/abs/2501.12432)
Append: [Understanding Multimodal LLMs Under Distribution Shifts: An Information-Theoretic Approach](https://arxiv.org/abs/2502.00577)
Append: [Polynomial, trigonometric, and tropical activations](https://arxiv.org/abs/2502.01247)
Append: [Preference Leakage: A Contamination Problem in LLM-as-a-judge](https://arxiv.org/abs/2502.01534)
Append: [ACECODER: Acing Coder RL via Automated Test-Case Synthesis](https://arxiv.org/abs/2502.01718)
Append: [DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation](https://arxiv.org/abs/2502.03930)
Append: [When More is Less: Understanding Chain-of-Thought Length in LLMs](https://arxiv.org/abs/2502.07266)
Append: [QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language](https://arxiv.org/abs/2502.09723)
Append: [SMART: Self-Aware Agent for Tool Overuse Mitigation](https://arxiv.org/abs/2502.11435)
Append: [Why Vision Language Models Struggle with Visual Arithmetic? Towards Enhanced Chart and Geometry Understanding](https://arxiv.org/abs/2502.11492)
Append: [APB: Accelerating Distributed Long-Context Inference by Passing Compressed Context Blocks across GPUs](https://arxiv.org/abs/2502.12085)
Append: [HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2502.12442)
Append: [Multi-Step Alignment as Markov Games: An Optimistic Online Gradient Descent Approach with Convergence Guarantees](https://arxiv.org/abs/2502.12678)
Append: [K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](https://arxiv.org/abs/2502.13344)
Append: [Automated Knowledge Component Generation and Knowledge Tracing for Coding Problems](https://arxiv.org/abs/2502.18632)
Append: [TheoremExplainAgent: Towards Video-based Multimodal Explanations for LLM Theorem Understanding](https://arxiv.org/abs/2502.19400)
Append: [Beyond the Tip of Efficiency: Uncovering the Submerged Threats of Jailbreak Attacks in Small Language Models](https://arxiv.org/abs/2502.19883)
Append: [Generalizable Prompt Learning of CLIP: A Brief Overview](https://arxiv.org/abs/2503.01263)
Append: [Language Models, Graph Searching, and Supervision Adulteration: When More Supervision is Less and How to Make More More](https://arxiv.org/abs/2503.10542)
Append: [Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding](https://arxiv.org/abs/2503.13377)
Append: [Mixture of Lookup Experts](https://arxiv.org/abs/2503.15798)
Append: [MoLAE: Mixture of Latent Experts for Parameter-Efficient Language Models](https://arxiv.org/abs/2503.23100)
Append: [An Illusion of Progress? Assessing the Current State of Web Agents](https://arxiv.org/abs/2504.01382)
Append: [Sugar-Coated Poison: Benign Generation Unlocks LLM Jailbreaking](https://arxiv.org/abs/2504.05652)
Append: [FamilyTool: A Multi-hop Personalized Tool Use Benchmark](https://arxiv.org/abs/2504.06766)
Append: [DocAgent: A Multi-Agent System for Automated Code Documentation Generation](https://arxiv.org/abs/2504.08725)
Append: [ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search](https://arxiv.org/abs/2504.10893)
Append: [AI Idea Bench 2025: AI Research Idea Generation Benchmark](https://arxiv.org/abs/2504.14191)
Append: [Roll the dice & look before you leap: Going beyond the creative limits of next-token prediction](https://arxiv.org/abs/2504.15266)
Append: [IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery](https://arxiv.org/abs/2504.16728)
Append: [RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2504.20073)
Append: [Reinforcement Learning for Reasoning in Large Language Models with One Training Example](https://arxiv.org/abs/2504.20571)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation](https://arxiv.org/abs/2505.03273)
Append: [Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety](https://arxiv.org/abs/2505.06843)
Append: [LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs](https://arxiv.org/abs/2505.08704)
Append: [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings](https://arxiv.org/abs/2505.13718)
Append: [PandaGuard: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)
Append: [GraphemeAug: A Systematic Approach to Synthesized Hard Negative Keyword Spotting Examples](https://arxiv.org/abs/2505.14814)
append_entries: 568
Finish: 2025-05-27 04:28:29.763551
------------------------------------------------------
Started: 2025-05-27 06:26:35.991993
Existing_entries: 1568
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1424
Summarized using GPT-3.5-turbo
Append: [AAAR-1.0: Assessing AI's Potential to Assist Research](https://arxiv.org/abs/2410.22394)
Token length: 1597
Summarized using GPT-3.5-turbo
Append: [ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL](https://arxiv.org/abs/2412.10138)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [BriLLM: Brain-inspired Large Language Model](https://arxiv.org/abs/2503.11299)
Token length: 1456
Summarized using GPT-3.5-turbo
Append: [FastCuRL: Curriculum Reinforcement Learning with Stage-wise Context Scaling for Efficient Training R1-like Reasoning Models](https://arxiv.org/abs/2503.17287)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization](https://arxiv.org/abs/2505.02172)
Token length: 988
Summarized using GPT-3.5-turbo
Append: [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)
Token length: 1747
Summarized using GPT-3.5-turbo
Append: [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision](https://arxiv.org/abs/2505.14996)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
Token length: 1374
Summarized using GPT-3.5-turbo
Append: [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
Token length: 1301
Summarized using GPT-3.5-turbo
Append: [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
Token length: 943
Summarized using GPT-3.5-turbo
Append: ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
Token length: 1603
Summarized using GPT-3.5-turbo
Append: [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
Token length: 1398
Summarized using GPT-3.5-turbo
Append: [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
Token length: 1855
Summarized using GPT-3.5-turbo
Append: [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
append_entries: 25
Finish: 2025-05-27 06:27:36.820994
------------------------------------------------------
Started: 2025-05-27 08:22:28.722995
Existing_entries: 1025
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
append_entries: 2
Finish: 2025-05-27 08:22:33.710128
------------------------------------------------------
Started: 2025-05-27 10:18:35.287357
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 10:18:36.515366
------------------------------------------------------
Started: 2025-05-27 12:35:18.008371
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 12:35:19.218601
------------------------------------------------------
Started: 2025-05-27 14:16:23.343462
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 14:16:24.474794
------------------------------------------------------
Started: 2025-05-27 16:21:03.442434
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 16:21:04.618719
------------------------------------------------------
Started: 2025-05-27 18:22:09.997551
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 18:22:11.236560
------------------------------------------------------
Started: 2025-05-27 20:18:37.073028
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 20:18:38.191566
------------------------------------------------------
Started: 2025-05-27 22:15:37.517071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-27 22:15:38.707862
------------------------------------------------------
Started: 2025-05-28 01:19:45.327893
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 01:19:46.475260
------------------------------------------------------
Started: 2025-05-28 03:11:16.977459
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 03:11:18.268401
------------------------------------------------------
Started: 2025-05-28 04:25:12.991782
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs](https://arxiv.org/abs/2505.20309)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [Arctic-Text2SQL-R1: Simple Rewards, Strong Reasoning in Text-to-SQL](https://arxiv.org/abs/2505.20315)
Token length: 1325
Summarized using GPT-3.5-turbo
Append: [Beyond Demonstrations: Dynamic Vector Construction from Latent Representations](https://arxiv.org/abs/2505.20318)
Token length: 1127
Summarized using GPT-3.5-turbo
Append: [Less Context, Same Performance: A RAG Framework for Resource-Efficient LLM-Based Clinical NLP](https://arxiv.org/abs/2505.20320)
Token length: 1563
Summarized using GPT-3.5-turbo
Append: [BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases](https://arxiv.org/abs/2505.20321)
Token length: 1173
Summarized using GPT-3.5-turbo
Append: [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
Token length: 1431
Summarized using GPT-3.5-turbo
Append: [PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus](https://arxiv.org/abs/2505.20323)
Token length: 1299
Summarized using GPT-3.5-turbo
Append: [Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic Confidence](https://arxiv.org/abs/2505.20325)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [Multi-Scale Manifold Alignment: A Unified Framework for Enhanced Explainability of Large Language Models](https://arxiv.org/abs/2505.20333)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Lookahead Q-Cache: Achieving More Consistent KV Cache Eviction via Pseudo Query](https://arxiv.org/abs/2505.20334)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [Language Model Distillation: A Temporal Difference Imitation Learning Perspective](https://arxiv.org/abs/2505.20335)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [MOSLIM:Align with diverse preferences in prompts through reward classification](https://arxiv.org/abs/2505.20336)
Token length: 1885
Summarized using GPT-3.5-turbo
Append: [Assessing the Capability of LLMs in Solving POSCOMP Questions](https://arxiv.org/abs/2505.20338)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [Dynamic Manifold Evolution Theory: Modeling and Stability Analysis of Latent Representations in Large Language Models](https://arxiv.org/abs/2505.20340)
Token length: 1748
Summarized using GPT-3.5-turbo
Append: [Do LLMs have a Gender (Entropy) Bias?](https://arxiv.org/abs/2505.20343)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data](https://arxiv.org/abs/2505.20347)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
Token length: 1508
Summarized using GPT-3.5-turbo
Append: [Enhancing Logical Reasoning in Language Models via Symbolically-Guided Monte Carlo Process Supervision](https://arxiv.org/abs/2505.20415)
Token length: 1419
Summarized using GPT-3.5-turbo
Append: [GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation](https://arxiv.org/abs/2505.20416)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [SEMMA: A Semantic Aware Knowledge Graph Foundation Model](https://arxiv.org/abs/2505.20422)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [The UD-NewsCrawl Treebank: Reflections and Challenges from a Large-scale Tagalog Syntactic Annotation Project](https://arxiv.org/abs/2505.20428)
Token length: 1035
Summarized using GPT-3.5-turbo
Append: [PreP-OCR: A Complete Pipeline for Document Image Restoration and Enhanced OCR Accuracy](https://arxiv.org/abs/2505.20429)
Token length: 1588
Summarized using GPT-3.5-turbo
Append: [HAMburger: Accelerating LLM Inference via Token Smashing](https://arxiv.org/abs/2505.20438)
Token length: 1011
Summarized using GPT-3.5-turbo
Append: [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
Token length: 1469
Summarized using GPT-3.5-turbo
Append: [Amulet: Putting Complex Multi-Turn Conversations on the Stand with LLM Juries](https://arxiv.org/abs/2505.20451)
Token length: 1526
Summarized using GPT-3.5-turbo
Append: [Conversation Kernels: A Flexible Mechanism to Learn Relevant Context for Online Conversation Understanding](https://arxiv.org/abs/2505.20482)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [InFact: Informativeness Alignment for Improved LLM Factuality](https://arxiv.org/abs/2505.20487)
Token length: 1138
Summarized using GPT-3.5-turbo
Append: [Inceptive Transformers: Enhancing Contextual Representations through Multi-Scale Feature Learning Across Domains and Languages](https://arxiv.org/abs/2505.20496)
Token length: 1255
Summarized using GPT-3.5-turbo
Append: [Beyond Keywords: Evaluating Large Language Model Classification of Nuanced Ableism](https://arxiv.org/abs/2505.20500)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [Gatsby Without the 'E': Crafting Lipograms with LLMs](https://arxiv.org/abs/2505.20501)
Token length: 1479
Summarized using GPT-3.5-turbo
Append: [Large Language Models for IT Automation Tasks: Are We There Yet?](https://arxiv.org/abs/2505.20505)
Token length: 842
Summarized using GPT-3.5-turbo
Append: [ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis](https://arxiv.org/abs/2505.20506)
Token length: 915
Summarized using GPT-3.5-turbo
Append: [Multimodal Emotion Recognition in Conversations: A Survey of Methods, Trends, Challenges and Prospects](https://arxiv.org/abs/2505.20511)
Token length: 1548
Summarized using GPT-3.5-turbo
Append: [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Paths Not Taken: Understanding and Mending the Multilingual Factual Recall Pipeline](https://arxiv.org/abs/2505.20546)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)
Token length: 1663
Summarized using GPT-3.5-turbo
Append: [Emotion Classification In-Context in Spanish](https://arxiv.org/abs/2505.20571)
Token length: 1666
Summarized using GPT-3.5-turbo
Append: [Effectiveness of Prompt Optimization in NL2SQL Systems](https://arxiv.org/abs/2505.20591)
Token length: 955
Summarized using GPT-3.5-turbo
Append: [Towards Pretraining Robust ASR Foundation Model with Acoustic-Aware Data Augmentation](https://arxiv.org/abs/2505.20606)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation](https://arxiv.org/abs/2505.20622)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization](https://arxiv.org/abs/2505.20624)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration](https://arxiv.org/abs/2505.20625)
Token length: 1555
Summarized using GPT-3.5-turbo
Append: [Test-Time Learning for Large Language Models](https://arxiv.org/abs/2505.20633)
Token length: 1279
Summarized using GPT-3.5-turbo
Append: [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/abs/2505.20645)
Token length: 1446
Summarized using GPT-3.5-turbo
Append: [FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information](https://arxiv.org/abs/2505.20650)
Token length: 1156
Summarized using GPT-3.5-turbo
Append: [Chinese Cyberbullying Detection: Dataset, Method, and Validation](https://arxiv.org/abs/2505.20654)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [Enhancing Transformation from Natural Language to Signal Temporal Logic Using LLMs with Diverse External Knowledge](https://arxiv.org/abs/2505.20658)
Token length: 1126
Summarized using GPT-3.5-turbo
Append: [BacktrackAgent: Enhancing GUI Agent with Error Detection and Backtracking Mechanism](https://arxiv.org/abs/2505.20660)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [Self-Route: Automatic Mode Switching via Capability Estimation for Efficient Reasoning](https://arxiv.org/abs/2505.20664)
Append: [Pretraining Language Models to Ponder in Continuous Space](https://arxiv.org/abs/2505.20674)
Append: [SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations](https://arxiv.org/abs/2505.20679)
Append: [Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages](https://arxiv.org/abs/2505.20693)
Append: [Beyond Templates: Dynamic Adaptation of Reasoning Demonstrations via Feasibility-Aware Exploration](https://arxiv.org/abs/2505.20700)
Append: [Dissecting Physics Reasoning in Small Language Models: A Multi-Dimensional Analysis from an Educational Perspective](https://arxiv.org/abs/2505.20707)
Append: [SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution](https://arxiv.org/abs/2505.20732)
Append: [Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator](https://arxiv.org/abs/2505.20738)
Append: [CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models](https://arxiv.org/abs/2505.20767)
Append: [SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences](https://arxiv.org/abs/2505.20776)
Append: [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/abs/2505.20779)
Append: [Improved Representation Steering for Language Models](https://arxiv.org/abs/2505.20809)
Append: [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
Append: [Rethinking Information Synthesis in Multimodal Question Answering A Multi-Agent Perspective](https://arxiv.org/abs/2505.20816)
Append: [Tracing and Reversing Rank-One Model Edits](https://arxiv.org/abs/2505.20819)
Append: [Reinforced Informativeness Optimization for Long-Form Retrieval-Augmented Generation](https://arxiv.org/abs/2505.20825)
Append: [AdParaphrase v2.0: Generating Attractive Ad Texts Using a Preference-Annotated Paraphrase Dataset](https://arxiv.org/abs/2505.20826)
Append: [Concealment of Intent: A Game-Theoretic Analysis](https://arxiv.org/abs/2505.20841)
Append: [Divide-Then-Align: Honest Alignment based on the Knowledge Boundary of RAG](https://arxiv.org/abs/2505.20871)
Append: [Can LLMs Learn to Map the World from Local Descriptions?](https://arxiv.org/abs/2505.20874)
Append: [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
Append: [MSA at SemEval-2025 Task 3: High Quality Weak Labeling and LLM Ensemble Verification for Multilingual Hallucination Detection](https://arxiv.org/abs/2505.20880)
Append: [EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2505.20888)
Append: [Dub-S2ST: Textless Speech-to-Speech Translation for Seamless Dubbing](https://arxiv.org/abs/2505.20899)
Append: [A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models](https://arxiv.org/abs/2505.20901)
Append: [Towards Objective Fine-tuning: How LLMs' Prior Knowledge Causes Potential Poor Calibration?](https://arxiv.org/abs/2505.20903)
Append: [Automated Privacy Information Annotation in Large Language Model Interactions](https://arxiv.org/abs/2505.20910)
Append: [Automatic Transmission for LLM Tiers: Optimizing Cost and Accuracy in Large Language Models](https://arxiv.org/abs/2505.20921)
Append: [Multi-objective Large Language Model Alignment with Hierarchical Experts](https://arxiv.org/abs/2505.20925)
Append: [Information-Theoretic Complementary Prompts for Improved Continual Text Classification](https://arxiv.org/abs/2505.20933)
Append: [On VLMs for Diverse Tasks in Multimodal Meme Classification](https://arxiv.org/abs/2505.20937)
Append: [Research Community Perspectives on "Intelligence" and Large Language Models](https://arxiv.org/abs/2505.20959)
Append: [Context-Aware Content Moderation for German Newspaper Comments](https://arxiv.org/abs/2505.20963)
Append: [Personalized Query Auto-Completion for Long and Short-Term Interests with Adaptive Detoxification Generation](https://arxiv.org/abs/2505.20966)
Append: [Reason-Align-Respond: Aligning LLM Reasoning with Knowledge Graphs for KGQA](https://arxiv.org/abs/2505.20971)
Append: [Contrastive Learning on LLM Back Generation Treebank for Cross-domain Constituency Parsing](https://arxiv.org/abs/2505.20976)
Append: [Evaluating and Steering Modality Preferences in Multimodal Large Language Model](https://arxiv.org/abs/2505.20977)
Append: [Who Reasons in the Large Language Models?](https://arxiv.org/abs/2505.20993)
Append: [Articulatory strategy in vowel production as a basis for speaker discrimination](https://arxiv.org/abs/2505.20995)
Append: [Uncertainty Unveiled: Can Exposure to More In-context Examples Mitigate Uncertainty for Large Language Models?](https://arxiv.org/abs/2505.21003)
Append: [LLMs are Frequency Pattern Learners in Natural Language Inference](https://arxiv.org/abs/2505.21011)
Append: [Def-DTS: Deductive Reasoning for Open-domain Dialogue Topic Segmentation](https://arxiv.org/abs/2505.21033)
Append: [FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis](https://arxiv.org/abs/2505.21040)
Append: [Visual Cues Enhance Predictive Turn-Taking for Two-Party Human Interaction](https://arxiv.org/abs/2505.21043)
Append: [Predicting Implicit Arguments in Procedural Video Instructions](https://arxiv.org/abs/2505.21068)
Append: [Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation](https://arxiv.org/abs/2505.21072)
Append: [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/abs/2505.21082)
Append: [BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and Cultural Knowledge](https://arxiv.org/abs/2505.21092)
Append: [Thinker: Learning to Think Fast and Slow](https://arxiv.org/abs/2505.21097)
Append: [A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction](https://arxiv.org/abs/2505.21109)
Append: [Will It Still Be True Tomorrow? Multilingual Evergreen Question Classification to Improve Trustworthy QA](https://arxiv.org/abs/2505.21115)
Append: [Scaling and Prompting for Improved End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2505.21137)
Append: [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/abs/2505.21138)
Append: [Assessment of L2 Oral Proficiency using Speech Large Language Models](https://arxiv.org/abs/2505.21148)
Append: [M-Wanda: Improving One-Shot Pruning for Multilingual LLMs](https://arxiv.org/abs/2505.21171)
Append: [TAT-R1: Terminology-Aware Translation with Reinforcement Learning and Word Alignment](https://arxiv.org/abs/2505.21172)
Append: [Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.21178)
Append: [Exploring the Latent Capacity of LLMs for One-Step Text Generation](https://arxiv.org/abs/2505.21189)
Append: [Lunguage: A Benchmark for Structured and Sequential Chest X-ray Interpretation](https://arxiv.org/abs/2505.21190)
Append: [Unveiling Instruction-Specific Neurons & Experts: An Analytical Framework for LLM's Instruction-Following Capabilities](https://arxiv.org/abs/2505.21191)
Append: [Pretrained LLMs Learn Multiple Types of Uncertainty](https://arxiv.org/abs/2505.21218)
Append: [A Representation Level Analysis of NMT Model Robustness to Grammatical Errors](https://arxiv.org/abs/2505.21224)
Append: [LMCD: Language Models are Zeroshot Cognitive Diagnosis Learners](https://arxiv.org/abs/2505.21239)
Append: [Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings](https://arxiv.org/abs/2505.21242)
Append: [ReSCORE: Label-free Iterative Retriever Training for Multi-hop Question Answering with Relevance-Consistency Supervision](https://arxiv.org/abs/2505.21250)
Append: [Multilingual Pretraining for Pixel Language Models](https://arxiv.org/abs/2505.21265)
Append: [rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset](https://arxiv.org/abs/2505.21297)
Append: [How Humans and LLMs Organize Conceptual Knowledge: Exploring Subordinate Categories in Italian](https://arxiv.org/abs/2505.21301)
Append: [Charting the Landscape of African NLP: Mapping Progress and Shaping the Road Ahead](https://arxiv.org/abs/2505.21315)
Append: [Leveraging large language models and traditional machine learning ensembles for ADHD detection from narrative transcripts](https://arxiv.org/abs/2505.21324)
Append: [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/abs/2505.21342)
Append: [Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning](https://arxiv.org/abs/2505.21354)
Append: [Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History](https://arxiv.org/abs/2505.21362)
Append: [Analyzing values about gendered language reform in LLMs' revisions](https://arxiv.org/abs/2505.21378)
Append: [PHISH in MESH: Korean Adversarial Phonetic Substitution and Phonetic-Semantic Feature Integration Defense](https://arxiv.org/abs/2505.21380)
Append: [AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs](https://arxiv.org/abs/2505.21389)
Append: [Improving Research Idea Generation Through Data: An Empirical Investigation in Social Science](https://arxiv.org/abs/2505.21396)
Append: [DecisionFlow: Advancing Large Language Model as Principled Decision Maker](https://arxiv.org/abs/2505.21397)
Append: [Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling](https://arxiv.org/abs/2505.21399)
Append: [RelationalFactQA: A Benchmark for Evaluating Tabular Fact Retrieval from Large Language Models](https://arxiv.org/abs/2505.21409)
Append: [Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity](https://arxiv.org/abs/2505.21411)
Append: [RefTool: Enhancing Model Reasoning with Reference-Guided Tool Creation](https://arxiv.org/abs/2505.21413)
Append: [Towards Better Instruction Following Retrieval Models](https://arxiv.org/abs/2505.21439)
Append: [Words Like Knives: Backstory-Personalized Modeling and Detection of Violent Communication](https://arxiv.org/abs/2505.21451)
Append: [Do LLMs Need to Think in One Language? Correlation between Latent Language and Task Performance](https://arxiv.org/abs/2505.21458)
Append: [Accelerating Diffusion Language Model Inference via Efficient KV Caching and Guided Diffusion](https://arxiv.org/abs/2505.21467)
Append: [Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration](https://arxiv.org/abs/2505.21471)
Append: [Are Language Models Consequentialist or Deontological Moral Reasoners?](https://arxiv.org/abs/2505.21479)
Append: [UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents](https://arxiv.org/abs/2505.21496)
Append: [Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making](https://arxiv.org/abs/2505.21503)
Append: [How does Alignment Enhance LLMs' Multilingual Capabilities? A Language Neurons Perspective](https://arxiv.org/abs/2505.21505)
Append: [InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning](https://arxiv.org/abs/2505.18291)
Append: [Cultural Awareness in Vision-Language Models: A Cross-Country Exploration](https://arxiv.org/abs/2505.20326)
Append: [Towards Emotionally Consistent Text-Based Speech Editing: Introducing EmoCorrector and The ECD-TSE Dataset](https://arxiv.org/abs/2505.20341)
Append: [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
Append: [What Changed? Detecting and Evaluating Instruction-Guided Image Edits with Multimodal Large Language Models](https://arxiv.org/abs/2505.20405)
Append: [SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents](https://arxiv.org/abs/2505.20411)
Append: [The Impact of a Chatbot's Ephemerality-Framing on Self-Disclosure Perceptions](https://arxiv.org/abs/2505.20464)
Append: [BrainStratify: Coarse-to-Fine Disentanglement of Intracranial Neural Dynamics](https://arxiv.org/abs/2505.20480)
Append: [Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review](https://arxiv.org/abs/2505.20503)
Append: [Project Riley: Multimodal Multi-Agent LLM Collaboration with Emotional Reasoning and Voting](https://arxiv.org/abs/2505.20521)
Append: [Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
Append: [Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning](https://arxiv.org/abs/2505.20561)
Append: [Comparisons between a Large Language Model-based Real-Time Compound Diagnostic Medical AI Interface and Physicians for Common Internal Medicine Cases using Simulated Patients](https://arxiv.org/abs/2505.20609)
Append: [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
Append: [SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis](https://arxiv.org/abs/2505.20630)
Append: [TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform for Terpenoid Research](https://arxiv.org/abs/2505.20663)
Append: [Can we Debias Social Stereotypes in AI-Generated Images? Examining Text-to-Image Outputs and User Perceptions](https://arxiv.org/abs/2505.20692)
Append: [MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware Multi-Segment Grounding](https://arxiv.org/abs/2505.20715)
Append: [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
Append: [An LLM-as-Judge Metric for Bridging the Gap with Human Evaluation in SE Tasks](https://arxiv.org/abs/2505.20854)
Append: [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
Append: [Cross from Left to Right Brain: Adaptive Text Dreamer for Vision-and-Language Navigation](https://arxiv.org/abs/2505.20897)
Append: [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
Append: [Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers](https://arxiv.org/abs/2505.21024)
Append: [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
Append: [Creativity in LLM-based Multi-Agent Systems: A Survey](https://arxiv.org/abs/2505.21116)
Append: [Leveraging GANs for citation intent classification and its impact on citation network analysis](https://arxiv.org/abs/2505.21162)
Append: [PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing](https://arxiv.org/abs/2505.21184)
Append: [PSRB: A Comprehensive Benchmark for Evaluating Persian ASR Systems](https://arxiv.org/abs/2505.21230)
Append: [Breaking the Ceiling: Exploring the Potential of Jailbreak Attacks through Expanding Strategy Space](https://arxiv.org/abs/2505.21277)
Append: [Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants](https://arxiv.org/abs/2505.21304)
Append: [Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks](https://arxiv.org/abs/2505.21329)
Append: [The Multilingual Divide and Its Impact on Global AI Safety](https://arxiv.org/abs/2505.21344)
Append: [ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models](https://arxiv.org/abs/2505.21465)
Append: [Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration](https://arxiv.org/abs/2505.21472)
Append: [Hardware-Efficient Attention for Fast Decoding](https://arxiv.org/abs/2505.21487)
Append: [Reinforcing General Reasoning without Verifiers](https://arxiv.org/abs/2505.21493)
Append: [Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers](https://arxiv.org/abs/2505.21497)
Append: [ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models](https://arxiv.org/abs/2505.21500)
Append: [WizardLM: Empowering large pre-trained language models to follow complex instructions](https://arxiv.org/abs/2304.12244)
Append: [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568)
Append: [Tradeoffs Between Alignment and Helpfulness in Language Models with Steering Methods](https://arxiv.org/abs/2401.16332)
Append: [LLMs with Industrial Lens: Deciphering the Challenges and Prospects -- A Survey](https://arxiv.org/abs/2402.14558)
Append: [An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment](https://arxiv.org/abs/2403.04963)
Append: [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518)
Append: [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [Predicting drug-gene relations via analogy tasks with word embeddings](https://arxiv.org/abs/2406.00984)
Append: [NAP^2: A Benchmark for Naturalness and Privacy-Preserving Text Rewriting by Learning from Human](https://arxiv.org/abs/2406.03749)
Append: [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/abs/2406.14230)
Append: [Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?](https://arxiv.org/abs/2407.00996)
Append: [Fine-Tuning on Diverse Reasoning Chains Drives Within-Inference CoT Refinement in LLMs](https://arxiv.org/abs/2407.03181)
Append: [Autoregressive Speech Synthesis without Vector Quantization](https://arxiv.org/abs/2407.08551)
Append: [Sentiment Reasoning for Healthcare](https://arxiv.org/abs/2407.21054)
Append: [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/abs/2408.13533)
Append: [GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding](https://arxiv.org/abs/2409.04183)
Append: [Rethinking Semantic Parsing for Large Language Models: Enhancing LLM Performance with Semantic Hints](https://arxiv.org/abs/2409.14469)
Append: [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/abs/2410.01651)
Append: [Subtle Errors in Reasoning: Preference Learning via Error-injected Self-editing](https://arxiv.org/abs/2410.06638)
Append: [Conversational Code Generation: a Case Study of Designing a Dialogue System for Generating Driving Scenarios for Testing Autonomous Vehicles](https://arxiv.org/abs/2410.09829)
Append: [The Best of Both Worlds: Bridging Quality and Diversity in Data Selection with Bipartite Graph](https://arxiv.org/abs/2410.12458)
Append: [BQA: Body Language Question Answering Dataset for Video Large Language Models](https://arxiv.org/abs/2410.13206)
Append: [Aggregation Artifacts in Subjective Tasks Collapse Large Language Models' Posteriors](https://arxiv.org/abs/2410.13776)
Append: [Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs](https://arxiv.org/abs/2410.14641)
Append: [Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch](https://arxiv.org/abs/2410.18693)
Append: [Frequency matters: Modeling irregular morphological patterns in Spanish with Transformers](https://arxiv.org/abs/2410.21013)
Append: [STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing](https://arxiv.org/abs/2411.00387)
Append: [Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2411.01834)
Append: [Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection](https://arxiv.org/abs/2411.07446)
Append: [Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation](https://arxiv.org/abs/2411.12719)
Append: [DRPruning: Efficient Large Language Model Pruning through Distributionally Robust Optimization](https://arxiv.org/abs/2411.14055)
Append: [How Private are Language Models in Abstractive Summarization?](https://arxiv.org/abs/2412.12040)
Append: [Knowledge Boundary of Large Language Models: A Survey](https://arxiv.org/abs/2412.12472)
Append: [ProgCo: Program Helps Self-Correction of Large Language Models](https://arxiv.org/abs/2501.01264)
Append: [VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models](https://arxiv.org/abs/2501.04962)
Append: [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)
Append: [Tuning LLM Judge Design Decisions for 1/1000 of the Cost](https://arxiv.org/abs/2501.17178)
Append: [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/abs/2501.18922)
Append: [Thinking beyond the anthropomorphic paradigm benefits LLM research](https://arxiv.org/abs/2502.09192)
Append: [The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions](https://arxiv.org/abs/2502.09674)
Append: [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)
Append: [ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition](https://arxiv.org/abs/2502.11198)
Append: [SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs](https://arxiv.org/abs/2502.12134)
Append: [Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues](https://arxiv.org/abs/2502.12187)
Append: [Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text](https://arxiv.org/abs/2502.12953)
Append: [Agentic Medical Knowledge Graphs Enhance Medical Question Answering: Bridging the Gap Between LLMs and Evolving Medical Knowledge](https://arxiv.org/abs/2502.13010)
Append: [Can Community Notes Replace Professional Fact-Checkers?](https://arxiv.org/abs/2502.14132)
Append: [Behavioral Analysis of Information Salience in Large Language Models](https://arxiv.org/abs/2502.14613)
Append: [Predicting Through Generation: Why Generation Is Better for Prediction](https://arxiv.org/abs/2502.17817)
Append: [Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework](https://arxiv.org/abs/2502.18874)
Append: [Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases](https://arxiv.org/abs/2502.19249)
Append: [Plan2Align: Predictive Planning Based Test-Time Preference Alignment for Large Language Models](https://arxiv.org/abs/2502.20795)
Append: [The Power of Personality: A Human Simulation Perspective to Investigate Large Language Model Agents](https://arxiv.org/abs/2502.20859)
Append: [Beware of Your Po! Measuring and Mitigating AI Safety Risks in Role-Play Fine-Tuning of LLMs](https://arxiv.org/abs/2502.20968)
Append: [MA-LoT: Model-Collaboration Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving](https://arxiv.org/abs/2503.03205)
Append: [HalluCounter: Reference-free LLM Hallucination Detection in the Wild!](https://arxiv.org/abs/2503.04615)
Append: [How to Protect Yourself from 5G Radiation? Investigating LLM Responses to Implicit Misinformation](https://arxiv.org/abs/2503.09598)
Append: [No LLM is Free From Bias: A Comprehensive Study of Bias Evaluation in Large Language Models](https://arxiv.org/abs/2503.11985)
Append: [Enabling Inclusive Systematic Reviews: Incorporating Preprint Articles with Large Language Model-Driven Evaluations](https://arxiv.org/abs/2503.13857)
Append: [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)
Append: [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)
Append: [Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2505.05111)
Append: [Beyond 'Aha!': Toward Systematic Meta-Abilities Alignment in Large Reasoning Models](https://arxiv.org/abs/2505.10554)
Append: [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)
Append: [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
Append: [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
Append: [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
Append: [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
Append: [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)
Append: [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
Append: [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
Append: [CLEVRER-Humans: Describing Physical and Causal Events the Human Way](https://arxiv.org/abs/2310.03635)
Append: [Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification](https://arxiv.org/abs/2402.04068)
Append: [GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement](https://arxiv.org/abs/2406.11546)
Append: [Can Large Language Models Understand Symbolic Graphics Programs?](https://arxiv.org/abs/2408.08313)
Append: ["Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree": Zero-Shot Decision Tree Induction and Embedding with Large Language Models](https://arxiv.org/abs/2409.18594)
Append: [EPIC: Efficient Position-Independent Caching for Serving Large Language Models](https://arxiv.org/abs/2410.15332)
Append: [Yesterday's News: Benchmarking Multi-Dimensional Out-of-Distribution Generalization of Misinformation Detection Models](https://arxiv.org/abs/2410.18122)
Append: [Interlocking-free Selective Rationalization Through Genetic-based Learning](https://arxiv.org/abs/2412.10312)
Append: [Leveraging Large Language Models for Active Merchant Non-player Characters](https://arxiv.org/abs/2412.11189)
Append: [Transparent and Coherent Procedural Mistake Detection](https://arxiv.org/abs/2412.11927)
Append: [Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey](https://arxiv.org/abs/2412.20367)
Append: [Efficiently Scaling LLM Reasoning with Certaindex](https://arxiv.org/abs/2412.20993)
Append: [More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives](https://arxiv.org/abs/2501.04070)
Append: [vCache: Verified Semantic Prompt Caching](https://arxiv.org/abs/2502.03771)
Append: [A Lightweight Method to Disrupt Memorized Sequences in LLM](https://arxiv.org/abs/2502.05159)
Append: [Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond](https://arxiv.org/abs/2502.05374)
Append: [Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection](https://arxiv.org/abs/2502.06042)
Append: [GeLLMO: Generalizing Large Language Models for Multi-property Molecule Optimization](https://arxiv.org/abs/2502.13398)
Append: [Training a Generally Curious Agent](https://arxiv.org/abs/2502.17543)
Append: [Voting or Consensus? Decision-Making in Multi-Agent Debate](https://arxiv.org/abs/2502.19130)
Append: [Path Pooling: Training-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2503.05203)
Append: [ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2503.09501)
Append: [Exploring the Necessity of Reasoning in LLM-based Agent Scenarios](https://arxiv.org/abs/2503.11074)
Append: [SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models](https://arxiv.org/abs/2503.13503)
Append: [LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers](https://arxiv.org/abs/2503.14434)
Append: [Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models](https://arxiv.org/abs/2503.20576)
Append: [Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations](https://arxiv.org/abs/2505.04948)
Append: [ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention](https://arxiv.org/abs/2505.10222)
Append: [MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly](https://arxiv.org/abs/2505.10610)
Append: [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
Append: [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training](https://arxiv.org/abs/2505.14681)
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
append_entries: 291
Finish: 2025-05-28 04:27:04.742668
------------------------------------------------------
Started: 2025-05-28 06:25:15.080654
Existing_entries: 1291
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1412
Summarized using GPT-3.5-turbo
Append: [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)
Token length: 1463
Summarized using GPT-3.5-turbo
Append: [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [OrionBench: A Benchmark for Chart and Human-Recognizable Object Detection in Infographics](https://arxiv.org/abs/2505.17473)
Token length: 1465
Summarized using GPT-3.5-turbo
Append: [NeUQI: Near-Optimal Uniform Quantization Parameter Initialization](https://arxiv.org/abs/2505.17595)
Token length: 1114
Summarized using GPT-3.5-turbo
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2505.17997)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Structured Thinking Matters: Improving LLMs Generalization in Causal Inference Tasks](https://arxiv.org/abs/2505.18034)
append_entries: 7
Finish: 2025-05-28 06:25:32.168821
------------------------------------------------------
Started: 2025-05-28 08:22:24.863839
Existing_entries: 1007
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 08:22:25.514336
------------------------------------------------------
Started: 2025-05-28 10:18:49.376337
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 10:18:50.115461
------------------------------------------------------
Started: 2025-05-28 12:35:04.812962
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 12:35:05.455127
------------------------------------------------------
Started: 2025-05-28 14:17:45.260354
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 14:17:45.923769
------------------------------------------------------
Started: 2025-05-28 16:19:39.824501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 16:19:40.545935
------------------------------------------------------
Started: 2025-05-28 18:22:08.933407
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 18:22:09.589879
------------------------------------------------------
Started: 2025-05-28 20:18:47.335795
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 20:18:47.984769
------------------------------------------------------
Started: 2025-05-28 22:15:33.698776
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-28 22:15:34.344525
------------------------------------------------------
Started: 2025-05-29 01:19:39.834538
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 01:19:40.479882
------------------------------------------------------
Started: 2025-05-29 03:11:33.079014
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 03:11:33.738525
------------------------------------------------------
Started: 2025-05-29 04:26:33.202518
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/abs/2505.21578)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/abs/2505.21598)
Token length: 1485
Summarized using GPT-3.5-turbo
Append: [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
Token length: 1393
Summarized using GPT-3.5-turbo
Append: [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
Token length: 1510
Summarized using GPT-3.5-turbo
Append: [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/abs/2505.21701)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/abs/2505.21710)
Token length: 1059
Summarized using GPT-3.5-turbo
Append: [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/abs/2505.21757)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/abs/2505.21781)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
Token length: 886
Summarized using GPT-3.5-turbo
Append: [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/abs/2505.21816)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Representative Language Generation](https://arxiv.org/abs/2505.21819)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/abs/2505.21859)
Token length: 1105
Summarized using GPT-3.5-turbo
Append: [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/abs/2505.21889)
Token length: 1454
Summarized using GPT-3.5-turbo
Append: [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
Token length: 1933
Summarized using GPT-3.5-turbo
Append: [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
Token length: 1160
Summarized using GPT-3.5-turbo
Append: [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/abs/2505.21937)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/abs/2505.21940)
Token length: 774
Summarized using GPT-3.5-turbo
Append: [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/abs/2505.21941)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/abs/2505.21958)
Token length: 1573
Summarized using GPT-3.5-turbo
Append: [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
Token length: 1124
Summarized using GPT-3.5-turbo
Append: [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/abs/2505.21967)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)
Token length: 1442
Summarized using GPT-3.5-turbo
Append: [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/abs/2505.21997)
Token length: 1222
Summarized using GPT-3.5-turbo
Append: [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/abs/2505.21999)
Token length: 1606
Summarized using GPT-3.5-turbo
Append: [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/abs/2505.22017)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/abs/2505.22018)
Token length: 1958
Summarized using GPT-3.5-turbo
Append: [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
Token length: 751
Summarized using GPT-3.5-turbo
Append: [Voice Adaptation for Swiss German](https://arxiv.org/abs/2505.22054)
Token length: 949
Summarized using GPT-3.5-turbo
Append: [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/abs/2505.22076)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.22095)
Token length: 1415
Summarized using GPT-3.5-turbo
Append: [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
Token length: 1537
Summarized using GPT-3.5-turbo
Append: [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/abs/2505.22101)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
Token length: 1158
Summarized using GPT-3.5-turbo
Append: [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/abs/2505.22113)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
Token length: 1201
Summarized using GPT-3.5-turbo
Append: [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)
Append: [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/abs/2505.22120)
Append: [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/abs/2505.22131)
Append: [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
Append: [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
Append: [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)
Append: [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)
Append: [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
Append: [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)
Append: [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/abs/2505.22172)
Append: [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
Append: [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
Append: [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
Append: [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
Append: [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
Append: [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/abs/2505.22236)
Append: [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
Append: [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
Append: [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/abs/2505.22273)
Append: [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
Append: [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/abs/2505.22293)
Append: [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
Append: [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)
Append: [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
Append: [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)
Append: [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)
Append: [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
Append: [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
Append: [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/abs/2505.22354)
Append: [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)
Append: [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/abs/2505.22430)
Append: [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
Append: [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/abs/2505.22501)
Append: [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)
Append: [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)
Append: [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
Append: [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/abs/2505.22563)
Append: [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
Append: [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
Append: [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/abs/2505.22582)
Append: [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/abs/2505.22586)
Append: [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
Append: [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)
Append: [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
Append: [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)
Append: [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
Append: [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
Append: [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)
Append: [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)
Append: [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/abs/2505.22653)
Append: [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/abs/2505.22661)
Append: [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
Append: [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
Append: [Complexity counts: global and local perspectives on Indo-Aryan numeral systems](https://arxiv.org/abs/2505.21510)
Append: [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
Append: [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
Append: [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
Append: [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
Append: [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
Append: [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
Append: [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
Append: [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
Append: [From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism](https://arxiv.org/abs/2505.21753)
Append: [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
Append: [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
Append: [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
Append: [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
Append: [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
Append: [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
Append: [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
Append: [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
Append: [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
Append: [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
Append: [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
Append: [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
Append: [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)
Append: [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
Append: [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
Append: [Visual Cues Support Robust Turn-taking Prediction in Noise](https://arxiv.org/abs/2505.22088)
Append: [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
Append: [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
Append: [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
Append: [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
Append: [Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis](https://arxiv.org/abs/2505.22231)
Append: [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
Append: [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
Append: [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
Append: [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
Append: [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
Append: [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
Append: [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
Append: [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
Append: [Effective Context in Neural Speech Models](https://arxiv.org/abs/2505.22487)
Append: [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
Append: [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
Append: [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
Append: [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
Append: [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
Append: [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
Append: [Machine Translation Models are Zero-Shot Detectors of Translation Direction](https://arxiv.org/abs/2401.06769)
Append: [Tracking Semantic Change in Slovene: A Novel Dataset and Optimal Transport-Based Distance](https://arxiv.org/abs/2402.16596)
Append: [Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems](https://arxiv.org/abs/2404.06762)
Append: [Mitigating Text Toxicity with Counterfactual Generation](https://arxiv.org/abs/2405.09948)
Append: [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)
Append: [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)
Append: [Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?](https://arxiv.org/abs/2406.14737)
Append: [Large Vocabulary Size Improves Large Language Models](https://arxiv.org/abs/2406.16508)
Append: [Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification](https://arxiv.org/abs/2407.07004)
Append: [Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering](https://arxiv.org/abs/2409.04122)
Append: [Nonlinear second-order dynamics describe labial constriction trajectories across languages and contexts](https://arxiv.org/abs/2410.08351)
Append: [Which Demographics do LLMs Default to During Annotation?](https://arxiv.org/abs/2410.08820)
Append: [Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2410.13080)
Append: [SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior](https://arxiv.org/abs/2410.16665)
Append: [Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching](https://arxiv.org/abs/2410.18436)
Append: [Understanding Synthetic Context Extension via Retrieval Heads](https://arxiv.org/abs/2410.22316)
Append: [Controllable Context Sensitivity and the Knob Behind It](https://arxiv.org/abs/2411.07404)
Append: [LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [Overcoming Non-monotonicity in Transducer-based Streaming Generation](https://arxiv.org/abs/2411.17170)
Append: [ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning](https://arxiv.org/abs/2412.11418)
Append: [Core Context Aware Transformers for Long Context Language Modeling](https://arxiv.org/abs/2412.12465)
Append: [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/abs/2412.16926)
Append: [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/abs/2501.00777)
Append: [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/abs/2501.03124)
Append: [LLMs Reproduce Stereotypes of Sexual and Gender Minorities](https://arxiv.org/abs/2501.05926)
Append: [Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts](https://arxiv.org/abs/2501.06365)
Append: [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://arxiv.org/abs/2501.13567)
Append: [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/abs/2501.13953)
Append: [Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains](https://arxiv.org/abs/2501.14431)
Append: [Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2502.00602)
Append: [Advancing Reasoning in Large Language Models: Promising Methods and Approaches](https://arxiv.org/abs/2502.03671)
Append: [Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring](https://arxiv.org/abs/2502.05242)
Append: [LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation](https://arxiv.org/abs/2502.07365)
Append: [CoSER: Coordinating LLM-Based Persona Simulation of Established Roles](https://arxiv.org/abs/2502.09082)
Append: [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/abs/2502.11100)
Append: [ReLearn: Unlearning via Learning for Large Language Models](https://arxiv.org/abs/2502.11190)
Append: [Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning](https://arxiv.org/abs/2502.11441)
Append: [BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages](https://arxiv.org/abs/2502.11926)
Append: [ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails](https://arxiv.org/abs/2502.13458)
Append: [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913)
Append: [Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering](https://arxiv.org/abs/2502.14245)
Append: [Self-Taught Agentic Long Context Understanding](https://arxiv.org/abs/2502.15920)
Append: [Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations](https://arxiv.org/abs/2503.00134)
Append: [Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication](https://arxiv.org/abs/2503.04395)
Append: [Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation](https://arxiv.org/abs/2503.08057)
Append: [Explicit Learning and the LLM in Machine Translation](https://arxiv.org/abs/2503.09454)
Append: [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)
Append: [Constrained Discrete Diffusion](https://arxiv.org/abs/2503.09790)
Append: [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond](https://arxiv.org/abs/2503.10460)
Append: [TLUE: A Tibetan Language Understanding Evaluation Benchmark](https://arxiv.org/abs/2503.12051)
Append: [Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA](https://arxiv.org/abs/2503.17933)
Append: [Sun-Shine: A Foundation Large Language Model for Tibetan Culture and Heritage](https://arxiv.org/abs/2503.18288)
Append: [Token embeddings violate the manifold hypothesis](https://arxiv.org/abs/2504.01002)
Append: [Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices](https://arxiv.org/abs/2504.03312)
Append: [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/abs/2504.03561)
Append: [AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments](https://arxiv.org/abs/2504.05104)
Append: [Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations](https://arxiv.org/abs/2504.06792)
Append: [Layers at Similar Depths Generate Similar Activations Across LLM Architectures](https://arxiv.org/abs/2504.08775)
Append: [GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM](https://arxiv.org/abs/2504.12339)
Append: [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)
Append: [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)
Append: [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)
Append: [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
Append: [Language-Specific Latent Process Hinders Cross-Lingual Performance](https://arxiv.org/abs/2505.13141)
Append: [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
Append: [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
Append: [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
Append: [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
Append: [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
Append: [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)
Append: [Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models](https://arxiv.org/abs/2311.04378)
Append: [Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures](https://arxiv.org/abs/2407.19580)
Append: [VQ-CTAP: Cross-Modal Fine-Grained Sequence Representation Learning for Speech Processing](https://arxiv.org/abs/2408.05758)
Append: [Fine-Grained and Thematic Evaluation of LLMs in Social Deduction Game](https://arxiv.org/abs/2408.09946)
Append: [On the Within-class Variation Issue in Alzheimer's Disease Detection](https://arxiv.org/abs/2409.16322)
Append: [Exploring the Limitations of Mamba in COPY and CoT Reasoning](https://arxiv.org/abs/2410.03810)
Append: [AdvAgent: Controllable Blackbox Red-teaming on Web Agents](https://arxiv.org/abs/2410.17401)
Append: [Natural Language Reinforcement Learning](https://arxiv.org/abs/2411.14251)
Append: [AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling](https://arxiv.org/abs/2411.17284)
Append: [Preference Adaptive and Sequential Text-to-Image Generation](https://arxiv.org/abs/2412.10419)
Append: [You Do Not Fully Utilize Transformer's Representation Capacity](https://arxiv.org/abs/2502.09245)
Append: [Non-Markovian Discrete Diffusion with Causal Language Models](https://arxiv.org/abs/2502.09767)
Append: [Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models](https://arxiv.org/abs/2502.09863)
Append: [Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration](https://arxiv.org/abs/2502.11882)
Append: [MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections](https://arxiv.org/abs/2502.12170)
Append: [WiseMind: Recontextualizing AI with a Knowledge-Guided, Theory-Informed Multi-Agent Framework for Instrumental and Humanistic Benefits](https://arxiv.org/abs/2502.20689)
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Append: [WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation](https://arxiv.org/abs/2503.07265)
Append: [Tempest: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search](https://arxiv.org/abs/2503.10619)
Append: [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Append: [Benchmarking LLMs' Swarm intelligence](https://arxiv.org/abs/2505.04364)
Append: [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
Append: [Towards Practical Defect-Focused Automated Code Review](https://arxiv.org/abs/2505.17928)
Append: [Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding](https://arxiv.org/abs/2505.18079)
Append: [Bridging Supervised Learning and Reinforcement Learning in Math Reasoning](https://arxiv.org/abs/2505.18116)
append_entries: 247
Finish: 2025-05-29 04:28:44.012511
------------------------------------------------------
Started: 2025-05-29 06:25:38.653807
Existing_entries: 1247
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1426
Summarized using GPT-3.5-turbo
Append: [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/abs/2502.16514)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [LINGOLY-TOO: Disentangling Reasoning from Knowledge with Templatised Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)
Token length: 1337
Summarized using GPT-3.5-turbo
Append: [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/abs/2503.10688)
append_entries: 3
Finish: 2025-05-29 06:25:48.119924
------------------------------------------------------
Started: 2025-05-29 08:22:27.216220
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 08:22:27.795545
------------------------------------------------------
Started: 2025-05-29 10:18:44.271650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 10:18:44.822579
------------------------------------------------------
Started: 2025-05-29 12:34:00.300977
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 12:34:00.858918
------------------------------------------------------
Started: 2025-05-29 14:16:13.619860
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 14:16:14.203877
------------------------------------------------------
Started: 2025-05-29 16:20:19.301069
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 16:20:19.908487
------------------------------------------------------
Started: 2025-05-29 18:22:50.090030
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 18:22:50.694769
------------------------------------------------------
Started: 2025-05-29 20:18:51.623273
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 20:18:52.274538
------------------------------------------------------
Started: 2025-05-29 22:15:53.687654
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-29 22:15:54.281277
------------------------------------------------------
Started: 2025-05-30 01:17:33.438446
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 01:17:34.049622
------------------------------------------------------
Started: 2025-05-30 03:09:40.627553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 03:09:41.185059
------------------------------------------------------
Started: 2025-05-30 04:24:45.166820
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
Token length: 801
Summarized using GPT-3.5-turbo
Append: [Climate Finance Bench](https://arxiv.org/abs/2505.22752)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
Token length: 1621
Summarized using GPT-3.5-turbo
Append: [StressTest: Can YOUR Speech LM Handle the Stress?](https://arxiv.org/abs/2505.22765)
Token length: 947
Summarized using GPT-3.5-turbo
Append: [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
Token length: 1609
Summarized using GPT-3.5-turbo
Append: [Counting trees: A treebank-driven exploration of syntactic variation in speech and writing across languages](https://arxiv.org/abs/2505.22774)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
Token length: 1816
Summarized using GPT-3.5-turbo
Append: [Can Large Language Models Match the Conclusions of Systematic Reviews?](https://arxiv.org/abs/2505.22787)
Token length: 1016
Summarized using GPT-3.5-turbo
Append: [Towards a More Generalized Approach in Open Relation Extraction](https://arxiv.org/abs/2505.22801)
Token length: 1065
Summarized using GPT-3.5-turbo
Append: [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/abs/2505.22823)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
Token length: 927
Summarized using GPT-3.5-turbo
Append: [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [GateNLP at SemEval-2025 Task 10: Hierarchical Three-Step Prompting for Multilingual Narrative Classification](https://arxiv.org/abs/2505.22867)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [When Models Reason in Your Language: Controlling Thinking Trace Language Comes at the Cost of Accuracy](https://arxiv.org/abs/2505.22888)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [VIGNETTE: Socially Grounded Bias Evaluation for Vision-Language Models](https://arxiv.org/abs/2505.22897)
Token length: 862
Summarized using GPT-3.5-turbo
Append: [Talent or Luck? Evaluating Attribution Bias in Large Language Models](https://arxiv.org/abs/2505.22910)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room](https://arxiv.org/abs/2505.22919)
Token length: 1693
Summarized using GPT-3.5-turbo
Append: [Structured Memory Mechanisms for Stable Context Representation in Large Language Models](https://arxiv.org/abs/2505.22921)
Token length: 1360
Summarized using GPT-3.5-turbo
Append: [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Improving QA Efficiency with DistilBERT: Fine-Tuning and Inference on mobile Intel CPUs](https://arxiv.org/abs/2505.22937)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
Token length: 982
Summarized using GPT-3.5-turbo
Append: [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
Token length: 1567
Summarized using GPT-3.5-turbo
Append: [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [StrucSum: Graph-Structured Reasoning for Long Document Extractive Summarization with LLMs](https://arxiv.org/abs/2505.22950)
Token length: 1061
Summarized using GPT-3.5-turbo
Append: [LLMs for Argument Mining: Detection, Extraction, and Relationship Classification of pre-defined Arguments in Online Comments](https://arxiv.org/abs/2505.22956)
Token length: 1710
Summarized using GPT-3.5-turbo
Append: [LLM-based HSE Compliance Assessment: Benchmark, Performance, and Advancements](https://arxiv.org/abs/2505.22959)
Token length: 1810
Summarized using GPT-3.5-turbo
Append: [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
Token length: 1206
Summarized using GPT-3.5-turbo
Append: [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Detecting Stealthy Backdoor Samples based on Intra-class Distance for Large Language Models](https://arxiv.org/abs/2505.23015)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Spac](https://arxiv.org/abs/2505.23029)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Can Modern NLP Systems Reliably Annotate Chest Radiography Exams? A Pre-Purchase Evaluation and Comparative Study of Solutions from AWS, Google, Azure, John Snow Labs, and Open-Source Models on an Independent Pediatric Dataset](https://arxiv.org/abs/2505.23030)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [Machine-Facing English: Defining a Hybrid Register Shaped by Human-AI Discourse](https://arxiv.org/abs/2505.23035)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Improving Multilingual Social Media Insights: Aspect-based Comment Analysis](https://arxiv.org/abs/2505.23037)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [EL4NER: Ensemble Learning for Named Entity Recognition via Multiple Small-Parameter Large Language Models](https://arxiv.org/abs/2505.23038)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [Query Routing for Retrieval-Augmented Language Models](https://arxiv.org/abs/2505.23052)
Token length: 1445
Summarized using GPT-3.5-turbo
Append: [Self-Correcting Code Generation Using Small Language Models](https://arxiv.org/abs/2505.23060)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [SNS-Bench-VL: Benchmarking Multimodal Large Language Models in Social Networking Services](https://arxiv.org/abs/2505.23065)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [Generating Diverse Training Samples for Relation Extraction with Large Language Models](https://arxiv.org/abs/2505.23108)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
Append: [PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics](https://arxiv.org/abs/2505.23126)
Append: [Enhancing Large Language Models'Machine Translation via Dynamic Focus Anchoring](https://arxiv.org/abs/2505.23140)
Append: [Cross-Domain Bilingual Lexicon Induction via Pretrained Language Models](https://arxiv.org/abs/2505.23146)
Append: [Tell, Don't Show: Leveraging Language Models' Abstractive Retellings to Model Literary Themes](https://arxiv.org/abs/2505.23166)
Append: [ZIPA: A family of efficient models for multilingual phone recognition](https://arxiv.org/abs/2505.23170)
Append: [Map&Make: Schema Guided Text to Table Generation](https://arxiv.org/abs/2505.23174)
Append: [Infinite-Instruct: Synthesizing Scaling Code instruction Data with Bidirectional Synthesis and Static Verification](https://arxiv.org/abs/2505.23177)
Append: [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
Append: [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
Append: [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
Append: [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
Append: [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
Append: [ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering](https://arxiv.org/abs/2505.23242)
Append: [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)
Append: [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
Append: [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
Append: [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/abs/2505.23291)
Append: [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
Append: [EmoBench-UA: A Benchmark Dataset for Emotion Detection in Ukrainian](https://arxiv.org/abs/2505.23297)
Append: [Data-efficient Meta-models for Evaluation of Context-based Questions and Answers in LLMs](https://arxiv.org/abs/2505.23299)
Append: [Generalized Category Discovery in Event-Centric Contexts: Latent Pattern Mining with LLMs](https://arxiv.org/abs/2505.23304)
Append: [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
Append: [Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO](https://arxiv.org/abs/2505.23316)
Append: [Neither Stochastic Parroting nor AGI: LLMs Solve Tasks through Context-Directed Extrapolation from Training Data Priors](https://arxiv.org/abs/2505.23323)
Append: [Discriminative Policy Optimization for Token-Level Reward Models](https://arxiv.org/abs/2505.23363)
Append: [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
Append: [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404)
Append: [From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs](https://arxiv.org/abs/2505.23410)
Append: [The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence](https://arxiv.org/abs/2505.23420)
Append: [UAQFact: Evaluating Factual Knowledge Utilization of LLMs on Unanswerable Questions](https://arxiv.org/abs/2505.23461)
Append: [Evaluating the performance and fragility of large language models on the self-assessment for neurological surgeons](https://arxiv.org/abs/2505.23477)
Append: [Revisiting Overthinking in Long Chain-of-Thought from the Perspective of Self-Doubt](https://arxiv.org/abs/2505.23480)
Append: [Spoken Language Modeling with Duration-Penalized Self-Supervised Units](https://arxiv.org/abs/2505.23494)
Append: [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
Append: [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
Append: [Probability-Consistent Preference Optimization for Enhanced LLM Reasoning](https://arxiv.org/abs/2505.23540)
Append: [Translation in the Wild](https://arxiv.org/abs/2505.23548)
Append: [Understanding Refusal in Language Models with Sparse Autoencoders](https://arxiv.org/abs/2505.23556)
Append: [Evaluating AI capabilities in detecting conspiracy theories on YouTube](https://arxiv.org/abs/2505.23570)
Append: [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
Append: [Table-R1: Inference-Time Scaling for Table Reasoning](https://arxiv.org/abs/2505.23621)
Append: [Characterizing the Expressivity of Transformer Language Models](https://arxiv.org/abs/2505.23623)
Append: [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
Append: [GeNRe: A French Gender-Neutral Rewriting System Using Collective Nouns](https://arxiv.org/abs/2505.23630)
Append: [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
Append: [ARC: Argument Representation and Coverage Analysis for Zero-Shot Long Document Summarization with Instruction Following LLMs](https://arxiv.org/abs/2505.23654)
Append: [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
Append: [ToolHaystack: Stress-Testing Tool-Augmented Language Models in Realistic Long-Term Interactions](https://arxiv.org/abs/2505.23662)
Append: [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
Append: [Automatic classification of stop realisation with wav2vec2.0](https://arxiv.org/abs/2505.23688)
Append: [Child-Directed Language Does Not Consistently Boost Syntax Learning in Language Models](https://arxiv.org/abs/2505.23689)
Append: [Can LLMs Reason Abstractly Over Math Word Problems Without CoT? Disentangling Abstract Formulation From Arithmetic Computation](https://arxiv.org/abs/2505.23701)
Append: [SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models](https://arxiv.org/abs/2505.23713)
Append: [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
Append: [Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models](https://arxiv.org/abs/2505.23715)
Append: [Label-Guided In-Context Learning for Named Entity Recognition](https://arxiv.org/abs/2505.23722)
Append: [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
Append: [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
Append: [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
Append: [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
Append: [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
Append: [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
Append: [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
Append: [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
Append: [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
Append: [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/abs/2505.22793)
Append: [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
Append: [Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge](https://arxiv.org/abs/2505.22863)
Append: [Conversational Alignment with Artificial Intelligence in Context](https://arxiv.org/abs/2505.22907)
Append: [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
Append: [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
Append: [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
Append: [TailorSQL: An NL2SQL System Tailored to Your Query Workload](https://arxiv.org/abs/2505.23039)
Append: [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
Append: [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
Append: [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
Append: [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
Append: [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
Append: [Nosey: Open-source hardware for acoustic nasalance](https://arxiv.org/abs/2505.23339)
Append: [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
Append: [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
Append: [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
Append: [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.23493)
Append: [Identity resolution of software metadata using Large Language Models](https://arxiv.org/abs/2505.23500)
Append: [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
Append: [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
Append: [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
Append: [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
Append: [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
Append: [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
Append: [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
Append: [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
Append: [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
Append: [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
Append: [ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation](https://arxiv.org/abs/2405.17057)
Append: [mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus](https://arxiv.org/abs/2406.08707)
Append: [Neuro-symbolic Training for Reasoning over Spatial Language](https://arxiv.org/abs/2406.13828)
Append: [CLEME2.0: Towards Interpretable Evaluation by Disentangling Edits for Grammatical Error Correction](https://arxiv.org/abs/2407.00934)
Append: [ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts](https://arxiv.org/abs/2407.09447)
Append: [$T^5Score$: A Methodology for Automatically Assessing the Quality of LLM Generated Multi-Document Topic Sets](https://arxiv.org/abs/2407.17390)
Append: [BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models](https://arxiv.org/abs/2408.04556)
Append: [X-TURING: Towards an Enhanced and Efficient Turing Test for Long-Term Dialogue Agents](https://arxiv.org/abs/2408.09853)
Append: [Resolving Lexical Bias in Model Editing](https://arxiv.org/abs/2408.10411)
Append: [Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding with LLMs](https://arxiv.org/abs/2410.03553)
Append: [Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs](https://arxiv.org/abs/2410.11001)
Append: [On the Risk of Evidence Pollution for Malicious Social Text Detection in the Era of LLMs](https://arxiv.org/abs/2410.12600)
Append: [BenchmarkCards: Large Language Model and Risk Reporting](https://arxiv.org/abs/2410.12974)
Append: [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/abs/2410.16502)
Append: [Reducing Tool Hallucination via Reliability Alignment](https://arxiv.org/abs/2412.04141)
Append: [C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model Evaluation](https://arxiv.org/abs/2412.04947)
Append: [EXIT: Context-Aware Extractive Compression for Enhancing Retrieval-Augmented Generation](https://arxiv.org/abs/2412.12559)
Append: [FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning](https://arxiv.org/abs/2412.12567)
Append: [AntiLeakBench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge](https://arxiv.org/abs/2412.13670)
Append: [SimGRAG: Leveraging Similar Subgraphs for Knowledge Graphs Driven Retrieval-Augmented Generation](https://arxiv.org/abs/2412.15272)
Append: [Human-Readable Adversarial Prompts: An Investigation into LLM Vulnerabilities Using Situational Context](https://arxiv.org/abs/2412.16359)
Append: [Divide and Conquer: A Hybrid Strategy Defeats Multimodal Large Language Models](https://arxiv.org/abs/2412.16555)
Append: [A Reality Check on Context Utilisation for Retrieval-Augmented Generation](https://arxiv.org/abs/2412.17031)
Append: [Tensor Product Attention Is All You Need](https://arxiv.org/abs/2501.06425)
Append: [Enhancing Automated Interpretability with Output-Centric Feature Descriptions](https://arxiv.org/abs/2501.08319)
Append: [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/abs/2501.13977)
Append: [Chain of Grounded Objectives: Bridging Process and Goal-oriented Prompting for Code Generation](https://arxiv.org/abs/2501.13978)
Append: [Decomposed Opinion Summarization with Verified Aspect-Aware Modules](https://arxiv.org/abs/2501.17191)
Append: [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/abs/2502.01179)
Append: [Fast Large Language Model Collaborative Decoding via Speculation](https://arxiv.org/abs/2502.01662)
Append: [SPRI: Aligning Large Language Models with Context-Situated Principles](https://arxiv.org/abs/2502.03397)
Append: [Toward universal steering and monitoring of AI models](https://arxiv.org/abs/2502.03708)
Append: [CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance](https://arxiv.org/abs/2502.04350)
Append: [Position: Scaling LLM Agents Requires Asymptotic Analysis with LLM Primitives](https://arxiv.org/abs/2502.04358)
Append: [Uncertainty Quantification for LLMs through Minimum Bayes Risk: Bridging Confidence and Consistency](https://arxiv.org/abs/2502.04964)
Append: [Jailbreaking to Jailbreak](https://arxiv.org/abs/2502.09638)
Append: [Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages](https://arxiv.org/abs/2502.10852)
Append: [Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability](https://arxiv.org/abs/2502.11115)
Append: [Token Pruning in Multimodal Large Language Models: Are We Solving the Right Problem?](https://arxiv.org/abs/2502.11501)
Append: [Understanding In-Context Machine Translation for Low-Resource Languages: A Case Study on Manchu](https://arxiv.org/abs/2502.11862)
Append: [LongFaith: Enhancing Long-Context Reasoning in LLMs with Faithful Synthetic Data](https://arxiv.org/abs/2502.12583)
Append: [Reasoning-to-Defend: Safety-Aware Reasoning Can Defend Large Language Models from Jailbreaking](https://arxiv.org/abs/2502.12970)
Append: [Do we still need Human Annotators? Prompting Large Language Models for Aspect Sentiment Quad Prediction](https://arxiv.org/abs/2502.13044)
Append: [FlexDuo: A Pluggable System for Enabling Full-Duplex Capabilities in Speech Dialogue Systems](https://arxiv.org/abs/2502.13472)
Append: [LoRA-MGPO: Mitigating Double Descent in Low-Rank Adaptation via Momentum-Guided Perturbation Optimization](https://arxiv.org/abs/2502.14538)
Append: [Length-Controlled Margin-Based Preference Optimization without Reference Model](https://arxiv.org/abs/2502.14643)
Append: [SOTOPIA-$\Omega$: Dynamic Strategy Injection Learning and Social Instruction Following Evaluation for Social Agents](https://arxiv.org/abs/2502.15538)
Append: [ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2502.15543)
Append: [DReSD: Dense Retrieval for Speculative Decoding](https://arxiv.org/abs/2502.15572)
Append: [Instruction-Tuning LLMs for Event Extraction with Annotation Guidelines](https://arxiv.org/abs/2502.16377)
Append: [A Survey of Uncertainty Estimation Methods on Large Language Models](https://arxiv.org/abs/2503.00172)
Append: [What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2503.09894)
Append: [DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation](https://arxiv.org/abs/2503.10452)
Append: [Enhancing Retrieval for ESGLLM via ESG-CID -- A Disclosure Content Index Finetuning Dataset for Mapping GRI and ESRS](https://arxiv.org/abs/2503.10674)
Append: [HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model](https://arxiv.org/abs/2503.12941)
Append: [FutureGen: LLM-RAG Approach to Generate the Future Work of Scientific Article](https://arxiv.org/abs/2503.16561)
Append: [Temporal Relation Extraction in Clinical Texts: A Span-based Graph Transformer Approach](https://arxiv.org/abs/2503.18085)
Append: [Multi-Modal Framing Analysis of News](https://arxiv.org/abs/2503.20960)
Append: [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/abs/2503.22353)
Append: [Agentic Knowledgeable Self-awareness](https://arxiv.org/abs/2504.03553)
Append: [NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables](https://arxiv.org/abs/2504.06560)
Append: [DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?](https://arxiv.org/abs/2504.08120)
Append: [LLMs Can Achieve High-quality Simultaneous Machine Translation as Efficiently as Offline](https://arxiv.org/abs/2504.09570)
Append: [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)
Append: [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)
Append: [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Append: [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)
Append: [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
Append: [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)
Append: [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
Append: [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)
Append: [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)
Append: [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)
Append: [Hijacking Large Language Models via Adversarial In-Context Learning](https://arxiv.org/abs/2311.09948)
Append: [Theoretical guarantees on the best-of-n alignment policy](https://arxiv.org/abs/2401.01879)
Append: [Learning to Poison Large Language Models for Downstream Manipulation](https://arxiv.org/abs/2402.13459)
Append: [BioVL-QR: Egocentric Biochemical Vision-and-Language Dataset Using Micro QR Codes](https://arxiv.org/abs/2404.03161)
Append: [Shortcut-connected Expert Parallelism for Accelerating Mixture-of-Experts](https://arxiv.org/abs/2404.05019)
Append: [Sample-Efficient Human Evaluation of Large Language Models via Maximum Discrepancy Competition](https://arxiv.org/abs/2404.08008)
Append: [Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning](https://arxiv.org/abs/2408.14774)
Append: [Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents](https://arxiv.org/abs/2408.16081)
Append: [On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists](https://arxiv.org/abs/2409.13931)
Append: [CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning](https://arxiv.org/abs/2410.02229)
Append: [GIVE: Structured Reasoning of Large Language Models with Knowledge Graph Inspired Veracity Extrapolation](https://arxiv.org/abs/2410.08475)
Append: [Can We Predict Performance of Large Models across Vision-Language Tasks?](https://arxiv.org/abs/2410.10112)
Append: [GraphNarrator: Generating Textual Explanations for Graph Neural Networks](https://arxiv.org/abs/2410.15268)
Append: [Improving Parallel Program Performance with LLM Optimizers via Agent-System Interfaces](https://arxiv.org/abs/2410.15625)
Append: [GWQ: Gradient-Aware Weight Quantization for Large Language Models](https://arxiv.org/abs/2411.00850)
Append: [SequentialBreak: Large Language Models Can be Fooled by Embedding Jailbreak Prompts into Sequential Prompt Chains](https://arxiv.org/abs/2411.06426)
Append: [VideoRAG: Retrieval-Augmented Generation over Video Corpus](https://arxiv.org/abs/2501.05874)
Append: [Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection](https://arxiv.org/abs/2502.01699)
Append: [Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View](https://arxiv.org/abs/2502.11256)
Append: [GSQ-Tuning: Group-Shared Exponents Integer in Fully Quantized Training for LLMs On-Device Fine-tuning](https://arxiv.org/abs/2502.12913)
Append: [K-Paths: Reasoning over Graph Paths for Drug Repurposing and Drug Interaction Prediction](https://arxiv.org/abs/2502.13344)
Append: [STeCa: Step-level Trajectory Calibration for LLM Agent Learning](https://arxiv.org/abs/2502.14276)
Append: [Learning to Reason from Feedback at Test-Time](https://arxiv.org/abs/2502.15771)
Append: [Dataset Featurization: Uncovering Natural Language Features through Unsupervised Data Reconstruction](https://arxiv.org/abs/2502.17541)
Append: [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/abs/2503.16814)
Append: [Learning to Reason under Off-Policy Guidance](https://arxiv.org/abs/2504.14945)
Append: [How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias](https://arxiv.org/abs/2505.00926)
append_entries: 245
Finish: 2025-05-30 04:26:30.689264
------------------------------------------------------
Started: 2025-05-30 06:24:32.402844
Existing_entries: 1245
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1400
Summarized using GPT-3.5-turbo
Append: [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
append_entries: 1
Finish: 2025-05-30 06:24:35.163004
------------------------------------------------------
Started: 2025-05-30 08:21:46.605609
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 08:21:47.187615
------------------------------------------------------
Started: 2025-05-30 10:17:39.524530
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 10:17:40.120347
------------------------------------------------------
Started: 2025-05-30 12:33:50.138874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 12:33:50.749088
------------------------------------------------------
Started: 2025-05-30 14:17:03.695096
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 14:17:04.344905
------------------------------------------------------
Started: 2025-05-30 16:20:42.470038
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 16:20:43.105595
------------------------------------------------------
Started: 2025-05-30 18:22:43.112111
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 18:22:43.744087
------------------------------------------------------
Started: 2025-05-30 20:18:13.976261
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 20:18:14.510861
------------------------------------------------------
Started: 2025-05-30 22:15:35.697715
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-30 22:15:36.259501
------------------------------------------------------
Started: 2025-05-31 01:17:44.922081
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 01:17:45.492159
------------------------------------------------------
Started: 2025-05-31 03:08:38.585071
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 03:08:39.127535
------------------------------------------------------
Started: 2025-05-31 04:20:30.513549
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 04:20:30.590506
------------------------------------------------------
Started: 2025-05-31 06:22:02.359108
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 06:22:02.415079
------------------------------------------------------
Started: 2025-05-31 08:20:00.796072
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 08:20:00.854968
------------------------------------------------------
Started: 2025-05-31 10:16:15.967913
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 10:16:16.026289
------------------------------------------------------
Started: 2025-05-31 12:30:39.237802
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 12:30:39.312090
------------------------------------------------------
Started: 2025-05-31 14:14:14.855388
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 14:14:14.962627
------------------------------------------------------
Started: 2025-05-31 16:18:36.607553
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 16:18:36.715546
------------------------------------------------------
Started: 2025-05-31 18:21:13.424259
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 18:21:13.479007
------------------------------------------------------
Started: 2025-05-31 20:16:59.111552
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 20:16:59.175262
------------------------------------------------------
Started: 2025-05-31 22:14:57.376650
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-05-31 22:14:57.484143
------------------------------------------------------
Started: 2025-06-01 01:42:10.073194
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 01:42:10.137267
------------------------------------------------------
Started: 2025-06-01 03:38:35.378486
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 03:38:35.438782
------------------------------------------------------
Started: 2025-06-01 04:32:47.254844
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 04:32:47.321983
------------------------------------------------------
Started: 2025-06-01 06:23:28.372292
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 06:23:28.479077
------------------------------------------------------
Started: 2025-06-01 08:20:29.011882
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 08:20:29.071643
------------------------------------------------------
Started: 2025-06-01 10:16:57.112343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 10:16:57.166072
------------------------------------------------------
Started: 2025-06-01 12:31:28.090267
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 12:31:28.149102
------------------------------------------------------
Started: 2025-06-01 14:14:01.708030
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 14:14:01.764645
------------------------------------------------------
Started: 2025-06-01 16:19:04.204415
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 16:19:04.279091
------------------------------------------------------
Started: 2025-06-01 18:20:58.866517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 18:20:58.952196
------------------------------------------------------
Started: 2025-06-01 20:17:33.337730
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 20:17:33.410518
------------------------------------------------------
Started: 2025-06-01 22:15:24.282501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-01 22:15:24.340714
------------------------------------------------------
Started: 2025-06-02 01:22:49.566735
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 01:22:49.640402
------------------------------------------------------
Started: 2025-06-02 03:18:35.955103
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 03:18:36.016221
------------------------------------------------------
Started: 2025-06-02 04:33:13.543829
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale](https://arxiv.org/abs/2505.23785)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework](https://arxiv.org/abs/2505.23788)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Conversational Exploration of Literature Landscape with LitChat](https://arxiv.org/abs/2505.23789)
Token length: 1634
Summarized using GPT-3.5-turbo
Append: [Rethinking the Understanding Ability across LLMs through Mutual Information](https://arxiv.org/abs/2505.23790)
Token length: 1645
Summarized using GPT-3.5-turbo
Append: [R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.23794)
Token length: 850
Summarized using GPT-3.5-turbo
Append: [Emergent LLM behaviors are observationally equivalent to data leakage](https://arxiv.org/abs/2505.23796)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Detection of Suicidal Risk on Social Media: A Hybrid Model](https://arxiv.org/abs/2505.23797)
Token length: 1531
Summarized using GPT-3.5-turbo
Append: [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)
Token length: 1290
Summarized using GPT-3.5-turbo
Append: [Estimating LLM Consistency: A User Baseline vs Surrogate Metrics](https://arxiv.org/abs/2505.23799)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks](https://arxiv.org/abs/2505.23801)
Token length: 1911
Summarized using GPT-3.5-turbo
Append: [MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks](https://arxiv.org/abs/2505.23802)
Token length: 1362
Summarized using GPT-3.5-turbo
Append: [Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies](https://arxiv.org/abs/2505.23804)
Token length: 1256
Summarized using GPT-3.5-turbo
Append: [MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation](https://arxiv.org/abs/2505.23806)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [DLP: Dynamic Layerwise Pruning in Large Language Models](https://arxiv.org/abs/2505.23807)
Token length: 1379
Summarized using GPT-3.5-turbo
Append: [DenseLoRA: Dense Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2505.23808)
Token length: 802
Summarized using GPT-3.5-turbo
Append: [LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion](https://arxiv.org/abs/2505.23809)
Token length: 1423
Summarized using GPT-3.5-turbo
Append: [MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation](https://arxiv.org/abs/2505.23810)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content](https://arxiv.org/abs/2505.23812)
Token length: 1313
Summarized using GPT-3.5-turbo
Append: [Aligning LLMs by Predicting Preferences from User Writing Samples](https://arxiv.org/abs/2505.23815)
Token length: 1453
Summarized using GPT-3.5-turbo
Append: [A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs](https://arxiv.org/abs/2505.23816)
Token length: 1434
Summarized using GPT-3.5-turbo
Append: [Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams](https://arxiv.org/abs/2505.23818)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks](https://arxiv.org/abs/2505.23820)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction](https://arxiv.org/abs/2505.23822)
Token length: 1198
Summarized using GPT-3.5-turbo
Append: [RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery](https://arxiv.org/abs/2505.23823)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation](https://arxiv.org/abs/2505.23824)
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/abs/2505.23827)
Token length: 1238
Summarized using GPT-3.5-turbo
Append: [BiasFilter: An Inference-Time Debiasing Framework for Large Language Models](https://arxiv.org/abs/2505.23829)
Token length: 1932
Summarized using GPT-3.5-turbo
Append: [EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models](https://arxiv.org/abs/2505.23830)
Token length: 1667
Summarized using GPT-3.5-turbo
Append: [ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.23831)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation](https://arxiv.org/abs/2505.23832)
Token length: 1550
Summarized using GPT-3.5-turbo
Append: [Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective](https://arxiv.org/abs/2505.23833)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things](https://arxiv.org/abs/2505.23835)
Token length: 1511
Summarized using GPT-3.5-turbo
Append: [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
Token length: 1678
Summarized using GPT-3.5-turbo
Append: [CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language](https://arxiv.org/abs/2505.23837)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities](https://arxiv.org/abs/2505.23838)
Token length: 1439
Summarized using GPT-3.5-turbo
Append: [Measuring Sycophancy of Language Models in Multi-turn Dialogues](https://arxiv.org/abs/2505.23840)
Token length: 1497
Summarized using GPT-3.5-turbo
Append: [Document Valuation in LLM Summaries: A Cluster Shapley Approach](https://arxiv.org/abs/2505.23842)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks](https://arxiv.org/abs/2505.23843)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation](https://arxiv.org/abs/2505.23844)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs](https://arxiv.org/abs/2505.23845)
Token length: 1963
Summarized using GPT-3.5-turbo
Append: [Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations](https://arxiv.org/abs/2505.23846)
Token length: 1076
Summarized using GPT-3.5-turbo
Append: [Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models](https://arxiv.org/abs/2505.23848)
Token length: 1960
Summarized using GPT-3.5-turbo
Append: [ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark](https://arxiv.org/abs/2505.23851)
Token length: 1764
Summarized using GPT-3.5-turbo
Append: [Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease](https://arxiv.org/abs/2505.23852)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [Revisiting Uncertainty Estimation and Calibration of Large Language Models](https://arxiv.org/abs/2505.23854)
Token length: 1311
Summarized using GPT-3.5-turbo
Append: [OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities](https://arxiv.org/abs/2505.23856)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation](https://arxiv.org/abs/2505.23867)
Token length: 803
Summarized using GPT-3.5-turbo
Append: [One Task Vector is not Enough: A Large-Scale Study for In-Context Learning](https://arxiv.org/abs/2505.23911)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation](https://arxiv.org/abs/2505.23912)
Append: [Probing Association Biases in LLM Moderation Over-Sensitivity](https://arxiv.org/abs/2505.23914)
Append: [ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents](https://arxiv.org/abs/2505.23923)
Append: [Scaling up the think-aloud method](https://arxiv.org/abs/2505.23931)
Append: [SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving](https://arxiv.org/abs/2505.23932)
Append: [Retrieval Augmented Generation based Large Language Models for Causality Mining](https://arxiv.org/abs/2505.23944)
Append: [A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models](https://arxiv.org/abs/2505.23945)
Append: [FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression](https://arxiv.org/abs/2505.23966)
Append: [Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs](https://arxiv.org/abs/2505.23996)
Append: [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
Append: [Large Language Model Meets Constraint Propagation](https://arxiv.org/abs/2505.24012)
Append: [BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2505.24016)
Append: [Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine](https://arxiv.org/abs/2505.24028)
Append: [The Surprising Soupability of Documents in State Space Models](https://arxiv.org/abs/2505.24033)
Append: [MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering](https://arxiv.org/abs/2505.24040)
Append: [TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine](https://arxiv.org/abs/2505.24063)
Append: [HardTests: Synthesizing High-Quality Test Cases for LLM Coding](https://arxiv.org/abs/2505.24098)
Append: [Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning](https://arxiv.org/abs/2505.24105)
Append: [The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It](https://arxiv.org/abs/2505.24119)
Append: [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](https://arxiv.org/abs/2505.24133)
Append: [CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer](https://arxiv.org/abs/2505.24143)
Append: [Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability](https://arxiv.org/abs/2505.24147)
Append: [LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing](https://arxiv.org/abs/2505.24163)
Append: [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2505.24164)
Append: [Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection](https://arxiv.org/abs/2505.24165)
Append: [Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation](https://arxiv.org/abs/2505.24174)
Append: [Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models](https://arxiv.org/abs/2505.24187)
Append: [CLaSp: In-Context Layer Skip for Self-Speculative Decoding](https://arxiv.org/abs/2505.24196)
Append: [Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling](https://arxiv.org/abs/2505.24199)
Append: [Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?](https://arxiv.org/abs/2505.24211)
Append: [Semi-structured LLM Reasoners Can Be Rigorously Audited](https://arxiv.org/abs/2505.24217)
Append: [ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation](https://arxiv.org/abs/2505.24219)
Append: [Automated Structured Radiology Report Generation](https://arxiv.org/abs/2505.24223)
Append: [Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization](https://arxiv.org/abs/2505.24229)
Append: [Advantageous Parameter Expansion Training Makes Better Large Language Models](https://arxiv.org/abs/2505.24241)
Append: [Mamba Knockout for Unraveling Factual Information Flow](https://arxiv.org/abs/2505.24244)
Append: [Proactive Guidance of Multi-Turn Conversation in Industrial Search](https://arxiv.org/abs/2505.24251)
Append: [Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games](https://arxiv.org/abs/2505.24255)
Append: [Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation](https://arxiv.org/abs/2505.24263)
Append: [Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations](https://arxiv.org/abs/2505.24264)
Append: [ScienceMeter: Tracking Scientific Knowledge Updates in Language Models](https://arxiv.org/abs/2505.24302)
Append: [HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification](https://arxiv.org/abs/2505.24319)
Append: [Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents](https://arxiv.org/abs/2505.24331)
Append: [Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning](https://arxiv.org/abs/2505.24332)
Append: [Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings](https://arxiv.org/abs/2505.24341)
Append: [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/abs/2505.24347)
Append: [Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research](https://arxiv.org/abs/2505.24354)
Append: [Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model](https://arxiv.org/abs/2505.24355)
Append: [Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion](https://arxiv.org/abs/2505.24362)
Append: [LLM Inference Enhanced by External Knowledge: A Survey](https://arxiv.org/abs/2505.24377)
Append: [ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24388)
Append: [LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory](https://arxiv.org/abs/2505.24409)
Append: [MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs](https://arxiv.org/abs/2505.24423)
Append: [Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts](https://arxiv.org/abs/2505.24427)
Append: [Model Unlearning via Sparse Autoencoder Subspace Guided Projections](https://arxiv.org/abs/2505.24428)
Append: [Exploring the Impact of Occupational Personas on Domain-Specific QA](https://arxiv.org/abs/2505.24448)
Append: [When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways](https://arxiv.org/abs/2505.24449)
Append: [Domain Pre-training Impact on Representations](https://arxiv.org/abs/2505.24455)
Append: [CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation](https://arxiv.org/abs/2505.24456)
Append: [VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation](https://arxiv.org/abs/2505.24472)
Append: [Towards Effective Code-Integrated Reasoning](https://arxiv.org/abs/2505.24480)
Append: [TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence](https://arxiv.org/abs/2505.24500)
Append: [Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors](https://arxiv.org/abs/2505.24523)
Append: [Limited-Resource Adapters Are Regularizers, Not Linguists](https://arxiv.org/abs/2505.24525)
Append: [DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance](https://arxiv.org/abs/2505.24532)
Append: [Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections](https://arxiv.org/abs/2505.24538)
Append: [Localizing Persona Representations in LLMs](https://arxiv.org/abs/2505.24539)
Append: [Cross-Attention Speculative Decoding](https://arxiv.org/abs/2505.24544)
Append: [A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings](https://arxiv.org/abs/2505.24550)
Append: [CREFT: Sequential Multi-Agent LLM for Character Relation Extraction](https://arxiv.org/abs/2505.24553)
Append: [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)
Append: [Improving Language and Modality Transfer in Translation by Character-level Modeling](https://arxiv.org/abs/2505.24561)
Append: [NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization](https://arxiv.org/abs/2505.24575)
Append: [GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training](https://arxiv.org/abs/2505.24581)
Append: [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)
Append: [Explainable Depression Detection using Masked Hard Instance Mining](https://arxiv.org/abs/2505.24609)
Append: [When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation](https://arxiv.org/abs/2505.24613)
Append: [Harnessing Large Language Models for Scientific Novelty Detection](https://arxiv.org/abs/2505.24615)
Append: [Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX](https://arxiv.org/abs/2505.24616)
Append: [Interpretable phenotyping of Heart Failure patients with Dutch discharge letters](https://arxiv.org/abs/2505.24619)
Append: [Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization](https://arxiv.org/abs/2505.24621)
Append: [The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2505.24630)
Append: [Disentangling Language and Culture for Evaluating Multilingual Large Language Models](https://arxiv.org/abs/2505.24635)
Append: [Efficient Text Encoders for Labor Market Analysis](https://arxiv.org/abs/2505.24640)
Append: [Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching](https://arxiv.org/abs/2505.24643)
Append: [PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder](https://arxiv.org/abs/2505.24646)
Append: [MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR](https://arxiv.org/abs/2505.24656)
Append: [Multiple LLM Agents Debate for Equitable Cultural Alignment](https://arxiv.org/abs/2505.24671)
Append: [TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis](https://arxiv.org/abs/2505.24672)
Append: [A Simple Linear Patch Revives Layer-Pruned Large Language Models](https://arxiv.org/abs/2505.24680)
Append: [Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation](https://arxiv.org/abs/2505.24683)
Append: [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)
Append: [BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization](https://arxiv.org/abs/2505.24689)
Append: [Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios](https://arxiv.org/abs/2505.24691)
Append: [Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison](https://arxiv.org/abs/2505.24701)
Append: [HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America](https://arxiv.org/abs/2505.24712)
Append: [Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification](https://arxiv.org/abs/2505.24713)
Append: [FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation](https://arxiv.org/abs/2505.24714)
Append: [Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.24726)
Append: [Circuit Stability Characterizes Language Model Generalization](https://arxiv.org/abs/2505.24731)
Append: [Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation](https://arxiv.org/abs/2505.24754)
Append: [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/abs/2505.24757)
Append: [From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning](https://arxiv.org/abs/2505.24768)
Append: [Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?](https://arxiv.org/abs/2505.24778)
Append: [Drop Dropout on Single-Epoch Language Model Pretraining](https://arxiv.org/abs/2505.24788)
Append: [Guiding Generative Storytelling with Knowledge Graphs](https://arxiv.org/abs/2505.24803)
Append: [LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://arxiv.org/abs/2505.24826)
Append: [Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.24830)
Append: [How much do language models memorize?](https://arxiv.org/abs/2505.24832)
Append: [Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks](https://arxiv.org/abs/2505.24834)
Append: [MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs](https://arxiv.org/abs/2505.24858)
Append: [AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time](https://arxiv.org/abs/2505.24863)
Append: [ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models](https://arxiv.org/abs/2505.24864)
Append: [Boosting In-Context Learning in LLMs Through the Lens of Classical Supervised Learning](https://arxiv.org/abs/2505.23783)
Append: [SkewRoute: Training-Free LLM Routing for Knowledge Graph Retrieval-Augmented Generation via Score Skewness of Retrieved Context](https://arxiv.org/abs/2505.23841)
Append: [Using Reasoning Models to Generate Search Heuristics that Solve Open Instances of Combinatorial Design Problems](https://arxiv.org/abs/2505.23881)
Append: [BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning](https://arxiv.org/abs/2505.23883)
Append: [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
Append: [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
Append: [ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding](https://arxiv.org/abs/2505.23922)
Append: [Information Structure in Mappings: An Approach to Learning, Representation, and Generalisation](https://arxiv.org/abs/2505.23960)
Append: [Large Language Models for Controllable Multi-property Multi-objective Molecule Optimization](https://arxiv.org/abs/2505.23987)
Append: [Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins](https://arxiv.org/abs/2505.24004)
Append: [Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code Workflows](https://arxiv.org/abs/2505.24189)
Append: [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)
Append: [Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC](https://arxiv.org/abs/2505.24200)
Append: [Reasoning Can Hurt the Inductive Abilities of Large Language Models](https://arxiv.org/abs/2505.24225)
Append: [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232)
Append: [An Adversary-Resistant Multi-Agent LLM System via Credibility Scoring](https://arxiv.org/abs/2505.24239)
Append: [Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules](https://arxiv.org/abs/2505.24292)
Append: [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
Append: [SwiftEval: Developing a Language-Specific Benchmark for LLM-generated Code Evaluation](https://arxiv.org/abs/2505.24324)
Append: [GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models](https://arxiv.org/abs/2505.24340)
Append: [KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval](https://arxiv.org/abs/2505.24342)
Append: [Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models](https://arxiv.org/abs/2505.24379)
Append: [Optimizing the Interface Between Knowledge Graphs and LLMs for Complex Reasoning](https://arxiv.org/abs/2505.24478)
Append: [Leveraging Knowledge Graphs and LLMs for Structured Generation of Misinformation](https://arxiv.org/abs/2505.24479)
Append: [AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders](https://arxiv.org/abs/2505.24519)
Append: [Beyond Linear Steering: Unified Multi-Attribute Control for Language Models](https://arxiv.org/abs/2505.24535)
Append: [Identifying Primary Stress Across Related Languages and Dialects with Transformer-based Speech Encoder Models](https://arxiv.org/abs/2505.24571)
Append: [Causal-aware Large Language Models: Enhancing Decision-Making Through Learning, Adapting and Acting](https://arxiv.org/abs/2505.24710)
Append: [CoRet: Improved Retriever for Code Editing](https://arxiv.org/abs/2505.24715)
Append: ["Dyadosyncrasy", Idiosyncrasy and Demographic Factors in Turn-Taking](https://arxiv.org/abs/2505.24736)
Append: [SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training](https://arxiv.org/abs/2505.24749)
Append: [REASONING GYM: Reasoning Environments for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.24760)
Append: [Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation](https://arxiv.org/abs/2505.24787)
Append: [PhySense: Principle-Based Physics Reasoning Benchmarking for Large Language Models](https://arxiv.org/abs/2505.24823)
Append: [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/abs/2505.24840)
Append: [Chameleon: A Flexible Data-mixing Framework for Language Model Pretraining and Finetuning](https://arxiv.org/abs/2505.24844)
Append: [MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning](https://arxiv.org/abs/2505.24846)
Append: [Harnessing Negative Signals: Reinforcement Distillation from Teacher Data for LLM Reasoning](https://arxiv.org/abs/2505.24850)
Append: [Beyond Multiple Choice: Evaluating Steering Vectors for Adaptive Free-Form Summarization](https://arxiv.org/abs/2505.24859)
Append: [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
Append: [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/abs/2505.24872)
Append: [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/abs/2505.24875)
Append: [Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks](https://arxiv.org/abs/2505.24876)
Append: [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
Append: [Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models](https://arxiv.org/abs/2401.08491)
Append: [An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4](https://arxiv.org/abs/2403.02839)
Append: [Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization](https://arxiv.org/abs/2405.14189)
Append: [Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning](https://arxiv.org/abs/2406.10099)
Append: [Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration](https://arxiv.org/abs/2406.16469)
Append: [NativQA: Multilingual Culturally-Aligned Natural Query for LLMs](https://arxiv.org/abs/2407.09823)
Append: [Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment](https://arxiv.org/abs/2407.14878)
Append: [MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU](https://arxiv.org/abs/2408.08144)
Append: [EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts](https://arxiv.org/abs/2408.12226)
Append: [BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla](https://arxiv.org/abs/2409.11638)
Append: [DemoShapley: Valuation of Demonstrations for In-Context Learning](https://arxiv.org/abs/2410.07523)
Append: [ChuLo: Chunk-Level Key Information Representation for Long Document Processing](https://arxiv.org/abs/2410.11119)
Append: [From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence](https://arxiv.org/abs/2410.13460)
Append: [Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models](https://arxiv.org/abs/2410.14248)
Append: [Let's Be Self-generated via Step by Step: A Curriculum Learning Approach to Automated Reasoning with Large Language Models](https://arxiv.org/abs/2410.21728)
Append: [Dialectal Coverage And Generalization in Arabic Speech Recognition](https://arxiv.org/abs/2411.05872)
Append: [Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework](https://arxiv.org/abs/2411.06160)
Append: [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116)
Append: [DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling](https://arxiv.org/abs/2412.04905)
Append: [Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation](https://arxiv.org/abs/2412.08473)
Append: [RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios](https://arxiv.org/abs/2412.08972)
Append: [A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI](https://arxiv.org/abs/2412.13942)
Append: [Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph](https://arxiv.org/abs/2412.15268)
Append: [Contrastive Learning for Task-Independent SpeechLLM-Pretraining](https://arxiv.org/abs/2412.15712)
Append: [CLIX: Cross-Lingual Explanations of Idiomatic Expressions](https://arxiv.org/abs/2501.03191)
Append: [AlphaPO: Reward Shape Matters for LLM Alignment](https://arxiv.org/abs/2501.03884)
Append: [Autonomy-of-Experts Models](https://arxiv.org/abs/2501.13074)
Append: [M+: Extending MemoryLLM with Scalable Long-Term Memory](https://arxiv.org/abs/2502.00592)
Append: [Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations](https://arxiv.org/abs/2502.01349)
Append: [Boosting Multimodal Reasoning with Automated Structured Thinking](https://arxiv.org/abs/2502.02339)
Append: [A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)](https://arxiv.org/abs/2502.02659)
Append: [Exploring Imbalanced Annotations for Effective In-Context Learning](https://arxiv.org/abs/2502.04037)
Append: [SparQLe: Speech Queries to Text Translation Through LLMs](https://arxiv.org/abs/2502.09284)
Append: [Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction](https://arxiv.org/abs/2502.11084)
Append: [VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment](https://arxiv.org/abs/2502.11361)
Append: [ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models](https://arxiv.org/abs/2502.11404)
Append: [GLTW: Joint Improved Graph Transformer and LLM via Three-Word Language for Knowledge Graph Completion](https://arxiv.org/abs/2502.11471)
Append: [MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training](https://arxiv.org/abs/2502.11541)
Append: [LLM Agents Making Agent Tools](https://arxiv.org/abs/2502.11705)
Append: ["See the World, Discover Knowledge": A Chinese Factuality Evaluation for Large Vision Language Models](https://arxiv.org/abs/2502.11718)
Append: [CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual LLMs](https://arxiv.org/abs/2502.12476)
Append: [StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following](https://arxiv.org/abs/2502.14494)
Append: [Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs](https://arxiv.org/abs/2502.14561)
Append: [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)
Append: [Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs](https://arxiv.org/abs/2502.14830)
Append: [TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding](https://arxiv.org/abs/2502.15197)
Append: [Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning](https://arxiv.org/abs/2502.15401)
Append: [Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation](https://arxiv.org/abs/2502.15434)
Append: [All That Glitters is Not Novel: Plagiarism in AI Generated Research](https://arxiv.org/abs/2502.16487)
Append: [All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark](https://arxiv.org/abs/2502.16989)
Append: [Do Language Models Understand Honorific Systems in Javanese?](https://arxiv.org/abs/2502.20864)
Append: [SwiLTra-Bench: The Swiss Legal Translation Benchmark](https://arxiv.org/abs/2503.01372)
Append: [Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering](https://arxiv.org/abs/2503.01606)
Append: [Vision-Language Models Struggle to Align Entities across Modalities](https://arxiv.org/abs/2503.03854)
Append: [HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks](https://arxiv.org/abs/2503.04378)
Append: [ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs](https://arxiv.org/abs/2503.05268)
Append: [DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs](https://arxiv.org/abs/2503.07067)
Append: [TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification](https://arxiv.org/abs/2503.15289)
Append: [MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering](https://arxiv.org/abs/2503.18491)
Append: [Inverse Reinforcement Learning with Dynamic Reward Scaling for LLM Alignment](https://arxiv.org/abs/2503.18991)
Append: [An Explicit Syllogistic Legal Reasoning Framework for Large Language Models](https://arxiv.org/abs/2504.04042)
Append: [RAISE: Reinforced Adaptive Instruction Selection For Large Language Models](https://arxiv.org/abs/2504.07282)
Append: [GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction](https://arxiv.org/abs/2504.10792)
Append: [From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs](https://arxiv.org/abs/2504.11277)
Append: [What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns](https://arxiv.org/abs/2504.15815)
Append: [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)
Append: [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)
Append: [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
Append: [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
Append: [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)
Append: [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
Append: [Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning](https://arxiv.org/abs/2303.14537)
Append: [Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models](https://arxiv.org/abs/2312.02219)
Append: [StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis](https://arxiv.org/abs/2312.10741)
Append: [Model Extrapolation Expedites Alignment](https://arxiv.org/abs/2404.16792)
Append: [Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis](https://arxiv.org/abs/2405.21075)
Append: [GUICourse: From General Vision Language Models to Versatile GUI Agents](https://arxiv.org/abs/2406.11317)
Append: [Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead](https://arxiv.org/abs/2407.00066)
Append: [VITA: Towards Open-Source Interactive Omni Multimodal LLM](https://arxiv.org/abs/2408.05211)
Append: [Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models](https://arxiv.org/abs/2408.09429)
Append: [On the Vulnerability of Applying Retrieval-Augmented Generation within Knowledge-Intensive Application Domains](https://arxiv.org/abs/2409.17275)
Append: [SVIP: Towards Verifiable Inference of Open-source Large Language Models](https://arxiv.org/abs/2410.22307)
Append: [ChinaTravel: An Open-Ended Benchmark for Language Agents in Chinese Travel Planning](https://arxiv.org/abs/2412.13682)
Append: [GeAR: Generation Augmented Retrieval](https://arxiv.org/abs/2501.02772)
Append: [Latent-space adversarial training with post-aware calibration for defending large language models against jailbreak attacks](https://arxiv.org/abs/2501.10639)
Append: [Safety Reasoning with Guidelines](https://arxiv.org/abs/2502.04040)
Append: [Scalable Oversight for Superhuman AI via Recursive Self-Critiquing](https://arxiv.org/abs/2502.04675)
Append: [Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options](https://arxiv.org/abs/2502.12929)
Append: [You need to MIMIC to get FAME: Solving Meeting Transcript Scarcity with a Multi-Agent Conversations](https://arxiv.org/abs/2502.13001)
Append: [Repo2Run: Automated Building Executable Environment for Code Repository at Scale](https://arxiv.org/abs/2502.13681)
Append: [PairBench: Are Vision-Language Models Reliable at Comparing What They See?](https://arxiv.org/abs/2502.15210)
Append: [Recurrent Knowledge Identification and Fusion for Language Model Continual Learning](https://arxiv.org/abs/2502.17510)
Append: [SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning](https://arxiv.org/abs/2502.19668)
Append: [Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives](https://arxiv.org/abs/2503.14604)
Append: [FactSelfCheck: Fact-Level Black-Box Hallucination Detection for LLMs](https://arxiv.org/abs/2503.17229)
Append: [Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models](https://arxiv.org/abs/2503.18034)
Append: [Efficient Adaptation For Remote Sensing Visual Grounding](https://arxiv.org/abs/2503.23083)
Append: [AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2504.00587)
Append: [Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning](https://arxiv.org/abs/2504.02922)
Append: [Learning to Reason Over Time: Timeline Self-Reflection for Improved Temporal Reasoning in Language Models](https://arxiv.org/abs/2504.05258)
Append: [Versatile Framework for Song Generation with Prompt-based Control](https://arxiv.org/abs/2504.19062)
Append: [Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks](https://arxiv.org/abs/2504.21578)
Append: [Using Knowledge Graphs to harvest datasets for efficient CLIP model training](https://arxiv.org/abs/2505.02746)
Append: [X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP](https://arxiv.org/abs/2505.05528)
Append: [NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition](https://arxiv.org/abs/2505.08052)
Append: [Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models](https://arxiv.org/abs/2505.10844)
Append: [Mitigating Subgroup Disparities in Multi-Label Speech Emotion Recognition: A Pseudo-Labeling and Unsupervised Learning Approach](https://arxiv.org/abs/2505.14449)
Append: [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
Append: [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
Append: [Safety Alignment Can Be Not Superficial With Explicit Safety Signals](https://arxiv.org/abs/2505.17072)
Append: [VideoGameBench: Can Vision-Language Models complete popular video games?](https://arxiv.org/abs/2505.18134)
append_entries: 319
Finish: 2025-06-02 04:35:15.492298
------------------------------------------------------
Started: 2025-06-02 06:26:32.396058
Existing_entries: 1319
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1153
Summarized using GPT-3.5-turbo
Append: [KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search](https://arxiv.org/abs/2501.18922)
Token length: 1012
Summarized using GPT-3.5-turbo
Append: [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
Token length: 1705
Summarized using GPT-3.5-turbo
Append: [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)
Token length: 1364
Summarized using GPT-3.5-turbo
Append: [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
append_entries: 7
Finish: 2025-06-02 06:26:50.136532
------------------------------------------------------
Started: 2025-06-02 08:23:47.813908
Existing_entries: 1007
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 08:23:48.462235
------------------------------------------------------
Started: 2025-06-02 10:19:09.196669
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 10:19:09.945006
------------------------------------------------------
Started: 2025-06-02 12:35:06.710087
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 12:35:07.357906
------------------------------------------------------
Started: 2025-06-02 14:17:38.361956
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 14:17:39.110603
------------------------------------------------------
Started: 2025-06-02 16:21:45.299168
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 16:21:45.997137
------------------------------------------------------
Started: 2025-06-02 18:23:42.330764
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 18:23:43.010052
------------------------------------------------------
Started: 2025-06-02 20:18:53.728029
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 20:18:54.384695
------------------------------------------------------
Started: 2025-06-02 22:16:29.051270
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-02 22:16:29.742481
------------------------------------------------------
Started: 2025-06-03 01:20:58.432863
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 01:20:59.184316
------------------------------------------------------
Started: 2025-06-03 03:13:55.835708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 03:13:56.539851
------------------------------------------------------
Started: 2025-06-03 04:27:34.349244
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 687
Summarized using GPT-3.5-turbo
Append: [Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese](https://arxiv.org/abs/2506.00019)
Token length: 1665
Summarized using GPT-3.5-turbo
Append: [Scaling Physical Reasoning with the PHYSICS Dataset](https://arxiv.org/abs/2506.00022)
Token length: 1527
Summarized using GPT-3.5-turbo
Append: [From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling](https://arxiv.org/abs/2506.00027)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists](https://arxiv.org/abs/2506.00042)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs](https://arxiv.org/abs/2506.00061)
Token length: 973
Summarized using GPT-3.5-turbo
Append: [Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling](https://arxiv.org/abs/2506.00064)
Token length: 1146
Summarized using GPT-3.5-turbo
Append: [You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models](https://arxiv.org/abs/2506.00065)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages](https://arxiv.org/abs/2506.00068)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [Evaluating the Sensitivity of LLMs to Prior Context](https://arxiv.org/abs/2506.00069)
Token length: 1087
Summarized using GPT-3.5-turbo
Append: [Gaussian mixture models as a proxy for interacting language models](https://arxiv.org/abs/2506.00077)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [COSMIC: Generalized Refusal Direction Identification in LLM Activations](https://arxiv.org/abs/2506.00085)
Token length: 1833
Summarized using GPT-3.5-turbo
Append: [SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset](https://arxiv.org/abs/2506.00087)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.00088)
Token length: 1877
Summarized using GPT-3.5-turbo
Append: [Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards](https://arxiv.org/abs/2506.00103)
Token length: 837
Summarized using GPT-3.5-turbo
Append: [Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models](https://arxiv.org/abs/2506.00134)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [LaMP-QA: A Benchmark for Personalized Long-form Question Answering](https://arxiv.org/abs/2506.00137)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry](https://arxiv.org/abs/2506.00145)
Token length: 941
Summarized using GPT-3.5-turbo
Append: [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences](https://arxiv.org/abs/2506.00195)
Token length: 1557
Summarized using GPT-3.5-turbo
Append: [Structuring Radiology Reports: Challenging LLMs with Lightweight Models](https://arxiv.org/abs/2506.00200)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Structure-Aware Fill-in-the-Middle Pretraining for Code](https://arxiv.org/abs/2506.00204)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [REIC: RAG-Enhanced Intent Classification at Scale](https://arxiv.org/abs/2506.00210)
Token length: 1802
Summarized using GPT-3.5-turbo
Append: [ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering](https://arxiv.org/abs/2506.00232)
Token length: 1965
Summarized using GPT-3.5-turbo
Append: [MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility](https://arxiv.org/abs/2506.00235)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain](https://arxiv.org/abs/2506.00250)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
Token length: 1269
Summarized using GPT-3.5-turbo
Append: [The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection](https://arxiv.org/abs/2506.00256)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [MultiHoax: A Dataset of Multi-hop False-Premise Questions](https://arxiv.org/abs/2506.00264)
Token length: 940
Summarized using GPT-3.5-turbo
Append: [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/abs/2506.00267)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings](https://arxiv.org/abs/2506.00277)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation](https://arxiv.org/abs/2506.00288)
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [DLM-One: Diffusion Language Models for One-Step Sequence Generation](https://arxiv.org/abs/2506.00290)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs](https://arxiv.org/abs/2506.00304)
Token length: 1211
Summarized using GPT-3.5-turbo
Append: [Lossless Token Sequence Compression via Meta-Tokens](https://arxiv.org/abs/2506.00307)
Token length: 1587
Summarized using GPT-3.5-turbo
Append: [An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3](https://arxiv.org/abs/2506.00312)
Token length: 1021
Summarized using GPT-3.5-turbo
Append: [SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation](https://arxiv.org/abs/2506.00319)
Token length: 1420
Summarized using GPT-3.5-turbo
Append: [TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering](https://arxiv.org/abs/2506.00331)
Token length: 1137
Summarized using GPT-3.5-turbo
Append: [Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus](https://arxiv.org/abs/2506.00332)
Token length: 1117
Summarized using GPT-3.5-turbo
Append: [Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models](https://arxiv.org/abs/2506.00334)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning](https://arxiv.org/abs/2506.00338)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs](https://arxiv.org/abs/2506.00344)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG](https://arxiv.org/abs/2506.00381)
Token length: 1437
Summarized using GPT-3.5-turbo
Append: [Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training](https://arxiv.org/abs/2506.00386)
Token length: 1296
Summarized using GPT-3.5-turbo
Append: [SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL](https://arxiv.org/abs/2506.00391)
Token length: 1237
Summarized using GPT-3.5-turbo
Append: [Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively](https://arxiv.org/abs/2506.00396)
Token length: 1268
Summarized using GPT-3.5-turbo
Append: [Scaling Textual Gradients via Sampling-Based Momentum](https://arxiv.org/abs/2506.00400)
Token length: 1048
Summarized using GPT-3.5-turbo
Append: [Causal Structure Discovery for Error Diagnostics of Children's ASR](https://arxiv.org/abs/2506.00402)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Accelerating Diffusion LLMs via Adaptive Parallel Decoding](https://arxiv.org/abs/2506.00413)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Dual Debiasing for Noisy In-Context Learning for Text Generation](https://arxiv.org/abs/2506.00418)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions](https://arxiv.org/abs/2506.00421)
Append: [DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition](https://arxiv.org/abs/2506.00422)
Append: [Inter-Passage Verification for Multi-evidence Multi-answer QA](https://arxiv.org/abs/2506.00425)
Append: [G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models](https://arxiv.org/abs/2506.00445)
Append: [Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization](https://arxiv.org/abs/2506.00448)
Append: [Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data](https://arxiv.org/abs/2506.00469)
Append: [EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models](https://arxiv.org/abs/2506.00479)
Append: [PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings](https://arxiv.org/abs/2506.00481)
Append: [Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models](https://arxiv.org/abs/2506.00483)
Append: [Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection](https://arxiv.org/abs/2506.00488)
Append: [Exploring In-context Example Generation for Machine Translation](https://arxiv.org/abs/2506.00507)
Append: [Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems](https://arxiv.org/abs/2506.00509)
Append: [Evaluating the Evaluation of Diversity in Commonsense Generation](https://arxiv.org/abs/2506.00514)
Append: [CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention](https://arxiv.org/abs/2506.00519)
Append: [Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning](https://arxiv.org/abs/2506.00527)
Append: [Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing](https://arxiv.org/abs/2506.00536)
Append: [ARIA: Training Language Agents with Intention-Driven Reward Aggregation](https://arxiv.org/abs/2506.00539)
Append: [Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages](https://arxiv.org/abs/2506.00549)
Append: [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
Append: [The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation](https://arxiv.org/abs/2506.00583)
Append: [Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems](https://arxiv.org/abs/2506.00585)
Append: [PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements](https://arxiv.org/abs/2506.00608)
Append: [Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation](https://arxiv.org/abs/2506.00612)
Append: [Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples](https://arxiv.org/abs/2506.00622)
Append: [LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech](https://arxiv.org/abs/2506.00628)
Append: [Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings](https://arxiv.org/abs/2506.00634)
Append: [ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances](https://arxiv.org/abs/2506.00636)
Append: [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
Append: [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
Append: [Clinical Annotations for Automatic Stuttering Severity Assessment](https://arxiv.org/abs/2506.00644)
Append: [GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction](https://arxiv.org/abs/2506.00649)
Append: [Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques](https://arxiv.org/abs/2506.00658)
Append: [SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues](https://arxiv.org/abs/2506.00668)
Append: [DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for Biomedical Multi-Hop QA](https://arxiv.org/abs/2506.00671)
Append: [Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments](https://arxiv.org/abs/2506.00694)
Append: [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
Append: [Chain-of-Thought Training for Open E2E Spoken Dialogue Systems](https://arxiv.org/abs/2506.00722)
Append: [Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models](https://arxiv.org/abs/2506.00726)
Append: [Narrative Media Framing in Political Discourse](https://arxiv.org/abs/2506.00737)
Append: [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/abs/2506.00739)
Append: [Length Aware Speech Translation for Video Dubbing](https://arxiv.org/abs/2506.00740)
Append: [Data Swarms: Optimizable Generation of Synthetic Evaluation Data](https://arxiv.org/abs/2506.00741)
Append: [Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection](https://arxiv.org/abs/2506.00743)
Append: [Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations](https://arxiv.org/abs/2506.00748)
Append: [Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons](https://arxiv.org/abs/2506.00759)
Append: [Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models](https://arxiv.org/abs/2506.00773)
Append: [Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge](https://arxiv.org/abs/2506.00777)
Append: [KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision](https://arxiv.org/abs/2506.00783)
Append: [Research Borderlands: Analysing Writing Across Research Cultures](https://arxiv.org/abs/2506.00784)
Append: [RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.00789)
Append: [Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering](https://arxiv.org/abs/2506.00806)
Append: [GuessBench: Sensemaking Multimodal Creativity in the Wild](https://arxiv.org/abs/2506.00814)
Append: [From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses](https://arxiv.org/abs/2506.00815)
Append: [One for All: Update Parameterized Knowledge Across Multiple Models](https://arxiv.org/abs/2506.00817)
Append: [Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks](https://arxiv.org/abs/2506.00823)
Append: [HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs](https://arxiv.org/abs/2506.00826)
Append: [COMPKE: Complex Question Answering under Knowledge Editing](https://arxiv.org/abs/2506.00829)
Append: [Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](https://arxiv.org/abs/2506.00842)
Append: [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
Append: [How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation](https://arxiv.org/abs/2506.00859)
Append: [L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models](https://arxiv.org/abs/2506.00863)
Append: [What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning](https://arxiv.org/abs/2506.00869)
Append: [CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning](https://arxiv.org/abs/2506.00875)
Append: [Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning](https://arxiv.org/abs/2506.00876)
Append: [Improve MLLM Benchmark Efficiency through Interview](https://arxiv.org/abs/2506.00883)
Append: [Affordance Benchmark for MLLMs](https://arxiv.org/abs/2506.00893)
Append: [SocialEval: Evaluating Social Intelligence of Large Language Models](https://arxiv.org/abs/2506.00900)
Append: [Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages](https://arxiv.org/abs/2506.00912)
Append: [How do Transformer Embeddings Represent Compositions? A Functional Analysis](https://arxiv.org/abs/2506.00914)
Append: [anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding](https://arxiv.org/abs/2506.00942)
Append: [Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection](https://arxiv.org/abs/2506.00955)
Append: [From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation](https://arxiv.org/abs/2506.00963)
Append: [ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness](https://arxiv.org/abs/2506.00964)
Append: [XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content](https://arxiv.org/abs/2506.00973)
Append: [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
Append: [LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World](https://arxiv.org/abs/2506.00980)
Append: [What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training](https://arxiv.org/abs/2506.00981)
Append: [Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering](https://arxiv.org/abs/2506.00985)
Append: [Talking to Data: Designing Smart Assistants for Humanities Databases](https://arxiv.org/abs/2506.00986)
Append: [Less is More: Local Intrinsic Dimensions of Contextual Language Models](https://arxiv.org/abs/2506.01034)
Append: [Probing Neural Topology of Large Language Models](https://arxiv.org/abs/2506.01042)
Append: [CHEER-Ekman: Fine-grained Embodied Emotion Classification](https://arxiv.org/abs/2506.01047)
Append: [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
Append: [How Programming Concepts and Neurons Are Shared in Code Language Models](https://arxiv.org/abs/2506.01074)
Append: [zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression](https://arxiv.org/abs/2506.01084)
Append: [Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements](https://arxiv.org/abs/2506.01089)
Append: [Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection](https://arxiv.org/abs/2506.01104)
Append: [From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models](https://arxiv.org/abs/2506.01133)
Append: [A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition](https://arxiv.org/abs/2506.01147)
Append: [Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish](https://arxiv.org/abs/2506.01156)
Append: [The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage](https://arxiv.org/abs/2506.01172)
Append: [LAQuer: Localized Attribution Queries in Content-grounded Generation](https://arxiv.org/abs/2506.01187)
Append: [Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages](https://arxiv.org/abs/2506.01190)
Append: [CoBRA: Quantifying Strategic Language Use and LLM Pragmatics](https://arxiv.org/abs/2506.01195)
Append: [Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures](https://arxiv.org/abs/2506.01197)
Append: [Trick or Neat: Adversarial Ambiguity and Language Model Evaluation](https://arxiv.org/abs/2506.01205)
Append: [Mamba Drafters for Speculative Decoding](https://arxiv.org/abs/2506.01206)
Append: [Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers](https://arxiv.org/abs/2506.01215)
Append: [Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean](https://arxiv.org/abs/2506.01237)
Append: [ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists](https://arxiv.org/abs/2506.01241)
Append: [MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine](https://arxiv.org/abs/2506.01252)
Append: [CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events](https://arxiv.org/abs/2506.01253)
Append: [Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management](https://arxiv.org/abs/2506.01254)
Append: [DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models](https://arxiv.org/abs/2506.01257)
Append: [Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis](https://arxiv.org/abs/2506.01262)
Append: [WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing](https://arxiv.org/abs/2506.01263)
Append: [Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines](https://arxiv.org/abs/2506.01265)
Append: [Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model](https://arxiv.org/abs/2506.01266)
Append: [Schema as Parameterized Tools for Universal Information Extraction](https://arxiv.org/abs/2506.01276)
Append: [VM14K: First Vietnamese Medical Benchmark](https://arxiv.org/abs/2506.01305)
Append: [A Platform for Investigating Public Health Content with Efficient Concern Classification](https://arxiv.org/abs/2506.01308)
Append: [Growing Through Experience: Scaling Episodic Grounding in Language Models](https://arxiv.org/abs/2506.01312)
Append: [Zero-Shot Text-to-Speech for Vietnamese](https://arxiv.org/abs/2506.01322)
Append: [Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines](https://arxiv.org/abs/2506.01329)
Append: [Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models](https://arxiv.org/abs/2506.01334)
Append: [The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology](https://arxiv.org/abs/2506.01340)
Append: [TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models](https://arxiv.org/abs/2506.01341)
Append: [Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents](https://arxiv.org/abs/2506.01344)
Append: [The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning](https://arxiv.org/abs/2506.01347)
Append: [KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors](https://arxiv.org/abs/2506.01357)
Append: [MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations](https://arxiv.org/abs/2506.01367)
Append: [AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation](https://arxiv.org/abs/2506.01381)
Append: [Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages](https://arxiv.org/abs/2506.01406)
Append: [Comparing LLM-generated and human-authored news text using formal syntactic theory](https://arxiv.org/abs/2506.01407)
Append: [UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment](https://arxiv.org/abs/2506.01419)
Append: [Self-Refining Language Model Anonymizers via Adversarial Distillation](https://arxiv.org/abs/2506.01420)
Append: [Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings](https://arxiv.org/abs/2506.01435)
Append: [Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data](https://arxiv.org/abs/2506.01439)
Append: [Building Entity Association Mining Framework for Knowledge Discovery](https://arxiv.org/abs/2506.01451)
Append: [TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge](https://arxiv.org/abs/2506.01458)
Append: [Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering](https://arxiv.org/abs/2506.01474)
Append: [LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification](https://arxiv.org/abs/2506.01484)
Append: [Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution](https://arxiv.org/abs/2506.01488)
Append: [Multilingual Definition Modeling](https://arxiv.org/abs/2506.01489)
Append: [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
Append: [Continual Speech Learning with Fused Speech Features](https://arxiv.org/abs/2506.01496)
Append: [Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes](https://arxiv.org/abs/2506.01512)
Append: [FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents](https://arxiv.org/abs/2506.01520)
Append: [V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat](https://arxiv.org/abs/2506.01524)
Append: [STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework](https://arxiv.org/abs/2506.01531)
Append: [Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries](https://arxiv.org/abs/2506.01535)
Append: [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
Append: [Prompt Engineering Large Language Models' Forecasting Capabilities](https://arxiv.org/abs/2506.01578)
Append: [Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings](https://arxiv.org/abs/2506.01587)
Append: [Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models](https://arxiv.org/abs/2506.01592)
Append: [MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
Append: [IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems](https://arxiv.org/abs/2506.01615)
Append: [Domain Lexical Knowledge-based Word Embedding Learning for Text Classification under Small Data](https://arxiv.org/abs/2506.01621)
Append: [MVAN: Multi-View Attention Networks for Fake News Detection on Social Media](https://arxiv.org/abs/2506.01627)
Append: [Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons](https://arxiv.org/abs/2506.01629)
Append: [ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge](https://arxiv.org/abs/2506.01646)
Append: [Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon](https://arxiv.org/abs/2506.01675)
Append: [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/abs/2506.01687)
Append: [When LLMs Team Up: The Emergence of Collaborative Affective Computing](https://arxiv.org/abs/2506.01698)
Append: [mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection](https://arxiv.org/abs/2506.01702)
Append: [Fairness Dynamics During Training](https://arxiv.org/abs/2506.01709)
Append: [Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning](https://arxiv.org/abs/2506.01710)
Append: [SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning](https://arxiv.org/abs/2506.01713)
Append: [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
Append: [Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training](https://arxiv.org/abs/2506.01732)
Append: [Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs](https://arxiv.org/abs/2506.01734)
Append: [Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning](https://arxiv.org/abs/2506.01748)
Append: [Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts](https://arxiv.org/abs/2506.01775)
Append: [MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation](https://arxiv.org/abs/2506.01776)
Append: [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
Append: [Human-Centric Evaluation for Foundation Models](https://arxiv.org/abs/2506.01793)
Append: [Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books](https://arxiv.org/abs/2506.01796)
Append: [Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives](https://arxiv.org/abs/2506.01807)
Append: [NAVER LABS Europe Submission to the Instruction-following Track](https://arxiv.org/abs/2506.01808)
Append: [Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high](https://arxiv.org/abs/2506.01814)
Append: [BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses](https://arxiv.org/abs/2506.01817)
Append: [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/abs/2506.01819)
Append: [CiteEval: Principle-Driven Citation Evaluation for Source Attribution](https://arxiv.org/abs/2506.01829)
Append: [Minimal Pair-Based Evaluation of Code-Switching](https://arxiv.org/abs/2506.01840)
Append: [Code-Switching and Syntax: A Large-Scale Experiment](https://arxiv.org/abs/2506.01846)
Append: [CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions](https://arxiv.org/abs/2506.01859)
Append: [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872)
Append: [Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis](https://arxiv.org/abs/2506.01918)
Append: [From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation](https://arxiv.org/abs/2506.01920)
Append: [Esoteric Language Models](https://arxiv.org/abs/2506.01928)
Append: [RewardBench 2: Advancing Reward Model Evaluation](https://arxiv.org/abs/2506.01937)
Append: [Novel Benchmark for NER in the Wastewater and Stormwater Domain](https://arxiv.org/abs/2506.01938)
Append: [Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.01939)
Append: [Self-ensemble: Mitigating Confidence Distortion for Large Language Models](https://arxiv.org/abs/2506.01951)
Append: [WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks](https://arxiv.org/abs/2506.01952)
Append: [DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation](https://arxiv.org/abs/2506.01954)
Append: [GLEN: Generative Retrieval via Lexical Index Learning](https://arxiv.org/abs/2311.03057)
Append: [Enhancing Finite State Machine Design Automation with Large Language Models and Prompt Engineering Techniques](https://arxiv.org/abs/2506.00001)
Append: [Probing Audio-Generation Capabilities of Text-Based Language Models](https://arxiv.org/abs/2506.00003)
Append: [Retrieval-Augmented Generation: A Comprehensive Survey of Architectures, Enhancements, and Robustness Frontiers](https://arxiv.org/abs/2506.00054)
Append: [Comparative analysis of privacy-preserving open-source LLMs regarding extraction of diagnostic information from clinical CMR imaging reports](https://arxiv.org/abs/2506.00060)
Append: [SafeCOMM: What about Safety Alignment in Fine-Tuned Telecom Large Language Models?](https://arxiv.org/abs/2506.00062)
Append: [Evaluating Prompt Engineering Techniques for Accuracy and Confidence Elicitation in Medical LLMs](https://arxiv.org/abs/2506.00072)
Append: [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
Append: [Optimizing Storytelling, Improving Audience Retention, and Reducing Waste in the Entertainment Industry](https://arxiv.org/abs/2506.00076)
Append: [Bottom-Up Perspectives on AI Governance: Insights from User Reviews of AI Products](https://arxiv.org/abs/2506.00080)
Append: [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095)
Append: [Children's Voice Privacy: First Steps And Emerging Challenges](https://arxiv.org/abs/2506.00100)
Append: [Disentangled Safety Adapters Enable Efficient Guardrails and Flexible Inference-Time Alignment](https://arxiv.org/abs/2506.00166)
Append: [Pushing the Limits of Beam Search Decoding for Transducer-based ASR models](https://arxiv.org/abs/2506.00185)
Append: [Control-R: Towards controllable test-time scaling](https://arxiv.org/abs/2506.00189)
Append: [Localized LoRA: A Structured Low-Rank Approximation for Efficient Fine-Tuning](https://arxiv.org/abs/2506.00236)
Append: [ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment](https://arxiv.org/abs/2506.00238)
Append: [Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise](https://arxiv.org/abs/2506.00242)
Append: [Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity](https://arxiv.org/abs/2506.00245)
Append: [MIR: Methodology Inspiration Retrieval for Scientific Research Problems](https://arxiv.org/abs/2506.00249)
Append: [GPR: Empowering Generation with Graph-Pretrained Retriever](https://arxiv.org/abs/2506.00261)
Append: [RoboMoRe: LLM-based Robot Co-design via Joint Optimization of Morphology and Reward](https://arxiv.org/abs/2506.00276)
Append: [MythTriage: Scalable Detection of Opioid Use Disorder Myths on a Video-Sharing Platform](https://arxiv.org/abs/2506.00308)
Append: [Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents](https://arxiv.org/abs/2506.00320)
Append: [Adapting General-Purpose Embedding Models to Private Datasets Using Keyword-based Retrieval](https://arxiv.org/abs/2506.00363)
Append: [Spectral Insights into Data-Oblivious Critical Layers in Large Language Models](https://arxiv.org/abs/2506.00382)
Append: [XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark](https://arxiv.org/abs/2506.00462)
Append: [BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM Evaluation](https://arxiv.org/abs/2506.00482)
Append: [FLoE: Fisher-Based Layer Selection for Efficient Sparse Adaptation of Low-Rank Experts](https://arxiv.org/abs/2506.00495)
Append: [CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing](https://arxiv.org/abs/2506.00530)
Append: [Con Instruction: Universal Jailbreaking of Multimodal Large Language Models via Non-Textual Modalities](https://arxiv.org/abs/2506.00548)
Append: [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
Append: [Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs](https://arxiv.org/abs/2506.00577)
Append: [Linear Representation Transferability Hypothesis: Leveraging Small Models to Steer Large Models](https://arxiv.org/abs/2506.00653)
Append: [Existing Large Language Model Unlearning Evaluations Are Inconclusive](https://arxiv.org/abs/2506.00688)
Append: [DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains](https://arxiv.org/abs/2506.00708)
Append: [Bregman Conditional Random Fields: Sequence Labeling with Parallelizable Inference Algorithms](https://arxiv.org/abs/2506.00732)
Append: [LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning](https://arxiv.org/abs/2506.00772)
Append: [HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models](https://arxiv.org/abs/2506.00805)
Append: [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
Append: [Towards Predicting Any Human Trajectory In Context](https://arxiv.org/abs/2506.00871)
Append: [CODEMENV: Benchmarking Large Language Models on Code Migration](https://arxiv.org/abs/2506.00894)
Append: [Position as Probability: Self-Supervised Transformers that Think Past Their Training for Length Extrapolation](https://arxiv.org/abs/2506.00920)
Append: [Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times](https://arxiv.org/abs/2506.00928)
Append: [Aligning VLM Assistants with Personalized Situated Cognition](https://arxiv.org/abs/2506.00930)
Append: [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)
Append: [Bridging the Gap: From Ad-hoc to Proactive Search in Conversations](https://arxiv.org/abs/2506.00983)
Append: [Simple Prompt Injection Attacks Can Leak Personal Data Observed by LLM Agents During Task Execution](https://arxiv.org/abs/2506.01055)
Append: [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
Append: [Earley-Driven Dynamic Pruning for Efficient Structured Decoding](https://arxiv.org/abs/2506.01151)
Append: [Confidence intervals for forced alignment boundaries using model ensembles](https://arxiv.org/abs/2506.01256)
Append: [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)
Append: [Overcoming Multi-step Complexity in Multimodal Theory-of-Mind Reasoning: A Scalable Bayesian Planner](https://arxiv.org/abs/2506.01301)
Append: [An Empirical Study of Group Conformity in Multi-Agent Systems](https://arxiv.org/abs/2506.01332)
Append: [Attention Is Not Always the Answer: Optimizing Voice Activity Detection with Simple Feature Fusion](https://arxiv.org/abs/2506.01365)
Append: [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
Append: [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
Append: [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
Append: [PGPO: Enhancing Agent Reasoning via Pseudocode-style Planning Guided Preference Optimization](https://arxiv.org/abs/2506.01475)
Append: [MUDI: A Multimodal Biomedical Dataset for Understanding Pharmacodynamic Drug-Drug Interactions](https://arxiv.org/abs/2506.01478)
Append: [LinearVC: Linear transformations of self-supervised features through the lens of voice conversion](https://arxiv.org/abs/2506.01510)
Append: [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
Append: [AIMSCheck: Leveraging LLMs for AI-Assisted Review of Modern Slavery Statements Across Jurisdictions](https://arxiv.org/abs/2506.01671)
Append: [GRAM: Generative Recommendation via Semantic-aware Multi-granular Late Fusion](https://arxiv.org/abs/2506.01673)
Append: [Respond Beyond Language: A Benchmark for Video Generation in Response to Realistic User Intents](https://arxiv.org/abs/2506.01689)
Append: [Self-Challenging Language Model Agents](https://arxiv.org/abs/2506.01716)
Append: [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/abs/2506.01789)
Append: [Unified Scaling Laws for Compressed Representations](https://arxiv.org/abs/2506.01863)
Append: [When Should Dense Retrievers Be Updated in Evolving Corpora? Detecting Out-of-Distribution Corpora Using GradNormIR](https://arxiv.org/abs/2506.01877)
Append: [WHEN TO ACT, WHEN TO WAIT: Modeling Structural Trajectories for Intent Triggerability in Task-Oriented Dialogue](https://arxiv.org/abs/2506.01881)
Append: [Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination](https://arxiv.org/abs/2506.01902)
Append: [Large language models can learn and generalize steganographic chain-of-thought under process supervision](https://arxiv.org/abs/2506.01926)
Append: [Dual-Process Image Generation](https://arxiv.org/abs/2506.01955)
Append: [On Meta-Prompting](https://arxiv.org/abs/2312.06562)
Append: [Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning](https://arxiv.org/abs/2312.17055)
Append: [Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You](https://arxiv.org/abs/2401.16092)
Append: [Do Large Language Models Latently Perform Multi-Hop Reasoning?](https://arxiv.org/abs/2402.16837)
Append: [White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs](https://arxiv.org/abs/2404.10508)
Append: [Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?](https://arxiv.org/abs/2404.12728)
Append: [Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment](https://arxiv.org/abs/2405.00557)
Append: [Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions](https://arxiv.org/abs/2405.03205)
Append: [LexGen: Domain-aware Multilingual Lexicon Generation](https://arxiv.org/abs/2405.11200)
Append: [Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness](https://arxiv.org/abs/2405.18915)
Append: [Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine](https://arxiv.org/abs/2406.02394)
Append: [Multi-Prompting Decoder Helps Better Language Understanding](https://arxiv.org/abs/2406.06279)
Append: [RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning Based on Emotional Information](https://arxiv.org/abs/2406.11093)
Append: [A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models](https://arxiv.org/abs/2406.11753)
Append: [BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning](https://arxiv.org/abs/2406.17764)
Append: [LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks](https://arxiv.org/abs/2406.18403)
Append: [Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation](https://arxiv.org/abs/2408.03505)
Append: [Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models](https://arxiv.org/abs/2408.13533)
Append: [Awes, Laws, and Flaws From Today's LLM Research](https://arxiv.org/abs/2408.15409)
Append: [Learning from Negative Samples in Generative Biomedical Entity Linking](https://arxiv.org/abs/2408.16493)
Append: [Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options](https://arxiv.org/abs/2409.00113)
Append: [STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond](https://arxiv.org/abs/2409.05367)
Append: [CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs](https://arxiv.org/abs/2409.05806)
Append: [Towards Diverse and Efficient Audio Captioning via Diffusion Models](https://arxiv.org/abs/2409.09401)
Append: [A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders](https://arxiv.org/abs/2409.14507)
Append: [Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach](https://arxiv.org/abs/2409.19458)
Append: [The Nature of NLP: Analyzing Contributions in NLP Papers](https://arxiv.org/abs/2409.19505)
Append: [AfriHuBERT: A self-supervised speech representation model for African languages](https://arxiv.org/abs/2409.20201)
Append: [Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models](https://arxiv.org/abs/2410.03026)
Append: [Can Language Models Reason about Individualistic Human Values and Preferences?](https://arxiv.org/abs/2410.03868)
Append: [Stereotype or Personalization? User Identity Biases Chatbot Recommendations](https://arxiv.org/abs/2410.05613)
Append: [MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment](https://arxiv.org/abs/2410.05873)
Append: [Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning](https://arxiv.org/abs/2410.06118)
Append: [Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models](https://arxiv.org/abs/2410.07176)
Append: [Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs](https://arxiv.org/abs/2410.08145)
Append: [RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates](https://arxiv.org/abs/2410.10075)
Append: [Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence](https://arxiv.org/abs/2410.11163)
Append: [BridG MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Bridging and Gradual MT](https://arxiv.org/abs/2410.11693)
Append: [Exploring Model Kinship for Merging Large Language Models](https://arxiv.org/abs/2410.12613)
Append: [A Little Human Data Goes A Long Way](https://arxiv.org/abs/2410.13098)
Append: [Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers](https://arxiv.org/abs/2410.13184)
Append: [BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla](https://arxiv.org/abs/2410.13281)
Append: [LoGU: Long-form Generation with Uncertainty Expressions](https://arxiv.org/abs/2410.14309)
Append: [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930)
Append: [CogSteer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models](https://arxiv.org/abs/2410.17714)
Append: [Scaling Diffusion Language Models via Adaptation from Autoregressive Models](https://arxiv.org/abs/2410.17891)
Append: [Improving Model Factuality with Fine-grained Critique-based Evaluator](https://arxiv.org/abs/2410.18359)
Append: [GrammaMT: Improving Machine Translation with Grammar-Informed In-Context Learning](https://arxiv.org/abs/2410.18702)
Append: [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)
Append: [TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models](https://arxiv.org/abs/2410.20445)
Append: [Anticipating Future with Large Language Model for Simultaneous Machine Translation](https://arxiv.org/abs/2410.22499)
Append: [STEM-POM: Evaluating Language Models Math-Symbol Reasoning in Document Parsing](https://arxiv.org/abs/2411.00387)
Append: [Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 14 Indian Languages](https://arxiv.org/abs/2411.04699)
Append: [KnowCoder-X: Boosting Multilingual Information Extraction via Code](https://arxiv.org/abs/2411.04794)
Append: [FactLens: Benchmarking Fine-Grained Fact Verification](https://arxiv.org/abs/2411.05980)
Append: [Neural Topic Modeling with Large Language Models in the Loop](https://arxiv.org/abs/2411.08534)
Append: [HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter](https://arxiv.org/abs/2411.15462)
Append: [Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?](https://arxiv.org/abs/2411.16679)
Append: [Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS](https://arxiv.org/abs/2411.18478)
Append: [If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World](https://arxiv.org/abs/2412.01617)
Append: [Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset](https://arxiv.org/abs/2412.02595)
Append: [CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels](https://arxiv.org/abs/2412.02819)
Append: [RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2412.02830)
Append: [PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from Related Example Banks](https://arxiv.org/abs/2412.05710)
Append: [Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis](https://arxiv.org/abs/2412.05862)
Append: [KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?](https://arxiv.org/abs/2412.08985)
Append: [On the Limit of Language Models as Planning Formalizers](https://arxiv.org/abs/2412.09879)
Append: [LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation](https://arxiv.org/abs/2412.10424)
Append: [INTERACT: Enabling Interactive, Question-Driven Learning in Large Language Models](https://arxiv.org/abs/2412.11388)
Append: [The Impact of Token Granularity on the Predictive Power of Language Model Surprisal](https://arxiv.org/abs/2412.11940)
Append: [Inferring Functionality of Attention Heads from their Parameters](https://arxiv.org/abs/2412.11965)
Append: [Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective](https://arxiv.org/abs/2412.12276)
Append: [Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport](https://arxiv.org/abs/2412.12569)
Append: [Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study](https://arxiv.org/abs/2412.13169)
Append: [SummExecEdit: A Factual Consistency Benchmark in Summarization with Executable Edits](https://arxiv.org/abs/2412.13378)
Append: [GAMEBoT: Transparent Assessment of LLM Reasoning in Games](https://arxiv.org/abs/2412.13602)
Append: [SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation](https://arxiv.org/abs/2412.13649)
Append: [Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation](https://arxiv.org/abs/2412.14050)
Append: [Neuron Empirical Gradient: Discovering and Quantifying Neurons Global Linear Controllability](https://arxiv.org/abs/2412.18053)
Append: [Improving Factuality with Explicit Working Memory](https://arxiv.org/abs/2412.18069)
Append: [Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm](https://arxiv.org/abs/2412.18120)
Append: [CoAM: Corpus of All-Type Multiword Expressions](https://arxiv.org/abs/2412.18151)
Append: [Token-Budget-Aware LLM Reasoning](https://arxiv.org/abs/2412.18547)
Append: ["My life is miserable, have to sign 500 autographs everyday": Exposing Humblebragging, the Brags in Disguise](https://arxiv.org/abs/2412.20057)
Append: [Towards Neural No-Resource Language Translation: A Comparative Evaluation of Approaches](https://arxiv.org/abs/2412.20584)
Append: [Enhancing Transformers for Generalizable First-Order Logical Entailment](https://arxiv.org/abs/2501.00759)
Append: [Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice](https://arxiv.org/abs/2501.00982)
Append: [Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback](https://arxiv.org/abs/2501.01377)
Append: [Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation](https://arxiv.org/abs/2501.01743)
Append: [Personalized Graph-Based Retrieval for Large Language Models](https://arxiv.org/abs/2501.02157)
Append: [Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection](https://arxiv.org/abs/2501.02295)
Append: [Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications](https://arxiv.org/abs/2501.02460)
Append: [Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation](https://arxiv.org/abs/2501.03545)
Append: [TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification](https://arxiv.org/abs/2501.03835)
Append: [Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models](https://arxiv.org/abs/2501.04945)
Append: [Effective faking of verbal deception detection with target-aligned adversarial attacks](https://arxiv.org/abs/2501.05962)
Append: [Curiosity-Driven Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2501.11463)
Append: [Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas](https://arxiv.org/abs/2501.11549)
Append: [Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction](https://arxiv.org/abs/2501.13125)
Append: [ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation](https://arxiv.org/abs/2501.14956)
Append: [Dialogue Systems for Emotional Support via Value Reinforcement](https://arxiv.org/abs/2501.17182)
Append: [How to Select Datapoints for Efficient Human Evaluation of NLG Models?](https://arxiv.org/abs/2501.18251)
Append: [Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation](https://arxiv.org/abs/2501.19017)
Append: [Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://arxiv.org/abs/2502.02508)
Append: [Reflection-Window Decoding: Text Generation with Selective Refinement](https://arxiv.org/abs/2502.03678)
Append: [Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition](https://arxiv.org/abs/2502.04795)
Append: [Iterative Deepening Sampling as Efficient Test-Time Scaling](https://arxiv.org/abs/2502.05449)
Append: [FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy](https://arxiv.org/abs/2502.05551)
Append: [Non-literal Understanding of Number Words by Language Models](https://arxiv.org/abs/2502.06204)
Append: [Position: It's Time to Act on the Risk of Efficient Personalized Text Generation](https://arxiv.org/abs/2502.06560)
Append: [Survey on Vision-Language-Action Models](https://arxiv.org/abs/2502.06851)
Append: [GCoT: Chain-of-Thought Prompt Learning for Graphs](https://arxiv.org/abs/2502.08092)
Append: [Quality-Aware Decoding: Unifying Quality Estimation and Decoding](https://arxiv.org/abs/2502.08561)
Append: [RoToR: Towards More Reliable Responses for Order-Invariant Inputs](https://arxiv.org/abs/2502.08662)
Append: [Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2502.08826)
Append: [Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages?](https://arxiv.org/abs/2502.08900)
Append: [Prediction hubs are context-informed frequent tokens in LLMs](https://arxiv.org/abs/2502.10201)
Append: [A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions](https://arxiv.org/abs/2502.11095)
Append: [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)
Append: [The Mirage of Model Editing: Revisiting Evaluation in the Wild](https://arxiv.org/abs/2502.11177)
Append: [LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing](https://arxiv.org/abs/2502.11368)
Append: [Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation](https://arxiv.org/abs/2502.11423)
Append: [If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?](https://arxiv.org/abs/2502.11469)
Append: [Diversity-oriented Data Augmentation with Large Language Models](https://arxiv.org/abs/2502.11671)
Append: [MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables](https://arxiv.org/abs/2502.11735)
Append: [SpeechT: Findings of the First Mentorship in Speech Translation](https://arxiv.org/abs/2502.12050)
Append: [Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges](https://arxiv.org/abs/2502.12378)
Append: [Theoretical Guarantees for Minimum Bayes Risk Decoding](https://arxiv.org/abs/2502.12685)
Append: [Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models](https://arxiv.org/abs/2502.12821)
Append: [Subword models struggle with word learning, but surprisal hides it](https://arxiv.org/abs/2502.12835)
Append: [HPSS: Heuristic Prompting Strategy Search for LLM Evaluators](https://arxiv.org/abs/2502.13031)
Append: [HumT DumT: Measuring and controlling human-like language in LLMs](https://arxiv.org/abs/2502.13259)
Append: [VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare](https://arxiv.org/abs/2502.13775)
Append: [TESS 2: A Large-Scale Generalist Diffusion Language Model](https://arxiv.org/abs/2502.13917)
Append: [RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13957)
Append: [Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above](https://arxiv.org/abs/2502.14127)
Append: [Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information](https://arxiv.org/abs/2502.14258)
Append: [SEA-HELM: Southeast Asian Holistic Evaluation of Language Models](https://arxiv.org/abs/2502.14301)
Append: [Data-Constrained Synthesis of Training Data for De-Identification](https://arxiv.org/abs/2502.14677)
Append: [Harnessing PDF Data for Improving Japanese Large Multimodal Models](https://arxiv.org/abs/2502.14778)
Append: [Mapping 1,000+ Language Models via the Log-Likelihood Vector](https://arxiv.org/abs/2502.16173)
Append: [NUTSHELL: A Dataset for Abstract Generation from Scientific Talks](https://arxiv.org/abs/2502.16942)
Append: [Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric](https://arxiv.org/abs/2502.17184)
Append: [Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs](https://arxiv.org/abs/2502.18795)
Append: [Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles](https://arxiv.org/abs/2502.18968)
Append: [TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency](https://arxiv.org/abs/2502.19163)
Append: [EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models](https://arxiv.org/abs/2502.19765)
Append: [FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving](https://arxiv.org/abs/2502.20238)
Append: [How Much is Enough? The Diminishing Returns of Tokenization Training Data](https://arxiv.org/abs/2502.20273)
Append: [Protecting multimodal large language models against misleading visualizations](https://arxiv.org/abs/2502.20503)
Append: [Enhancing Text Editing for Grammatical Error Correction: Arabic as a Case Study](https://arxiv.org/abs/2503.00985)
Append: [MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages](https://arxiv.org/abs/2503.01150)
Append: [A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models](https://arxiv.org/abs/2503.01854)
Append: [Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization](https://arxiv.org/abs/2503.02450)
Append: [EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States](https://arxiv.org/abs/2503.03340)
Append: [Large Language Models in Bioinformatics: A Survey](https://arxiv.org/abs/2503.04490)
Append: [Optimizing Multi-Hop Document Retrieval Through Intermediate Representations](https://arxiv.org/abs/2503.04796)
Append: [HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation](https://arxiv.org/abs/2503.04800)
Append: [CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization](https://arxiv.org/abs/2503.05750)
Append: [GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification](https://arxiv.org/abs/2503.05763)
Append: [Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation](https://arxiv.org/abs/2503.06594)
Append: [Implicit Reasoning in Transformers is Reasoning through Shortcuts](https://arxiv.org/abs/2503.07604)
Append: [Context-aware Biases for Length Extrapolation](https://arxiv.org/abs/2503.08067)
Append: [Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs](https://arxiv.org/abs/2503.10084)
Append: [ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning](https://arxiv.org/abs/2503.13089)
Append: [Navigating Rifts in Human-LLM Grounding: Study and Benchmark](https://arxiv.org/abs/2503.13975)
Append: [SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models](https://arxiv.org/abs/2503.15351)
Append: [A Dual-Directional Context-Aware Test-Time Learning for Text Classification](https://arxiv.org/abs/2503.15469)
Append: [Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content](https://arxiv.org/abs/2503.16031)
Append: [CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement](https://arxiv.org/abs/2503.17279)
Append: [ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems](https://arxiv.org/abs/2503.20756)
Append: [Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models](https://arxiv.org/abs/2503.20850)
Append: [Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models](https://arxiv.org/abs/2503.22877)
Append: [A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates](https://arxiv.org/abs/2504.01225)
Append: [Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation](https://arxiv.org/abs/2504.01919)
Append: [Post-Training Language Models for Continual Relation Extraction](https://arxiv.org/abs/2504.05214)
Append: [Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games](https://arxiv.org/abs/2504.06868)
Append: [Supervised Optimism Correction: Be Confident When LLMs Are Sure](https://arxiv.org/abs/2504.07527)
Append: [SD$^2$: Self-Distilled Sparse Drafters](https://arxiv.org/abs/2504.08838)
Append: [Parameterized Synthetic Text Generation with SimpleStories](https://arxiv.org/abs/2504.09184)
Append: [Guiding Reasoning in Small Language Models with LLM Assistance](https://arxiv.org/abs/2504.09923)
Append: [LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews](https://arxiv.org/abs/2504.11042)
Append: [Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction](https://arxiv.org/abs/2505.00814)
Append: [Bemba Speech Translation: Exploring a Low-Resource African Language](https://arxiv.org/abs/2505.02518)
Append: [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/abs/2505.07659)
Append: [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/abs/2505.07784)
Append: [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)
Append: [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)
Append: [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
Append: [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
Append: [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
Append: [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
Append: [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
Append: [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
Append: [Rank, Chunk and Expand: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
Append: [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)
Append: [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)
Append: [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
Append: [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective](https://arxiv.org/abs/2505.16900)
Append: [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)
Append: [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/abs/2505.17446)
Append: [Multimodal Conversation Structure Understanding](https://arxiv.org/abs/2505.17536)
Append: [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)
Append: [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)
Append: [Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments](https://arxiv.org/abs/2505.18927)
Append: [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)
Append: [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)
Append: [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
Append: [Efficient Speech Translation through Model Compression and Knowledge Distillation](https://arxiv.org/abs/2505.20237)
Append: [It's High Time: A Survey of Temporal Information Retrieval and Question Answering](https://arxiv.org/abs/2505.20243)
Append: [Learning distributed representations with efficient SoftMax normalization](https://arxiv.org/abs/2303.17475)
Append: [Towards a Neural Lambda Calculus: Neurosymbolic AI Applied to the Foundations of Functional Programming](https://arxiv.org/abs/2304.09276)
Append: [StarVector: Generating Scalable Vector Graphics Code from Images and Text](https://arxiv.org/abs/2312.11556)
Append: [SongComposer: A Large Language Model for Lyric and Melody Generation in Song Composition](https://arxiv.org/abs/2402.17645)
Append: [Calibration of Large Language Models on Code Summarization](https://arxiv.org/abs/2404.19318)
Append: [CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks](https://arxiv.org/abs/2406.13945)
Append: [CityGPT: Empowering Urban Spatial Cognition of Large Language Models](https://arxiv.org/abs/2406.13948)
Append: [Curriculum Learning with Quality-Driven Data Selection](https://arxiv.org/abs/2407.00102)
Append: [LETS-C: Leveraging Text Embedding for Time Series Classification](https://arxiv.org/abs/2407.06533)
Append: [Leveraging Variation Theory in Counterfactual Data Augmentation for Optimized Active Learning](https://arxiv.org/abs/2408.03819)
Append: [WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks](https://arxiv.org/abs/2409.04459)
Append: [ReflectDiffu:Reflect between Emotion-intent Contagion and Mimicry for Empathetic Response Generation via a RL-Diffusion Framework](https://arxiv.org/abs/2409.10289)
Append: [SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving Model Transformation](https://arxiv.org/abs/2410.03960)
Append: [SciEvo: A 2 Million, 30-Year Cross-disciplinary Dataset for Temporal Scientometric Analysis](https://arxiv.org/abs/2410.09510)
Append: [LLM-Mixer: Multiscale Mixing in LLMs for Time Series Forecasting](https://arxiv.org/abs/2410.11674)
Append: [Communication-Efficient and Tensorized Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2410.13097)
Append: [Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation](https://arxiv.org/abs/2410.13248)
Append: [ChitroJera: A Regionally Relevant Visual Question Answering Dataset for Bangla](https://arxiv.org/abs/2410.14991)
Append: [CLEAR: Character Unlearning in Textual and Visual Modalities](https://arxiv.org/abs/2410.18057)
Append: [Autoregressive Models in Vision: A Survey](https://arxiv.org/abs/2411.05902)
Append: [Probing LLM Hallucination from Within: Perturbation-Driven Approach via Internal Knowledge](https://arxiv.org/abs/2411.09689)
Append: [VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models](https://arxiv.org/abs/2411.17451)
Append: [Are Your LLMs Capable of Stable Reasoning?](https://arxiv.org/abs/2412.13147)
Append: [Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning](https://arxiv.org/abs/2412.13631)
Append: [Are We in the AI-Generated Text World Already? Quantifying and Monitoring AIGT on Social Media](https://arxiv.org/abs/2412.18148)
Append: [Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging](https://arxiv.org/abs/2412.20070)
Append: [Why Are Positional Encodings Nonessential for Deep Autoregressive Transformers? Revisiting a Petroglyph](https://arxiv.org/abs/2501.00659)
Append: [Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?](https://arxiv.org/abs/2501.02669)
Append: [Towards Early Prediction of Self-Supervised Speech Model Performance](https://arxiv.org/abs/2501.05966)
Append: [Feedback-Aware Monte Carlo Tree Search for Efficient Information Seeking in Goal-Oriented Conversations](https://arxiv.org/abs/2501.15056)
Append: [PIP: Perturbation-based Iterative Pruning for Large Language Models](https://arxiv.org/abs/2501.15278)
Append: [WhiSPA: Semantically and Psychologically Aligned Whisper with Self-Supervised Contrastive and Student-Teacher Learning](https://arxiv.org/abs/2501.16344)
Append: [The TIP of the Iceberg: Revealing a Hidden Class of Task-in-Prompt Adversarial Attacks on LLMs](https://arxiv.org/abs/2501.18626)
Append: [KVTuner: Sensitivity-Aware Layer-Wise Mixed-Precision KV Cache Quantization for Efficient and Nearly Lossless LLM Inference](https://arxiv.org/abs/2502.04420)
Append: [Safety at Scale: A Comprehensive Survey of Large Model Safety](https://arxiv.org/abs/2502.05206)
Append: [Bone Soups: A Seek-and-Soup Model Merging Approach for Controllable Multi-Objective Generation](https://arxiv.org/abs/2502.10762)
Append: [Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training](https://arxiv.org/abs/2502.11191)
Append: [How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training](https://arxiv.org/abs/2502.11196)
Append: [From Selection to Generation: A Survey of LLM-based Active Learning](https://arxiv.org/abs/2502.11767)
Append: [AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence](https://arxiv.org/abs/2502.13943)
Append: [Standard Benchmarks Fail - Auditing LLM Agents in Finance Must Prioritize Risk](https://arxiv.org/abs/2502.15865)
Append: [From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs](https://arxiv.org/abs/2502.17701)
Append: [ZEBRA: Leveraging Model-Behavioral Knowledge for Zero-Annotation Preference Dataset Construction](https://arxiv.org/abs/2502.18744)
Append: [Tokens for Learning, Tokens for Unlearning: Mitigating Membership Inference Attacks in Large Language Models via Dual-Purpose Training](https://arxiv.org/abs/2502.19726)
Append: [OmniRouter: Budget and Performance Controllable Multi-LLM Routing](https://arxiv.org/abs/2502.20576)
Append: [I see what you mean: Co-Speech Gestures for Reference Resolution in Multimodal Dialogue](https://arxiv.org/abs/2503.00071)
Append: [Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems](https://arxiv.org/abs/2503.00600)
Append: [Marco-o1 v2: Towards Widening The Distillation Bottleneck for Reasoning Models](https://arxiv.org/abs/2503.01461)
Append: [MMSciBench: Benchmarking Language Models on Chinese Multimodal Scientific Problems](https://arxiv.org/abs/2503.01891)
Append: [ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation](https://arxiv.org/abs/2503.07010)
Append: [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/abs/2503.09532)
Append: [MentalChat16K: A Benchmark Dataset for Conversational Mental Health Assistance](https://arxiv.org/abs/2503.13509)
Append: [Redefining Toxicity: An Objective and Context-Aware Approach for Stress-Level-Based Detection](https://arxiv.org/abs/2503.16072)
Append: [CodeReviewQA: The Code Review Comprehension Assessment for Large Language Models](https://arxiv.org/abs/2503.16167)
Append: [REALM: A Dataset of Real-World LLM Use Cases](https://arxiv.org/abs/2503.18792)
Append: [Large Language and Reasoning Models are Shallow Disjunctive Reasoners](https://arxiv.org/abs/2503.23487)
Append: [OmniCaptioner: One Captioner to Rule Them All](https://arxiv.org/abs/2504.07089)
Append: [Graph-Driven Multimodal Feature Learning Framework for Apparent Personality Assessment](https://arxiv.org/abs/2504.11515)
Append: [3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark](https://arxiv.org/abs/2504.13861)
Append: [Completing A Systematic Review in Hours instead of Months with Interactive AI Agents](https://arxiv.org/abs/2504.14822)
Append: [Acting Less is Reasoning More! Teaching Model to Act Efficiently](https://arxiv.org/abs/2504.14870)
Append: [Visualizing Public Opinion on X: A Real-Time Sentiment Dashboard Using VADER and DistilBERT](https://arxiv.org/abs/2504.15448)
Append: [(Im)possibility of Automated Hallucination Detection in Large Language Models](https://arxiv.org/abs/2504.17004)
Append: [Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning](https://arxiv.org/abs/2504.19583)
Append: [Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems](https://arxiv.org/abs/2505.00212)
Append: [Reassessing Large Language Model Boolean Query Generation for Systematic Reviews](https://arxiv.org/abs/2505.07155)
Append: [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
Append: [Forensic deepfake audio detection using segmental speech features](https://arxiv.org/abs/2505.13847)
Append: [RADAR: Enhancing Radiology Report Generation with Supplementary Knowledge Injection](https://arxiv.org/abs/2505.14318)
Append: [SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment](https://arxiv.org/abs/2505.14667)
Append: [TrimR: Verifier-based Training-Free Thinking Compression for Efficient Test-Time Scaling](https://arxiv.org/abs/2505.17155)
Append: [An End-to-End Approach for Child Reading Assessment in the Xhosa Language](https://arxiv.org/abs/2505.17371)
Append: [Co-Reinforcement Learning for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.17534)
Append: [One RL to See Them All: Visual Triple Unified Reinforcement Learning](https://arxiv.org/abs/2505.18129)
Append: [A Survey of LLM $\times$ DATA](https://arxiv.org/abs/2505.18458)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction](https://arxiv.org/abs/2505.20279)
append_entries: 596
Finish: 2025-06-03 04:29:26.961562
------------------------------------------------------
Started: 2025-06-03 06:26:30.174997
Existing_entries: 1596
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 899
Summarized using GPT-3.5-turbo
Append: [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/abs/2504.03561)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [The NaijaVoices Dataset: Cultivating Large-Scale, High-Quality, Culturally-Rich Speech Data for African Languages](https://arxiv.org/abs/2505.20564)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
Token length: 1937
Summarized using GPT-3.5-turbo
Append: [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)
Token length: 1215
Summarized using GPT-3.5-turbo
Append: [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
Token length: 1344
Summarized using GPT-3.5-turbo
Append: [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [AdvAgent: Controllable Blackbox Red-teaming on Web Agents](https://arxiv.org/abs/2410.17401)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [Wanda++: Pruning Large Language Models via Regional Gradients](https://arxiv.org/abs/2503.04992)
Token length: 1246
Summarized using GPT-3.5-turbo
Append: [Hierarchical Retrieval with Evidence Curation for Open-Domain Financial Question Answering on Standardized Documents](https://arxiv.org/abs/2505.20368)
Token length: 1692
Summarized using GPT-3.5-turbo
Append: [How Do Transformers Learn Variable Binding in Symbolic Programs?](https://arxiv.org/abs/2505.20896)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
Token length: 1742
Summarized using GPT-3.5-turbo
Append: [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
append_entries: 18
Finish: 2025-06-03 06:27:12.362394
------------------------------------------------------
Started: 2025-06-03 08:24:19.684449
Existing_entries: 1018
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [ScEdit: Script-based Assessment of Knowledge Editing](https://arxiv.org/abs/2505.23291)
append_entries: 1
Finish: 2025-06-03 08:24:22.928284
------------------------------------------------------
Started: 2025-06-03 10:18:56.570917
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 10:18:57.742727
------------------------------------------------------
Started: 2025-06-03 12:35:14.423829
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 12:35:15.610200
------------------------------------------------------
Started: 2025-06-03 14:17:35.886558
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 14:17:37.097031
------------------------------------------------------
Started: 2025-06-03 16:23:08.937482
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 16:23:10.090441
------------------------------------------------------
Started: 2025-06-03 18:24:16.542015
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 18:24:17.697837
------------------------------------------------------
Started: 2025-06-03 20:19:30.821899
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 20:19:32.051524
------------------------------------------------------
Started: 2025-06-03 22:16:30.484959
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-03 22:16:31.646851
------------------------------------------------------
Started: 2025-06-04 01:20:41.708384
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 01:20:42.967672
------------------------------------------------------
Started: 2025-06-04 03:14:18.165248
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 03:14:19.397618
------------------------------------------------------
Started: 2025-06-04 04:26:03.977868
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1193
Summarized using GPT-3.5-turbo
Append: [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/abs/2506.01961)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)
Token length: 931
Summarized using GPT-3.5-turbo
Append: [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/abs/2506.02018)
Token length: 895
Summarized using GPT-3.5-turbo
Append: [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/abs/2506.02019)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)
Token length: 1257
Summarized using GPT-3.5-turbo
Append: [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)
Token length: 1522
Summarized using GPT-3.5-turbo
Append: [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/abs/2506.02126)
Token length: 1547
Summarized using GPT-3.5-turbo
Append: [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
Token length: 1063
Summarized using GPT-3.5-turbo
Append: [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/abs/2506.02147)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/abs/2506.02157)
Token length: 927
Summarized using GPT-3.5-turbo
Append: [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/abs/2506.02172)
Token length: 1935
Summarized using GPT-3.5-turbo
Append: [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)
Token length: 1034
Summarized using GPT-3.5-turbo
Append: [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
Token length: 1188
Summarized using GPT-3.5-turbo
Append: [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/abs/2506.02239)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/abs/2506.02264)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/abs/2506.02283)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)
Token length: 1309
Summarized using GPT-3.5-turbo
Append: [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/abs/2506.02321)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)
Token length: 1316
Summarized using GPT-3.5-turbo
Append: [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338)
Token length: 1490
Summarized using GPT-3.5-turbo
Append: [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/abs/2506.02347)
Token length: 1450
Summarized using GPT-3.5-turbo
Append: [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/abs/2506.02350)
Token length: 1217
Summarized using GPT-3.5-turbo
Append: [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/abs/2506.02372)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)
Token length: 1870
Summarized using GPT-3.5-turbo
Append: [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)
Token length: 1008
Summarized using GPT-3.5-turbo
Append: [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/abs/2506.02425)
Token length: 1273
Summarized using GPT-3.5-turbo
Append: [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)
Token length: 905
Summarized using GPT-3.5-turbo
Append: [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/abs/2506.02431)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/abs/2506.02442)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/abs/2506.02460)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/abs/2506.02461)
Token length: 1069
Summarized using GPT-3.5-turbo
Append: [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)
Token length: 1449
Summarized using GPT-3.5-turbo
Append: [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/abs/2506.02480)
Token length: 1421
Summarized using GPT-3.5-turbo
Append: [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)
Token length: 953
Summarized using GPT-3.5-turbo
Append: [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/abs/2506.02483)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/abs/2506.02503)
Token length: 1409
Summarized using GPT-3.5-turbo
Append: [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)
Append: [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)
Append: [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/abs/2506.02519)
Append: [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)
Append: [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/abs/2506.02532)
Append: [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)
Append: [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)
Append: [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
Append: [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)
Append: [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/abs/2506.02573)
Append: [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)
Append: [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)
Append: [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/abs/2506.02591)
Append: [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592)
Append: [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)
Append: [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/abs/2506.02627)
Append: [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)
Append: [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)
Append: [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
Append: [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/abs/2506.02683)
Append: [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/abs/2506.02689)
Append: [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)
Append: [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)
Append: [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/abs/2506.02740)
Append: [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/abs/2506.02753)
Append: [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)
Append: [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
Append: [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)
Append: [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/abs/2506.02827)
Append: [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/abs/2506.02872)
Append: [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
Append: [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/abs/2506.02894)
Append: [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)
Append: [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)
Append: [A Controllable Examination for Long-Context Language Models](https://arxiv.org/abs/2506.02921)
Append: [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)
Append: [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)
Append: [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)
Append: [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)
Append: [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02961)
Append: [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/abs/2506.02973)
Append: [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/abs/2506.02979)
Append: [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)
Append: [It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems](https://arxiv.org/abs/2506.02995)
Append: [A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems](https://arxiv.org/abs/2506.02998)
Append: [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)
Append: [Coding Agents with Multimodal Browsing are Generalist Problem Solvers](https://arxiv.org/abs/2506.03011)
Append: [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)
Append: [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)
Append: [Literary Evidence Retrieval via Long-Context Language Models](https://arxiv.org/abs/2506.03090)
Append: [Beyond Text Compression: Evaluating Tokenizers Across Scales](https://arxiv.org/abs/2506.03101)
Append: [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
Append: [AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation](https://arxiv.org/abs/2506.03122)
Append: [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136)
Append: [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)
Append: [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)
Append: [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)
Append: [Generate, Not Recommend: Personalized Multimodal Content Generation](https://arxiv.org/abs/2506.01704)
Append: [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
Append: [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
Append: [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)
Append: [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)
Append: [Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition](https://arxiv.org/abs/2506.02059)
Append: [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
Append: [Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion](https://arxiv.org/abs/2506.02085)
Append: [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](https://arxiv.org/abs/2506.02088)
Append: [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
Append: [A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering](https://arxiv.org/abs/2506.02160)
Append: [Cocktail-Party Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.02178)
Append: [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
Append: [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/abs/2506.02229)
Append: [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
Append: [StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion](https://arxiv.org/abs/2506.02414)
Append: [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
Append: [BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](https://arxiv.org/abs/2506.02479)
Append: [Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs](https://arxiv.org/abs/2506.02529)
Append: [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
Append: [Synthetic Speech Source Tracing using Metric Learning](https://arxiv.org/abs/2506.02590)
Append: [Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](https://arxiv.org/abs/2506.02708)
Append: [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
Append: [An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models](https://arxiv.org/abs/2506.02730)
Append: [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
Append: [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
Append: [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)
Append: [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
Append: [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
Append: [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
Append: [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
Append: [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query](https://arxiv.org/abs/2506.03144)
Append: [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
Append: [Can Character-based Language Models Improve Downstream Task Performance in Low-Resource and Noisy Language Scenarios?](https://arxiv.org/abs/2110.13658)
Append: [TransAug: Translate as Augmentation for Sentence Embeddings](https://arxiv.org/abs/2111.00157)
Append: [Improving Transformer Performance for French Clinical Notes Classification Using Mixture of Experts on a Limited Dataset](https://arxiv.org/abs/2303.12892)
Append: [UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed Entities](https://arxiv.org/abs/2403.04247)
Append: [Revealing the Parallel Multilingual Learning within Large Language Models](https://arxiv.org/abs/2403.09073)
Append: [Checkpoint Merging via Bayesian Optimization in LLM Pretraining](https://arxiv.org/abs/2403.19390)
Append: [LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought](https://arxiv.org/abs/2405.06705)
Append: [SignMusketeers: An Efficient Multi-Stream Approach for Sign Language Translation at Scale](https://arxiv.org/abs/2406.06907)
Append: [Unmasking Database Vulnerabilities: Zero-Knowledge Schema Inference Attacks in Text-to-SQL Systems](https://arxiv.org/abs/2406.14545)
Append: [Free-text Rationale Generation under Readability Level Control](https://arxiv.org/abs/2407.01384)
Append: [UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs' Memorization](https://arxiv.org/abs/2407.03525)
Append: [Localizing and Mitigating Errors in Long-form Question Answering](https://arxiv.org/abs/2407.11930)
Append: [A Survey on Employing Large Language Models for Text-to-SQL Tasks](https://arxiv.org/abs/2407.15186)
Append: [Cross-Institutional Dental EHR Entity Extraction via Generative AI and Synthetic Notes](https://arxiv.org/abs/2407.21050)
Append: [Lower Layers Matter: Alleviating Hallucination via Multi-Layer Fusion Contrastive Decoding with Truthfulness Refocused](https://arxiv.org/abs/2408.08769)
Append: [XTRUST: On the Multilingual Trustworthiness of Large Language Models](https://arxiv.org/abs/2409.15762)
Append: [How to Connect Speech Foundation Models and Large Language Models? What Matters and What Does Not](https://arxiv.org/abs/2409.17044)
Append: [Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning](https://arxiv.org/abs/2410.00382)
Append: [CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark by Human-AI CulturalTeaming](https://arxiv.org/abs/2410.02677)
Append: [Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step](https://arxiv.org/abs/2410.03869)
Append: [Large Language Model Evaluation via Matrix Nuclear-Norm](https://arxiv.org/abs/2410.10672)
Append: [Watching the Watchers: Exposing Gender Disparities in Machine Translation Quality Estimation](https://arxiv.org/abs/2410.10995)
Append: [Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning](https://arxiv.org/abs/2410.11020)
Append: [BenchmarkCards: Standardized Documentation for Large Language Model Benchmarks](https://arxiv.org/abs/2410.12974)
Append: [A Complexity-Based Theory of Compositionality](https://arxiv.org/abs/2410.14817)
Append: [EoRA: Fine-tuning-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation](https://arxiv.org/abs/2410.21271)
Append: [Self-Evolved Reward Learning for LLMs](https://arxiv.org/abs/2411.00418)
Append: [Generative Emotion Cause Explanation in Multimodal Conversations](https://arxiv.org/abs/2411.02430)
Append: [What Goes Into a LM Acceptability Judgment? Rethinking the Impact of Frequency and Length](https://arxiv.org/abs/2411.02528)
Append: [SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications](https://arxiv.org/abs/2411.04975)
Append: [SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs](https://arxiv.org/abs/2411.07965)
Append: [Value-Spectrum: Quantifying Preferences of Vision-Language Models via Value Decomposition in Social Media Contexts](https://arxiv.org/abs/2411.11479)
Append: [SHuBERT: Self-Supervised Sign Language Representation Learning via Multi-Stream Cluster Prediction](https://arxiv.org/abs/2411.16765)
Append: [Is it the end of (generative) linguistics as we know it?](https://arxiv.org/abs/2412.12797)
Append: [Can Input Attributions Explain Inductive Reasoning in In-Context Learning?](https://arxiv.org/abs/2412.15628)
Append: [Computational Analysis of Character Development in Holocaust Testimonies](https://arxiv.org/abs/2412.17063)
Append: [Diving into Self-Evolving Training for Multimodal Reasoning](https://arxiv.org/abs/2412.17451)
Append: [Behind Closed Words: Creating and Investigating the forePLay Annotated Dataset for Polish Erotic Discourse](https://arxiv.org/abs/2412.17533)
Append: [Instruction-Following Pruning for Large Language Models](https://arxiv.org/abs/2501.02086)
Append: [FocalPO: Enhancing Preference Optimizing by Focusing on Correct Preference Rankings](https://arxiv.org/abs/2501.06645)
Append: [Large Language Models to Diffusion Finetuning](https://arxiv.org/abs/2501.15781)
Append: [UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models](https://arxiv.org/abs/2502.00334)
Append: [Inference-time sparse attention with asymmetric indexing](https://arxiv.org/abs/2502.08246)
Append: [Rethinking Evaluation Metrics for Grammatical Error Correction: Why Use a Different Evaluation Process than Human?](https://arxiv.org/abs/2502.09416)
Append: [Akan Cinematic Emotions (ACE): A Multimodal Multi-party Dataset for Emotion Recognition in Movie Dialogues](https://arxiv.org/abs/2502.10973)
Append: [Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models](https://arxiv.org/abs/2502.11075)
Append: [Can't See the Forest for the Trees: Benchmarking Multimodal Safety Awareness for Multimodal LLMs](https://arxiv.org/abs/2502.11184)
Append: [SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings](https://arxiv.org/abs/2502.12562)
Append: [A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary Position Embedding and Query-Aware Vector Quantization](https://arxiv.org/abs/2502.12665)
Append: [Q-STRUM Debate: Query-Driven Contrastive Summarization for Recommendation Comparison](https://arxiv.org/abs/2502.12921)
Append: [A Similarity Paradigm Through Textual Regularization Without Forgetting](https://arxiv.org/abs/2502.14376)
Append: [Social Genome: Grounded Social Reasoning Abilities of Multimodal Models](https://arxiv.org/abs/2502.15109)
Append: [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2502.17110)
Append: [CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought](https://arxiv.org/abs/2502.17214)
Append: [Towards Enhanced Immersion and Agency for LLM-based Interactive Drama](https://arxiv.org/abs/2502.17878)
Append: [DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers](https://arxiv.org/abs/2502.18460)
Append: [PolyPrompt: Automating Knowledge Extraction from Multilingual Language Models with Dynamic Prompt Generation](https://arxiv.org/abs/2502.19756)
Append: [Finite State Automata Inside Transformers with Chain-of-Thought: A Mechanistic Study on State Tracking](https://arxiv.org/abs/2502.20129)
Append: [Unnatural Languages Are Not Bugs but Features for LLMs](https://arxiv.org/abs/2503.01926)
Append: [Generator-Assistant Stepwise Rollback Framework for Large Language Model Agent](https://arxiv.org/abs/2503.02519)
Append: [Collapse of Dense Retrievers: Short, Early, and Literal Biases Outranking Factual Evidence](https://arxiv.org/abs/2503.05037)
Append: [OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses](https://arxiv.org/abs/2503.10927)
Append: [The time scale of redundancy between prosody and linguistic context](https://arxiv.org/abs/2503.11630)
Append: [Splintering Nonconcatenative Languages for Better Tokenization](https://arxiv.org/abs/2503.14433)
Append: [Meta-Learning Neural Mechanisms rather than Bayesian Priors](https://arxiv.org/abs/2503.16048)
Append: [Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility](https://arxiv.org/abs/2503.17579)
Append: [Negation: A Pink Elephant in the Large Language Models' Room?](https://arxiv.org/abs/2503.22395)
Append: [Efficient Annotator Reliability Assessment with EffiARA](https://arxiv.org/abs/2504.00589)
Append: [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)
Append: [A Fully Automated Pipeline for Conversational Discourse Annotation: Tree Scheme Generation and Labeling with Large Language Models](https://arxiv.org/abs/2504.08961)
Append: [d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2504.12216)
Append: [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)
Append: [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)
Append: [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)
Append: [KRISTEVA: Close Reading as a Novel Task for Benchmarking Interpretive Reasoning](https://arxiv.org/abs/2505.09825)
Append: [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
Append: [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)
Append: [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/abs/2505.15299)
Append: [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
Append: [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
Append: [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
Append: [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)
Append: [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
Append: [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)
Append: [Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms](https://arxiv.org/abs/2505.20322)
Append: [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
Append: [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
Append: [Threading the Needle: Reweaving Chain-of-Thought Reasoning to Explain Human Label Variation](https://arxiv.org/abs/2505.23368)
Append: [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
Append: [Conti Inc.: Understanding the Internal Discussions of a large Ransomware-as-a-Service Operator with Machine Learning](https://arxiv.org/abs/2308.16061)
Append: [MCU: An Evaluation Framework for Open-Ended Game Agents](https://arxiv.org/abs/2310.08367)
Append: [GPTVQ: The Blessing of Dimensionality for LLM Quantization](https://arxiv.org/abs/2402.15319)
Append: [AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs](https://arxiv.org/abs/2404.16873)
Append: [A Hitchhiker's Guide to Scaling Law Estimation](https://arxiv.org/abs/2410.11840)
Append: [Evaluating and Advancing Multimodal Large Language Models in Perception Ability Lens](https://arxiv.org/abs/2411.14725)
Append: [Superhuman performance of a large language model on the reasoning tasks of a physician](https://arxiv.org/abs/2412.10849)
Append: [SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](https://arxiv.org/abs/2412.15289)
Append: [Tracking the Feature Dynamics in LLM Training: A Mechanistic Study](https://arxiv.org/abs/2412.17626)
Append: [Ola: Pushing the Frontiers of Omni-Modal Language Model](https://arxiv.org/abs/2502.04328)
Append: [Logits are All We Need to Adapt Closed Models](https://arxiv.org/abs/2502.06806)
Append: [Symmetrical Visual Contrastive Optimization: Aligning Vision-Language Models with Minimal Contrastive Images](https://arxiv.org/abs/2502.13928)
Append: [Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment](https://arxiv.org/abs/2502.14354)
Append: [A Mousetrap: Fooling Large Reasoning Models for Jailbreak with Chain of Iterative Chaos](https://arxiv.org/abs/2502.15806)
Append: [Grounded Persuasive Language Generation for Automated Marketing](https://arxiv.org/abs/2502.16810)
Append: [ViDoRAG: Visual Document Retrieval-Augmented Generation via Dynamic Iterative Reasoning Agents](https://arxiv.org/abs/2502.18017)
Append: [We Should Chart an Atlas of All the World's Models](https://arxiv.org/abs/2503.10633)
Append: [Unique Hard Attention: A Tale of Two Sides](https://arxiv.org/abs/2503.14615)
Append: [Enhancing Ultra-Low-Bit Quantization of Large Language Models Through Saliency-Aware Partial Retraining](https://arxiv.org/abs/2504.13932)
Append: [Enhancing Target-unspecific Tasks through a Features Matrix](https://arxiv.org/abs/2505.03414)
Append: [X-Driver: Explainable Autonomous Driving with Vision-Language Models](https://arxiv.org/abs/2505.05098)
Append: [Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation](https://arxiv.org/abs/2505.13887)
Append: [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
Append: [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
append_entries: 253
Finish: 2025-06-04 04:27:52.014105
------------------------------------------------------
Started: 2025-06-04 06:25:03.273619
Existing_entries: 1253
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
Token length: 1476
Summarized using GPT-3.5-turbo
Append: [LiTEx: A Linguistic Taxonomy of Explanations for Understanding Within-Label Variation in Natural Language Inference](https://arxiv.org/abs/2505.22848)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [DyePack: Provably Flagging Test Set Contamination in LLMs Using Backdoors](https://arxiv.org/abs/2505.23001)
Token length: 1229
Summarized using GPT-3.5-turbo
Append: [Dataset Cartography for Large Language Model Alignment: Mapping and Diagnosing Preference Data](https://arxiv.org/abs/2505.23114)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Improving Multilingual Speech Models on ML-SUPERB 2.0: Fine-tuning with Data Augmentation and LID-Aware CTC](https://arxiv.org/abs/2505.24200)
append_entries: 5
Finish: 2025-06-04 06:25:16.311166
------------------------------------------------------
Started: 2025-06-04 08:23:18.428279
Existing_entries: 1005
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 08:23:19.043790
------------------------------------------------------
Started: 2025-06-04 10:18:38.145383
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 10:18:38.801781
------------------------------------------------------
Started: 2025-06-04 12:35:05.278879
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 12:35:05.888903
------------------------------------------------------
Started: 2025-06-04 14:14:10.602354
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 14:14:11.186674
------------------------------------------------------
Started: 2025-06-04 16:18:54.555966
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 16:18:55.218839
------------------------------------------------------
Started: 2025-06-04 18:23:33.708333
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 18:23:34.303250
------------------------------------------------------
Started: 2025-06-04 20:15:07.611205
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 20:15:08.252487
------------------------------------------------------
Started: 2025-06-04 22:14:55.438714
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-04 22:14:56.065797
------------------------------------------------------
Started: 2025-06-05 01:19:43.504710
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 01:19:44.099638
------------------------------------------------------
Started: 2025-06-05 03:16:07.467069
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 03:16:08.067841
------------------------------------------------------
Started: 2025-06-05 04:29:04.223573
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems](https://arxiv.org/abs/2506.03259)
Token length: 154
Summarized using GPT-3.5-turbo
Append: [A conclusive remark on linguistic theorizing and language modeling](https://arxiv.org/abs/2506.03268)
Token length: 1857
Summarized using GPT-3.5-turbo
Append: [FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes](https://arxiv.org/abs/2506.03278)
Token length: 1102
Summarized using GPT-3.5-turbo
Append: [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)
Token length: 1581
Summarized using GPT-3.5-turbo
Append: [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)
Token length: 954
Summarized using GPT-3.5-turbo
Append: [From Instructions to ODRL Usage Policies: An Ontology Guided Approach](https://arxiv.org/abs/2506.03301)
Token length: 951
Summarized using GPT-3.5-turbo
Append: [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/abs/2506.03310)
Token length: 922
Summarized using GPT-3.5-turbo
Append: [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/abs/2506.03360)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
Token length: 945
Summarized using GPT-3.5-turbo
Append: [DistRAG: Towards Distance-Based Spatial Reasoning in LLMs](https://arxiv.org/abs/2506.03424)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)
Token length: 884
Summarized using GPT-3.5-turbo
Append: [Culture Matters in Toxic Language Detection in Persian](https://arxiv.org/abs/2506.03458)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2506.03476)
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training](https://arxiv.org/abs/2506.03483)
Token length: 1700
Summarized using GPT-3.5-turbo
Append: [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)
Token length: 1622
Summarized using GPT-3.5-turbo
Append: [Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing](https://arxiv.org/abs/2506.03490)
Token length: 1672
Summarized using GPT-3.5-turbo
Append: [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](https://arxiv.org/abs/2506.03510)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals](https://arxiv.org/abs/2506.03519)
Token length: 1384
Summarized using GPT-3.5-turbo
Append: [TokAlign: Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/abs/2506.03523)
Token length: 1443
Summarized using GPT-3.5-turbo
Append: [Seed-Coder: Let the Code Model Curate Data for Itself](https://arxiv.org/abs/2506.03524)
Token length: 995
Summarized using GPT-3.5-turbo
Append: [Go-Browse: Training Web Agents with Structured Exploration](https://arxiv.org/abs/2506.03533)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)
Token length: 1464
Summarized using GPT-3.5-turbo
Append: [BPO: Revisiting Preference Modeling in Direct Preference Optimization](https://arxiv.org/abs/2506.03557)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch](https://arxiv.org/abs/2506.03558)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)
Token length: 1196
Summarized using GPT-3.5-turbo
Append: [FreePRM: Training Process Reward Models Without Ground Truth Process Labels](https://arxiv.org/abs/2506.03570)
Token length: 974
Summarized using GPT-3.5-turbo
Append: [Exchange of Perspective Prompting Enhances Reasoning in Large Language Models](https://arxiv.org/abs/2506.03573)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)
Token length: 1240
Summarized using GPT-3.5-turbo
Append: [Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models](https://arxiv.org/abs/2506.03580)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models](https://arxiv.org/abs/2506.03592)
Token length: 1618
Summarized using GPT-3.5-turbo
Append: [Is linguistically-motivated data augmentation worth it?](https://arxiv.org/abs/2506.03593)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Learning to Insert [PAUSE] Tokens for Better Reasoning](https://arxiv.org/abs/2506.03616)
Token length: 1507
Summarized using GPT-3.5-turbo
Append: [Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales](https://arxiv.org/abs/2506.03619)
Token length: 1499
Summarized using GPT-3.5-turbo
Append: [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)
Token length: 1515
Summarized using GPT-3.5-turbo
Append: [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)
Token length: 1385
Summarized using GPT-3.5-turbo
Append: [Trustworthy Medical Question Answering: An Evaluation-Centric Survey](https://arxiv.org/abs/2506.03659)
Token length: 795
Summarized using GPT-3.5-turbo
Append: [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/abs/2506.03665)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering](https://arxiv.org/abs/2506.03681)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/abs/2506.03690)
Token length: 1952
Summarized using GPT-3.5-turbo
Append: [AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism](https://arxiv.org/abs/2506.03700)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation](https://arxiv.org/abs/2506.03704)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition](https://arxiv.org/abs/2506.03722)
Token length: 1251
Summarized using GPT-3.5-turbo
Append: [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)
Append: [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)
Append: [Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services](https://arxiv.org/abs/2506.03761)
Append: [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)
Append: [ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations](https://arxiv.org/abs/2506.03763)
Append: [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/abs/2506.03781)
Append: [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)
Append: [Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts](https://arxiv.org/abs/2506.03793)
Append: [Automatic Correction of Writing Anomalies in Hausa Texts](https://arxiv.org/abs/2506.03820)
Append: [CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents](https://arxiv.org/abs/2506.03822)
Append: [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/abs/2506.03827)
Append: [Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain](https://arxiv.org/abs/2506.03832)
Append: [PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading](https://arxiv.org/abs/2506.03861)
Append: [EuroGEST: Investigating gender stereotypes in multilingual language models](https://arxiv.org/abs/2506.03867)
Append: [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)
Append: [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/abs/2506.03884)
Append: [Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation](https://arxiv.org/abs/2506.03887)
Append: [Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems](https://arxiv.org/abs/2506.03901)
Append: [The Harmonic Structure of Information Contours](https://arxiv.org/abs/2506.03902)
Append: [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/abs/2506.03913)
Append: [Compositional Generalisation for Explainable Hate Speech Detection](https://arxiv.org/abs/2506.03916)
Append: [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)
Append: [More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning](https://arxiv.org/abs/2506.03923)
Append: [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/abs/2506.03941)
Append: [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)
Append: [From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding](https://arxiv.org/abs/2506.03968)
Append: [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/abs/2506.03978)
Append: [Voice Activity Projection Model with Multimodal Encoders](https://arxiv.org/abs/2506.03980)
Append: [Around the World in 24 Hours: Probing LLM Knowledge of Time and Place](https://arxiv.org/abs/2506.03984)
Append: [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/abs/2506.03989)
Append: [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
Append: [Words of Warmth: Trust and Sociability Norms for over 26k English Words](https://arxiv.org/abs/2506.03993)
Append: [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/abs/2506.03994)
Append: [QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering](https://arxiv.org/abs/2506.04020)
Append: [AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data](https://arxiv.org/abs/2506.04032)
Append: [The mutual exclusivity bias of bilingual visually grounded speech models](https://arxiv.org/abs/2506.04037)
Append: [LexTime: A Benchmark for Temporal Ordering of Legal Events](https://arxiv.org/abs/2506.04041)
Append: [Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness](https://arxiv.org/abs/2506.04042)
Append: [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)
Append: [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)
Append: [On Support Samples of Next Word Prediction](https://arxiv.org/abs/2506.04047)
Append: [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)
Append: [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)
Append: [Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning](https://arxiv.org/abs/2506.04065)
Append: [LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward](https://arxiv.org/abs/2506.04070)
Append: [Controlling Difficulty of Generated Text for AI-Assisted Language Learning](https://arxiv.org/abs/2506.04072)
Append: [Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems](https://arxiv.org/abs/2506.04076)
Append: [A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions](https://arxiv.org/abs/2506.04077)
Append: [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
Append: [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
Append: [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
Append: [Rectified Sparse Attention](https://arxiv.org/abs/2506.04108)
Append: [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/abs/2506.04131)
Append: [Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?](https://arxiv.org/abs/2506.04139)
Append: [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142)
Append: [A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization](https://arxiv.org/abs/2506.04156)
Append: [SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling](https://arxiv.org/abs/2506.04179)
Append: [SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models](https://arxiv.org/abs/2506.04180)
Append: [Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models](https://arxiv.org/abs/2506.04182)
Append: [R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2506.04185)
Append: [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)
Append: [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
Append: [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2506.03197)
Append: [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
Append: [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
Append: [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
Append: [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/abs/2506.03444)
Append: [ProRank: Prompt Warmup via Reinforcement Learning for Small Language Models Reranking](https://arxiv.org/abs/2506.03487)
Append: [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525)
Append: [Preface to the Special Issue of the TAL Journal on Scholarly Document Processing](https://arxiv.org/abs/2506.03587)
Append: [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
Append: [Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models](https://arxiv.org/abs/2506.03606)
Append: [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
Append: [PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing](https://arxiv.org/abs/2506.03741)
Append: [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/abs/2506.03857)
Append: [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
Append: [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
Append: [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
Append: [CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking](https://arxiv.org/abs/2506.04019)
Append: [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
Append: [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
Append: [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)
Append: [MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos](https://arxiv.org/abs/2506.04141)
Append: [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
Append: [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
Append: [Transformers in Speech Processing: A Survey](https://arxiv.org/abs/2303.11607)
Append: [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)
Append: [Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models](https://arxiv.org/abs/2310.12049)
Append: [CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks](https://arxiv.org/abs/2406.02524)
Append: [AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models](https://arxiv.org/abs/2406.09295)
Append: [UBench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions](https://arxiv.org/abs/2406.12784)
Append: [UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs](https://arxiv.org/abs/2406.18173)
Append: [REAL: Response Embedding-based Alignment for LLMs](https://arxiv.org/abs/2409.17169)
Append: [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/abs/2410.01444)
Append: [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/abs/2410.09300)
Append: [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/abs/2410.16502)
Append: [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747)
Append: [Enabling LLM Knowledge Analysis via Extensive Materialization](https://arxiv.org/abs/2411.04920)
Append: [Improving Radiology Report Conciseness and Structure via Local Large Language Models](https://arxiv.org/abs/2411.05042)
Append: [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)
Append: [A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects](https://arxiv.org/abs/2411.10371)
Append: [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/abs/2412.05237)
Append: [Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation](https://arxiv.org/abs/2412.15255)
Append: [Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria](https://arxiv.org/abs/2412.21006)
Append: [ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability](https://arxiv.org/abs/2501.05855)
Append: [Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions](https://arxiv.org/abs/2501.16748)
Append: [ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Consensus Enforcement, and Column Exploration](https://arxiv.org/abs/2502.00675)
Append: [Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models](https://arxiv.org/abs/2502.13656)
Append: [Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946)
Append: [Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems](https://arxiv.org/abs/2502.14019)
Append: [Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of Topic Models](https://arxiv.org/abs/2502.14748)
Append: [D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model Generation Using Large Language Models](https://arxiv.org/abs/2502.16540)
Append: [Sliding Window Attention Training for Efficient Large Language Models](https://arxiv.org/abs/2502.18845)
Append: [Where Are We? Evaluating LLM Performance on African Languages](https://arxiv.org/abs/2502.19582)
Append: [DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation](https://arxiv.org/abs/2503.01622)
Append: [On the Acquisition of Shared Grammatical Representations in Bilingual Language Models](https://arxiv.org/abs/2503.03962)
Append: [Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities](https://arxiv.org/abs/2503.04721)
Append: [PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts](https://arxiv.org/abs/2503.06706)
Append: [LSC-Eval: A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data](https://arxiv.org/abs/2503.08042)
Append: [An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (HPLT)](https://arxiv.org/abs/2503.10267)
Append: [Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set](https://arxiv.org/abs/2503.10515)
Append: [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/abs/2503.15358)
Append: [Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey](https://arxiv.org/abs/2503.15850)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset](https://arxiv.org/abs/2503.23899)
Append: [Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding](https://arxiv.org/abs/2504.00030)
Append: [CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness](https://arxiv.org/abs/2504.05154)
Append: [Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation](https://arxiv.org/abs/2504.05276)
Append: [Identifying Aspects in Peer Reviews](https://arxiv.org/abs/2504.06910)
Append: [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)
Append: [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)
Append: [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)
Append: [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)
Append: [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
Append: [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)
Append: [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/abs/2505.17427)
Append: [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
Append: [AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy](https://arxiv.org/abs/2505.20538)
Append: [STEER-BENCH: A Benchmark for Evaluating the Steerability of Large Language Models](https://arxiv.org/abs/2505.20645)
Append: [Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties](https://arxiv.org/abs/2505.20875)
Append: [LLMs Think, But Not In Your Flow: Reasoning-Level Personalization for Black-Box Large Language Models](https://arxiv.org/abs/2505.21082)
Append: [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
Append: [Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration](https://arxiv.org/abs/2505.24688)
Append: [Trust-Oriented Adaptive Guardrails for Large Language Models](https://arxiv.org/abs/2408.08959)
Append: [VinePPO: Refining Credit Assignment in RL Training of LLMs](https://arxiv.org/abs/2410.01679)
Append: [A LLM-Powered Automatic Grading Framework with Human-Level Guidelines Optimization](https://arxiv.org/abs/2410.02165)
Append: [MMedPO: Aligning Medical Vision-Language Models with Clinical-Aware Multimodal Preference Optimization](https://arxiv.org/abs/2412.06141)
Append: [From Intention To Implementation: Automating Biomedical Research via LLMs](https://arxiv.org/abs/2412.09429)
Append: [Scaling Laws for Floating Point Quantization Training](https://arxiv.org/abs/2501.02423)
Append: [Sleepless Nights, Sugary Days: Creating Synthetic Users with Health Conditions for Realistic Coaching Agent Interactions](https://arxiv.org/abs/2502.13135)
Append: [InSerter: Speech Instruction Following with Unsupervised Interleaved Pre-training](https://arxiv.org/abs/2503.02769)
Append: [Dynamic Benchmarking of Reasoning Capabilities in Code Large Language Models Under Data Contamination](https://arxiv.org/abs/2503.04149)
Append: [ROGRAG: A Robustly Optimized GraphRAG Framework](https://arxiv.org/abs/2503.06474)
Append: [GRU: Mitigating the Trade-off between Unlearning and Retention for Large Language Models](https://arxiv.org/abs/2503.09117)
Append: [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)
Append: [Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory](https://arxiv.org/abs/2505.10981)
Append: [PAST: Phonetic-Acoustic Speech Tokenizer](https://arxiv.org/abs/2505.14470)
Append: [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
Append: [SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)
Append: [What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals](https://arxiv.org/abs/2505.20730)
Append: [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
Append: [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
Append: [Large Language Models are Locally Linear Mappings](https://arxiv.org/abs/2505.24293)
append_entries: 212
Finish: 2025-06-05 04:31:02.700935
------------------------------------------------------
Started: 2025-06-05 06:25:51.527102
Existing_entries: 1212
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1565
Summarized using GPT-3.5-turbo
Append: [LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions](https://arxiv.org/abs/2505.23811)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Bench4KE: Benchmarking Automated Competency Question Generation](https://arxiv.org/abs/2505.24554)
Token length: 1413
Summarized using GPT-3.5-turbo
Append: [SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability](https://arxiv.org/abs/2503.09532)
append_entries: 3
Finish: 2025-06-05 06:26:00.068334
------------------------------------------------------
Started: 2025-06-05 08:22:37.157954
Existing_entries: 1003
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 08:22:37.649875
------------------------------------------------------
Started: 2025-06-05 10:18:56.635777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 10:18:57.102918
------------------------------------------------------
Started: 2025-06-05 12:35:01.137934
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 12:35:01.631488
------------------------------------------------------
Started: 2025-06-05 14:17:04.033968
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 14:17:04.532926
------------------------------------------------------
Started: 2025-06-05 16:21:07.165321
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 16:21:07.632087
------------------------------------------------------
Started: 2025-06-05 18:25:29.907511
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 18:25:30.433833
------------------------------------------------------
Started: 2025-06-05 20:15:44.468539
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 20:15:44.963811
------------------------------------------------------
Started: 2025-06-05 22:14:57.758999
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-05 22:14:58.265566
------------------------------------------------------
Started: 2025-06-06 01:19:43.480677
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 01:19:44.043306
------------------------------------------------------
Started: 2025-06-06 03:13:27.971425
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 03:13:28.519673
------------------------------------------------------
Started: 2025-06-06 04:28:52.277781
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1532
Summarized using GPT-3.5-turbo
Append: [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [Effects of Speaker Count, Duration, and Accent Diversity on Zero-Shot Accent Robustness in Low-Resource ASR](https://arxiv.org/abs/2506.04364)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [Mechanistic Decomposition of Sentence Representations](https://arxiv.org/abs/2506.04373)
Token length: 1328
Summarized using GPT-3.5-turbo
Append: [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
Token length: 1551
Summarized using GPT-3.5-turbo
Append: [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
Token length: 1219
Summarized using GPT-3.5-turbo
Append: [Unpacking Let Alone: Human-Scale Models Generalize to a Rare Construction in Form but not Meaning](https://arxiv.org/abs/2506.04408)
Token length: 615
Summarized using GPT-3.5-turbo
Append: [Empaths at SemEval-2025 Task 11: Retrieval-Augmented Approach to Perceived Emotions Prediction](https://arxiv.org/abs/2506.04409)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [Zero-Shot Open-Schema Entity Structure Discovery](https://arxiv.org/abs/2506.04458)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Aligning Large Language Models with Implicit Preferences from User-Generated Content](https://arxiv.org/abs/2506.04463)
Token length: 959
Summarized using GPT-3.5-turbo
Append: [SQLens: An End-to-End Framework for Error Detection and Correction in Text-to-SQL](https://arxiv.org/abs/2506.04494)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [DRE: An Effective Dual-Refined Method for Integrating Small and Large Language Models in Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2506.04516)
Token length: 1083
Summarized using GPT-3.5-turbo
Append: [Please Translate Again: Two Simple Experiments on Whether Human-Like Reasoning Helps Translation](https://arxiv.org/abs/2506.04521)
Token length: 766
Summarized using GPT-3.5-turbo
Append: [Is It JUST Semantics? A Case Study of Discourse Particle Understanding in LLMs](https://arxiv.org/abs/2506.04534)
Token length: 450
Summarized using GPT-3.5-turbo
Append: [BSBench: will your LLM find the largest prime number?](https://arxiv.org/abs/2506.04535)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [SSA-COMET: Do LLMs Outperform Learned Metrics in Evaluating MT for Under-Resourced African Languages?](https://arxiv.org/abs/2506.04557)
Token length: 1612
Summarized using GPT-3.5-turbo
Append: [Demonstrations of Integrity Attacks in Multi-Agent Systems](https://arxiv.org/abs/2506.04572)
Token length: 1573
Summarized using GPT-3.5-turbo
Append: [Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis](https://arxiv.org/abs/2506.04574)
Token length: 1703
Summarized using GPT-3.5-turbo
Append: [Are LLMs Reliable Translators of Logical Reasoning Across Lexically Diversified Contexts?](https://arxiv.org/abs/2506.04575)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Selecting Demonstrations for Many-Shot In-Context Learning via Gradient Matching](https://arxiv.org/abs/2506.04579)
Token length: 1183
Summarized using GPT-3.5-turbo
Append: [SUCEA: Reasoning-Intensive Retrieval for Adversarial Fact-checking through Claim Decomposition and Editing](https://arxiv.org/abs/2506.04583)
Token length: 1276
Summarized using GPT-3.5-turbo
Append: [MuSciClaims: Multimodal Scientific Claim Verification](https://arxiv.org/abs/2506.04585)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [LESS: Large Language Model Enhanced Semi-Supervised Learning for Speech Foundational Models](https://arxiv.org/abs/2506.04586)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification](https://arxiv.org/abs/2506.04592)
Token length: 1113
Summarized using GPT-3.5-turbo
Append: [A MISMATCHED Benchmark for Scientific Natural Language Inference](https://arxiv.org/abs/2506.04603)
Token length: 838
Summarized using GPT-3.5-turbo
Append: [Revisiting Test-Time Scaling: A Survey and a Diversity-Aware Method for Efficient Reasoning](https://arxiv.org/abs/2506.04611)
Token length: 1751
Summarized using GPT-3.5-turbo
Append: [Subjective Perspectives within Learned Representations Predict High-Impact Innovation](https://arxiv.org/abs/2506.04616)
Token length: 945
Summarized using GPT-3.5-turbo
Append: [Static Word Embeddings for Sentence Semantic Representation](https://arxiv.org/abs/2506.04624)
Token length: 1878
Summarized using GPT-3.5-turbo
Append: [Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning](https://arxiv.org/abs/2506.04625)
Token length: 1043
Summarized using GPT-3.5-turbo
Append: [ViCocktail: Automated Multi-Modal Data Collection for Vietnamese Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.04635)
Token length: 1359
Summarized using GPT-3.5-turbo
Append: [TaDA: Training-free recipe for Decoding with Adaptive KV Cache Compression and Mean-centering](https://arxiv.org/abs/2506.04642)
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Flex-TravelPlanner: A Benchmark for Flexible Planning with Language Agents](https://arxiv.org/abs/2506.04649)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Normative Conflicts and Shallow AI Alignment](https://arxiv.org/abs/2506.04679)
Token length: 844
Summarized using GPT-3.5-turbo
Append: [MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models](https://arxiv.org/abs/2506.04688)
Token length: 1797
Summarized using GPT-3.5-turbo
Append: [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
Token length: 888
Summarized using GPT-3.5-turbo
Append: [Cracking the Code: Enhancing Implicit Hate Speech Detection through Coding Classification](https://arxiv.org/abs/2506.04693)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [Accelerated Test-Time Scaling with Model-Free Speculative Sampling](https://arxiv.org/abs/2506.04708)
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [IIITH-BUT system for IWSLT 2025 low-resource Bhojpuri to Hindi speech translation](https://arxiv.org/abs/2506.04714)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat](https://arxiv.org/abs/2506.04721)
Token length: 1372
Summarized using GPT-3.5-turbo
Append: [Lifelong Evolution: Collaborative Learning between Large and Small Language Models for Continuous Emergent Fake News Detection](https://arxiv.org/abs/2506.04739)
Token length: 1010
Summarized using GPT-3.5-turbo
Append: [Identifying Reliable Evaluation Metrics for Scientific Text Revision](https://arxiv.org/abs/2506.04772)
Token length: 1323
Summarized using GPT-3.5-turbo
Append: [Fine-Grained Interpretation of Political Opinions in Large Language Models](https://arxiv.org/abs/2506.04774)
Token length: 1660
Summarized using GPT-3.5-turbo
Append: [MMSU: A Massive Multi-task Spoken Language Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.04779)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques](https://arxiv.org/abs/2506.04788)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Dissecting Logical Reasoning in LLMs: A Fine-Grained Evaluation and Supervision Study](https://arxiv.org/abs/2506.04810)
Token length: 1655
Summarized using GPT-3.5-turbo
Append: [Design of intelligent proofreading system for English translation based on CNN and BERT](https://arxiv.org/abs/2506.04811)
Token length: 1052
Summarized using GPT-3.5-turbo
Append: [Evaluating Vision-Language and Large Language Models for Automated Student Assessment in Indonesian Classrooms](https://arxiv.org/abs/2506.04822)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [A Reasoning-Based Approach to Cryptic Crossword Clue Solving](https://arxiv.org/abs/2506.04824)
Append: [Joint Evaluation of Answer and Reasoning Consistency for Hallucination Detection in Large Reasoning Models](https://arxiv.org/abs/2506.04832)
Append: [MockConf: A Student Interpretation Dataset: Analysis, Word- and Span-level Alignment and Baselines](https://arxiv.org/abs/2506.04848)
Append: [Multiple-Choice Question Generation Using Large Language Models: Methodology and Educator Insights](https://arxiv.org/abs/2506.04851)
Append: [Prompting LLMs: Length Control for Isometric Machine Translation](https://arxiv.org/abs/2506.04855)
Append: [Evaluating the Effectiveness of Linguistic Knowledge in Pretrained Language Models: A Case Study of Universal Dependencies](https://arxiv.org/abs/2506.04887)
Append: [ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests](https://arxiv.org/abs/2506.04894)
Append: [Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots](https://arxiv.org/abs/2506.04907)
Append: [A Practitioner's Guide to Building ASR Models for Low-Resource Languages: A Case Study on Scottish Gaelic](https://arxiv.org/abs/2506.04915)
Append: [Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback](https://arxiv.org/abs/2506.04920)
Append: [ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT](https://arxiv.org/abs/2506.04929)
Append: [From Struggle (06-2024) to Mastery (02-2025) LLMs Conquer Advanced Algorithm Exams and Pave the Way for Editorial Generation](https://arxiv.org/abs/2506.04965)
Append: [Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering](https://arxiv.org/abs/2506.04981)
Append: [SCOP: Evaluating the Comprehension Process of Large Language Models from a Cognitive View](https://arxiv.org/abs/2506.05000)
Append: [ComfyUI-Copilot: An Intelligent Assistant for Automated Workflow Development](https://arxiv.org/abs/2506.05010)
Append: [Controlling Summarization Length Through EOS Token Weighting](https://arxiv.org/abs/2506.05017)
Append: [Automatic Robustness Stress Testing of LLMs as Mathematical Problem Solvers](https://arxiv.org/abs/2506.05038)
Append: [TALL -- A Trainable Architecture for Enhancing LLM Performance in Low-Resource Languages](https://arxiv.org/abs/2506.05057)
Append: [Debatable Intelligence: Benchmarking LLM Judges via Debate Speech Evaluation](https://arxiv.org/abs/2506.05062)
Append: [Does It Make Sense to Speak of Introspection in Large Language Models?](https://arxiv.org/abs/2506.05068)
Append: [RIVAL: Reinforcement Learning with Iterative and Adversarial Optimization for Machine Translation](https://arxiv.org/abs/2506.05070)
Append: [Just a Scratch: Enhancing LLM Capabilities for Self-harm Detection through Intent Differentiation and Emoji Interpretation](https://arxiv.org/abs/2506.05073)
Append: [Parking, Perception, and Retail: Street-Level Determinants of Community Vitality in Harbin](https://arxiv.org/abs/2506.05080)
Append: [CL-ISR: A Contrastive Learning and Implicit Stance Reasoning Framework for Misleading Text Detection on Social Media](https://arxiv.org/abs/2506.05107)
Append: [The NTNU System at the S&I Challenge 2025 SLA Open Track](https://arxiv.org/abs/2506.05121)
Append: [DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning](https://arxiv.org/abs/2506.05128)
Append: [Information Locality as an Inductive Bias for Neural Language Models](https://arxiv.org/abs/2506.05136)
Append: [AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models](https://arxiv.org/abs/2506.05140)
Append: [Do Large Language Models Judge Error Severity Like Humans?](https://arxiv.org/abs/2506.05142)
Append: [Knowledgeable-r1: Policy Optimization for Knowledge Exploration in Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05154)
Append: [Dissecting Bias in LLMs: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2506.05166)
Append: [ECoRAG: Evidentiality-guided Compression for Long Context RAG](https://arxiv.org/abs/2506.05167)
Append: [Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models](https://arxiv.org/abs/2506.05176)
Append: [Counterfactual reasoning: an analysis of in-context emergence](https://arxiv.org/abs/2506.05188)
Append: [RELIC: Evaluating Compositional Instruction Following via Language Recognition](https://arxiv.org/abs/2506.05205)
Append: [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text](https://arxiv.org/abs/2506.05209)
Append: [Improving Low-Resource Morphological Inflection via Self-Supervised Objectives](https://arxiv.org/abs/2506.05227)
Append: [Towards a Unified System of Representation for Continuity and Discontinuity in Natural Language](https://arxiv.org/abs/2506.05235)
Append: [CLATTER: Comprehensive Entailment Reasoning for Hallucination Detection](https://arxiv.org/abs/2506.05243)
Append: [Micro-Act: Mitigate Knowledge Conflict in Question Answering via Actionable Self-Reasoning](https://arxiv.org/abs/2506.05278)
Append: [ProRefine: Inference-time Prompt Refinement with Textual Feedback](https://arxiv.org/abs/2506.05305)
Append: [Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models](https://arxiv.org/abs/2506.05314)
Append: [Search Arena: Analyzing Search-Augmented LLMs](https://arxiv.org/abs/2506.05334)
Append: [Flattery, Fluff, and Fog: Diagnosing and Mitigating Idiosyncratic Biases in Preference Models](https://arxiv.org/abs/2506.05339)
Append: [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
Append: [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
Append: [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)
Append: [Can we reconstruct a dysarthric voice with the large speech model Parler TTS?](https://arxiv.org/abs/2506.04397)
Append: [Matter-of-Fact: A Benchmark for Verifying the Feasibility of Literature-Supported Claims in Materials Science](https://arxiv.org/abs/2506.04410)
Append: [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
Append: [Understanding and Meeting Practitioner Needs When Measuring Representational Harms Caused by LLM-Based Systems](https://arxiv.org/abs/2506.04482)
Append: [Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model](https://arxiv.org/abs/2506.04518)
Append: [Grapheme-Coherent Phonemic and Prosodic Annotation of Speech by Implicit and Explicit Grapheme Conditioning](https://arxiv.org/abs/2506.04527)
Append: [Clustering and Median Aggregation Improve Differentially Private Inference](https://arxiv.org/abs/2506.04566)
Append: [EMO-Debias: Benchmarking Gender Debiasing Techniques in Multi-Label Speech Emotion Recognition](https://arxiv.org/abs/2506.04652)
Append: [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
Append: [LLM-based phoneme-to-grapheme for phoneme-based speech recognition](https://arxiv.org/abs/2506.04711)
Append: [Evaluation is All You Need: Strategic Overclaiming of LLM Reasoning Capabilities Through Evaluation Design](https://arxiv.org/abs/2506.04734)
Append: [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
Append: [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
Append: [From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs](https://arxiv.org/abs/2506.04831)
Append: [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
Append: [Dissecting Long Reasoning Models: An Empirical Study](https://arxiv.org/abs/2506.04913)
Append: [Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings](https://arxiv.org/abs/2506.04997)
Append: [Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics](https://arxiv.org/abs/2506.05087)
Append: [CIVET: Systematic Evaluation of Understanding in VLMs](https://arxiv.org/abs/2506.05146)
Append: [LLM-First Search: Self-Guided Exploration of the Solution Space](https://arxiv.org/abs/2506.05213)
Append: [Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning](https://arxiv.org/abs/2506.05214)
Append: [Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts](https://arxiv.org/abs/2506.05229)
Append: [MesaNet: Sequence Modeling by Locally Optimal Test-Time Training](https://arxiv.org/abs/2506.05233)
Append: [Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia Games](https://arxiv.org/abs/2506.05309)
Append: [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)
Append: [Unleashing Hour-Scale Video Training for Long Video-Language Understanding](https://arxiv.org/abs/2506.05332)
Append: [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/abs/2506.05333)
Append: [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345)
Append: [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
Append: [The Vector Grounding Problem](https://arxiv.org/abs/2304.01481)
Append: [Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation](https://arxiv.org/abs/2402.12649)
Append: [Mosaic-IT: Cost-Free Compositional Data Synthesis for Instruction Tuning](https://arxiv.org/abs/2405.13326)
Append: [The Impossibility of Fair LLMs](https://arxiv.org/abs/2406.03198)
Append: [Multi-Head RAG: Solving Multi-Aspect Problems with LLMs](https://arxiv.org/abs/2406.05085)
Append: [Investigating Distributions of Telecom Adapted Sentence Embeddings for Document Retrieval](https://arxiv.org/abs/2406.12336)
Append: [Leveraging LLMs for Bangla Grammar Error Correction:Error Categorization, Synthetic Data, and Model Evaluation](https://arxiv.org/abs/2406.14284)
Append: [DataGen: Unified Synthetic Dataset Generation via Large Language Models](https://arxiv.org/abs/2406.18966)
Append: [Inducing lexicons of in-group language with socio-temporal context](https://arxiv.org/abs/2409.19257)
Append: [An Exploration of Self-Supervised Mutual Information Alignment for Multi-Task Settings](https://arxiv.org/abs/2410.01704)
Append: [Self-Correction is More than Refinement: A Learning Framework for Visual and Language Reasoning Tasks](https://arxiv.org/abs/2410.04055)
Append: [Evaluating Morphological Compositional Generalization in Large Language Models](https://arxiv.org/abs/2410.12656)
Append: [Not All Options Are Created Equal: Textual Option Weighting for Token-Efficient LLM-Based Knowledge Tracing](https://arxiv.org/abs/2410.12872)
Append: [Isolated Causal Effects of Natural Language](https://arxiv.org/abs/2410.14812)
Append: [What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective](https://arxiv.org/abs/2410.23743)
Append: [FastDraft: How to Train Your Draft](https://arxiv.org/abs/2411.11055)
Append: [MuLan: Adapting Multilingual Diffusion Models for Hundreds of Languages with Negligible Cost](https://arxiv.org/abs/2412.01271)
Append: [The broader spectrum of in-context learning](https://arxiv.org/abs/2412.03782)
Append: [GRAF: Graph Retrieval Augmented by Facts for Romanian Legal Multi-Choice Question Answering](https://arxiv.org/abs/2412.04119)
Append: [The Lessons of Developing Process Reward Models in Mathematical Reasoning](https://arxiv.org/abs/2501.07301)
Append: [Explainability in Practice: A Survey of Explainable NLP Across Various Domains](https://arxiv.org/abs/2502.00837)
Append: [Mind the Confidence Gap: Overconfidence, Calibration, and Distractor Effects in Large Language Models](https://arxiv.org/abs/2502.11028)
Append: [On the Robust Approximation of ASR Metrics](https://arxiv.org/abs/2502.12408)
Append: [Lost in Transcription, Found in Distribution Shift: Demystifying Hallucination in Speech Foundation Models](https://arxiv.org/abs/2502.12414)
Append: [Should I Trust You? Detecting Deception in Negotiations using Counterfactual RL](https://arxiv.org/abs/2502.12436)
Append: [Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity](https://arxiv.org/abs/2502.13063)
Append: [A Survey on Data Contamination for Large Language Models](https://arxiv.org/abs/2502.14425)
Append: [Towards Robust ESG Analysis Against Greenwashing Risks: Aspect-Action Analysis with Cross-Category Generalization](https://arxiv.org/abs/2502.15821)
Append: [SNaRe: Domain-aware Data Generation for Low-Resource Event Detection](https://arxiv.org/abs/2502.17394)
Append: [From Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors](https://arxiv.org/abs/2503.00038)
Append: [Argument Summarization and its Evaluation in the Era of Large Language Models](https://arxiv.org/abs/2503.00847)
Append: [ATLaS: Agent Tuning via Learning Critical Steps](https://arxiv.org/abs/2503.02197)
Append: [When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding Models Against Misinformation Edits](https://arxiv.org/abs/2503.03417)
Append: [Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment](https://arxiv.org/abs/2503.04647)
Append: [COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing](https://arxiv.org/abs/2503.21670)
Append: [Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](https://arxiv.org/abs/2503.22353)
Append: [Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?](https://arxiv.org/abs/2503.24102)
Append: [NorEval: A Norwegian Language Understanding and Generation Evaluation Benchmark](https://arxiv.org/abs/2504.07749)
Append: [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)
Append: [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)
Append: [Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency and Domain Knowledge](https://arxiv.org/abs/2505.05946)
Append: [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)
Append: [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
Append: [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
Append: [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
Append: [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)
Append: [WiNGPT-3.0 Technical Report](https://arxiv.org/abs/2505.17387)
Append: [MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering](https://arxiv.org/abs/2505.18247)
Append: [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
Append: [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
Append: [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
Append: [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)
Append: [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
Append: [In-context Language Learning for Endangered Languages in Speech Recognition](https://arxiv.org/abs/2505.20445)
Append: [CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature](https://arxiv.org/abs/2505.20779)
Append: [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
Append: [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
Append: [Uncovering Visual-Semantic Psycholinguistic Properties from the Distributional Structure of Text Embedding Space](https://arxiv.org/abs/2505.23029)
Append: [MMBoundary: Advancing MLLM Knowledge Boundary Awareness through Reasoning Step Confidence Calibration](https://arxiv.org/abs/2505.23224)
Append: [Adaptive Jailbreaking Strategies Based on the Semantic Understanding Capabilities of Large Language Models](https://arxiv.org/abs/2505.23404)
Append: [ValueSim: Generating Backstories to Model Individual Value Systems](https://arxiv.org/abs/2505.23827)
Append: [Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction](https://arxiv.org/abs/2505.24347)
Append: [Scaling Trends in Language Model Robustness](https://arxiv.org/abs/2407.18213)
Append: [Focus On This, Not That! Steering LLMs with Adaptive Feature Specification](https://arxiv.org/abs/2410.22944)
Append: [Failure Modes of LLMs for Causal Reasoning on Narratives](https://arxiv.org/abs/2410.23884)
Append: [DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts](https://arxiv.org/abs/2412.10510)
Append: [HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing](https://arxiv.org/abs/2412.16187)
Append: [Can Large Language Models Understand Intermediate Representations in Compilers?](https://arxiv.org/abs/2502.06854)
Append: [EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents](https://arxiv.org/abs/2502.09560)
Append: [GoRA: Gradient-driven Adaptive Low Rank Adaptation](https://arxiv.org/abs/2502.12171)
Append: [Empowering LLMs with Logical Reasoning: A Comprehensive Survey](https://arxiv.org/abs/2502.15652)
Append: [Contrastive Visual Data Augmentation](https://arxiv.org/abs/2502.17709)
Append: [TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice](https://arxiv.org/abs/2502.18504)
Append: [ChatWise: A Strategy-Guided Chatbot for Enhancing Cognitive Support in Older Adults](https://arxiv.org/abs/2503.05740)
Append: [LLM Social Simulations Are a Promising Research Method](https://arxiv.org/abs/2504.02234)
Append: [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)
Append: [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
Append: [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
Append: [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
Append: [FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models](https://arxiv.org/abs/2505.19536)
Append: [Position is Power: System Prompts as a Mechanism of Bias in Large Language Models (LLMs)](https://arxiv.org/abs/2505.21091)
Append: [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
Append: [Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition](https://arxiv.org/abs/2505.22251)
Append: [WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions](https://arxiv.org/abs/2505.24195)
Append: [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
Append: [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/abs/2505.24875)
append_entries: 211
Finish: 2025-06-06 04:31:46.266073
------------------------------------------------------
Started: 2025-06-06 06:25:45.101117
Existing_entries: 1211
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 06:25:45.596120
------------------------------------------------------
Started: 2025-06-06 08:22:11.482493
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 08:22:11.940539
------------------------------------------------------
Started: 2025-06-06 10:18:44.038705
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 10:18:44.501287
------------------------------------------------------
Started: 2025-06-06 12:33:47.158301
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 12:33:47.694350
------------------------------------------------------
Started: 2025-06-06 14:16:27.459708
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 14:16:27.934053
------------------------------------------------------
Started: 2025-06-06 16:21:41.270655
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 16:21:41.828328
------------------------------------------------------
Started: 2025-06-06 18:23:35.195437
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 18:23:35.732169
------------------------------------------------------
Started: 2025-06-06 20:19:06.450826
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 20:19:06.908542
------------------------------------------------------
Started: 2025-06-06 22:15:59.923676
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-06 22:16:00.386451
------------------------------------------------------
Started: 2025-06-07 01:19:10.586998
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 01:19:11.055590
------------------------------------------------------
Started: 2025-06-07 03:11:19.553744
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 03:11:20.057006
------------------------------------------------------
Started: 2025-06-07 04:19:59.517040
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 04:19:59.576112
------------------------------------------------------
Started: 2025-06-07 06:22:12.142867
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 06:22:12.215288
------------------------------------------------------
Started: 2025-06-07 08:19:49.865984
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 08:19:49.949167
------------------------------------------------------
Started: 2025-06-07 10:17:02.596306
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 10:17:02.652156
------------------------------------------------------
Started: 2025-06-07 12:31:04.203250
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 12:31:04.308931
------------------------------------------------------
Started: 2025-06-07 14:13:58.716874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 14:13:58.775227
------------------------------------------------------
Started: 2025-06-07 16:18:52.181858
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 16:18:52.236105
------------------------------------------------------
Started: 2025-06-07 18:21:01.102549
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 18:21:01.156106
------------------------------------------------------
Started: 2025-06-07 20:16:45.597812
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 20:16:45.672207
------------------------------------------------------
Started: 2025-06-07 22:15:12.966292
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-07 22:15:13.044882
------------------------------------------------------
Started: 2025-06-08 01:27:14.292596
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 01:27:14.351494
------------------------------------------------------
Started: 2025-06-08 03:21:48.697274
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 03:21:48.768108
------------------------------------------------------
Started: 2025-06-08 04:27:35.690859
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 04:27:35.823752
------------------------------------------------------
Started: 2025-06-08 06:23:19.838208
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 06:23:19.895383
------------------------------------------------------
Started: 2025-06-08 08:19:38.365624
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 08:19:38.450881
------------------------------------------------------
Started: 2025-06-08 10:16:14.838488
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 10:16:14.911399
------------------------------------------------------
Started: 2025-06-08 12:30:55.622514
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 12:30:55.677771
------------------------------------------------------
Started: 2025-06-08 14:14:20.747874
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 14:14:20.832008
------------------------------------------------------
Started: 2025-06-08 16:18:51.893768
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 16:18:51.976594
------------------------------------------------------
Started: 2025-06-08 18:20:42.233150
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 18:20:42.301914
------------------------------------------------------
Started: 2025-06-08 20:17:04.510527
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 20:17:04.576963
------------------------------------------------------
Started: 2025-06-08 22:14:53.927759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-08 22:14:54.008999
------------------------------------------------------
Started: 2025-06-09 01:24:22.883059
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 01:24:22.941949
------------------------------------------------------
Started: 2025-06-09 03:19:54.379887
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 03:19:54.440874
------------------------------------------------------
Started: 2025-06-09 04:32:39.886499
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [EvidenceOutcomes: a Dataset of Clinical Trial Publications with Clinically Meaningful Outcomes](https://arxiv.org/abs/2506.05380)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [LLMs Can Also Do Well! Breaking Barriers in Semantic Role Labeling via Large Language Models](https://arxiv.org/abs/2506.05385)
Token length: 1331
Summarized using GPT-3.5-turbo
Append: [Beyond RAG: Reinforced Reasoning Augmented Generation for Clinical Notes](https://arxiv.org/abs/2506.05386)
Token length: 1024
Summarized using GPT-3.5-turbo
Append: [Advancing Decoding Strategies: Enhancements in Locally Typical Sampling for LLMs](https://arxiv.org/abs/2506.05387)
Token length: 1120
Summarized using GPT-3.5-turbo
Append: [taz2024full: Analysing German Newspapers for Gender Bias and Discrimination across Decades](https://arxiv.org/abs/2506.05388)
Token length: 1305
Summarized using GPT-3.5-turbo
Append: [Understanding Gender Bias in AI-Generated Product Descriptions](https://arxiv.org/abs/2506.05390)
Token length: 1496
Summarized using GPT-3.5-turbo
Append: [Are Large Language Models Good Temporal Graph Learners?](https://arxiv.org/abs/2506.05393)
Token length: 1344
Summarized using GPT-3.5-turbo
Append: [Auto Review: Second Stage Error Detection for Highly Accurate Information Extraction from Phone Conversations](https://arxiv.org/abs/2506.05400)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs](https://arxiv.org/abs/2506.05410)
Token length: 851
Summarized using GPT-3.5-turbo
Append: [SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs](https://arxiv.org/abs/2506.05413)
Token length: 891
Summarized using GPT-3.5-turbo
Append: [Automatically Detecting Amusing Games in Wordle](https://arxiv.org/abs/2506.05415)
Token length: 1025
Summarized using GPT-3.5-turbo
Append: [MLLM-CL: Continual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2506.05453)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [Multidimensional Analysis of Specific Language Impairment Using Unsupervised Learning Through PCA and Clustering](https://arxiv.org/abs/2506.05498)
Token length: 1842
Summarized using GPT-3.5-turbo
Append: [Improving LLMs with a knowledge from databases](https://arxiv.org/abs/2506.05560)
Token length: 662
Summarized using GPT-3.5-turbo
Append: [Combating Misinformation in the Arab World: Challenges & Opportunities](https://arxiv.org/abs/2506.05582)
Token length: 857
Summarized using GPT-3.5-turbo
Append: [UTSA-NLP at ArchEHR-QA 2025: Improving EHR Question Answering via Self-Consistency Prompting](https://arxiv.org/abs/2506.05589)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [SynthesizeMe! Inducing Persona-Guided Prompts for Personalized Reward Models in LLMs](https://arxiv.org/abs/2506.05598)
Token length: 1232
Summarized using GPT-3.5-turbo
Append: [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
Token length: 975
Summarized using GPT-3.5-turbo
Append: [Mitigating Confounding in Speech-Based Dementia Detection through Weight Masking](https://arxiv.org/abs/2506.05610)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs](https://arxiv.org/abs/2506.05629)
Token length: 836
Summarized using GPT-3.5-turbo
Append: [IYKYK: Using language models to decode extremist cryptolects](https://arxiv.org/abs/2506.05635)
Token length: 1089
Summarized using GPT-3.5-turbo
Append: [A Fictional Q&A Dataset for Studying Memorization and Knowledge Acquisition](https://arxiv.org/abs/2506.05639)
Token length: 1429
Summarized using GPT-3.5-turbo
Append: [Can LLMs Express Personality Across Cultures? Introducing CulturalPersonas for Evaluating Trait Alignment](https://arxiv.org/abs/2506.05670)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Zero-Shot Event Causality Identification via Multi-source Evidence Fuzzy Aggregation with Large Language Models](https://arxiv.org/abs/2506.05675)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [A Unified Representation for Continuity and Discontinuity: Syntactic and Computational Motivations](https://arxiv.org/abs/2506.05686)
Token length: 1462
Summarized using GPT-3.5-turbo
Append: [When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.05690)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
Token length: 762
Summarized using GPT-3.5-turbo
Append: [RKEFino1: A Regulation Knowledge-Enhanced Large Language Model](https://arxiv.org/abs/2506.05700)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [Large Language Models are Good Relational Learners](https://arxiv.org/abs/2506.05725)
Token length: 1397
Summarized using GPT-3.5-turbo
Append: [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [LLM-Symbolic Integration for Robust Temporal Tabular Reasoning](https://arxiv.org/abs/2506.05746)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Writing-RL: Advancing Long-form Writing via Adaptive Curriculum Reinforcement Learning](https://arxiv.org/abs/2506.05760)
Token length: 1189
Summarized using GPT-3.5-turbo
Append: [BioMol-MQA: A Multi-Modal Question Answering Dataset For LLM Reasoning Over Bio-Molecular Interactions](https://arxiv.org/abs/2506.05766)
Token length: 927
Summarized using GPT-3.5-turbo
Append: [dots.llm1 Technical Report](https://arxiv.org/abs/2506.05767)
Token length: 1247
Summarized using GPT-3.5-turbo
Append: [Discrete Minds in a Continuous World: Do Language Models Know Time Passes?](https://arxiv.org/abs/2506.05790)
Token length: 1045
Summarized using GPT-3.5-turbo
Append: [MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning](https://arxiv.org/abs/2506.05813)
Token length: 1407
Summarized using GPT-3.5-turbo
Append: [FinanceReasoning: Benchmarking Financial Numerical Reasoning More Credible, Comprehensive and Challenging](https://arxiv.org/abs/2506.05828)
Token length: 1727
Summarized using GPT-3.5-turbo
Append: [Cross-lingual Collapse: How Language-Centric Foundation Models Shape Reasoning in Large Language Models](https://arxiv.org/abs/2506.05850)
Token length: 1927
Summarized using GPT-3.5-turbo
Append: [Route-and-Reason: Scaling Large Language Model Reasoning with Reinforced Model Router](https://arxiv.org/abs/2506.05901)
Token length: 1438
Summarized using GPT-3.5-turbo
Append: [Generating Grounded Responses to Counter Misinformation via Learning Efficient Fine-Grained Critiques](https://arxiv.org/abs/2506.05924)
Token length: 632
Summarized using GPT-3.5-turbo
Append: [LengClaro2023: A Dataset of Administrative Texts in Spanish with Plain Language adaptations](https://arxiv.org/abs/2506.05927)
Token length: 1425
Summarized using GPT-3.5-turbo
Append: [MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.05928)
Token length: 1234
Summarized using GPT-3.5-turbo
Append: [DynamicMind: A Tri-Mode Thinking System for Large Language Models](https://arxiv.org/abs/2506.05936)
Token length: 1576
Summarized using GPT-3.5-turbo
Append: [IntentionESC: An Intention-Centered Framework for Enhancing Emotional Support in Dialogue Systems](https://arxiv.org/abs/2506.05947)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [NameTag 3: A Tool and a Service for Multilingual/Multitagset NER](https://arxiv.org/abs/2506.05949)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Elementary Math Word Problem Generation using Large Language Models](https://arxiv.org/abs/2506.05950)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Let's Put Ourselves in Sally's Shoes: Shoes-of-Others Prefixing Improves Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.05970)
Token length: 575
Summarized using GPT-3.5-turbo
Append: [LTG at SemEval-2025 Task 10: Optimizing Context for Classification of Narrative Roles](https://arxiv.org/abs/2506.05976)
Token length: 883
Summarized using GPT-3.5-turbo
Append: [Tau-Eval: A Unified Evaluation Framework for Useful and Private Text Anonymization](https://arxiv.org/abs/2506.05979)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [A Culturally-Rich Romanian NLP Dataset from "Who Wants to Be a Millionaire?" Videos](https://arxiv.org/abs/2506.05991)
Append: [Token Signature: Predicting Chain-of-Thought Gains with Token Decoding Feature in Large Language Models](https://arxiv.org/abs/2506.06008)
Append: [Unlocking Recursive Thinking of LLMs: Alignment via Refinement](https://arxiv.org/abs/2506.06009)
Append: [AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search](https://arxiv.org/abs/2506.06017)
Append: [When to Trust Context: Self-Reflective Debates for Context Reliability](https://arxiv.org/abs/2506.06020)
Append: [Large Language Models are Demonstration Pre-Selectors for Themselves](https://arxiv.org/abs/2506.06033)
Append: [MATP-BENCH: Can MLLM Be a Good Automated Theorem Prover for Multimodal Problems?](https://arxiv.org/abs/2506.06034)
Append: [Hey, That's My Data! Label-Only Dataset Inference in Large Language Models](https://arxiv.org/abs/2506.06057)
Append: [Simple Yet Effective: Extracting Private Data Across Clients in Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.06060)
Append: [Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning](https://arxiv.org/abs/2506.06069)
Append: [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091)
Append: [Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning](https://arxiv.org/abs/2506.06093)
Append: [Bridging the Gap: In-Context Learning for Modeling Human Disagreement](https://arxiv.org/abs/2506.06113)
Append: [Phonetically-Augmented Discriminative Rescoring for Voice Search Error Correction](https://arxiv.org/abs/2506.06117)
Append: [Let's CONFER: A Dataset for Evaluating Natural Language Inference Models on CONditional InFERence and Presupposition](https://arxiv.org/abs/2506.06133)
Append: [semantic-features: A User-Friendly Tool for Studying Contextual Word Embeddings in Interpretable Semantic Spaces](https://arxiv.org/abs/2506.06169)
Append: [Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach](https://arxiv.org/abs/2506.06175)
Append: [Detecting Voice Phishing with Precision: Fine-Tuning Small Language Models](https://arxiv.org/abs/2506.06180)
Append: [Building Models of Neurological Language](https://arxiv.org/abs/2506.06208)
Append: [PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts](https://arxiv.org/abs/2506.06211)
Append: [Can Theoretical Physics Research Benefit from Language Agents?](https://arxiv.org/abs/2506.06214)
Append: [Explaining Matters: Leveraging Definitions and Semantic Expansion for Sexism Detection](https://arxiv.org/abs/2506.06238)
Append: [Bridging External and Parametric Knowledge: Mitigating Hallucination of LLMs with Shared-Private Semantic Synergy in Dual-Stream Knowledge](https://arxiv.org/abs/2506.06240)
Append: [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)
Append: [AdvSumm: Adversarial Training for Bias Mitigation in Text Summarization](https://arxiv.org/abs/2506.06273)
Append: [Attention-based transformer models for image captioning across languages: An in-depth survey and evaluation](https://arxiv.org/abs/2506.05399)
Append: [Can Vision Language Models Infer Human Gaze Direction? A Controlled Study](https://arxiv.org/abs/2506.05412)
Append: [Coordinated Robustness Evaluation Framework for Vision-Language Models](https://arxiv.org/abs/2506.05429)
Append: [LLMs Can Compensate for Deficiencies in Visual Representations](https://arxiv.org/abs/2506.05439)
Append: [BYO-Eval: Build Your Own Dataset for Fine-Grained Visual Assessment of Multimodal Language Models](https://arxiv.org/abs/2506.05440)
Append: [Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety](https://arxiv.org/abs/2506.05451)
Append: [MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning](https://arxiv.org/abs/2506.05523)
Append: [When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration](https://arxiv.org/abs/2506.05579)
Append: [MMTU: A Massive Multi-Task Table Understanding and Reasoning Benchmark](https://arxiv.org/abs/2506.05587)
Append: [SoK: Are Watermarks in LLMs Ready for Deployment?](https://arxiv.org/abs/2506.05594)
Append: [Deployability-Centric Infrastructure-as-Code Generation: An LLM-based Iterative Framework](https://arxiv.org/abs/2506.05623)
Append: [Projectable Models: One-Shot Generation of Small Specialized Transformers from Large Ones](https://arxiv.org/abs/2506.05641)
Append: [BAQ: Efficient Bit Allocation Quantization for Large Language Models](https://arxiv.org/abs/2506.05664)
Append: [Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning](https://arxiv.org/abs/2506.05671)
Append: [Contextually Guided Transformers via Low-Rank Adaptation](https://arxiv.org/abs/2506.05672)
Append: [Voice Impression Control in Zero-Shot TTS](https://arxiv.org/abs/2506.05688)
Append: [Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective](https://arxiv.org/abs/2506.05754)
Append: [Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?](https://arxiv.org/abs/2506.05765)
Append: [CodeContests+: High-Quality Test Case Generation for Competitive Programming](https://arxiv.org/abs/2506.05817)
Append: [Proactive Assistant Dialogue Generation from Streaming Egocentric Videos](https://arxiv.org/abs/2506.05904)
Append: [Audio-Aware Large Language Models as Judges for Speaking Styles](https://arxiv.org/abs/2506.05984)
Append: [Bootstrapping World Models from Dynamics Models in Multimodal Foundation Models](https://arxiv.org/abs/2506.06006)
Append: [CO-VADA: A Confidence-Oriented Voice Augmentation Debiasing Approach for Fair Speech Emotion Recognition](https://arxiv.org/abs/2506.06071)
Append: [Label-Context-Dependent Internal Language Model Estimation for CTC](https://arxiv.org/abs/2506.06096)
Append: [Table-r1: Self-supervised and Reinforcement Learning for Program-based Table Reasoning in Small Language Models](https://arxiv.org/abs/2506.06137)
Append: [CLaMR: Contextualized Late-Interaction for Multimodal Content Retrieval](https://arxiv.org/abs/2506.06144)
Append: [Masked Language Models are Good Heterogeneous Graph Generalizers](https://arxiv.org/abs/2506.06157)
Append: [The Lock-in Hypothesis: Stagnation by Algorithm](https://arxiv.org/abs/2506.06166)
Append: [Corrector Sampling in Language Models](https://arxiv.org/abs/2506.06215)
Append: [PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time](https://arxiv.org/abs/2506.06254)
Append: [Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding](https://arxiv.org/abs/2506.06275)
Append: [CAT-LLM: Style-enhanced Large Language Models with Text Style Definition for Chinese Article-style Transfer](https://arxiv.org/abs/2401.05707)
Append: [TMT: Tri-Modal Translation between Speech, Image, and Text by Processing Different Modalities as Different Languages](https://arxiv.org/abs/2402.16021)
Append: [Multi-Agent Collaboration via Cross-Team Orchestration](https://arxiv.org/abs/2406.08979)
Append: [Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport](https://arxiv.org/abs/2406.12329)
Append: [HIGHT: Hierarchical Graph Tokenization for Molecule-Language Alignment](https://arxiv.org/abs/2406.14021)
Append: [Banyan: Improved Representation Learning with Explicit Structure](https://arxiv.org/abs/2407.17771)
Append: [A semantic embedding space based on large language models for modelling human beliefs](https://arxiv.org/abs/2408.07237)
Append: [Where is the signal in tokenization space?](https://arxiv.org/abs/2408.08541)
Append: [WER We Stand: Benchmarking Urdu ASR Models](https://arxiv.org/abs/2409.11252)
Append: [Judgment of Learning: A Human Ability Beyond Generative Artificial Intelligence](https://arxiv.org/abs/2410.13392)
Append: [The Impact of Inference Acceleration on Bias of LLMs](https://arxiv.org/abs/2410.22118)
Append: [Unveiling Topological Structures from Language: A Comprehensive Survey of Topological Data Analysis Applications in NLP](https://arxiv.org/abs/2411.10298)
Append: [The Synergy of LLMs & RL Unlocks Offline Learning of Generalizable Language-Conditioned Policies with Low-fidelity Data](https://arxiv.org/abs/2412.06877)
Append: [Benchmarking and Improving Large Vision-Language Models for Fundamental Visual Graph Understanding and Reasoning](https://arxiv.org/abs/2412.13540)
Append: [Reasoning Through Execution: Unifying Process and Outcome Rewards for Code Generation](https://arxiv.org/abs/2412.15118)
Append: [ResearchTown: Simulator of Human Research Community](https://arxiv.org/abs/2412.17767)
Append: [MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models](https://arxiv.org/abs/2501.00316)
Append: [GRASP: Replace Redundant Layers with Adaptive Singular Parameters for Efficient Model Compression](https://arxiv.org/abs/2501.00339)
Append: [Emergent Response Planning in LLMs](https://arxiv.org/abs/2502.06258)
Append: [DPO-Shift: Shifting the Distribution of Direct Preference Optimization](https://arxiv.org/abs/2502.07599)
Append: [Lost in the Passage: Passage-level In-context Learning Does Not Necessarily Need a "Passage"](https://arxiv.org/abs/2502.10634)
Append: [Towards Effective Extraction and Evaluation of Factual Claims](https://arxiv.org/abs/2502.10855)
Append: [TituLLMs: A Family of Bangla LLMs with Comprehensive Benchmarking](https://arxiv.org/abs/2502.11187)
Append: [On the Query Complexity of Verifier-Assisted Language Generation](https://arxiv.org/abs/2502.12123)
Append: [The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text](https://arxiv.org/abs/2502.14921)
Append: [MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models](https://arxiv.org/abs/2502.16671)
Append: [Improving Customer Service with Automatic Topic Detection in User Emails](https://arxiv.org/abs/2502.19115)
Append: [Emergent Symbolic Mechanisms Support Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2502.20332)
Append: [GraphCheck: Multi-Path Fact-Checking with Entity-Relationship Graphs](https://arxiv.org/abs/2502.20785)
Append: [Adversarial Tokenization](https://arxiv.org/abs/2503.02174)
Append: [TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge](https://arxiv.org/abs/2503.04381)
Append: [DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi](https://arxiv.org/abs/2503.04768)
Append: [Taming Knowledge Conflicts in Language Models](https://arxiv.org/abs/2503.10996)
Append: [Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning](https://arxiv.org/abs/2504.05632)
Append: [Analyzing LLMs' Knowledge Boundary Cognition Across Languages Through the Lens of Internal Representations](https://arxiv.org/abs/2504.13816)
Append: [Detect, Explain, Escalate: Low-Carbon Dialogue Breakdown Management for LLM-Powered Agents](https://arxiv.org/abs/2504.18839)
Append: [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)
Append: [Automated Journalistic Questions: A New Method for Extracting 5W1H in French](https://arxiv.org/abs/2505.14804)
Append: [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
Append: [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
Append: [MEDAL: A Framework for Benchmarking LLMs as Multilingual Open-Domain Chatbots and Dialogue Evaluators](https://arxiv.org/abs/2505.22777)
Append: [LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews](https://arxiv.org/abs/2505.24757)
Append: [Structure Guided Large Language Model for SQL Generation](https://arxiv.org/abs/2402.13284)
Append: [Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](https://arxiv.org/abs/2407.02814)
Append: [CoIR: A Comprehensive Benchmark for Code Information Retrieval Models](https://arxiv.org/abs/2407.02883)
Append: [Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/abs/2410.01744)
Append: [AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML](https://arxiv.org/abs/2410.02958)
Append: [PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning](https://arxiv.org/abs/2410.08811)
Append: [ProSec: Fortifying Code LLMs with Proactive Security Alignment](https://arxiv.org/abs/2411.12882)
Append: [Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency](https://arxiv.org/abs/2411.16525)
Append: [Training Software Engineering Agents and Verifiers with SWE-Gym](https://arxiv.org/abs/2412.21139)
Append: [MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding](https://arxiv.org/abs/2501.18362)
Append: [Peri-LN: Revisiting Normalization Layer in the Transformer Architecture](https://arxiv.org/abs/2502.02732)
Append: [LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws](https://arxiv.org/abs/2502.12120)
Append: [Investigating Non-Transitivity in LLM-as-a-Judge](https://arxiv.org/abs/2502.14074)
Append: [CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness](https://arxiv.org/abs/2502.14914)
Append: [Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations](https://arxiv.org/abs/2502.18147)
Append: [Instructor-Worker Large Language Model System for Policy Recommendation: a Case Study on Air Quality Analysis of the January 2025 Los Angeles Wildfires](https://arxiv.org/abs/2503.00566)
Append: [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613)
Append: [TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining](https://arxiv.org/abs/2504.02107)
Append: [Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement Learning](https://arxiv.org/abs/2504.13818)
Append: [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
append_entries: 167
Finish: 2025-06-09 04:34:27.742381
------------------------------------------------------
Started: 2025-06-09 06:26:40.329003
Existing_entries: 1167
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1142
Summarized using GPT-3.5-turbo
Append: [LLM in the Loop: Creating the ParaDeHate Dataset for Hate Speech Detoxification](https://arxiv.org/abs/2506.01484)
Token length: 1132
Summarized using GPT-3.5-turbo
Append: [Tug-of-war between idiom's figurative and literal meanings in LLMs](https://arxiv.org/abs/2506.01723)
Token length: 654
Summarized using GPT-3.5-turbo
Append: [Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor](https://arxiv.org/abs/2506.01819)
Token length: 1546
Summarized using GPT-3.5-turbo
Append: [Generalizable LLM Learning of Graph Synthetic Data with Reinforcement Learning](https://arxiv.org/abs/2506.00845)
append_entries: 4
Finish: 2025-06-09 06:26:50.586352
------------------------------------------------------
Started: 2025-06-09 08:24:27.043037
Existing_entries: 1004
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 08:24:27.477205
------------------------------------------------------
Started: 2025-06-09 10:18:55.652327
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 10:18:56.047213
------------------------------------------------------
Started: 2025-06-09 12:35:09.756193
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 12:35:10.199835
------------------------------------------------------
Started: 2025-06-09 14:17:44.962293
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 14:17:45.335246
------------------------------------------------------
Started: 2025-06-09 16:21:34.841037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 16:21:35.252025
------------------------------------------------------
Started: 2025-06-09 18:24:41.511163
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 18:24:41.895846
------------------------------------------------------
Started: 2025-06-09 20:18:53.510189
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 20:18:53.952318
------------------------------------------------------
Started: 2025-06-09 22:16:28.538421
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-09 22:16:28.910521
------------------------------------------------------
Started: 2025-06-10 01:21:30.772727
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 01:21:31.179626
------------------------------------------------------
Started: 2025-06-10 03:16:20.160407
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 03:16:20.567011
------------------------------------------------------
Started: 2025-06-10 04:28:53.156548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
Token length: 1144
Summarized using GPT-3.5-turbo
Append: [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
Token length: 754
Summarized using GPT-3.5-turbo
Append: [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
Token length: 1534
Summarized using GPT-3.5-turbo
Append: [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
Token length: 1202
Summarized using GPT-3.5-turbo
Append: [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
Token length: 1019
Summarized using GPT-3.5-turbo
Append: [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
Token length: 1127
Summarized using GPT-3.5-turbo
Append: [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
Token length: 1336
Summarized using GPT-3.5-turbo
Append: [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
Token length: 1897
Summarized using GPT-3.5-turbo
Append: [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
Token length: 1401
Summarized using GPT-3.5-turbo
Append: [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
Token length: 1320
Summarized using GPT-3.5-turbo
Append: [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
Token length: 901
Summarized using GPT-3.5-turbo
Append: [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
Token length: 1549
Summarized using GPT-3.5-turbo
Append: [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
Token length: 1244
Summarized using GPT-3.5-turbo
Append: [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
Token length: 1386
Summarized using GPT-3.5-turbo
Append: [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
Token length: 1226
Summarized using GPT-3.5-turbo
Append: [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
Token length: 1070
Summarized using GPT-3.5-turbo
Append: [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
Token length: 1291
Summarized using GPT-3.5-turbo
Append: [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
Token length: 1500
Summarized using GPT-3.5-turbo
Append: [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
Token length: 1178
Summarized using GPT-3.5-turbo
Append: [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
Token length: 1409
Summarized using GPT-3.5-turbo
Append: [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
Token length: 1427
Summarized using GPT-3.5-turbo
Append: [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
Token length: 1300
Summarized using GPT-3.5-turbo
Append: [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
Token length: 1292
Summarized using GPT-3.5-turbo
Append: [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
Token length: 1602
Summarized using GPT-3.5-turbo
Append: [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
Token length: 1242
Summarized using GPT-3.5-turbo
Append: [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
Token length: 675
Summarized using GPT-3.5-turbo
Append: [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
Token length: 1062
Summarized using GPT-3.5-turbo
Append: [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
Token length: 805
Summarized using GPT-3.5-turbo
Append: [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
Token length: 1085
Summarized using GPT-3.5-turbo
Append: [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
Token length: 1367
Summarized using GPT-3.5-turbo
Append: [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
Token length: 797
Summarized using GPT-3.5-turbo
Append: [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
Token length: 1117
Summarized using GPT-3.5-turbo
Append: [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
Append: [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
Append: [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
Append: [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
Append: [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
Append: [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
Append: [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
Append: [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
Append: [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
Append: [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
Append: [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
Append: [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
Append: [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
Append: [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
Append: [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
Append: [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
Append: [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
Append: [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
Append: [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
Append: [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
Append: [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
Append: [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
Append: [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
Append: [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Disserta\c{c}\~oes e Trabalhos de Gradua\c{c}\~ao em SI -- XXI Simp\'osio Brasileiro de Sistemas de Informa\c{c}\~ao](https://arxiv.org/abs/2506.07169)
Append: [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
Append: [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
Append: [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
Append: [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
Append: [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
Append: [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
Append: [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
Append: [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
Append: [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
Append: [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
Append: [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
Append: [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
Append: [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356)
Append: [SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation](https://arxiv.org/abs/2506.07423)
Append: [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
Append: [Conjoined Predication and Scalar Implicature](https://arxiv.org/abs/2506.07429)
Append: [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
Append: [LG-ANNA-Embedding technical report](https://arxiv.org/abs/2506.07438)
Append: [Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling](https://arxiv.org/abs/2506.07453)
Append: [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
Append: [From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered](https://arxiv.org/abs/2506.07461)
Append: [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
Append: [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/abs/2506.07479)
Append: [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
Append: [DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech](https://arxiv.org/abs/2506.07502)
Append: [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
Append: [DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction](https://arxiv.org/abs/2506.07510)
Append: [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/abs/2506.07523)
Append: [Bit-level BPE: Below the byte boundary](https://arxiv.org/abs/2506.07541)
Append: [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
Append: [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
Append: [Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque](https://arxiv.org/abs/2506.07597)
Append: [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
Append: [Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation](https://arxiv.org/abs/2506.07617)
Append: [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
Append: [Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation](https://arxiv.org/abs/2506.07626)
Append: [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
Append: [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
Append: [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
Append: [Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation](https://arxiv.org/abs/2506.07646)
Append: [Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping](https://arxiv.org/abs/2506.07658)
Append: [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
Append: [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
Append: [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
Append: [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
Append: [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
Append: [Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility](https://arxiv.org/abs/2506.07719)
Append: [Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU](https://arxiv.org/abs/2506.07726)
Append: [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
Append: [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
Append: [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
Append: [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://arxiv.org/abs/2506.07818)
Append: [Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning](https://arxiv.org/abs/2506.07851)
Append: [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
Append: [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
Append: [Quantum Graph Transformer for NLP Sentiment Classification](https://arxiv.org/abs/2506.07937)
Append: [Statistical Hypothesis Testing for Auditing Robustness in Language Models](https://arxiv.org/abs/2506.07947)
Append: [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
Append: [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
Append: [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
Append: [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
Append: [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
Append: [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
Append: [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
Append: [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
Append: [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
Append: [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
Append: [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
Append: [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
Append: [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
Append: [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
Append: [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
Append: [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
Append: [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
Append: [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
Append: [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
Append: [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
Append: [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
Append: [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
Append: [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
Append: [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
Append: [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
Append: [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
Append: [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
Append: [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
Append: [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
Append: [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
Append: [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
Append: [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
Append: [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
Append: [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
Append: [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
Append: [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
Append: [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
Append: [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
Append: [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
Append: [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
Append: [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
Append: [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
Append: [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
Append: [Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition](https://arxiv.org/abs/2506.07515)
Append: [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
Append: [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
Append: [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
Append: [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
Append: [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
Append: [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
Append: [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
Append: [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
Append: [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
Append: [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
Append: [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
Append: [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
Append: [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
Append: [$\tau^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
Append: [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
Append: [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
Append: [Highly Fast Text Segmentation With Pairwise Markov Chains](https://arxiv.org/abs/2102.11037)
Append: [ViMMRC 2.0 -- Enhancing Machine Reading Comprehension on Vietnamese Literature Text](https://arxiv.org/abs/2303.18162)
Append: [Rational Decision-Making Agent with Internalized Utility Judgment](https://arxiv.org/abs/2308.12519)
Append: [AfroBench: How Good are Large Language Models on African Languages?](https://arxiv.org/abs/2311.07978)
Append: [When Attention Collapses: How Degenerate Layers in LLMs Enable Smaller, Stronger Models](https://arxiv.org/abs/2404.08634)
Append: [Can Perplexity Predict Fine-tuning Performance? An Investigation of Tokenization Effects on Sequential Language Models for Nepali](https://arxiv.org/abs/2404.18071)
Append: [Knowledge-to-Jailbreak: Investigating Knowledge-driven Jailbreaking Attacks for Large Language Models](https://arxiv.org/abs/2406.11682)
Append: [ZeroDL: Zero-shot Distribution Learning for Text Clustering via Large Language Models](https://arxiv.org/abs/2406.13342)
Append: [AutoPal: Autonomous Adaptation to Users for Personal AI Companionship](https://arxiv.org/abs/2406.13960)
Append: [Modality-Specialized Synergizers for Interleaved Vision-Language Generalists](https://arxiv.org/abs/2407.03604)
Append: [Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts](https://arxiv.org/abs/2407.09590)
Append: [Cool-Fusion: Fuse Large Language Models without Training](https://arxiv.org/abs/2407.19807)
Append: [Synergizing Unsupervised Episode Detection with LLMs for Large-Scale News Events](https://arxiv.org/abs/2408.04873)
Append: [WeQA: A Benchmark for Retrieval Augmented Generation in Wind Energy Domain](https://arxiv.org/abs/2408.11800)
Append: [Open-FinLLMs: Open Multimodal Large Language Models for Financial Applications](https://arxiv.org/abs/2408.11878)
Append: [PECAN: LLM-Guided Dynamic Progress Control with Attention-Guided Hierarchical Weighted Graph for Long-Document QA](https://arxiv.org/abs/2410.04790)
Append: [TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training](https://arxiv.org/abs/2410.06511)
Append: [Efficient Pretraining Data Selection for Language Models via Multi-Actor Collaboration](https://arxiv.org/abs/2410.08102)
Append: [Assessing Dialect Fairness and Robustness of Large Language Models in Reasoning Tasks](https://arxiv.org/abs/2410.11005)
Append: [LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs](https://arxiv.org/abs/2410.14182)
Append: [SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment](https://arxiv.org/abs/2410.14676)
Append: [Diversity Explains Inference Scaling Laws: Through a Case Study of Minimum Bayes Risk Decoding](https://arxiv.org/abs/2410.15021)
Append: [Value Residual Learning](https://arxiv.org/abs/2410.17897)
Append: [Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks](https://arxiv.org/abs/2411.05361)
Append: [Epistemic Integrity in Large Language Models](https://arxiv.org/abs/2411.06528)
Append: [Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning](https://arxiv.org/abs/2411.17679)
Append: [MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache](https://arxiv.org/abs/2411.18077)
Append: [LLMs Can Simulate Standardized Patients via Agent Coevolution](https://arxiv.org/abs/2412.11716)
Append: [Evaluating Zero-Shot Multilingual Aspect-Based Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2412.12564)
Append: [DISC: Plug-and-Play Decoding Intervention with Similarity of Characters for Chinese Spelling Check](https://arxiv.org/abs/2412.12863)
Append: [Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models](https://arxiv.org/abs/2412.14133)
Append: [On Adversarial Robustness of Language Models in Transfer Learning](https://arxiv.org/abs/2501.00066)
Append: [Eliciting In-context Retrieval and Reasoning for Long-context Large Language Models](https://arxiv.org/abs/2501.08248)
Append: [ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models](https://arxiv.org/abs/2501.13397)
Append: [Unraveling Token Prediction Refinement and Identifying Essential Layers in Language Models](https://arxiv.org/abs/2501.15054)
Append: [Latent Thought Models with Variational Bayes Inference-Time Computation](https://arxiv.org/abs/2502.01567)
Append: [Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning](https://arxiv.org/abs/2502.01968)
Append: [Minerva: A Programmable Memory Test Benchmark for Language Models](https://arxiv.org/abs/2502.03358)
Append: [Sparse Autoencoders for Hypothesis Generation](https://arxiv.org/abs/2502.04382)
Append: [Retrieval-augmented Large Language Models for Financial Time Series Forecasting](https://arxiv.org/abs/2502.05878)
Append: [RomanLens: The Role Of Latent Romanization In Multilinguality In LLMs](https://arxiv.org/abs/2502.07424)
Append: [Measuring Diversity in Synthetic Datasets](https://arxiv.org/abs/2502.08512)
Append: [Gumbel Reranking: Differentiable End-to-End Reranker Optimization](https://arxiv.org/abs/2502.11116)
Append: [CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?](https://arxiv.org/abs/2502.11300)
Append: [Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More](https://arxiv.org/abs/2502.11494)
Append: [AdaSplash: Adaptive Sparse Flash Attention](https://arxiv.org/abs/2502.12082)
Append: [From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap for Text Length Control via MARKERGEN](https://arxiv.org/abs/2502.13544)
Append: [MMTEB: Massive Multilingual Text Embedding Benchmark](https://arxiv.org/abs/2502.13595)
Append: [Enhancing Input-Label Mapping in In-Context Learning with Contrastive Decoding](https://arxiv.org/abs/2502.13738)
Append: [From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions](https://arxiv.org/abs/2502.13791)
Append: [ParallelComp: Parallel Long-Context Compressor for Length Extrapolation](https://arxiv.org/abs/2502.14317)
Append: [Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis](https://arxiv.org/abs/2502.14767)
Append: [DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance](https://arxiv.org/abs/2502.16886)
Append: [NeoBERT: A Next-Generation BERT](https://arxiv.org/abs/2502.19587)
Append: [Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses in LLMs](https://arxiv.org/abs/2503.00979)
Append: [Examining the Mental Health Impact of Misinformation on Social Media Using a Hybrid Transformer-Based Approach](https://arxiv.org/abs/2503.02333)
Append: [Generalized Interpolating Discrete Diffusion](https://arxiv.org/abs/2503.04482)
Append: [Sentence-level Reward Model can Generalize Better for Aligning LLM from Human Preference](https://arxiv.org/abs/2503.04793)
Append: [RONA: Pragmatically Diverse Image Captioning with Coherence Relations](https://arxiv.org/abs/2503.10997)
Append: [nvBench 2.0: Resolving Ambiguity in Text-to-Visualization through Stepwise Reasoning](https://arxiv.org/abs/2503.12880)
Append: [Right Answer, Wrong Score: Uncovering the Inconsistencies of LLM Evaluation in Multiple-Choice Question Answering](https://arxiv.org/abs/2503.14996)
Append: [Imagine to Hear: Auditory Knowledge Generation can be an Effective Assistant for Language Models](https://arxiv.org/abs/2503.16853)
Append: [sudo rm -rf agentic_security](https://arxiv.org/abs/2503.20279)
Append: [ThinkEdit: Interpretable Weight Editing to Mitigate Overly Short Thinking in Reasoning Models](https://arxiv.org/abs/2503.22048)
Append: [Taxonomizing Representational Harms using Speech Act Theory](https://arxiv.org/abs/2504.00928)
Append: [Legal Mathematical Reasoning with LLMs: Procedural Alignment through Two-Stage Reinforcement Learning](https://arxiv.org/abs/2504.02590)
Append: [Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs](https://arxiv.org/abs/2504.04745)
Append: [LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models](https://arxiv.org/abs/2504.10415)
Append: [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)
Append: [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)
Append: [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)
Append: [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
Append: [Toward Reliable Scientific Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)
Append: [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)
Append: [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
Append: [Power-Law Decay Loss for Large Language Model Finetuning: A Theory Perspective](https://arxiv.org/abs/2505.16900)
Append: [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)
Append: [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)
Append: [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)
Append: [When Two LLMs Debate, Both Think They'll Win](https://arxiv.org/abs/2505.19184)
Append: [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)
Append: [APE: Selective Fine-tuning with Acceptance Criteria for Language Model Adaptation](https://arxiv.org/abs/2505.19912)
Append: [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
Append: [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)
Append: [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
Append: [Large Language Models Often Know When They Are Being Evaluated](https://arxiv.org/abs/2505.23836)
Append: [Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws](https://arxiv.org/abs/2505.24009)
Append: [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
Append: [SecFormer: Fast and Accurate Privacy-Preserving Inference for Transformer Models via SMPC](https://arxiv.org/abs/2401.00793)
Append: [Active Preference Optimization for Sample Efficient RLHF](https://arxiv.org/abs/2402.10500)
Append: [Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models](https://arxiv.org/abs/2403.20331)
Append: [Binary Classifier Optimization for Large Language Model Alignment](https://arxiv.org/abs/2404.04656)
Append: [JailbreakLens: Visual Analysis of Jailbreak Attacks Against Large Language Models](https://arxiv.org/abs/2404.08793)
Append: [Outlier-weighed Layerwise Sampling for LLM Fine-tuning](https://arxiv.org/abs/2405.18380)
Append: [Watermarking Language Models with Error Correcting Codes](https://arxiv.org/abs/2406.10281)
Append: [Data Shapley in One Training Run](https://arxiv.org/abs/2406.11011)
Append: [Is poisoning a real threat to LLM alignment? Maybe more so than you think](https://arxiv.org/abs/2406.12091)
Append: [Beyond Numeric Rewards: In-Context Dueling Bandits with LLM Agents](https://arxiv.org/abs/2407.01887)
Append: [Selective Prompt Anchoring for Code Generation](https://arxiv.org/abs/2408.09121)
Append: [HSF: Defending against Jailbreak Attacks with Hidden State Filtering](https://arxiv.org/abs/2409.03788)
Append: [RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking](https://arxiv.org/abs/2409.17458)
Append: [Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization](https://arxiv.org/abs/2409.18433)
Append: [Parameter-Efficient Fine-Tuning of State Space Models](https://arxiv.org/abs/2410.09016)
Append: [How Does DPO Reduce Toxicity? A Mechanistic Neuron-Level Analysis](https://arxiv.org/abs/2411.06424)
Append: [Unveiling and Addressing Pseudo Forgetting in Large Language Models](https://arxiv.org/abs/2411.11932)
Append: [Watermark under Fire: A Robustness Evaluation of LLM Watermarking](https://arxiv.org/abs/2411.13425)
Append: [Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features](https://arxiv.org/abs/2412.00142)
Append: [Semantic Exploration with Adaptive Gating for Efficient Problem Solving with Language Models](https://arxiv.org/abs/2501.05752)
Append: [Scalable Vision Language Model Training via High Quality Data Curation](https://arxiv.org/abs/2501.05952)
Append: [From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs](https://arxiv.org/abs/2501.16207)
Append: [Scaling Inference-Efficient Language Models](https://arxiv.org/abs/2501.18107)
Append: [BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning](https://arxiv.org/abs/2501.18858)
Append: [DeepRAG: Thinking to Retrieve Step by Step for Large Language Models](https://arxiv.org/abs/2502.01142)
Append: [Multi-agent Architecture Search via Agentic Supernet](https://arxiv.org/abs/2502.04180)
Append: [SyncMind: Measuring Agent Out-of-Sync Recovery in Collaborative Software Engineering](https://arxiv.org/abs/2502.06994)
Append: [Automated Capability Discovery via Foundation Model Self-Exploration](https://arxiv.org/abs/2502.07577)
Append: [When Incentives Backfire, Data Stops Being Human](https://arxiv.org/abs/2502.07732)
Append: [Theoretical Benefit and Limitation of Diffusion Language Model](https://arxiv.org/abs/2502.09622)
Append: [DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products](https://arxiv.org/abs/2502.10297)
Append: [PlanGenLLMs: A Modern Survey of LLM Planning Capabilities](https://arxiv.org/abs/2502.11221)
Append: [DISC: DISC: Dynamic Decomposition Improves LLM Inference Scaling](https://arxiv.org/abs/2502.16706)
Append: [Synthetic Text Generation for Training Large Language Models via Gradient Matching](https://arxiv.org/abs/2502.17607)
Append: [AMPO: Active Multi-Preference Optimization for Self-play Preference Selection](https://arxiv.org/abs/2502.18293)
Append: [PhantomWiki: On-Demand Datasets for Reasoning and Retrieval Evaluation](https://arxiv.org/abs/2502.20377)
Append: [EgoNormia: Benchmarking Physical Social Norm Understanding](https://arxiv.org/abs/2502.20490)
Append: [HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models](https://arxiv.org/abs/2502.20811)
Append: [LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement](https://arxiv.org/abs/2503.00493)
Append: [BRIDGE: Bootstrapping Text to Control Time-Series Generation via Multi-Agent Iterative Optimization and Diffusion Modeling](https://arxiv.org/abs/2503.02445)
Append: [Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs with Semantic Space](https://arxiv.org/abs/2503.11586)
Append: [FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA](https://arxiv.org/abs/2503.11880)
Append: [AdaReTaKe: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding](https://arxiv.org/abs/2503.12559)
Append: [EconEvals: Benchmarks and Litmus Tests for LLM Agents in Unknown Environments](https://arxiv.org/abs/2503.18825)
Append: [Exploring Training and Inference Scaling Laws in Generative Retrieval](https://arxiv.org/abs/2503.18941)
Append: [Representation Bending for Large Language Model Safety](https://arxiv.org/abs/2504.01550)
Append: [Skywork R1V: Pioneering Multimodal Reasoning with Chain-of-Thought](https://arxiv.org/abs/2504.05599)
Append: [MIB: A Mechanistic Interpretability Benchmark](https://arxiv.org/abs/2504.13151)
Append: [STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings](https://arxiv.org/abs/2504.13416)
Append: [A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment](https://arxiv.org/abs/2504.15585)
Append: [Robustifying Vision-Language Models via Dynamic Token Reweighting](https://arxiv.org/abs/2505.17132)
Append: [Attention with Trained Embeddings Provably Selects Important Tokens](https://arxiv.org/abs/2505.17282)
Append: [ChartGalaxy: A Dataset for Infographic Chart Understanding and Generation](https://arxiv.org/abs/2505.18668)
Append: [Can MLLMs Guide Me Home? A Benchmark Study on Fine-Grained Visual Reasoning from Transit Maps](https://arxiv.org/abs/2505.18675)
Append: [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
Append: [Scaling over Scaling: Exploring Test-Time Scaling Plateau in Large Reasoning Models](https://arxiv.org/abs/2505.20522)
Append: [AI Scientists Fail Without Strong Implementation Capability](https://arxiv.org/abs/2506.01372)
append_entries: 335
Finish: 2025-06-10 04:30:37.340953
------------------------------------------------------
Started: 2025-06-10 06:25:35.476474
Existing_entries: 1335
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race](https://arxiv.org/abs/2506.00253)
Token length: 1467
Summarized using GPT-3.5-turbo
Append: [SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions](https://arxiv.org/abs/2506.00643)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons](https://arxiv.org/abs/2506.00759)
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction](https://arxiv.org/abs/2506.00975)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
Token length: 932
Summarized using GPT-3.5-turbo
Append: [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)
Token length: 1460
Summarized using GPT-3.5-turbo
Append: [SATA: A Paradigm for LLM Jailbreak via Simple Assistive Task Linkage](https://arxiv.org/abs/2412.15289)
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Grounded Persuasive Language Generation for Automated Marketing](https://arxiv.org/abs/2502.16810)
Token length: 1071
Summarized using GPT-3.5-turbo
Append: [Comba: Improving Bilinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
Token length: 1013
Summarized using GPT-3.5-turbo
Append: [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
append_entries: 10
Finish: 2025-06-10 06:25:56.949819
------------------------------------------------------
Started: 2025-06-10 08:23:36.330405
Existing_entries: 1010
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 08:23:37.049459
------------------------------------------------------
Started: 2025-06-10 10:18:34.824332
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 10:18:35.504727
------------------------------------------------------
Started: 2025-06-10 12:35:26.193721
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 12:35:26.959875
------------------------------------------------------
Started: 2025-06-10 14:17:23.456915
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 14:17:24.164479
------------------------------------------------------
Started: 2025-06-10 16:21:39.126706
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 16:21:39.845226
------------------------------------------------------
Started: 2025-06-10 18:23:31.568342
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 18:23:32.310931
------------------------------------------------------
Started: 2025-06-10 20:19:01.714533
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 20:19:02.483642
------------------------------------------------------
Started: 2025-06-10 22:16:15.583122
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-10 22:16:16.287235
------------------------------------------------------
Started: 2025-06-11 01:21:18.317909
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 01:21:19.050825
------------------------------------------------------
Started: 2025-06-11 03:15:20.291069
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 03:15:20.966332
------------------------------------------------------
Started: 2025-06-11 04:32:29.737823
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 960
Summarized using GPT-3.5-turbo
Append: [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
Token length: 1928
Summarized using GPT-3.5-turbo
Append: [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
Token length: 1608
Summarized using GPT-3.5-turbo
Append: [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
Token length: 1150
Summarized using GPT-3.5-turbo
Append: [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
Token length: 1716
Summarized using GPT-3.5-turbo
Append: [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
Token length: 1659
Summarized using GPT-3.5-turbo
Append: [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
Token length: 1306
Summarized using GPT-3.5-turbo
Append: [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
Token length: 1711
Summarized using GPT-3.5-turbo
Append: [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
Token length: 1123
Summarized using GPT-3.5-turbo
Append: [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
Token length: 1330
Summarized using GPT-3.5-turbo
Append: [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
Token length: 1588
Summarized using GPT-3.5-turbo
Append: [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
Token length: 1287
Summarized using GPT-3.5-turbo
Append: [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
Token length: 1092
Summarized using GPT-3.5-turbo
Append: [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
Token length: 1339
Summarized using GPT-3.5-turbo
Append: [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
Token length: 1050
Summarized using GPT-3.5-turbo
Append: [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
Token length: 1916
Summarized using GPT-3.5-turbo
Append: [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
Token length: 1209
Summarized using GPT-3.5-turbo
Append: [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
Token length: 1277
Summarized using GPT-3.5-turbo
Append: [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
Token length: 1170
Summarized using GPT-3.5-turbo
Append: [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
Token length: 1509
Summarized using GPT-3.5-turbo
Append: [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
Token length: 1477
Summarized using GPT-3.5-turbo
Append: [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
Token length: 1272
Summarized using GPT-3.5-turbo
Append: [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
Token length: 744
Summarized using GPT-3.5-turbo
Append: [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
Token length: 1639
Summarized using GPT-3.5-turbo
Append: [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
Token length: 800
Summarized using GPT-3.5-turbo
Append: [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
Token length: 1168
Summarized using GPT-3.5-turbo
Append: [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
Token length: 1095
Summarized using GPT-3.5-turbo
Append: [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
Token length: 805
Summarized using GPT-3.5-turbo
Append: [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
Token length: 1346
Summarized using GPT-3.5-turbo
Append: [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
Token length: 1917
Summarized using GPT-3.5-turbo
Append: [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
Token length: 1411
Summarized using GPT-3.5-turbo
Append: [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
Token length: 1036
Summarized using GPT-3.5-turbo
Append: [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
Token length: 1066
Summarized using GPT-3.5-turbo
Append: [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
Token length: 774
Summarized using GPT-3.5-turbo
Append: [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
Token length: 1179
Summarized using GPT-3.5-turbo
Append: [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
Token length: 1348
Summarized using GPT-3.5-turbo
Append: [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
Token length: 718
Summarized using GPT-3.5-turbo
Append: [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
Token length: 1560
Summarized using GPT-3.5-turbo
Append: [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
Token length: 1129
Summarized using GPT-3.5-turbo
Append: [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
Token length: 1145
Summarized using GPT-3.5-turbo
Append: [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
Token length: 1118
Summarized using GPT-3.5-turbo
Append: [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
Append: [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
Append: [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
Append: [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
Append: [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
Append: [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
Append: [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
Append: [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
Append: [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
Append: [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
Append: [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
Append: [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
Append: [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
Append: [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
Append: [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
Append: [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
Append: [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
Append: [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
Append: [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
Append: [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
Append: [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
Append: [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
Append: [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
Append: [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
Append: [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
Append: [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
Append: [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
Append: [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
Append: [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
Append: [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
Append: [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
Append: [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
Append: [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
Append: [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
Append: [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
Append: [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
Append: [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
Append: [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
Append: [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
Append: [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
Append: [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
Append: [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
Append: [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
Append: [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
Append: [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
Append: [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
Append: [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
Append: [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
Append: [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
Append: [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
Append: [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
Append: [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
Append: [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
Append: [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
Append: [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
Append: [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
Append: [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
Append: [A Decomposition-Based Approach for Evaluating and Analyzing Inter-Annotator Disagreement](https://arxiv.org/abs/2206.05446)
Append: [A Survey on Long Text Modeling with Transformers](https://arxiv.org/abs/2302.14502)
Append: [Cross-lingual Transfer in Programming Languages: An Extensive Empirical Study](https://arxiv.org/abs/2310.16937)
Append: [Poro 34B and the Blessing of Multilinguality](https://arxiv.org/abs/2404.01856)
Append: [P-React: Synthesizing Topic-Adaptive Reactions of Personality Traits via Mixture of Specialized LoRA Experts](https://arxiv.org/abs/2406.12548)
Append: [SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs](https://arxiv.org/abs/2406.19593)
Append: [High-Throughput Phenotyping of Clinical Text Using Large Language Models](https://arxiv.org/abs/2408.01214)
Append: [Exploring SSL Discrete Speech Features for Zipformer-based Contextual ASR](https://arxiv.org/abs/2409.08797)
Append: [Guidelines for Fine-grained Sentence-level Arabic Readability Annotation](https://arxiv.org/abs/2410.08674)
Append: [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/abs/2410.15639)
Append: [FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs](https://arxiv.org/abs/2410.19317)
Append: [SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script](https://arxiv.org/abs/2410.20682)
Append: [Length-Induced Embedding Collapse in PLM-based Models](https://arxiv.org/abs/2410.24200)
Append: [The BS-meter: A ChatGPT-Trained Instrument to Detect Sloppy Language-Games](https://arxiv.org/abs/2411.15129)
Append: [From Language Models over Tokens to Language Models over Characters](https://arxiv.org/abs/2412.03719)
Append: [JuStRank: Benchmarking LLM Judges for System Ranking](https://arxiv.org/abs/2412.09569)
Append: [Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence](https://arxiv.org/abs/2412.13949)
Append: [Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models](https://arxiv.org/abs/2502.02444)
Append: [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/abs/2502.02958)
Append: [LLM Alignment as Retriever Optimization: An Information Retrieval Perspective](https://arxiv.org/abs/2502.03699)
Append: [In Praise of Stubbornness: An Empirical Case for Cognitive-Dissonance Aware Continual Update of Knowledge in LLMs](https://arxiv.org/abs/2502.04390)
Append: [Accelerating LLM Inference with Lossless Speculative Decoding Algorithms for Heterogeneous Vocabularies](https://arxiv.org/abs/2502.05202)
Append: [R.R.: Unveiling LLM Training Privacy through Recollection and Ranking](https://arxiv.org/abs/2502.12658)
Append: [Retrieval-augmented systems can be dangerous medical communicators](https://arxiv.org/abs/2502.14898)
Append: [Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews](https://arxiv.org/abs/2502.15226)
Append: [Self-Training Elicits Concise Reasoning in Large Language Models](https://arxiv.org/abs/2502.20122)
Append: [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/abs/2503.04556)
Append: [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/abs/2503.10354)
Append: [Enhancing Arabic Automated Essay Scoring with Synthetic Data and Error Injection](https://arxiv.org/abs/2503.17739)
Append: [Summarizing Speech: A Comprehensive Survey](https://arxiv.org/abs/2504.08024)
Append: [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)
Append: [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)
Append: [Value Portrait: Assessing Language Models' Values through Psychometrically and Ecologically Valid Items](https://arxiv.org/abs/2505.01015)
Append: [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)
Append: [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)
Append: [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
Append: [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)
Append: [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
Append: [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)
Append: [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)
Append: [CASPER: A Large Scale Spontaneous Speech Dataset](https://arxiv.org/abs/2506.00267)
Append: [AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation](https://arxiv.org/abs/2506.00551)
Append: [DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments](https://arxiv.org/abs/2506.00739)
Append: [TL;DR: Too Long, Do Re-weighting for Efficient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
Append: [Speech Synthesis By Unrolling Diffusion Process using Neural Network Layers](https://arxiv.org/abs/2309.09652)
Append: [Exploring the Escalation of Source Bias in User, Data, and Recommender System Feedback Loop](https://arxiv.org/abs/2405.17998)
Append: [Textual Unlearning Gives a False Sense of Unlearning](https://arxiv.org/abs/2406.13348)
Append: [Human-like object concept representations emerge naturally in multimodal large language models](https://arxiv.org/abs/2407.01067)
Append: [How transformers learn structured data: insights from hierarchical filtering](https://arxiv.org/abs/2408.15138)
Append: [TPP-LLM: Modeling Temporal Point Processes by Efficiently Fine-Tuning Large Language Models](https://arxiv.org/abs/2410.02062)
Append: [NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples](https://arxiv.org/abs/2410.14669)
Append: [xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs](https://arxiv.org/abs/2410.16267)
Append: [Phonology-Guided Speech-to-Speech Translation for African Languages](https://arxiv.org/abs/2410.23323)
Append: [DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models](https://arxiv.org/abs/2411.03250)
Append: [PrisonBreak: Jailbreaking Large Language Models with Fewer Than Twenty-Five Targeted Bit-flips](https://arxiv.org/abs/2412.07192)
Append: [RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation](https://arxiv.org/abs/2501.08617)
Append: [TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation](https://arxiv.org/abs/2502.07306)
Append: [Self-Supervised Transformers as Iterative Solution Improvers for Constraint Satisfaction](https://arxiv.org/abs/2502.15794)
Append: ["Would You Want an AI Tutor?" Understanding Stakeholder Perceptions of LLM-based Systems in the Classroom](https://arxiv.org/abs/2503.02885)
Append: [Big Help or Big Brother? Auditing Tracking, Profiling, and Personalization in Generative AI Assistants](https://arxiv.org/abs/2503.16586)
Append: [Understanding Bias Reinforcement in LLM Agents Debate](https://arxiv.org/abs/2503.16814)
Append: [Quamba2: A Robust and Scalable Post-training Quantization Framework for Selective State Space Models](https://arxiv.org/abs/2503.22879)
Append: [Towards Reliable Proof Generation with LLMs: A Neuro-Symbolic Approach](https://arxiv.org/abs/2505.14479)
Append: [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
Append: [From Output to Evaluation: Does Raw Instruction-Tuned Code LLMs Output Suffice for Fill-in-the-Middle Code Generation?](https://arxiv.org/abs/2505.18789)
Append: [Optimized Text Embedding Models and Benchmarks for Amharic Passage Retrieval](https://arxiv.org/abs/2505.19356)
Append: [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
append_entries: 173
Finish: 2025-06-11 04:38:59.537315
------------------------------------------------------
Started: 2025-06-11 06:25:19.660076
Existing_entries: 1173
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1281
Summarized using GPT-3.5-turbo
Append: [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
Token length: 1586
Summarized using GPT-3.5-turbo
Append: [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
append_entries: 2
Finish: 2025-06-11 06:25:25.586577
------------------------------------------------------
Started: 2025-06-11 08:22:47.650704
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 08:22:48.046564
------------------------------------------------------
Started: 2025-06-11 10:18:28.392346
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 10:18:28.823092
------------------------------------------------------
Started: 2025-06-11 12:35:18.226380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 12:35:18.629053
------------------------------------------------------
Started: 2025-06-11 14:16:02.974704
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 14:16:03.423820
------------------------------------------------------
Started: 2025-06-11 16:22:22.608987
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 16:22:23.052565
------------------------------------------------------
Started: 2025-06-11 18:24:00.194775
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 18:24:00.622549
------------------------------------------------------
Started: 2025-06-11 20:17:28.100501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 20:17:28.517716
------------------------------------------------------
Started: 2025-06-11 22:16:10.217761
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-11 22:16:10.694314
------------------------------------------------------
Started: 2025-06-12 01:19:59.842415
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 01:20:00.242887
------------------------------------------------------
Started: 2025-06-12 03:13:59.449133
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 03:13:59.858371
------------------------------------------------------
Started: 2025-06-12 04:27:32.553389
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1233
Summarized using GPT-3.5-turbo
Append: [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
Token length: 907
Summarized using GPT-3.5-turbo
Append: [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
Token length: 1733
Summarized using GPT-3.5-turbo
Append: [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
Token length: 1068
Summarized using GPT-3.5-turbo
Append: [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
Token length: 1082
Summarized using GPT-3.5-turbo
Append: [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
Token length: 1004
Summarized using GPT-3.5-turbo
Append: [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
Token length: 1962
Summarized using GPT-3.5-turbo
Append: [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
Token length: 1394
Summarized using GPT-3.5-turbo
Append: [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
Token length: 518
Summarized using GPT-3.5-turbo
Append: [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
Token length: 1018
Summarized using GPT-3.5-turbo
Append: [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
Token length: 1136
Summarized using GPT-3.5-turbo
Append: [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
Token length: 1135
Summarized using GPT-3.5-turbo
Append: [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
Token length: 1943
Summarized using GPT-3.5-turbo
Append: [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
Token length: 1366
Summarized using GPT-3.5-turbo
Append: [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
Token length: 966
Summarized using GPT-3.5-turbo
Append: [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
Token length: 1098
Summarized using GPT-3.5-turbo
Append: [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
Token length: 1677
Summarized using GPT-3.5-turbo
Append: [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
Token length: 1940
Summarized using GPT-3.5-turbo
Append: [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
Token length: 1768
Summarized using GPT-3.5-turbo
Append: [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
Token length: 1695
Summarized using GPT-3.5-turbo
Append: [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
Token length: 1107
Summarized using GPT-3.5-turbo
Append: [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
Token length: 1203
Summarized using GPT-3.5-turbo
Append: [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
Token length: 985
Summarized using GPT-3.5-turbo
Append: [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
Token length: 1335
Summarized using GPT-3.5-turbo
Append: [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
Token length: 1652
Summarized using GPT-3.5-turbo
Append: [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
Token length: 1016
Summarized using GPT-3.5-turbo
Append: [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
Token length: 1031
Summarized using GPT-3.5-turbo
Append: [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
Token length: 1851
Summarized using GPT-3.5-turbo
Append: [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
Token length: 1094
Summarized using GPT-3.5-turbo
Append: [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
Token length: 1403
Summarized using GPT-3.5-turbo
Append: [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
Token length: 1535
Summarized using GPT-3.5-turbo
Append: [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
Token length: 1574
Summarized using GPT-3.5-turbo
Append: [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
Token length: 1342
Summarized using GPT-3.5-turbo
Append: [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
Token length: 1552
Summarized using GPT-3.5-turbo
Append: [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
Append: [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
Append: [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
Append: [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
Append: [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
Append: [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
Append: [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
Append: [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
Append: [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
Append: [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
Append: [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
Append: [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
Append: [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
Append: [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
Append: [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
Append: [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
Append: [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
Append: [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
Append: [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
Append: [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
Append: [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
Append: [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
Append: [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
Append: [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
Append: [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
Append: [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
Append: [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
Append: [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
Append: [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
Append: [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
Append: [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
Append: [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
Append: [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
Append: [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
Append: [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
Append: [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
Append: [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
Append: [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
Append: [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
Append: [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
Append: [AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes](https://arxiv.org/abs/2305.14725)
Append: [DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing](https://arxiv.org/abs/2402.16733)
Append: [Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2404.01129)
Append: [Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR](https://arxiv.org/abs/2405.14259)
Append: [Language Models Resist Alignment: Evidence From Data Compression](https://arxiv.org/abs/2406.06144)
Append: [Standard Language Ideology in AI-Generated Language](https://arxiv.org/abs/2406.08726)
Append: [Raising the Bar: Investigating the Values of Large Language Models via Generative Evolving Testing](https://arxiv.org/abs/2406.14230)
Append: [CaLMQA: Exploring culturally specific long-form question answering across 23 languages](https://arxiv.org/abs/2406.17761)
Append: [CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses](https://arxiv.org/abs/2407.13329)
Append: [MMREC: LLM Based Multi-Modal Recommender System](https://arxiv.org/abs/2408.04211)
Append: [LogProber: Disentangling confidence from contamination in LLM responses](https://arxiv.org/abs/2408.14352)
Append: [Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic](https://arxiv.org/abs/2408.16326)
Append: [Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models](https://arxiv.org/abs/2409.00598)
Append: [MOSAIC: Multiple Observers Spotting AI Content](https://arxiv.org/abs/2409.07615)
Append: [Explaining word embeddings with perfect fidelity: Case study in research impact prediction](https://arxiv.org/abs/2409.15912)
Append: [GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment](https://arxiv.org/abs/2410.08193)
Append: [How Do Multilingual Language Models Remember Facts?](https://arxiv.org/abs/2410.14387)
Append: [Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models](https://arxiv.org/abs/2410.17131)
Append: [Code-Switching Curriculum Learning for Multilingual Transfer in LLMs](https://arxiv.org/abs/2411.02460)
Append: [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/abs/2411.12768)
Append: [Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning](https://arxiv.org/abs/2411.17304)
Append: [Retrofitting Large Language Models with Dynamic Tokenization](https://arxiv.org/abs/2411.18553)
Append: [Steps are all you need: Rethinking STEM Education with Prompt Engineering](https://arxiv.org/abs/2412.05023)
Append: [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/abs/2412.05342)
Append: [Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering](https://arxiv.org/abs/2412.05453)
Append: [7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/abs/2412.06845)
Append: [Irony Detection, Reasoning and Understanding in Zero-shot Learning](https://arxiv.org/abs/2501.16884)
Append: [Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](https://arxiv.org/abs/2502.16033)
Append: [Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation](https://arxiv.org/abs/2502.19830)
Append: [AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification](https://arxiv.org/abs/2503.01940)
Append: [Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization](https://arxiv.org/abs/2503.02450)
Append: [Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering](https://arxiv.org/abs/2503.11314)
Append: [MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering](https://arxiv.org/abs/2503.18491)
Append: [Style over Substance: Distilled Language Models Reason Via Stylistic Replication](https://arxiv.org/abs/2504.01738)
Append: [One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image](https://arxiv.org/abs/2504.02132)
Append: [Assessment of Evolving Large Language Models in Upper Secondary Mathematics](https://arxiv.org/abs/2504.12347)
Append: [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)
Append: [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)
Append: [Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective](https://arxiv.org/abs/2505.07859)
Append: [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)
Append: [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
Append: [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)
Append: [Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis](https://arxiv.org/abs/2505.24593)
Append: [Writing-Zero: Bridge the Gap Between Non-verifiable Tasks and Verifiable Rewards](https://arxiv.org/abs/2506.00103)
Append: [LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech](https://arxiv.org/abs/2506.00628)
Append: [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/abs/2506.01687)
Append: [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
Append: [Listen, Chat, and Remix: Text-Guided Soundscape Remixing for Enhanced Auditory Experience](https://arxiv.org/abs/2402.03710)
Append: [Using Shapley interactions to understand how models use structure](https://arxiv.org/abs/2403.13106)
Append: [LLM2TEA: Agentic AI Designer Finds Innovative Objects with Generative Evolutionary Multitasking](https://arxiv.org/abs/2406.14917)
Append: [Code-Switching Red-Teaming: LLM Evaluation for Safety and Multilingual Understanding](https://arxiv.org/abs/2406.15481)
Append: [The Remarkable Robustness of LLMs: Stages of Inference?](https://arxiv.org/abs/2406.19384)
Append: [AcTracer: Active Testing of Large Language Model via Multi-Stage Sampling](https://arxiv.org/abs/2408.03573)
Append: [EMMA: Efficient Visual Alignment in Multi-Modal LLMs](https://arxiv.org/abs/2410.02080)
Append: [Beyond Bradley-Terry Models: A General Preference Model for Language Model Alignment](https://arxiv.org/abs/2410.02197)
Append: [MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning](https://arxiv.org/abs/2411.12977)
Append: [Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey](https://arxiv.org/abs/2412.20367)
Append: [ICONS: Influence Consensus for Vision-Language Data Selection](https://arxiv.org/abs/2501.00654)
Append: [Reasoning Language Models: A Blueprint](https://arxiv.org/abs/2501.11223)
Append: [Trustworthy AI: Safety, Bias, and Privacy -- A Survey](https://arxiv.org/abs/2502.10450)
Append: [Rethinking Diverse Human Preference Learning through Principal Component Analysis](https://arxiv.org/abs/2502.13131)
Append: [AAD-LLM: Neural Attention-Driven Auditory Scene Understanding](https://arxiv.org/abs/2502.16794)
Append: [ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2502.19409)
Append: [Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis](https://arxiv.org/abs/2502.20383)
Append: [Chem42: a Family of chemical Language Models for Target-aware Ligand Generation](https://arxiv.org/abs/2503.16563)
Append: [CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction](https://arxiv.org/abs/2504.15629)
Append: [Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection](https://arxiv.org/abs/2504.17834)
Append: [OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)
append_entries: 157
Finish: 2025-06-12 04:29:23.772729
------------------------------------------------------
Started: 2025-06-12 06:25:52.034641
Existing_entries: 1157
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1151
Summarized using GPT-3.5-turbo
Append: [Rethinking Text-based Protein Understanding: Retrieval or LLM?](https://arxiv.org/abs/2505.20354)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)
append_entries: 2
Finish: 2025-06-12 06:25:56.133874
------------------------------------------------------
Started: 2025-06-12 08:22:53.691038
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 08:22:54.072592
------------------------------------------------------
Started: 2025-06-12 10:18:51.706568
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 10:18:52.109448
------------------------------------------------------
Started: 2025-06-12 12:34:37.667645
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 12:34:38.098023
------------------------------------------------------
Started: 2025-06-12 14:17:08.463940
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 14:17:08.868055
------------------------------------------------------
Started: 2025-06-12 16:20:57.527784
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 16:20:57.993118
------------------------------------------------------
Started: 2025-06-12 18:23:15.910733
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 18:23:16.309032
------------------------------------------------------
Started: 2025-06-12 20:18:56.719447
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 20:18:57.129318
------------------------------------------------------
Started: 2025-06-12 22:16:03.857411
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-12 22:16:04.278322
------------------------------------------------------
Started: 2025-06-13 01:21:19.173822
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 01:21:19.593838
------------------------------------------------------
Started: 2025-06-13 03:15:36.098798
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 03:15:36.542134
------------------------------------------------------
Started: 2025-06-13 04:28:38.654655
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [A Survey of Automatic Evaluation Methods on Text, Visual and Speech Generations](https://arxiv.org/abs/2506.10019)
Token length: 987
Summarized using GPT-3.5-turbo
Append: [TaskCraft: Automated Generation of Agentic Tasks](https://arxiv.org/abs/2506.10055)
Token length: 1850
Summarized using GPT-3.5-turbo
Append: [A quantum semantic framework for natural language processing](https://arxiv.org/abs/2506.10077)
Token length: 898
Summarized using GPT-3.5-turbo
Append: [Chat-of-Thought: Collaborative Multi-Agent System for Generating Domain Specific Information](https://arxiv.org/abs/2506.10086)
Token length: 779
Summarized using GPT-3.5-turbo
Append: [When Meaning Stays the Same, but Models Drift: Evaluating Quality of Service under Token-Level Behavioral Instability in LLMs](https://arxiv.org/abs/2506.10095)
Token length: 1699
Summarized using GPT-3.5-turbo
Append: [ChartReasoner: Code-Driven Modality Bridging for Long-Chain Reasoning in Chart Question Answering](https://arxiv.org/abs/2506.10116)
Token length: 1164
Summarized using GPT-3.5-turbo
Append: [Unsupervised Elicitation of Language Models](https://arxiv.org/abs/2506.10139)
Token length: 1343
Summarized using GPT-3.5-turbo
Append: [When Large Language Models are Reliable for Judging Empathic Communication](https://arxiv.org/abs/2506.10150)
Token length: 968
Summarized using GPT-3.5-turbo
Append: [Analyzing Emotions in Bangla Social Media Comments Using Machine Learning and LIME](https://arxiv.org/abs/2506.10154)
Token length: 1057
Summarized using GPT-3.5-turbo
Append: [Measuring Corporate Human Capital Disclosures: Lexicon, Data, Code, and Research Opportunities](https://arxiv.org/abs/2506.10155)
Token length: 1358
Summarized using GPT-3.5-turbo
Append: [Can LLMs Generate Good Stories? Insights and Challenges from a Narrative Planning Perspective](https://arxiv.org/abs/2506.10161)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval](https://arxiv.org/abs/2506.10202)
Token length: 1524
Summarized using GPT-3.5-turbo
Append: [TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and Novel Tic-Tac-Toe-style Games](https://arxiv.org/abs/2506.10209)
Token length: 1235
Summarized using GPT-3.5-turbo
Append: [Classifying Unreliable Narrators with Large Language Models](https://arxiv.org/abs/2506.10231)
Token length: 1141
Summarized using GPT-3.5-turbo
Append: [ToxSyn-PT: A Large-Scale Synthetic Dataset for Hate Speech Detection in Portuguese](https://arxiv.org/abs/2506.10245)
Token length: 1589
Summarized using GPT-3.5-turbo
Append: [Do Language Models Have Bayesian Brains? Distinguishing Stochastic and Deterministic Decision Patterns within Large Language Models](https://arxiv.org/abs/2506.10268)
Token length: 1289
Summarized using GPT-3.5-turbo
Append: [ClusterUCB: Efficient Gradient-Based Data Selection for Targeted Fine-Tuning of LLMs](https://arxiv.org/abs/2506.10288)
Token length: 1929
Summarized using GPT-3.5-turbo
Append: [Flick: Few Labels Text Classification using K-Aware Intermediate Learning in Multi-Task Low-Resource Languages](https://arxiv.org/abs/2506.10292)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: ["Check My Work?": Measuring Sycophancy in a Simulated Educational Context](https://arxiv.org/abs/2506.10297)
Token length: 1002
Summarized using GPT-3.5-turbo
Append: [Scheduled Interleaved Speech-Text Training for Speech-to-Speech Translation with LLMs](https://arxiv.org/abs/2506.10299)
Token length: 1053
Summarized using GPT-3.5-turbo
Append: [Code Execution as Grounded Supervision for LLM Reasoning](https://arxiv.org/abs/2506.10343)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [TableRAG: A Retrieval Augmented Generation Framework for Heterogeneous Document Reasoning](https://arxiv.org/abs/2506.10380)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [PAG: Multi-Turn Reinforced LLM Self-Correction with Policy as Generative Verifier](https://arxiv.org/abs/2506.10406)
Token length: 827
Summarized using GPT-3.5-turbo
Append: [Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?](https://arxiv.org/abs/2506.10415)
Token length: 1033
Summarized using GPT-3.5-turbo
Append: [Beyond the Battlefield: Framing Analysis of Media Coverage in Conflict Reporting](https://arxiv.org/abs/2506.10421)
Token length: 1317
Summarized using GPT-3.5-turbo
Append: [Fast on the Easy, Deep on the Hard: Efficient Reasoning via Powered Length Penalty](https://arxiv.org/abs/2506.10446)
Token length: 1099
Summarized using GPT-3.5-turbo
Append: [Table-Text Alignment: Explaining Claim Verification Against Tables in Scientific Papers](https://arxiv.org/abs/2506.10486)
Token length: 1119
Summarized using GPT-3.5-turbo
Append: [Surface Fairness, Deep Bias: A Comparative Study of Bias in Language Models](https://arxiv.org/abs/2506.10491)
Token length: 1162
Summarized using GPT-3.5-turbo
Append: [Beyond Single-User Dialogue: Assessing Multi-User Dialogue State Tracking Capabilities of Large Language Models](https://arxiv.org/abs/2506.10504)
Token length: 1578
Summarized using GPT-3.5-turbo
Append: [Reliable Reasoning Path: Distilling Effective Guidance for LLM Reasoning with Knowledge Graphs](https://arxiv.org/abs/2506.10508)
Token length: 942
Summarized using GPT-3.5-turbo
Append: [Unsupervised Protoform Reconstruction through Parsimonious Rule-guided Heuristics and Evolutionary Search](https://arxiv.org/abs/2506.10614)
Token length: 910
Summarized using GPT-3.5-turbo
Append: [SDialog: A Python Toolkit for Synthetic Dialogue Generation and Analysis](https://arxiv.org/abs/2506.10622)
Token length: 1149
Summarized using GPT-3.5-turbo
Append: [NeuralNexus at BEA 2025 Shared Task: Retrieval-Augmented Prompting for Mistake Identification in AI Tutors](https://arxiv.org/abs/2506.10627)
Token length: 1009
Summarized using GPT-3.5-turbo
Append: [Spelling-out is not Straightforward: LLMs' Capability of Tokenization from Token to Characters](https://arxiv.org/abs/2506.10641)
Token length: 1254
Summarized using GPT-3.5-turbo
Append: [Large Language Models for Detection of Life-Threatening Texts](https://arxiv.org/abs/2506.10687)
Token length: 618
Summarized using GPT-3.5-turbo
Append: [Inferring Adjective Hypernyms with Language Models to Increase the Connectivity of Open English Wordnet](https://arxiv.org/abs/2506.10715)
Token length: 1363
Summarized using GPT-3.5-turbo
Append: [PREMISE: Scalable and Strategic Prompt Optimization for Efficient Mathematical Reasoning in Large Models](https://arxiv.org/abs/2506.10716)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Beyond True or False: Retrieval-Augmented Hierarchical Analysis of Nuanced Claims](https://arxiv.org/abs/2506.10728)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora](https://arxiv.org/abs/2506.10737)
Token length: 1388
Summarized using GPT-3.5-turbo
Append: [One Tokenizer To Rule Them All: Emergent Language Plasticity via Multilingual Tokenizers](https://arxiv.org/abs/2506.10766)
Token length: 977
Summarized using GPT-3.5-turbo
Append: [Different Questions, Different Models: Fine-Grained Evaluation of Uncertainty and Calibration in Clinical QA with LLMs](https://arxiv.org/abs/2506.10769)
Token length: 1023
Summarized using GPT-3.5-turbo
Append: [Improving Named Entity Transcription with Contextual LLM-based Revision](https://arxiv.org/abs/2506.10779)
Token length: 1568
Summarized using GPT-3.5-turbo
Append: [Mitigating Negative Interference in Multilingual Sequential Knowledge Editing through Null-Space Constraints](https://arxiv.org/abs/2506.10800)
Token length: 1468
Summarized using GPT-3.5-turbo
Append: [ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization](https://arxiv.org/abs/2506.10822)
Token length: 678
Summarized using GPT-3.5-turbo
Append: [CIIR@LiveRAG 2025: Optimizing Multi-Agent Retrieval Augmented Generation through Self-Training](https://arxiv.org/abs/2506.10844)
Token length: 1307
Summarized using GPT-3.5-turbo
Append: [Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles](https://arxiv.org/abs/2506.10848)
Token length: 936
Summarized using GPT-3.5-turbo
Append: [Analyzing the relationships between pretraining language, phonetic, tonal, and speaker information in self-supervised speech models](https://arxiv.org/abs/2506.10855)
Token length: 1185
Summarized using GPT-3.5-turbo
Append: [Enhancing Medical Dialogue Generation through Knowledge Refinement and Dynamic Prompt Adjustment](https://arxiv.org/abs/2506.10877)
Token length: 807
Summarized using GPT-3.5-turbo
Append: [Slimming Down LLMs Without Losing Their Minds](https://arxiv.org/abs/2506.10885)
Token length: 1690
Summarized using GPT-3.5-turbo
Append: [Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers](https://arxiv.org/abs/2506.10887)
Append: [BioClinical ModernBERT: A State-of-the-Art Long-Context Encoder for Biomedical and Clinical NLP](https://arxiv.org/abs/2506.10896)
Append: [Beyond Gold Standards: Epistemic Ensemble of LLM Judges for Formal Mathematical Reasoning](https://arxiv.org/abs/2506.10903)
Append: [Magistral](https://arxiv.org/abs/2506.10910)
Append: [Decomposing MLP Activations into Interpretable Features via Semi-Nonnegative Matrix Factorization](https://arxiv.org/abs/2506.10920)
Append: [Dynamic Epistemic Friction in Dialogue](https://arxiv.org/abs/2506.10934)
Append: [Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture without Training](https://arxiv.org/abs/2506.10952)
Append: [ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark](https://arxiv.org/abs/2506.10960)
Append: [AutoMind: Adaptive Knowledgeable Agent for Automated Data Science](https://arxiv.org/abs/2506.10974)
Append: [How Well Can Reasoning Models Identify and Recover from Unhelpful Thoughts?](https://arxiv.org/abs/2506.10979)
Append: [Multimodal Cinematic Video Synthesis Using Text-to-Image and Audio Generation Models](https://arxiv.org/abs/2506.10005)
Append: [Multimodal Large Language Models: A Survey](https://arxiv.org/abs/2506.10016)
Append: [From Threat to Tool: Leveraging Refusal-Aware Injection Attacks for Safety Alignment](https://arxiv.org/abs/2506.10020)
Append: [Private Memorization Editing: Turning Memorization into a Defense to Strengthen Data Privacy in Large Language Models](https://arxiv.org/abs/2506.10024)
Append: [Evaluation empirique de la s\'ecurisation et de l'alignement de ChatGPT et Gemini: analyse comparative des vuln\'erabilit\'es par exp\'erimentations de jailbreaks](https://arxiv.org/abs/2506.10029)
Append: [GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models](https://arxiv.org/abs/2506.10047)
Append: [Omni-DPO: A Dual-Perspective Paradigm for Dynamic Preference Learning of LLMs](https://arxiv.org/abs/2506.10054)
Append: [One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence](https://arxiv.org/abs/2506.10157)
Append: [Disclosure Audits for LLM Agents](https://arxiv.org/abs/2506.10171)
Append: [Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods](https://arxiv.org/abs/2506.10236)
Append: [Discrete Audio Tokens: More Than a Survey!](https://arxiv.org/abs/2506.10274)
Append: [AC/DC: LLM-based Audio Comprehension via Dialogue Continuation](https://arxiv.org/abs/2506.10312)
Append: [Detecting Sockpuppetry on Wikipedia Using Meta-Learning](https://arxiv.org/abs/2506.10314)
Append: [Provably Learning from Language Feedback](https://arxiv.org/abs/2506.10341)
Append: [An Analysis of Datasets, Metrics and Models in Keyphrase Generation](https://arxiv.org/abs/2506.10346)
Append: [Can We Infer Confidential Properties of Training Data from LLMs?](https://arxiv.org/abs/2506.10364)
Append: [Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning](https://arxiv.org/abs/2506.10378)
Append: [Time-IMM: A Dataset and Benchmark for Irregular Multimodal Multivariate Time Series](https://arxiv.org/abs/2506.10412)
Append: [PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs](https://arxiv.org/abs/2506.10423)
Append: [Towards Robust Multimodal Emotion Recognition under Missing Modalities and Distribution Shifts](https://arxiv.org/abs/2506.10452)
Append: [Scientists' First Exam: Probing Cognitive Abilities of MLLM via Perception, Understanding, and Reasoning](https://arxiv.org/abs/2506.10521)
Append: [Encoding call-by-push-value in the pi-calculus](https://arxiv.org/abs/2506.10584)
Append: [Deep Learning-Based Digitization of Overlapping ECG Images with Open-Source Python Code](https://arxiv.org/abs/2506.10617)
Append: [Conversational Search: From Fundamentals to Frontiers in the LLM Era](https://arxiv.org/abs/2506.10635)
Append: [Robust Unsupervised Adaptation of a Speech Recogniser Using Entropy Minimisation and Speaker Codes](https://arxiv.org/abs/2506.10653)
Append: [TeleMath: A Benchmark for Large Language Models in Telecom Mathematical Problem Solving](https://arxiv.org/abs/2506.10674)
Append: [Neural at ArchEHR-QA 2025: Agentic Prompt Optimization for Evidence-Grounded Clinical Question Answering](https://arxiv.org/abs/2506.10751)
Append: [FASCIST-O-METER: Classifier for Neo-fascist Discourse Online](https://arxiv.org/abs/2506.10789)
Append: [VideoDeepResearch: Long Video Understanding With Agentic Tool Using](https://arxiv.org/abs/2506.10821)
Append: [The Diffusion Duality](https://arxiv.org/abs/2506.10892)
Append: [Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?](https://arxiv.org/abs/2506.10912)
Append: [Robustly Improving LLM Fairness in Realistic Settings via Interpretability](https://arxiv.org/abs/2506.10922)
Append: [VINCIE: Unlocking In-context Image Editing from Video](https://arxiv.org/abs/2506.10941)
Append: [GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models](https://arxiv.org/abs/2506.10946)
Append: [Build the web for agents, not agents for the web](https://arxiv.org/abs/2506.10953)
Append: [MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning](https://arxiv.org/abs/2506.10963)
Append: [ConvD: Attention Enhanced Dynamic Convolutional Embeddings for Knowledge Graph Completion](https://arxiv.org/abs/2312.07589)
Append: [Weak-to-Strong Jailbreaking on Large Language Models](https://arxiv.org/abs/2401.17256)
Append: [Visually Descriptive Language Model for Vector Graphics Reasoning](https://arxiv.org/abs/2404.06479)
Append: [IndoToxic2024: A Demographically-Enriched Dataset of Hate Speech and Toxicity Types for Indonesian Language](https://arxiv.org/abs/2406.19349)
Append: [Benchmarking LLMs for Environmental Review and Permitting](https://arxiv.org/abs/2407.07321)
Append: [Multi-group Uncertainty Quantification for Long-form Text Generation](https://arxiv.org/abs/2407.21057)
Append: [SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language Models](https://arxiv.org/abs/2408.08545)
Append: [The Fellowship of the LLMs: Multi-Agent Workflows for Synthetic Preference Optimization Dataset Generation](https://arxiv.org/abs/2408.08688)
Append: [Paired Completion: Flexible Quantification of Issue-framing at Scale with LLMs](https://arxiv.org/abs/2408.09742)
Append: [Efficient Length-Generalizable Attention via Causal Retrieval for Long-Context Language Modeling](https://arxiv.org/abs/2410.01651)
Append: [Efficiently Identifying Watermarked Segments in Mixed-Source Texts](https://arxiv.org/abs/2410.03600)
Append: [Persistent Topological Features in Large Language Models](https://arxiv.org/abs/2410.11042)
Append: [On Many-Shot In-Context Learning for Long-Context Evaluation](https://arxiv.org/abs/2411.07130)
Append: [Squeezed Attention: Accelerating Long Context Length LLM Inference](https://arxiv.org/abs/2411.09688)
Append: [Prompt-based Depth Pruning of Large Language Models](https://arxiv.org/abs/2502.04348)
Append: [Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges](https://arxiv.org/abs/2502.12378)
Append: [BeamLoRA: Beam-Constraint Low-Rank Adaptation](https://arxiv.org/abs/2502.13604)
Append: [Measuring Chain of Thought Faithfulness by Unlearning Reasoning Steps](https://arxiv.org/abs/2502.14829)
Append: [Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models](https://arxiv.org/abs/2502.15010)
Append: [Mind the Style Gap: Meta-Evaluation of Style and Attribute Transfer Metrics](https://arxiv.org/abs/2502.15022)
Append: [The Esethu Framework: Reimagining Sustainable Dataset Governance and Curation for Low-Resource Languages](https://arxiv.org/abs/2502.15916)
Append: [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/abs/2502.19148)
Append: [Large Language Models for Multilingual Previously Fact-Checked Claim Detection](https://arxiv.org/abs/2503.02737)
Append: [Improving LLM Safety Alignment with Dual-Objective Optimization](https://arxiv.org/abs/2503.03710)
Append: [Social Bias Benchmark for Generation: A Comparison of Generation and QA-Based Evaluations](https://arxiv.org/abs/2503.06987)
Append: [Large Language Models for Outpatient Referral: Problem Definition, Benchmarking and Challenges](https://arxiv.org/abs/2503.08292)
Append: [Computation Mechanism Behind LLM Position Generalization](https://arxiv.org/abs/2503.13305)
Append: [PLAY2PROMPT: Zero-shot Tool Instruction Optimization for LLM Agents via Tool Play](https://arxiv.org/abs/2503.14432)
Append: [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)
Append: [IPA-CHILDES & G2P+: Feature-Rich Resources for Cross-Lingual Phonology and Phonemic Language Modeling](https://arxiv.org/abs/2504.03036)
Append: [BabyLM's First Words: Word Segmentation as a Phonological Probing Task](https://arxiv.org/abs/2504.03338)
Append: [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)
Append: [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)
Append: [Sailing by the Stars: A Survey on Reward Models and Learning Strategies for Learning from Rewards](https://arxiv.org/abs/2505.02686)
Append: [Research Borderlands: Analysing Writing Across Research Cultures](https://arxiv.org/abs/2506.00784)
Append: [SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models](https://arxiv.org/abs/2506.01062)
Append: [iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering](https://arxiv.org/abs/2506.01784)
Append: [Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance](https://arxiv.org/abs/2402.08680)
Append: [PRSA: Prompt Stealing Attacks against Real-World Prompt Services](https://arxiv.org/abs/2402.19200)
Append: [Failure Modes of LLMs for Causal Reasoning on Narratives](https://arxiv.org/abs/2410.23884)
Append: [Debiasing Watermarks for Large Language Models via Maximal Coupling](https://arxiv.org/abs/2411.11203)
Append: [Great Models Think Alike and this Undermines AI Oversight](https://arxiv.org/abs/2502.04313)
Append: [A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce](https://arxiv.org/abs/2504.11343)
Append: [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
Append: [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
Append: [The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and Transactions in Consumer Markets](https://arxiv.org/abs/2506.00073)
Append: [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
Append: [CHANCERY: Evaluating Corporate Governance Reasoning Capabilities in Language Models](https://arxiv.org/abs/2506.04636)
Append: [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
append_entries: 144
Finish: 2025-06-13 04:30:25.623282
------------------------------------------------------
Started: 2025-06-13 06:25:30.616398
Existing_entries: 1144
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1014
Summarized using GPT-3.5-turbo
Append: [Identifying Reliable Evaluation Metrics for Scientific Text Revision](https://arxiv.org/abs/2506.04772)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [Context Is Not Comprehension](https://arxiv.org/abs/2506.04907)
append_entries: 2
Finish: 2025-06-13 06:25:34.925491
------------------------------------------------------
Started: 2025-06-13 08:22:45.594802
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 08:22:45.952744
------------------------------------------------------
Started: 2025-06-13 10:18:31.793981
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 10:18:32.267309
------------------------------------------------------
Started: 2025-06-13 12:34:29.461747
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 12:34:29.825573
------------------------------------------------------
Started: 2025-06-13 14:16:45.536597
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 14:16:45.872708
------------------------------------------------------
Started: 2025-06-13 16:21:07.790554
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 16:21:08.144450
------------------------------------------------------
Started: 2025-06-13 18:23:35.792284
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 18:23:36.158667
------------------------------------------------------
Started: 2025-06-13 20:18:25.304133
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 20:18:25.731964
------------------------------------------------------
Started: 2025-06-13 22:16:00.474079
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-13 22:16:00.820197
------------------------------------------------------
Started: 2025-06-14 01:18:05.558142
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 01:18:05.904606
------------------------------------------------------
Started: 2025-06-14 03:09:46.767517
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 03:09:47.119832
------------------------------------------------------
Started: 2025-06-14 04:20:28.613627
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 04:20:28.676882
------------------------------------------------------
Started: 2025-06-14 06:22:13.792303
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 06:22:13.855355
------------------------------------------------------
Started: 2025-06-14 08:19:47.543123
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 08:19:47.617712
------------------------------------------------------
Started: 2025-06-14 10:16:20.968631
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 10:16:21.026051
------------------------------------------------------
Started: 2025-06-14 12:31:03.004511
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 12:31:03.109193
------------------------------------------------------
Started: 2025-06-14 14:14:01.968645
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 14:14:02.030481
------------------------------------------------------
Started: 2025-06-14 16:19:04.997302
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 16:19:05.074649
------------------------------------------------------
Started: 2025-06-14 18:20:36.348469
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 18:20:36.410831
------------------------------------------------------
Started: 2025-06-14 20:16:53.502558
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 20:16:53.613041
------------------------------------------------------
Started: 2025-06-14 22:15:26.618558
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-14 22:15:26.678122
------------------------------------------------------
Started: 2025-06-15 01:37:02.032821
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 01:37:02.093431
------------------------------------------------------
Started: 2025-06-15 03:22:05.100443
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 03:22:05.184858
------------------------------------------------------
Started: 2025-06-15 04:26:28.488459
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 04:26:28.545407
------------------------------------------------------
Started: 2025-06-15 06:22:20.947827
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 06:22:21.021354
------------------------------------------------------
Started: 2025-06-15 08:19:52.354240
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 08:19:52.413587
------------------------------------------------------
Started: 2025-06-15 10:17:30.061432
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 10:17:30.131830
------------------------------------------------------
Started: 2025-06-15 12:31:24.917332
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 12:31:24.977664
------------------------------------------------------
Started: 2025-06-15 14:14:06.731638
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 14:14:06.786544
------------------------------------------------------
Started: 2025-06-15 16:19:08.420063
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 16:19:08.508015
------------------------------------------------------
Started: 2025-06-15 18:21:16.252466
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 18:21:16.344721
------------------------------------------------------
Started: 2025-06-15 20:17:23.065907
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 20:17:23.123213
------------------------------------------------------
Started: 2025-06-15 22:15:17.539911
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-15 22:15:17.602232
------------------------------------------------------
Started: 2025-06-16 01:23:30.845755
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 01:23:30.917893
------------------------------------------------------
Started: 2025-06-16 03:19:45.643135
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 03:19:45.743046
------------------------------------------------------
Started: 2025-06-16 04:32:45.645366
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1826
Summarized using GPT-3.5-turbo
Append: [TeleEval-OS: Performance evaluations of large language models for operations scheduling](https://arxiv.org/abs/2506.11017)
Token length: 1448
Summarized using GPT-3.5-turbo
Append: [Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2506.11063)
Token length: 858
Summarized using GPT-3.5-turbo
Append: [Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study](https://arxiv.org/abs/2506.11065)
Token length: 1195
Summarized using GPT-3.5-turbo
Append: [A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes](https://arxiv.org/abs/2506.11067)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models](https://arxiv.org/abs/2506.11068)
Token length: 1291
Summarized using GPT-3.5-turbo
Append: [Targeted control of fast prototyping through domain-specific interface](https://arxiv.org/abs/2506.11070)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention](https://arxiv.org/abs/2506.11073)
Token length: 1597
Summarized using GPT-3.5-turbo
Append: [CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling](https://arxiv.org/abs/2506.11077)
Token length: 1536
Summarized using GPT-3.5-turbo
Append: [RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs](https://arxiv.org/abs/2506.11078)
Token length: 1405
Summarized using GPT-3.5-turbo
Append: [MANBench: Is Your Multimodal Model Smarter than Human?](https://arxiv.org/abs/2506.11080)
Token length: 1459
Summarized using GPT-3.5-turbo
Append: [SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs](https://arxiv.org/abs/2506.11081)
Token length: 1351
Summarized using GPT-3.5-turbo
Append: [PRISM: A Transformer-based Language Model of Structured Clinical Event Data](https://arxiv.org/abs/2506.11082)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [RedDebate: Safer Responses through Multi-Agent Red Teaming Debates](https://arxiv.org/abs/2506.11083)
Token length: 1159
Summarized using GPT-3.5-turbo
Append: [Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing](https://arxiv.org/abs/2506.11088)
Token length: 978
Summarized using GPT-3.5-turbo
Append: [Customizing Speech Recognition Model with Large Language Model Feedback](https://arxiv.org/abs/2506.11091)
Token length: 1227
Summarized using GPT-3.5-turbo
Append: [Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation](https://arxiv.org/abs/2506.11092)
Token length: 1925
Summarized using GPT-3.5-turbo
Append: [The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs](https://arxiv.org/abs/2506.11094)
Token length: 1167
Summarized using GPT-3.5-turbo
Append: [Persistent Homology of Topic Networks for the Prediction of Reader Curiosity](https://arxiv.org/abs/2506.11095)
Token length: 1744
Summarized using GPT-3.5-turbo
Append: [C-SEO Bench: Does Conversational SEO Work?](https://arxiv.org/abs/2506.11097)
Token length: 1583
Summarized using GPT-3.5-turbo
Append: [Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey](https://arxiv.org/abs/2506.11102)
Token length: 1653
Summarized using GPT-3.5-turbo
Append: [You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model](https://arxiv.org/abs/2506.11103)
Token length: 1224
Summarized using GPT-3.5-turbo
Append: [DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration](https://arxiv.org/abs/2506.11104)
Token length: 1022
Summarized using GPT-3.5-turbo
Append: [Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation](https://arxiv.org/abs/2506.11105)
Token length: 1381
Summarized using GPT-3.5-turbo
Append: [Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking](https://arxiv.org/abs/2506.11106)
Token length: 848
Summarized using GPT-3.5-turbo
Append: [History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM](https://arxiv.org/abs/2506.11108)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization](https://arxiv.org/abs/2506.11109)
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models](https://arxiv.org/abs/2506.11110)
Token length: 1858
Summarized using GPT-3.5-turbo
Append: [Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions](https://arxiv.org/abs/2506.11111)
Token length: 619
Summarized using GPT-3.5-turbo
Append: [Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)](https://arxiv.org/abs/2506.11112)
Token length: 1131
Summarized using GPT-3.5-turbo
Append: [Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](https://arxiv.org/abs/2506.11113)
Token length: 1369
Summarized using GPT-3.5-turbo
Append: [KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations](https://arxiv.org/abs/2506.11114)
Token length: 1214
Summarized using GPT-3.5-turbo
Append: [Incorporating Domain Knowledge into Materials Tokenization](https://arxiv.org/abs/2506.11115)
Token length: 1582
Summarized using GPT-3.5-turbo
Append: [Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models](https://arxiv.org/abs/2506.11116)
Token length: 1733
Summarized using GPT-3.5-turbo
Append: [ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research](https://arxiv.org/abs/2506.11117)
Token length: 1747
Summarized using GPT-3.5-turbo
Append: [Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech](https://arxiv.org/abs/2506.11119)
Token length: 1321
Summarized using GPT-3.5-turbo
Append: [SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models](https://arxiv.org/abs/2506.11120)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR](https://arxiv.org/abs/2506.11121)
Token length: 1191
Summarized using GPT-3.5-turbo
Append: [ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams](https://arxiv.org/abs/2506.11125)
Token length: 1322
Summarized using GPT-3.5-turbo
Append: [GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions](https://arxiv.org/abs/2506.11127)
Token length: 1474
Summarized using GPT-3.5-turbo
Append: [Stronger Language Models Produce More Human-Like Errors](https://arxiv.org/abs/2506.11128)
Token length: 1109
Summarized using GPT-3.5-turbo
Append: [Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK](https://arxiv.org/abs/2506.11129)
Token length: 1051
Summarized using GPT-3.5-turbo
Append: [A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](https://arxiv.org/abs/2506.11130)
Token length: 765
Summarized using GPT-3.5-turbo
Append: [Large Language Models and Emergence: A Complex Systems Perspective](https://arxiv.org/abs/2506.11135)
Token length: 1862
Summarized using GPT-3.5-turbo
Append: [Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models](https://arxiv.org/abs/2506.11137)
Token length: 1184
Summarized using GPT-3.5-turbo
Append: [RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?](https://arxiv.org/abs/2506.11243)
Token length: 999
Summarized using GPT-3.5-turbo
Append: [Iterative Multilingual Spectral Attribute Erasure](https://arxiv.org/abs/2506.11244)
Token length: 1275
Summarized using GPT-3.5-turbo
Append: [No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning](https://arxiv.org/abs/2506.11246)
Token length: 1263
Summarized using GPT-3.5-turbo
Append: [Learning a Continue-Thinking Token for Enhanced Test-Time Scaling](https://arxiv.org/abs/2506.11274)
Token length: 1165
Summarized using GPT-3.5-turbo
Append: [Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning](https://arxiv.org/abs/2506.11300)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Don't Pay Attention](https://arxiv.org/abs/2506.11305)
Append: [Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly](https://arxiv.org/abs/2506.11338)
Append: [From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review](https://arxiv.org/abs/2506.11343)
Append: [Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models](https://arxiv.org/abs/2506.11344)
Append: [The Biased Samaritan: LLM biases in Perceived Kindness](https://arxiv.org/abs/2506.11361)
Append: [A Variational Approach for Mitigating Entity Bias in Relation Extraction](https://arxiv.org/abs/2506.11381)
Append: [Curriculum-Guided Layer Scaling for Language Model Pretraining](https://arxiv.org/abs/2506.11389)
Append: [Predicting Early-Onset Colorectal Cancer with Large Language Models](https://arxiv.org/abs/2506.11410)
Append: [Efficient Long-Context LLM Inference via KV Cache Clustering](https://arxiv.org/abs/2506.11418)
Append: [Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards](https://arxiv.org/abs/2506.11425)
Append: [KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models](https://arxiv.org/abs/2506.11432)
Append: [AbsenceBench: Language Models Can't Tell What's Missing](https://arxiv.org/abs/2506.11440)
Append: [A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems](https://arxiv.org/abs/2506.11467)
Append: [Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards](https://arxiv.org/abs/2506.11474)
Append: [ImmunoFOMO: Are Language Models missing what oncologists see?](https://arxiv.org/abs/2506.11478)
Append: [Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models](https://arxiv.org/abs/2506.11485)
Append: [Lag-Relative Sparse Attention In Long Context Training](https://arxiv.org/abs/2506.11498)
Append: [On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval](https://arxiv.org/abs/2506.11499)
Append: [From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation](https://arxiv.org/abs/2506.11557)
Append: [Are LLMs Good Text Diacritizers? An Arabic and Yor\`ub\'a Case Study](https://arxiv.org/abs/2506.11602)
Append: [SceneGram: Conceptualizing and Describing Tangrams in Scene Context](https://arxiv.org/abs/2506.11631)
Append: [LoRA-Gen: Specializing Large Language Model via Online LoRA Generation](https://arxiv.org/abs/2506.11638)
Append: [Converting Annotated Clinical Cases into Structured Case Report Forms](https://arxiv.org/abs/2506.11666)
Append: [Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE](https://arxiv.org/abs/2506.11673)
Append: [LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach](https://arxiv.org/abs/2506.11681)
Append: [Configurable Preference Tuning with Rubric-Guided Synthetic Data](https://arxiv.org/abs/2506.11702)
Append: [The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference](https://arxiv.org/abs/2506.11728)
Append: [DART: Distilling Autoregressive Reasoning to Silent Thought](https://arxiv.org/abs/2506.11752)
Append: [DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents](https://arxiv.org/abs/2506.11763)
Append: [Long-Short Alignment for Effective Long-Context Modeling in LLMs](https://arxiv.org/abs/2506.11769)
Append: [Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models](https://arxiv.org/abs/2506.11798)
Append: [Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?](https://arxiv.org/abs/2506.11807)
Append: [Post Persona Alignment for Multi-Session Dialogue Generation](https://arxiv.org/abs/2506.11857)
Append: [Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache](https://arxiv.org/abs/2506.11886)
Append: [GeistBERT: Breathing Life into German NLP](https://arxiv.org/abs/2506.11903)
Append: [Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study](https://arxiv.org/abs/2506.11919)
Append: [Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback](https://arxiv.org/abs/2506.11930)
Append: [Improving Large Language Model Safety with Contrastive Representation Learning](https://arxiv.org/abs/2506.11938)
Append: [code_transformed: The Influence of Large Language Models on Code](https://arxiv.org/abs/2506.12014)
Append: [Developing a Dyslexia Indicator Using Eye Tracking](https://arxiv.org/abs/2506.11004)
Append: [A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects](https://arxiv.org/abs/2506.11012)
Append: [Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox](https://arxiv.org/abs/2506.11022)
Append: [Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models](https://arxiv.org/abs/2506.11031)
Append: [CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.11034)
Append: [Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity](https://arxiv.org/abs/2506.11035)
Append: [Large Language models for Time Series Analysis: Techniques, Applications, and Challenges](https://arxiv.org/abs/2506.11040)
Append: [CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs](https://arxiv.org/abs/2506.11059)
Append: [PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding](https://arxiv.org/abs/2506.11064)
Append: [Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition](https://arxiv.org/abs/2506.11069)
Append: [Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling](https://arxiv.org/abs/2506.11072)
Append: [Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts](https://arxiv.org/abs/2506.11079)
Append: [LeanExplore: A search engine for Lean 4 declarations](https://arxiv.org/abs/2506.11085)
Append: [ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models](https://arxiv.org/abs/2506.11087)
Append: [Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM](https://arxiv.org/abs/2506.11089)
Append: [Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting](https://arxiv.org/abs/2506.11096)
Append: [Knowledge Graph Embeddings with Representing Relations as Annular Sectors](https://arxiv.org/abs/2506.11099)
Append: [LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic](https://arxiv.org/abs/2506.11221)
Append: [LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation](https://arxiv.org/abs/2506.11237)
Append: [GLAP: General contrastive audio-text pretraining across domains and languages](https://arxiv.org/abs/2506.11350)
Append: [Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables](https://arxiv.org/abs/2506.11375)
Append: [Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning](https://arxiv.org/abs/2506.11376)
Append: [LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model](https://arxiv.org/abs/2506.11402)
Append: [Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs](https://arxiv.org/abs/2506.11415)
Append: [AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction](https://arxiv.org/abs/2506.11475)
Append: [Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs](https://arxiv.org/abs/2506.11515)
Append: [Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning](https://arxiv.org/abs/2506.11516)
Append: [RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning](https://arxiv.org/abs/2506.11555)
Append: [DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs](https://arxiv.org/abs/2506.11558)
Append: [VLM@school -- Evaluation of AI image understanding on German middle school knowledge](https://arxiv.org/abs/2506.11604)
Append: [(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test](https://arxiv.org/abs/2506.11620)
Append: [Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model](https://arxiv.org/abs/2506.11737)
Append: [On the Performance of LLMs for Real Estate Appraisal](https://arxiv.org/abs/2506.11812)
Append: [Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation](https://arxiv.org/abs/2506.11820)
Append: [Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment](https://arxiv.org/abs/2506.11880)
Append: [Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making](https://arxiv.org/abs/2506.11887)
Append: [TreeRL: LLM Reinforcement Learning with On-Policy Tree Search](https://arxiv.org/abs/2506.11902)
Append: [LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?](https://arxiv.org/abs/2506.11928)
Append: [Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task](https://arxiv.org/abs/2506.11986)
Append: [VGR: Visual Grounded Reasoning](https://arxiv.org/abs/2506.11991)
Append: [Generative Representational Learning of Foundation Models for Recommendation](https://arxiv.org/abs/2506.11999)
Append: [FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference](https://arxiv.org/abs/2405.04065)
Append: [JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models](https://arxiv.org/abs/2406.02050)
Append: [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)
Append: [TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models](https://arxiv.org/abs/2410.20445)
Append: [Deep Sparse Latent Feature Models for Knowledge Graph Completion](https://arxiv.org/abs/2411.15694)
Append: [MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets](https://arxiv.org/abs/2412.21015)
Append: [Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia Reflect on the Cross-Cultural Sociolinguistic Norms?](https://arxiv.org/abs/2501.03479)
Append: [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)
Append: [PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling](https://arxiv.org/abs/2502.01925)
Append: [TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages](https://arxiv.org/abs/2502.11020)
Append: [Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis](https://arxiv.org/abs/2502.11812)
Append: [Conformal Linguistic Calibration: Trading-off between Factuality and Specificity](https://arxiv.org/abs/2502.19110)
Append: [MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis](https://arxiv.org/abs/2502.19175)
Append: [How Much is Enough? The Diminishing Returns of Tokenization Training Data](https://arxiv.org/abs/2502.20273)
Append: [PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts](https://arxiv.org/abs/2503.06706)
Append: [Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts](https://arxiv.org/abs/2503.09347)
Append: [LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2503.21227)
Append: [Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)
Append: [D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model](https://arxiv.org/abs/2504.13439)
Append: [Long-context Non-factoid Question Answering in Indic Languages](https://arxiv.org/abs/2504.13615)
Append: [Understanding the Repeat Curse in Large Language Models from a Feature Perspective](https://arxiv.org/abs/2504.14218)
Append: [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)
Append: [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
Append: [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
Append: [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)
Append: [RSCF: Relation-Semantics Consistent Filter for Entity Embedding of Knowledge Graph](https://arxiv.org/abs/2505.20813)
Append: [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
Append: [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)
Append: [Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics](https://arxiv.org/abs/2506.00637)
Append: [VM14K: First Vietnamese Medical Benchmark](https://arxiv.org/abs/2506.01305)
Append: [Word Sense Detection Leveraging Maximum Mean Discrepancy](https://arxiv.org/abs/2506.01602)
Append: [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
Append: [MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions](https://arxiv.org/abs/2403.07910)
Append: [Ad Auctions for LLMs via Retrieval Augmented Generation](https://arxiv.org/abs/2406.09459)
Append: [An overview of domain-specific foundation model: key technologies, applications and challenges](https://arxiv.org/abs/2409.04267)
Append: [Jointly modelling the evolution of social structure and language in online communities](https://arxiv.org/abs/2409.19243)
Append: [Glider: Global and Local Instruction-Driven Expert Router](https://arxiv.org/abs/2410.07172)
Append: [Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts](https://arxiv.org/abs/2410.14375)
Append: [Transferable Post-training via Inverse Value Learning](https://arxiv.org/abs/2410.21027)
Append: [Entropy Controllable Direct Preference Optimization](https://arxiv.org/abs/2411.07595)
Append: [T1: Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling](https://arxiv.org/abs/2501.11651)
Append: [Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation](https://arxiv.org/abs/2501.18638)
Append: [Vision-Language Models for Edge Networks: A Comprehensive Survey](https://arxiv.org/abs/2502.07855)
Append: [ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness](https://arxiv.org/abs/2504.10514)
Append: [Enhancing multimodal analogical reasoning with Logic Augmented Generation](https://arxiv.org/abs/2504.11190)
Append: [FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on Technical Documents](https://arxiv.org/abs/2504.13128)
Append: [Attention Retrieves, MLP Memorizes: Disentangling Trainable Components in the Transformer](https://arxiv.org/abs/2506.01115)
Append: [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
Append: [Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model](https://arxiv.org/abs/2506.04518)
append_entries: 178
Finish: 2025-06-16 04:34:20.513055
------------------------------------------------------
Started: 2025-06-16 06:27:03.985428
Existing_entries: 1178
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [Cartridges: Lightweight and general-purpose long context representations via self-study](https://arxiv.org/abs/2506.06266)
append_entries: 1
Finish: 2025-06-16 06:27:06.303503
------------------------------------------------------
Started: 2025-06-16 08:24:18.714703
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 08:24:19.131393
------------------------------------------------------
Started: 2025-06-16 10:19:09.325759
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 10:19:09.725996
------------------------------------------------------
Started: 2025-06-16 12:35:49.948389
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 12:35:50.384859
------------------------------------------------------
Started: 2025-06-16 14:17:27.475282
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 14:17:27.907819
------------------------------------------------------
Started: 2025-06-16 16:21:24.333953
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 16:21:24.726644
------------------------------------------------------
Started: 2025-06-16 18:24:12.772896
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 18:24:13.171804
------------------------------------------------------
Started: 2025-06-16 20:18:39.371803
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 20:18:39.817317
------------------------------------------------------
Started: 2025-06-16 22:16:22.466165
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-16 22:16:22.857164
------------------------------------------------------
Started: 2025-06-17 01:21:19.824989
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 01:21:20.225266
------------------------------------------------------
Started: 2025-06-17 03:15:42.039790
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 03:15:42.447316
------------------------------------------------------
Started: 2025-06-17 04:28:58.198309
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1261
Summarized using GPT-3.5-turbo
Append: [Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading](https://arxiv.org/abs/2506.12066)
Token length: 1472
Summarized using GPT-3.5-turbo
Append: [ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour](https://arxiv.org/abs/2506.12090)
Token length: 1181
Summarized using GPT-3.5-turbo
Append: [Continuously Updating Digital Twins using Large Language Models](https://arxiv.org/abs/2506.12091)
Token length: 1368
Summarized using GPT-3.5-turbo
Append: [Enhancing Traffic Accident Classifications: Application of NLP Methods for City Safety](https://arxiv.org/abs/2506.12092)
Token length: 902
Summarized using GPT-3.5-turbo
Append: [UCD: Unlearning in LLMs via Contrastive Decoding](https://arxiv.org/abs/2506.12097)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Personalized LLM Decoding via Contrasting Personal Preference](https://arxiv.org/abs/2506.12109)
Token length: 1879
Summarized using GPT-3.5-turbo
Append: [Eliciting Reasoning in Language Models with Cognitive Tools](https://arxiv.org/abs/2506.12115)
Token length: 1259
Summarized using GPT-3.5-turbo
Append: [Unsupervised Document and Template Clustering using Multimodal Embeddings](https://arxiv.org/abs/2506.12116)
Token length: 1361
Summarized using GPT-3.5-turbo
Append: [Can Mixture-of-Experts Surpass Dense LLMs Under Strictly Equal Resources?](https://arxiv.org/abs/2506.12119)
Token length: 793
Summarized using GPT-3.5-turbo
Append: [Hatevolution: What Static Benchmarks Don't Tell Us](https://arxiv.org/abs/2506.12148)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Maximally-Informative Retrieval for State Space Model Generation](https://arxiv.org/abs/2506.12149)
Token length: 1258
Summarized using GPT-3.5-turbo
Append: [A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource Languages](https://arxiv.org/abs/2506.12158)
Token length: 1177
Summarized using GPT-3.5-turbo
Append: [Instruction Tuning and CoT Prompting for Contextual Medical QA with LLMs](https://arxiv.org/abs/2506.12182)
Token length: 1418
Summarized using GPT-3.5-turbo
Append: [Supernova Event Dataset: Interpreting Large Language Model's Personality through Critical Event Analysis](https://arxiv.org/abs/2506.12189)
Token length: 1601
Summarized using GPT-3.5-turbo
Append: [Infini-gram mini: Exact n-gram Search at the Internet Scale with FM-Index](https://arxiv.org/abs/2506.12229)
Token length: 1823
Summarized using GPT-3.5-turbo
Append: [Large Language Models for History, Philosophy, and Sociology of Science: Interpretive Uses, Methodological Challenges, and Critical Perspectives](https://arxiv.org/abs/2506.12242)
Token length: 1395
Summarized using GPT-3.5-turbo
Append: [The Behavior Gap: Evaluating Zero-shot LLM Agents in Complex Task-Oriented Dialogs](https://arxiv.org/abs/2506.12266)
Token length: 1406
Summarized using GPT-3.5-turbo
Append: [Med-U1: Incentivizing Unified Medical Reasoning in LLMs via Large-scale Reinforcement Learning](https://arxiv.org/abs/2506.12307)
Token length: 1056
Summarized using GPT-3.5-turbo
Append: [Phonikud: Hebrew Grapheme-to-Phoneme Conversion for Real-Time Text-to-Speech](https://arxiv.org/abs/2506.12311)
Token length: 752
Summarized using GPT-3.5-turbo
Append: [Intersectional Bias in Japanese Large Language Models from a Contextualized Perspective](https://arxiv.org/abs/2506.12327)
Token length: 1072
Summarized using GPT-3.5-turbo
Append: [Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs](https://arxiv.org/abs/2506.12338)
Token length: 1221
Summarized using GPT-3.5-turbo
Append: [Refract ICL: Rethinking Example Selection in the Era of Million-Token Models](https://arxiv.org/abs/2506.12346)
Token length: 1731
Summarized using GPT-3.5-turbo
Append: [Efficient Reasoning Through Suppression of Self-Affirmation Reflections in Large Reasoning Models](https://arxiv.org/abs/2506.12353)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics](https://arxiv.org/abs/2506.12365)
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [Understanding the Effect of Knowledge Graph Extraction Error on Downstream Graph Analyses: A Case Study on Affiliation Graphs](https://arxiv.org/abs/2506.12367)
Token length: 1194
Summarized using GPT-3.5-turbo
Append: [Training-free LLM Merging for Multi-task Learning](https://arxiv.org/abs/2506.12379)
Token length: 1077
Summarized using GPT-3.5-turbo
Append: [Recent Advances and Future Directions in Literature-Based Discovery](https://arxiv.org/abs/2506.12385)
Token length: 1176
Summarized using GPT-3.5-turbo
Append: [Group then Scale: Dynamic Mixture-of-Experts Multilingual Language Model](https://arxiv.org/abs/2506.12388)
Token length: 1376
Summarized using GPT-3.5-turbo
Append: [Exploring Cultural Variations in Moral Judgments with Large Language Models](https://arxiv.org/abs/2506.12433)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [From Outcomes to Processes: Guiding PRM Learning from ORM for Inference-Time Alignment](https://arxiv.org/abs/2506.12446)
Token length: 1347
Summarized using GPT-3.5-turbo
Append: [Language Surgery in Multilingual Large Language Models](https://arxiv.org/abs/2506.12450)
Token length: 1520
Summarized using GPT-3.5-turbo
Append: [A Pluggable Multi-Task Learning Framework for Sentiment-Aware Financial Relation Extraction](https://arxiv.org/abs/2506.12452)
Token length: 874
Summarized using GPT-3.5-turbo
Append: [TagRouter: Learning Route to LLMs through Tags for Open-Domain Text Generation Tasks](https://arxiv.org/abs/2506.12473)
Token length: 1040
Summarized using GPT-3.5-turbo
Append: [FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented Generation](https://arxiv.org/abs/2506.12494)
Token length: 1253
Summarized using GPT-3.5-turbo
Append: [Improving Factuality for Dialogue Response Generation via Graph-Based Knowledge Augmentation](https://arxiv.org/abs/2506.12496)
Token length: 1505
Summarized using GPT-3.5-turbo
Append: [Towards Fairness Assessment of Dutch Hate Speech Detection](https://arxiv.org/abs/2506.12502)
Token length: 1340
Summarized using GPT-3.5-turbo
Append: [Detection, Classification, and Mitigation of Gender Bias in Large Language Models](https://arxiv.org/abs/2506.12527)
Token length: 1207
Summarized using GPT-3.5-turbo
Append: [Speech-Language Models with Decoupled Tokenizers and Multi-Token Prediction](https://arxiv.org/abs/2506.12537)
Token length: 1223
Summarized using GPT-3.5-turbo
Append: [RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking](https://arxiv.org/abs/2506.12538)
Token length: 1541
Summarized using GPT-3.5-turbo
Append: [Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts](https://arxiv.org/abs/2506.12552)
Token length: 1155
Summarized using GPT-3.5-turbo
Append: [DoTA-RAG: Dynamic of Thought Aggregation RAG](https://arxiv.org/abs/2506.12571)
Token length: 1079
Summarized using GPT-3.5-turbo
Append: [Overview of the NLPCC 2025 Shared Task: Gender Bias Mitigation Challenge](https://arxiv.org/abs/2506.12574)
Token length: 1228
Summarized using GPT-3.5-turbo
Append: [Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders](https://arxiv.org/abs/2506.12576)
Token length: 1879
Summarized using GPT-3.5-turbo
Append: [OneEval: Benchmarking LLM Knowledge-intensive Reasoning over Diverse Knowledge Bases](https://arxiv.org/abs/2506.12577)
Token length: 1015
Summarized using GPT-3.5-turbo
Append: [An Exploration of Mamba for Speech Self-Supervised Models](https://arxiv.org/abs/2506.12606)
Token length: 1818
Summarized using GPT-3.5-turbo
Append: [Towards Building General Purpose Embedding Models for Industry 4.0 Agents](https://arxiv.org/abs/2506.12607)
Token length: 1171
Summarized using GPT-3.5-turbo
Append: [Konooz: Multi-domain Multi-dialect Corpus for Named Entity Recognition](https://arxiv.org/abs/2506.12615)
Token length: 1297
Summarized using GPT-3.5-turbo
Append: [OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics](https://arxiv.org/abs/2506.12618)
Token length: 1030
Summarized using GPT-3.5-turbo
Append: [Between Predictability and Randomness: Seeking Artistic Inspiration from AI Generative Models](https://arxiv.org/abs/2506.12634)
Token length: 1049
Summarized using GPT-3.5-turbo
Append: [How Grounded is Wikipedia? A Study on Structured Evidential Support](https://arxiv.org/abs/2506.12637)
Append: [Synthetic Socratic Debates: Examining Persona Effects on Moral Decision and Persuasion Dynamics](https://arxiv.org/abs/2506.12657)
Append: [Enhancing Clinical Models with Pseudo Data for De-identification](https://arxiv.org/abs/2506.12674)
Append: [Flexible Realignment of Language Models](https://arxiv.org/abs/2506.12704)
Append: [Rethinking Hate Speech Detection on Social Media: Can LLMs Replace Traditional Models?](https://arxiv.org/abs/2506.12744)
Append: [Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models](https://arxiv.org/abs/2506.12758)
Append: [Surprise Calibration for Better In-Context Learning](https://arxiv.org/abs/2506.12796)
Append: [Medical Argument Mining: Exploitation of Scarce Data Using NLI Systems](https://arxiv.org/abs/2506.12823)
Append: [Transforming Chatbot Text: A Sequence-to-Sequence Approach](https://arxiv.org/abs/2506.12843)
Append: [QFFT, Question-Free Fine-Tuning for Adaptive Reasoning](https://arxiv.org/abs/2506.12860)
Append: [ArgHiTZ at ArchEHR-QA 2025: A Two-Step Divide and Conquer Approach to Patient Question Answering for Top Factuality](https://arxiv.org/abs/2506.12886)
Append: [Assessing the Performance Gap Between Lexical and Semantic Models for Information Retrieval With Formulaic Legal Language](https://arxiv.org/abs/2506.12895)
Append: [JEBS: A Fine-grained Biomedical Lexical Simplification Task](https://arxiv.org/abs/2506.12898)
Append: [SciDA: Scientific Dynamic Assessor of LLMs](https://arxiv.org/abs/2506.12909)
Append: [PersonaFeedback: A Large-scale Human-annotated Benchmark For Personalization](https://arxiv.org/abs/2506.12915)
Append: [SoundMind: RL-Incentivized Logic Reasoning for Audio-Language Models](https://arxiv.org/abs/2506.12935)
Append: [CliniDial: A Naturally Occurring Multimodal Dialogue Dataset for Team Reflection in Action During Clinical Operation](https://arxiv.org/abs/2506.12936)
Append: [Assessing the Role of Data Quality in Training Bilingual Language Models](https://arxiv.org/abs/2506.12966)
Append: [Multi-document Summarization through Multi-document Event Relation Graph Reasoning in LLMs: a case study in Framing Bias Mitigation](https://arxiv.org/abs/2506.12978)
Append: [Large Language Models Enhanced by Plug and Play Syntactic Knowledge for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2506.12991)
Append: [Missing the human touch? A computational stylometry analysis of GPT-4 translations of online Chinese literature](https://arxiv.org/abs/2506.13013)
Append: [Edeflip: Supervised Word Translation between English and Yoruba](https://arxiv.org/abs/2506.13020)
Append: [Just Go Parallel: Improving the Multilingual Capabilities of Large Language Models](https://arxiv.org/abs/2506.13044)
Append: [CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model](https://arxiv.org/abs/2506.13055)
Append: [Multipole Attention for Efficient Long Context Reasoning](https://arxiv.org/abs/2506.13059)
Append: [MotiveBench: How Far Are We From Human-Like Motivational Reasoning in Large Language Models?](https://arxiv.org/abs/2506.13065)
Append: [FinLMM-R1: Enhancing Financial Reasoning in LMM through Scalable Data and Reward Design](https://arxiv.org/abs/2506.13066)
Append: [CHILL at SemEval-2025 Task 2: You Can't Just Throw Entities and Hope -- Make Your LLM to Get Them Right](https://arxiv.org/abs/2506.13070)
Append: [Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs](https://arxiv.org/abs/2506.13102)
Append: [Leveraging In-Context Learning for Language Model Agents](https://arxiv.org/abs/2506.13109)
Append: [CMU's IWSLT 2025 Simultaneous Speech Translation System](https://arxiv.org/abs/2506.13143)
Append: [Adapting LLMs for Minimal-edit Grammatical Error Correction](https://arxiv.org/abs/2506.13148)
Append: [Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging Unsubstantiated Claims and Ambiguous Pronouns](https://arxiv.org/abs/2506.13172)
Append: [Development of the user-friendly decision aid Rule-based Evaluation and Support Tool (REST) for optimizing the resources of an information extraction task](https://arxiv.org/abs/2506.13177)
Append: [Enhancing Large Language Models with Reliable Knowledge Graphs](https://arxiv.org/abs/2506.13178)
Append: [Dynamic Acoustic Model Architecture Optimization in Training for ASR](https://arxiv.org/abs/2506.13180)
Append: [Align-then-Unlearn: Embedding Alignment for LLM Unlearning](https://arxiv.org/abs/2506.13181)
Append: [Breaking Thought Patterns: A Multi-Dimensional Reasoning Framework for LLMs](https://arxiv.org/abs/2506.13192)
Append: [Do Music Preferences Reflect Cultural Values? A Cross-National Analysis Using Music Embedding and World Values Survey](https://arxiv.org/abs/2506.13199)
Append: [Capability Salience Vector: Fine-grained Alignment of Loss and Capabilities for Downstream Task Scaling Law](https://arxiv.org/abs/2506.13216)
Append: [IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation](https://arxiv.org/abs/2506.13229)
Append: [AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy](https://arxiv.org/abs/2506.13284)
Append: [Mitigating Safety Fallback in Editing-based Backdoor Injection on LLMs](https://arxiv.org/abs/2506.13285)
Append: [Seewo's Submission to MLC-SLM: Lessons learned from Speech Reasoning Language Models](https://arxiv.org/abs/2506.13300)
Append: [Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines](https://arxiv.org/abs/2506.13313)
Append: [Document-Level Tabular Numerical Cross-Checking: A Coarse-to-Fine Approach](https://arxiv.org/abs/2506.13328)
Append: [EAQuant: Enhancing Post-Training Quantization for MoE Models via Expert-Aware Optimization](https://arxiv.org/abs/2506.13329)
Append: [NTU Speechlab LLM-Based Multilingual ASR System for Interspeech MLC-SLM Challenge 2025](https://arxiv.org/abs/2506.13339)
Append: [Direct Reasoning Optimization: LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks](https://arxiv.org/abs/2506.13351)
Append: [StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns](https://arxiv.org/abs/2506.13356)
Append: [Efficient Medical VIE via Reinforcement Learning](https://arxiv.org/abs/2506.13363)
Append: [Enhancing Goal-oriented Proactive Dialogue Systems via Consistency Reflection and Correction](https://arxiv.org/abs/2506.13366)
Append: [Decompositional Reasoning for Graph Retrieval with Large Language Models](https://arxiv.org/abs/2506.13380)
Append: [Bi-directional Context-Enhanced Speech Large Language Models for Multilingual Conversational ASR](https://arxiv.org/abs/2506.13396)
Append: [RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis](https://arxiv.org/abs/2506.13405)
Append: [A Neural Model for Word Repetition](https://arxiv.org/abs/2506.13450)
Append: [Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study](https://arxiv.org/abs/2506.13464)
Append: [Enhancing Omics Cohort Discovery for Research on Neurodegeneration through Ontology-Augmented Embedding Models](https://arxiv.org/abs/2506.13467)
Append: [An Interdisciplinary Approach to Human-Centered Machine Translation](https://arxiv.org/abs/2506.13468)
Append: [Abstract, Align, Predict: Zero-Shot Stance Detection via Cognitive Inductive Reasoning](https://arxiv.org/abs/2506.13470)
Append: [ROSAQ: Rotation-based Saliency-Aware Weight Quantization for Efficiently Compressing Large Language Models](https://arxiv.org/abs/2506.13472)
Append: [Language Agents for Hypothesis-driven Clinical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2506.13474)
Append: [Position: Pause Recycling LoRAs and Prioritize Mechanisms to Uncover Limits and Effectiveness](https://arxiv.org/abs/2506.13479)
Append: [TurBLiMP: A Turkish Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2506.13487)
Append: [BOW: Bottlenecked Next Word Exploration](https://arxiv.org/abs/2506.13502)
Append: [K/DA: Automated Data Generation Pipeline for Detoxifying Implicitly Offensive Language in Korean](https://arxiv.org/abs/2506.13513)
Append: [TensorSLM: Energy-efficient Embedding Compression of Sub-billion Parameter Language Models on Low-end Devices](https://arxiv.org/abs/2506.13514)
Append: [Mixture of Weight-shared Heterogeneous Group Attention Experts for Dynamic Token-wise KV Optimization](https://arxiv.org/abs/2506.13541)
Append: [Understand the Implication: Learning to Think for Pragmatic Understanding](https://arxiv.org/abs/2506.13559)
Append: [Characterizing Linguistic Shifts in Croatian News via Diachronic Word Embeddings](https://arxiv.org/abs/2506.13569)
Append: [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585)
Append: [Qwen vs. Gemma Integration with Whisper: A Comparative Study in Multilingual SpeechLLM Systems](https://arxiv.org/abs/2506.13596)
Append: [CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation](https://arxiv.org/abs/2506.13599)
Append: [A Structured Bangla Dataset of Disease-Symptom Associations to Improve Diagnostic Accuracy](https://arxiv.org/abs/2506.13610)
Append: [An Empirical Study of LLM-as-a-Judge: How Design Choices Impact Evaluation Reliability](https://arxiv.org/abs/2506.13639)
Append: [EvolvTrip: Enhancing Literary Character Understanding with Temporal Theory-of-Mind Graphs](https://arxiv.org/abs/2506.13641)
Append: [Prefix-Tuning+: Modernizing Prefix-Tuning through Attention Independent Prefix Data](https://arxiv.org/abs/2506.13674)
Append: [Turning Down the Heat: A Critical Analysis of Min-p Sampling in Language Models](https://arxiv.org/abs/2506.13681)
Append: [Balancing Knowledge Delivery and Emotional Comfort in Healthcare Conversational Systems](https://arxiv.org/abs/2506.13692)
Append: [Instruction Following by Boosting Attention of Large Language Models](https://arxiv.org/abs/2506.13734)
Append: [LTRR: Learning To Rank Retrievers for LLMs](https://arxiv.org/abs/2506.13743)
Append: [Steering LLM Thinking with Budget Guidance](https://arxiv.org/abs/2506.13752)
Append: [CMT-LLM: Contextual Multi-Talker ASR Utilizing Large Language Models](https://arxiv.org/abs/2506.12059)
Append: [Seamless Dysfluent Speech Text Alignment for Disordered Speech Analysis](https://arxiv.org/abs/2506.12073)
Append: [Artificial Intelligence and Civil Discourse: How LLMs Moderate Climate Change Conversations](https://arxiv.org/abs/2506.12077)
Append: [Modeling Earth-Scale Human-Like Societies with One Billion Agents](https://arxiv.org/abs/2506.12078)
Append: [The CAISAR Platform: Extending the Reach of Machine Learning Specification and Verification](https://arxiv.org/abs/2506.12084)
Append: [Quantum-Inspired Differentiable Integral Neural Networks (QIDINNs): A Feynman-Based Architecture for Continuous Learning Over Streaming Data](https://arxiv.org/abs/2506.12111)
Append: [Generative or Discriminative? Revisiting Text Classification in the Era of Transformers](https://arxiv.org/abs/2506.12181)
Append: [From Emergence to Control: Probing and Modulating Self-Reflection in Language Models](https://arxiv.org/abs/2506.12217)
Append: [Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles](https://arxiv.org/abs/2506.12232)
Append: [Datrics Text2SQL: A Framework for Natural Language to SQL Query Generation](https://arxiv.org/abs/2506.12234)
Append: [ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration](https://arxiv.org/abs/2506.12248)
Append: [InfoFlood: Jailbreaking Large Language Models with Information Overload](https://arxiv.org/abs/2506.12274)
Append: [Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure](https://arxiv.org/abs/2506.12278)
Append: [Perspective on Utilizing Foundation Models for Laboratory Automation in Materials Research](https://arxiv.org/abs/2506.12312)
Append: [GSDNet: Revisiting Incomplete Multimodal-Diffusion from Graph Spectrum Perspective for Conversation Emotion Recognition](https://arxiv.org/abs/2506.12325)
Append: [Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek](https://arxiv.org/abs/2506.12349)
Append: [QiMeng-Attention: SOTA Attention Operator is generated by SOTA Attention Algorithm](https://arxiv.org/abs/2506.12355)
Append: [MM-R5: MultiModal Reasoning-Enhanced ReRanker via Reinforcement Learning for Document Retrieval](https://arxiv.org/abs/2506.12364)
Append: [ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities](https://arxiv.org/abs/2506.12376)
Append: [Model Merging for Knowledge Editing](https://arxiv.org/abs/2506.12384)
Append: [Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM](https://arxiv.org/abs/2506.12421)
Append: [AI Flow: Perspectives, Scenarios, and Approaches](https://arxiv.org/abs/2506.12479)
Append: [MALM: A Multi-Information Adapter for Large Language Models to Mitigate Hallucination](https://arxiv.org/abs/2506.12483)
Append: [Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization](https://arxiv.org/abs/2506.12484)
Append: [StreamMel: Real-Time Zero-shot Text-to-Speech via Interleaved Continuous Autoregressive Modeling](https://arxiv.org/abs/2506.12570)
Append: [MS4UI: A Dataset for Multi-modal Summarization of User Interface Instructional Videos](https://arxiv.org/abs/2506.12623)
Append: [SC-SOT: Conditioning the Decoder on Diarized Speaker Information for End-to-End Overlapped Speech Recognition](https://arxiv.org/abs/2506.12672)
Append: [SecurityLingua: Efficient Defense of LLM Jailbreak Attacks via Security-Aware Prompt Compression](https://arxiv.org/abs/2506.12707)
Append: [Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?](https://arxiv.org/abs/2506.12713)
Append: [Strategic Scaling of Test-Time Compute: A Bandit Learning Approach](https://arxiv.org/abs/2506.12721)
Append: [Rethinking DPO: The Role of Rejected Responses in Preference Misalignment](https://arxiv.org/abs/2506.12725)
Append: [WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench](https://arxiv.org/abs/2506.12841)
Append: [Identifying and Investigating Global News Coverage of Critical Events Such as Disasters and Terrorist Attacks](https://arxiv.org/abs/2506.12925)
Append: [Sectoral Coupling in Linguistic State Space](https://arxiv.org/abs/2506.12927)
Append: [HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance](https://arxiv.org/abs/2506.12937)
Append: [Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition](https://arxiv.org/abs/2506.12953)
Append: [Efficient Neuro-Symbolic Retrieval-Augmented Generation through Adaptive Query Routing](https://arxiv.org/abs/2506.12981)
Append: [Knowledge Graph Fusion with Large Language Models for Accurate, Explainable Manufacturing Process Planning](https://arxiv.org/abs/2506.13026)
Append: [Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning](https://arxiv.org/abs/2506.13051)
Append: [PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue](https://arxiv.org/abs/2506.13063)
Append: [Equitable Electronic Health Record Prediction with FAME: Fairness-Aware Multimodal Embedding](https://arxiv.org/abs/2506.13104)
Append: [Crime Hotspot Prediction Using Deep Graph Convolutional Networks](https://arxiv.org/abs/2506.13116)
Append: [ZINA: Multimodal Fine-grained Hallucination Detection and Editing](https://arxiv.org/abs/2506.13130)
Append: [Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence](https://arxiv.org/abs/2506.13187)
Append: [SPOT: Bridging Natural Language and Geospatial Search for Investigative Journalists](https://arxiv.org/abs/2506.13188)
Append: [Thought Crime: Backdoors and Emergent Misalignment in Reasoning Models](https://arxiv.org/abs/2506.13206)
Append: [Distinct Computations Emerge From Compositional Curricula in In-Context Learning](https://arxiv.org/abs/2506.13253)
Append: [AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining](https://arxiv.org/abs/2506.13274)
Append: [SeqPE: Transformer with Sequential Position Encoding](https://arxiv.org/abs/2506.13277)
Append: [Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers](https://arxiv.org/abs/2506.13342)
Append: [Leveraging Vision-Language Pre-training for Human Activity Recognition in Still Images](https://arxiv.org/abs/2506.13458)
Append: [Flexible-length Text Infilling for Discrete Diffusion Models](https://arxiv.org/abs/2506.13579)
Append: [Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model](https://arxiv.org/abs/2506.13642)
Append: [Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs](https://arxiv.org/abs/2506.13727)
Append: [Is Smaller Always Faster? Tradeoffs in Compressing Self-Supervised Speech Transformers](https://arxiv.org/abs/2211.09949)
Append: [Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning](https://arxiv.org/abs/2405.05955)
Append: [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)
Append: [WorldAPIs: The World Is Worth How Many APIs? A Thought Experiment](https://arxiv.org/abs/2407.07778)
Append: [Fast-and-Frugal Text-Graph Transformers are Effective Link Predictors](https://arxiv.org/abs/2408.06778)
Append: [AgentCourt: Simulating Court with Adversarial Evolvable Lawyer Agents](https://arxiv.org/abs/2408.08089)
Append: [EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling MiXed Emotions and Discourse Dynamics](https://arxiv.org/abs/2408.08782)
Append: [Improving Clinical Note Generation from Complex Doctor-Patient Conversation](https://arxiv.org/abs/2408.14568)
Append: [Co-occurrence is not Factual Association in Language Models](https://arxiv.org/abs/2409.14057)
Append: [Making LLMs Better Many-to-Many Speech-to-Text Translators with Curriculum Learning](https://arxiv.org/abs/2409.19510)
Append: [Scaling Laws For Mixed Qquantization](https://arxiv.org/abs/2410.06722)
Append: [Upcycling Large Language Models into Mixture of Experts](https://arxiv.org/abs/2410.07524)
Append: [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426)
Append: [EffiCoder: Enhancing Code Generation in Large Language Models through Efficiency-Aware Fine-tuning](https://arxiv.org/abs/2410.10209)
Append: [Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering](https://arxiv.org/abs/2410.12846)
Append: [POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization](https://arxiv.org/abs/2410.12999)
Append: [ReflecTool: Towards Reflection-Aware Tool-Augmented Clinical Agents](https://arxiv.org/abs/2410.17657)
Append: [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)
Append: [A dataset of questions on decision-theoretic reasoning in Newcomb-like problems](https://arxiv.org/abs/2411.10588)
Append: [Bridging Relevance and Reasoning: Rationale Distillation in Retrieval-Augmented Generation](https://arxiv.org/abs/2412.08519)
Append: [CoT-based Synthesizer: Enhancing LLM Performance through Answer Synthesis](https://arxiv.org/abs/2501.01668)
Append: [An Investigation into Value Misalignment in LLM-Generated Texts for Cultural Heritage](https://arxiv.org/abs/2501.02039)
Append: [Benchmarking Rotary Position Embeddings for Automatic Speech Recognition](https://arxiv.org/abs/2501.06051)
Append: [Foundations of Large Language Models](https://arxiv.org/abs/2501.09223)
Append: [Rethinking Table Instruction Tuning](https://arxiv.org/abs/2501.14693)
Append: [Activation-Informed Merging of Large Language Models](https://arxiv.org/abs/2502.02421)
Append: [Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search](https://arxiv.org/abs/2502.02508)
Append: [BOUQuET: dataset, Benchmark and Open initiative for Universal Quality Evaluation in Translation](https://arxiv.org/abs/2502.04314)
Append: [Fino1: On the Transferability of Reasoning-Enhanced LLMs and Reinforcement Learning to Finance](https://arxiv.org/abs/2502.08127)
Append: [Truth Knows No Language: Evaluating Truthfulness Beyond English](https://arxiv.org/abs/2502.09387)
Append: [SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models](https://arxiv.org/abs/2502.09604)
Append: [MTLM: Incorporating Bidirectional Text Information to Enhance Language Model Training in Speech Recognition Systems](https://arxiv.org/abs/2502.10058)
Append: [CMCTS: A Constrained Monte Carlo Tree Search Framework for Mathematical Reasoning in Large Language Model](https://arxiv.org/abs/2502.11169)
Append: [Idiosyncrasies in Large Language Models](https://arxiv.org/abs/2502.12150)
Append: [NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions](https://arxiv.org/abs/2502.13124)
Append: [ProMedTS: A Self-Supervised, Prompt-Guided Multimodal Approach for Integrating Medical Text and Time Series](https://arxiv.org/abs/2502.13509)
Append: [A Large and Balanced Corpus for Fine-grained Arabic Readability Assessment](https://arxiv.org/abs/2502.13520)
Append: [Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification](https://arxiv.org/abs/2502.14133)
Append: [Entity Framing and Role Portrayal in the News](https://arxiv.org/abs/2502.14718)
Append: [A Training-free LLM-based Approach to General Chinese Character Error Correction](https://arxiv.org/abs/2502.15266)
Append: [InfiniSST: Simultaneous Translation of Unbounded Speech with Large Language Model](https://arxiv.org/abs/2503.02969)
Append: [Efficient Safety Alignment of Large Language Models via Preference Re-ranking and Representation-based Reward Modeling](https://arxiv.org/abs/2503.10093)
Append: [REPA: Russian Error Types Annotation for Evaluating Text Generation and Judgment Capabilities](https://arxiv.org/abs/2503.13102)
Append: [MathFusion: Enhancing Mathematical Problem-solving of LLM through Instruction Fusion](https://arxiv.org/abs/2503.16212)
Append: [ShED-HD: A Shannon Entropy Distribution Framework for Lightweight Hallucination Detection on Edge Devices](https://arxiv.org/abs/2503.18242)
Append: [LinkAlign: Scalable Schema Linking for Real-World Large-Scale Multi-Database Text-to-SQL](https://arxiv.org/abs/2503.18596)
Append: [Efficient Inference for Large Reasoning Models: A Survey](https://arxiv.org/abs/2503.23077)
Append: [Evaluating how LLM annotations represent diverse views on contentious topics](https://arxiv.org/abs/2503.23243)
Append: [Experiential Semantic Information and Brain Alignment: Are Multimodal Models Better than Language Models?](https://arxiv.org/abs/2504.00942)
Append: [NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables](https://arxiv.org/abs/2504.06560)
Append: [Scholar Inbox: Personalized Paper Recommendations for Scientists](https://arxiv.org/abs/2504.08385)
Append: [Unsupervised Classification of English Words Based on Phonological Information: Discovery of Germanic and Latinate Clusters](https://arxiv.org/abs/2504.11770)
Append: [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)
Append: [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)
Append: [Toward Reasonable Parrots: Why Large Language Models Should Argue with Us by Design](https://arxiv.org/abs/2505.05298)
Append: [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)
Append: [Leveraging LLM and Self-Supervised Training Models for Speech Recognition in Chinese Dialects: A Comparative Analysis](https://arxiv.org/abs/2505.21138)
Append: [R-KV: Redundancy-aware KV Cache Compression for Reasoning Models](https://arxiv.org/abs/2505.24133)
Append: [Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus](https://arxiv.org/abs/2506.00332)
Append: [From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation](https://arxiv.org/abs/2506.00713)
Append: [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/abs/2506.03781)
Append: [MELABenchv1: Benchmarking Large Language Models against Smaller Fine-Tuned Models for Low-Resource Maltese NLP](https://arxiv.org/abs/2506.04385)
Append: [OPeRA: A Dataset of Observation, Persona, Rationale, and Action for Evaluating LLMs on Human Online Shopping Behavior Simulation](https://arxiv.org/abs/2506.05606)
Append: [Video Understanding with Large Language Models: A Survey](https://arxiv.org/abs/2312.17432)
Append: [Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversation](https://arxiv.org/abs/2402.11827)
Append: [Personalized Wireless Federated Learning for Large Language Models](https://arxiv.org/abs/2404.13238)
Append: [Efficient Sequential Decision Making with Large Language Models](https://arxiv.org/abs/2406.12125)
Append: [PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference](https://arxiv.org/abs/2406.15513)
Append: [Navigating LLM Ethics: Advancements, Challenges, and Future Directions](https://arxiv.org/abs/2406.18841)
Append: [SMILE: Speech Meta In-Context Learning for Low-Resource Language Automatic Speech Recognition](https://arxiv.org/abs/2409.10429)
Append: [RATIONALYST: Mining Implicit Rationales for Process Supervision of Reasoning](https://arxiv.org/abs/2410.01044)
Append: [How Much Can We Forget about Data Contamination?](https://arxiv.org/abs/2410.03249)
Append: [Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences](https://arxiv.org/abs/2410.21332)
Append: [Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse](https://arxiv.org/abs/2410.21333)
Append: [Regular-pattern-sensitive CRFs for Distant Label Interactions](https://arxiv.org/abs/2411.12484)
Append: [MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems](https://arxiv.org/abs/2412.15557)
Append: [Unifying Specialized Visual Encoders for Video Language Models](https://arxiv.org/abs/2501.01426)
Append: [Layer by Layer: Uncovering Hidden Representations in Language Models](https://arxiv.org/abs/2502.02013)
Append: [Scaling Laws for Upcycling Mixture-of-Experts Language Models](https://arxiv.org/abs/2502.03009)
Append: [Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training](https://arxiv.org/abs/2502.03460)
Append: [Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions](https://arxiv.org/abs/2502.04322)
Append: [Optimizing Temperature for Language Models with Multi-Sample Inference](https://arxiv.org/abs/2502.05234)
Append: [Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification](https://arxiv.org/abs/2502.07299)
Append: [HARBOR: Exploring Persona Dynamics in Multi-Agent Competition](https://arxiv.org/abs/2502.12149)
Append: [Less is More: Improving LLM Alignment via Preference Data Selection](https://arxiv.org/abs/2502.14560)
Append: [From Euler to AI: Unifying Formulas for Mathematical Constants](https://arxiv.org/abs/2502.17533)
Append: [Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models](https://arxiv.org/abs/2503.01208)
Append: [What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text](https://arxiv.org/abs/2503.04804)
Append: [Compute Optimal Scaling of Skills: Knowledge vs Reasoning](https://arxiv.org/abs/2503.10061)
Append: [Transformers without Normalization](https://arxiv.org/abs/2503.10622)
Append: [QualiSpeech: A Speech Quality Assessment Dataset with Natural Language Reasoning and Descriptions](https://arxiv.org/abs/2503.20290)
Append: [Affordable AI Assistants with Knowledge Graph of Thoughts](https://arxiv.org/abs/2504.02670)
Append: [On Synthesizing Data for Context Attribution in Question Answering](https://arxiv.org/abs/2504.05317)
Append: [JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2504.10512)
Append: [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
Append: [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
append_entries: 281
Finish: 2025-06-17 04:30:29.357021
------------------------------------------------------
Started: 2025-06-17 06:25:56.863881
Existing_entries: 1281
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1798
Summarized using GPT-3.5-turbo
Append: [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
Token length: 962
Summarized using GPT-3.5-turbo
Append: [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
Token length: 1719
Summarized using GPT-3.5-turbo
Append: [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
append_entries: 4
Finish: 2025-06-17 06:26:04.493447
------------------------------------------------------
Started: 2025-06-17 08:23:34.090262
Existing_entries: 1004
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 08:23:34.739431
------------------------------------------------------
Started: 2025-06-17 10:18:36.555311
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 10:18:37.158883
------------------------------------------------------
Started: 2025-06-17 12:35:52.739674
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 12:35:53.383146
------------------------------------------------------
Started: 2025-06-17 14:17:20.585328
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 14:17:21.204818
------------------------------------------------------
Started: 2025-06-17 16:21:45.352590
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 16:21:46.008851
------------------------------------------------------
Started: 2025-06-17 18:23:57.017037
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 18:23:57.615192
------------------------------------------------------
Started: 2025-06-17 20:18:36.665723
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 20:18:37.344744
------------------------------------------------------
Started: 2025-06-17 22:15:55.366501
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-17 22:15:56.058754
------------------------------------------------------
Started: 2025-06-18 01:20:37.824605
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 01:20:38.456571
------------------------------------------------------
Started: 2025-06-18 03:14:31.543791
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 03:14:32.140730
------------------------------------------------------
Started: 2025-06-18 04:29:13.017823
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1779
Summarized using GPT-3.5-turbo
Append: [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)
Token length: 1187
Summarized using GPT-3.5-turbo
Append: [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/abs/2506.13888)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [EmoNews: A Spoken Dialogue System for Expressive News Conversations](https://arxiv.org/abs/2506.13894)
Token length: 1631
Summarized using GPT-3.5-turbo
Append: [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/abs/2506.13901)
Token length: 1278
Summarized using GPT-3.5-turbo
Append: [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/abs/2506.13956)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [Are manual annotations necessary for statutory interpretations retrieval?](https://arxiv.org/abs/2506.13965)
Token length: 1619
Summarized using GPT-3.5-turbo
Append: [AI shares emotion with humans across languages and cultures](https://arxiv.org/abs/2506.13978)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/abs/2506.14012)
Token length: 1517
Summarized using GPT-3.5-turbo
Append: [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/abs/2506.14028)
Token length: 747
Summarized using GPT-3.5-turbo
Append: [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/abs/2506.14040)
Token length: 698
Summarized using GPT-3.5-turbo
Append: [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/abs/2506.14046)
Token length: 978
Summarized using GPT-3.5-turbo
Append: [Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data](https://arxiv.org/abs/2506.14064)
Token length: 937
Summarized using GPT-3.5-turbo
Append: [Abstract Meaning Representation for Hospital Discharge Summarization](https://arxiv.org/abs/2506.14101)
Token length: 870
Summarized using GPT-3.5-turbo
Append: [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)
Token length: 1457
Summarized using GPT-3.5-turbo
Append: [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
Token length: 1239
Summarized using GPT-3.5-turbo
Append: [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
Token length: 1190
Summarized using GPT-3.5-turbo
Append: [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.14158)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind](https://arxiv.org/abs/2506.14161)
Token length: 1225
Summarized using GPT-3.5-turbo
Append: [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/abs/2506.14175)
Token length: 1039
Summarized using GPT-3.5-turbo
Append: [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/abs/2506.14177)
Token length: 981
Summarized using GPT-3.5-turbo
Append: [AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](https://arxiv.org/abs/2506.14190)
Token length: 950
Summarized using GPT-3.5-turbo
Append: [MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment](https://arxiv.org/abs/2506.14199)
Token length: 1455
Summarized using GPT-3.5-turbo
Append: [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/abs/2506.14200)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation](https://arxiv.org/abs/2506.14203)
Token length: 1332
Summarized using GPT-3.5-turbo
Append: [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/abs/2506.14205)
Token length: 1432
Summarized using GPT-3.5-turbo
Append: [CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation](https://arxiv.org/abs/2506.14206)
Token length: 1357
Summarized using GPT-3.5-turbo
Append: [Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation](https://arxiv.org/abs/2506.14211)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [Chaining Event Spans for Temporal Relation Grounding](https://arxiv.org/abs/2506.14213)
Token length: 1910
Summarized using GPT-3.5-turbo
Append: [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/abs/2506.14234)
Token length: 877
Summarized using GPT-3.5-turbo
Append: [A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs](https://arxiv.org/abs/2506.14235)
Token length: 1518
Summarized using GPT-3.5-turbo
Append: [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/abs/2506.14248)
Token length: 1112
Summarized using GPT-3.5-turbo
Append: [From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents](https://arxiv.org/abs/2506.14285)
Token length: 1422
Summarized using GPT-3.5-turbo
Append: [Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent](https://arxiv.org/abs/2506.14302)
Token length: 1333
Summarized using GPT-3.5-turbo
Append: [Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics](https://arxiv.org/abs/2506.14335)
Token length: 1147
Summarized using GPT-3.5-turbo
Append: [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/abs/2506.14345)
Token length: 856
Summarized using GPT-3.5-turbo
Append: [Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits](https://arxiv.org/abs/2506.14370)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/abs/2506.14371)
Token length: 912
Summarized using GPT-3.5-turbo
Append: [Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding](https://arxiv.org/abs/2506.14397)
Token length: 1192
Summarized using GPT-3.5-turbo
Append: [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
Token length: 1754
Summarized using GPT-3.5-turbo
Append: [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429)
Token length: 1414
Summarized using GPT-3.5-turbo
Append: [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/abs/2506.14448)
Token length: 1617
Summarized using GPT-3.5-turbo
Append: [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474)
Token length: 1666
Summarized using GPT-3.5-turbo
Append: [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
Token length: 972
Summarized using GPT-3.5-turbo
Append: [M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/abs/2506.14532)
Token length: 1208
Summarized using GPT-3.5-turbo
Append: [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)
Token length: 1685
Summarized using GPT-3.5-turbo
Append: [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/abs/2506.14580)
Token length: 1648
Summarized using GPT-3.5-turbo
Append: [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)
Token length: 823
Summarized using GPT-3.5-turbo
Append: [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/abs/2506.14613)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/abs/2506.14625)
Append: [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)
Append: [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)
Append: [Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments](https://arxiv.org/abs/2506.14645)
Append: [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/abs/2506.14646)
Append: [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/abs/2506.14681)
Append: [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)
Append: [Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data](https://arxiv.org/abs/2506.14704)
Append: [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14731)
Append: [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/abs/2506.14758)
Append: [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)
Append: [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)
Append: [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
Append: [Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning](https://arxiv.org/abs/2506.13778)
Append: [AcademicBrowse: Benchmarking Academic Browse Ability of LLMs](https://arxiv.org/abs/2506.13784)
Append: [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
Append: [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
Append: [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
Append: [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
Append: [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
Append: [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
Append: [InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking](https://arxiv.org/abs/2506.14086)
Append: [Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints](https://arxiv.org/abs/2506.14104)
Append: [RadFabric: Agentic AI System with Reasoning Capability for Radiology](https://arxiv.org/abs/2506.14142)
Append: [Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment](https://arxiv.org/abs/2506.14148)
Append: [Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models](https://arxiv.org/abs/2506.14153)
Append: [Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios](https://arxiv.org/abs/2506.14204)
Append: [Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription](https://arxiv.org/abs/2506.14223)
Append: [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
Append: [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
Append: [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
Append: [Computational Studies in Influencer Marketing: A Systematic Literature Review](https://arxiv.org/abs/2506.14602)
Append: [VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning](https://arxiv.org/abs/2506.14629)
Append: [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
Append: [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/abs/2506.14766)
Append: [Compression of enumerations and gain](https://arxiv.org/abs/2304.03030)
Append: [FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback](https://arxiv.org/abs/2307.10867)
Append: [Exploring news intent and its application: A theory-driven approach](https://arxiv.org/abs/2312.16490)
Append: [Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification](https://arxiv.org/abs/2402.10735)
Append: [Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora](https://arxiv.org/abs/2406.13677)
Append: [ETM: Modern Insights into Perspective on Text-to-SQL Evaluation in the Age of Large Language Models](https://arxiv.org/abs/2407.07313)
Append: [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/abs/2410.01444)
Append: [Uncovering Overfitting in Large Language Model Editing](https://arxiv.org/abs/2410.07819)
Append: [Beyond Browsing: API-Based Web Agents](https://arxiv.org/abs/2410.16464)
Append: [Towards Better Open-Ended Text Generation: A Multicriteria Evaluation Framework](https://arxiv.org/abs/2410.18653)
Append: [Ensemble Watermarks for Large Language Models](https://arxiv.org/abs/2411.19563)
Append: [BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English](https://arxiv.org/abs/2412.04726)
Append: [Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache Compression](https://arxiv.org/abs/2412.05693)
Append: [ClusterChat: Multi-Feature Search for Corpus Exploration](https://arxiv.org/abs/2412.14533)
Append: [Language and Planning in Robotic Navigation: A Multilingual Evaluation of State-of-the-Art Models](https://arxiv.org/abs/2501.05478)
Append: [The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically Justify Replacing Human Annotators with LLMs](https://arxiv.org/abs/2501.10970)
Append: [Counterfactual-Consistency Prompting for Relative Temporal Understanding in Large Language Models](https://arxiv.org/abs/2502.11425)
Append: [Towards Geo-Culturally Grounded LLM Generations](https://arxiv.org/abs/2502.13497)
Append: [PredictaBoard: Benchmarking LLM Score Predictability](https://arxiv.org/abs/2502.14445)
Append: [Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models](https://arxiv.org/abs/2502.15910)
Append: [LongSpec: Long-Context Lossless Speculative Decoding with Efficient Drafting and Verification](https://arxiv.org/abs/2502.17421)
Append: [Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning](https://arxiv.org/abs/2502.20620)
Append: [SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling](https://arxiv.org/abs/2503.04619)
Append: [Effect of Selection Format on LLM Performance](https://arxiv.org/abs/2503.06926)
Append: [SOPBench: Evaluating Language Agents at Following Standard Operating Procedures and Constraints](https://arxiv.org/abs/2503.08669)
Append: [Do Construction Distributions Shape Formal Language Learning In German BabyLMs?](https://arxiv.org/abs/2503.11593)
Append: [Automated Construction of a Knowledge Graph of Nuclear Fusion Energy for Effective Elicitation and Retrieval of Information](https://arxiv.org/abs/2504.07738)
Append: [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)
Append: [Graph RAG for Legal Norms: A Hierarchical, Temporal and Deterministic Approach](https://arxiv.org/abs/2505.00039)
Append: [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)
Append: [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)
Append: [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
Append: [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2505.17238)
Append: [REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning](https://arxiv.org/abs/2505.20613)
Append: [EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG](https://arxiv.org/abs/2506.00854)
Append: [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
Append: [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
Append: [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
Append: [Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature](https://arxiv.org/abs/2308.12420)
Append: [Bridging Social Media and Search Engines: Dredge Words and the Detection of Unreliable Domains](https://arxiv.org/abs/2406.11423)
Append: [Do Large Language Models Exhibit Cognitive Dissonance? Studying the Difference Between Revealed Beliefs and Stated Answers](https://arxiv.org/abs/2406.14986)
Append: [Controllable and Reliable Knowledge-Intensive Task-Oriented Conversational Agents with Declarative Genie Worksheets](https://arxiv.org/abs/2407.05674)
Append: [Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents](https://arxiv.org/abs/2410.05243)
Append: [ONEBench to Test Them All: Sample-Level Benchmarking Over Open-Ended Capabilities](https://arxiv.org/abs/2412.06745)
Append: [Agent Laboratory: Using LLM Agents as Research Assistants](https://arxiv.org/abs/2501.04227)
Append: [From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors](https://arxiv.org/abs/2501.18045)
Append: [SAE-V: Interpreting Multimodal Models for Enhanced Alignment](https://arxiv.org/abs/2502.17514)
Append: [Reward Shaping to Mitigate Reward Hacking in RLHF](https://arxiv.org/abs/2502.18770)
Append: [OWLViz: An Open-World Benchmark for Visual Question Answering](https://arxiv.org/abs/2503.07631)
Append: [Chain-of-Thought Reasoning In The Wild Is Not Always Faithful](https://arxiv.org/abs/2503.08679)
Append: [Assessing Consistency and Reproducibility in the Outputs of Large Language Models: Evidence Across Diverse Finance and Accounting Tasks](https://arxiv.org/abs/2503.16974)
Append: [Inherent and emergent liability issues in LLM-based agentic systems: a principal-agent perspective](https://arxiv.org/abs/2504.03255)
Append: [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
Append: [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
Append: [Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models](https://arxiv.org/abs/2505.20612)
Append: [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
Append: [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
append_entries: 141
Finish: 2025-06-18 04:30:45.857841
------------------------------------------------------
Started: 2025-06-18 06:25:14.575700
Existing_entries: 1141
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1169
Summarized using GPT-3.5-turbo
Append: [Position: Editing Large Language Models Poses Serious Safety Risks](https://arxiv.org/abs/2502.02958)
append_entries: 1
Finish: 2025-06-18 06:25:16.948504
------------------------------------------------------
Started: 2025-06-18 08:23:04.665089
Existing_entries: 1001
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 08:23:05.007042
------------------------------------------------------
Started: 2025-06-18 10:18:24.081473
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 10:18:24.510375
------------------------------------------------------
Started: 2025-06-18 12:35:37.856203
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 12:35:38.220329
------------------------------------------------------
Started: 2025-06-18 14:16:57.575974
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 14:16:57.921369
------------------------------------------------------
Started: 2025-06-18 16:21:39.183904
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 16:21:39.596410
------------------------------------------------------
Started: 2025-06-18 18:23:39.804750
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 18:23:40.154300
------------------------------------------------------
Started: 2025-06-18 20:19:51.333192
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 20:19:51.765525
------------------------------------------------------
Started: 2025-06-18 22:15:58.301289
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-18 22:15:58.640863
------------------------------------------------------
Started: 2025-06-19 01:21:18.406145
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 01:21:18.747846
------------------------------------------------------
Started: 2025-06-19 03:15:22.599127
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 03:15:22.975099
------------------------------------------------------
Started: 2025-06-19 04:28:46.839137
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1387
Summarized using GPT-3.5-turbo
Append: [Adverse Event Extraction from Discharge Summaries: A New Dataset, Annotation Scheme, and Initial Findings](https://arxiv.org/abs/2506.14900)
Token length: 1410
Summarized using GPT-3.5-turbo
Append: [Combining Constrained and Unconstrained Decoding via Boosting: BoostCD and Its Application to Information Extraction](https://arxiv.org/abs/2506.14901)
Token length: 1329
Summarized using GPT-3.5-turbo
Append: [CrEst: Credibility Estimation for Contexts in LLMs via Weak Supervision](https://arxiv.org/abs/2506.14912)
Token length: 1691
Summarized using GPT-3.5-turbo
Append: [MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance](https://arxiv.org/abs/2506.14927)
Token length: 1433
Summarized using GPT-3.5-turbo
Append: [From Chat to Checkup: Can Large Language Models Assist in Diabetes Prediction?](https://arxiv.org/abs/2506.14949)
Token length: 863
Summarized using GPT-3.5-turbo
Append: [Memory Tokens: Large Language Models Can Generate Reversible Sentence Embeddings](https://arxiv.org/abs/2506.15001)
Token length: 1216
Summarized using GPT-3.5-turbo
Append: [Identifying social isolation themes in NVDRS text narratives using topic modeling and text-classification methods](https://arxiv.org/abs/2506.15030)
Token length: 1315
Summarized using GPT-3.5-turbo
Append: [Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form Generation](https://arxiv.org/abs/2506.15068)
Token length: 943
Summarized using GPT-3.5-turbo
Append: [Learning-Time Encoding Shapes Unlearning in LLMs](https://arxiv.org/abs/2506.15076)
Token length: 1266
Summarized using GPT-3.5-turbo
Append: [Improving Dialogue Discourse Parsing through Discourse-aware Utterance Clarification](https://arxiv.org/abs/2506.15081)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [CKD-EHR:Clinical Knowledge Distillation for Electronic Health Records](https://arxiv.org/abs/2506.15118)
Token length: 1350
Summarized using GPT-3.5-turbo
Append: [Modeling the One-to-Many Property in Open-Domain Dialogue with LLMs](https://arxiv.org/abs/2506.15131)
Token length: 961
Summarized using GPT-3.5-turbo
Append: [Thunder-Tok: Minimizing Tokens per Word in Tokenizing Korean Texts for Generative Language Models](https://arxiv.org/abs/2506.15138)
Token length: 1293
Summarized using GPT-3.5-turbo
Append: [Emergence of Primacy and Recency Effect in Mamba: A Mechanistic Point of View](https://arxiv.org/abs/2506.15156)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [A Comparative Study of Task Adaptation Techniques of Large Language Models for Identifying Sustainable Development Goals](https://arxiv.org/abs/2506.15208)
Token length: 1811
Summarized using GPT-3.5-turbo
Append: [ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning in LLMs](https://arxiv.org/abs/2506.15211)
Token length: 1489
Summarized using GPT-3.5-turbo
Append: [MinosEval: Distinguishing Factoid and Non-Factoid for Tailored Open-Ended QA Evaluation with LLMs](https://arxiv.org/abs/2506.15215)
Token length: 963
Summarized using GPT-3.5-turbo
Append: [Lost in Variation? Evaluating NLI Performance in Basque and Spanish Geographical Variants](https://arxiv.org/abs/2506.15239)
Token length: 1355
Summarized using GPT-3.5-turbo
Append: [Research on Graph-Retrieval Augmented Generation Based on Historical Text Knowledge Graphs](https://arxiv.org/abs/2506.15241)
Token length: 994
Summarized using GPT-3.5-turbo
Append: [TopClustRAG at SIGIR 2025 LiveRAG Challenge](https://arxiv.org/abs/2506.15246)
Token length: 1133
Summarized using GPT-3.5-turbo
Append: [Thunder-DeID: Accurate and Efficient De-identification Framework for Korean Court Judgments](https://arxiv.org/abs/2506.15266)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment](https://arxiv.org/abs/2506.15301)
Token length: 765
Summarized using GPT-3.5-turbo
Append: [ConLID: Supervised Contrastive Learning for Low-Resource Language Identification](https://arxiv.org/abs/2506.15304)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [DeVisE: Behavioral Testing of Medical Large Language Models](https://arxiv.org/abs/2506.15339)
Token length: 1250
Summarized using GPT-3.5-turbo
Append: [SANSKRITI: A Comprehensive Benchmark for Evaluating Language Models' Knowledge of Indian Culture](https://arxiv.org/abs/2506.15355)
Token length: 1545
Summarized using GPT-3.5-turbo
Append: [COSMMIC: Comment-Sensitive Multimodal Multilingual Indian Corpus for Summarization and Headline Generation](https://arxiv.org/abs/2506.15372)
Token length: 1746
Summarized using GPT-3.5-turbo
Append: [Targeted Lexical Injection: Unlocking Latent Cross-Lingual Alignment in Lugha-Llama via Early-Layer LoRA Fine-Tuning](https://arxiv.org/abs/2506.15415)
Token length: 1041
Summarized using GPT-3.5-turbo
Append: [Understanding GUI Agent Localization Biases through Logit Sharpness](https://arxiv.org/abs/2506.15425)
Token length: 1750
Summarized using GPT-3.5-turbo
Append: [AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need](https://arxiv.org/abs/2506.15451)
Token length: 1231
Summarized using GPT-3.5-turbo
Append: [RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation](https://arxiv.org/abs/2506.15455)
Token length: 1562
Summarized using GPT-3.5-turbo
Append: [Context-Informed Grounding Supervision](https://arxiv.org/abs/2506.15480)
Token length: 1436
Summarized using GPT-3.5-turbo
Append: [SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling](https://arxiv.org/abs/2506.15498)
Token length: 1519
Summarized using GPT-3.5-turbo
Append: [Enhancing Hyperbole and Metaphor Detection with Their Bidirectional Dynamic Interaction and Emotion Knowledge](https://arxiv.org/abs/2506.15504)
Token length: 1490
Summarized using GPT-3.5-turbo
Append: [Lessons from Training Grounded LLMs with Verifiable Rewards](https://arxiv.org/abs/2506.15522)
Token length: 1795
Summarized using GPT-3.5-turbo
Append: [RATTENTION: Towards the Minimal Sliding Window Size in Local-Global Attention Models](https://arxiv.org/abs/2506.15545)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [Approximating Language Model Training Data from Weights](https://arxiv.org/abs/2506.15553)
Token length: 1365
Summarized using GPT-3.5-turbo
Append: [PredGen: Accelerated Inference of Large Language Models through Input-Time Speculation for Real-Time Speech Interaction](https://arxiv.org/abs/2506.15556)
Token length: 1088
Summarized using GPT-3.5-turbo
Append: [Gender Inclusivity Fairness Index (GIFI): A Multilevel Framework for Evaluating Gender Diversity in Large Language Models](https://arxiv.org/abs/2506.15568)
Token length: 1037
Summarized using GPT-3.5-turbo
Append: [SciVer: Evaluating Foundation Models for Multimodal Scientific Claim Verification](https://arxiv.org/abs/2506.15569)
Token length: 1625
Summarized using GPT-3.5-turbo
Append: [DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement](https://arxiv.org/abs/2506.15583)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts](https://arxiv.org/abs/2506.15594)
Token length: 1482
Summarized using GPT-3.5-turbo
Append: [From Model to Classroom: Evaluating Generated MCQs for Portuguese with Narrative and Difficulty Concerns](https://arxiv.org/abs/2506.15598)
Token length: 1776
Summarized using GPT-3.5-turbo
Append: [The Compositional Architecture of Regret in Large Language Models](https://arxiv.org/abs/2506.15617)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Minding the Politeness Gap in Cross-cultural Communication](https://arxiv.org/abs/2506.15623)
Token length: 1163
Summarized using GPT-3.5-turbo
Append: [Revisiting Compositional Generalization Capability of Large Language Models Considering Instruction Following Ability](https://arxiv.org/abs/2506.15629)
Token length: 935
Summarized using GPT-3.5-turbo
Append: [Oldies but Goldies: The Potential of Character N-grams for Romanian Texts](https://arxiv.org/abs/2506.15650)
Token length: 1054
Summarized using GPT-3.5-turbo
Append: [CC-LEARN: Cohort-based Consistency Learning](https://arxiv.org/abs/2506.15662)
Token length: 957
Summarized using GPT-3.5-turbo
Append: [Leaky Thoughts: Large Reasoning Models Are Not Private Thinkers](https://arxiv.org/abs/2506.15674)
Token length: 1001
Summarized using GPT-3.5-turbo
Append: [Gender-Neutral Machine Translation Strategies in Practice](https://arxiv.org/abs/2506.15676)
Token length: 1265
Summarized using GPT-3.5-turbo
Append: [GenRecal: Generation after Recalibration from Large to Small Vision-Language Models](https://arxiv.org/abs/2506.15681)
Append: [PhantomHunter: Detecting Unseen Privately-Tuned LLM-Generated Text via Family-Aware Learning](https://arxiv.org/abs/2506.15683)
Append: [ETS: Open Vocabulary Electroencephalography-To-Text Decoding and Sentiment Classification](https://arxiv.org/abs/2506.14783)
Append: [SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2506.14791)
Append: [Assembly of Experts: Linear-time construction of the Chimera LLM variants with emergent and adaptable behaviors](https://arxiv.org/abs/2506.14794)
Append: [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
Append: [Detecting Narrative Shifts through Persistent Structures: A Topological Analysis of Media Discourse](https://arxiv.org/abs/2506.14836)
Append: [Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching](https://arxiv.org/abs/2506.14852)
Append: [Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective](https://arxiv.org/abs/2506.14965)
Append: [Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings](https://arxiv.org/abs/2506.14997)
Append: [Optimal Embedding Learning Rate in LLMs: The Effect of Vocabulary Size](https://arxiv.org/abs/2506.15025)
Append: [An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW](https://arxiv.org/abs/2506.15029)
Append: [Identifying economic narratives in large text corpora -- An integrated approach using Large Language Models](https://arxiv.org/abs/2506.15041)
Append: [SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning](https://arxiv.org/abs/2506.15154)
Append: [video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2506.15220)
Append: [When and How Unlabeled Data Provably Improve In-Context Learning](https://arxiv.org/abs/2506.15329)
Append: [Factorized RVQ-GAN For Disentangled Speech Tokenization](https://arxiv.org/abs/2506.15456)
Append: [Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework](https://arxiv.org/abs/2506.15538)
Append: [LoX: Low-Rank Extrapolation Robustifies LLM Safety Against Fine-tuning](https://arxiv.org/abs/2506.15606)
Append: [AutoRule: Reasoning Chain-of-thought Extracted Rule-based Rewards Improve Preference Learning](https://arxiv.org/abs/2506.15651)
Append: [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
Append: [Dense SAE Latents Are Features, Not Bugs](https://arxiv.org/abs/2506.15679)
Append: [HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction](https://arxiv.org/abs/2205.02225)
Append: [An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling](https://arxiv.org/abs/2402.13534)
Append: [Lean Workbook: A large-scale Lean problem set formalized from natural language math problems](https://arxiv.org/abs/2406.03847)
Append: [A Systematic Survey of Natural Language Processing for the Greek Language](https://arxiv.org/abs/2407.09861)
Append: [Robust Utility-Preserving Text Anonymization Based on Large Language Models](https://arxiv.org/abs/2407.11770)
Append: [RadioRAG: Online Retrieval-augmented Generation for Radiology Question Answering](https://arxiv.org/abs/2407.15621)
Append: [Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level](https://arxiv.org/abs/2410.06809)
Append: [Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs](https://arxiv.org/abs/2410.07009)
Append: [Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](https://arxiv.org/abs/2410.16930)
Append: [Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence](https://arxiv.org/abs/2410.17161)
Append: [LL\"aMmlein: Transparent, Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)
Append: [REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization](https://arxiv.org/abs/2412.03092)
Append: [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)
Append: [Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review](https://arxiv.org/abs/2412.18043)
Append: [Can LLMs Ask Good Questions?](https://arxiv.org/abs/2501.03491)
Append: [Perspective Transition of Large Language Models for Solving Subjective Tasks](https://arxiv.org/abs/2501.09265)
Append: [I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search](https://arxiv.org/abs/2502.14693)
Append: [CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale](https://arxiv.org/abs/2502.16645)
Append: [Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation](https://arxiv.org/abs/2502.19941)
Append: [PsychBench: A comprehensive and professional benchmark for evaluating the performance of LLM-assisted psychiatric clinical practice](https://arxiv.org/abs/2503.01903)
Append: [Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation](https://arxiv.org/abs/2503.08327)
Append: [Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing](https://arxiv.org/abs/2503.11895)
Append: [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)
Append: [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)
Append: [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
Append: [J4R: Learning to Judge with Equivalent Initial State Group Relative Policy Optimization](https://arxiv.org/abs/2505.13346)
Append: [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/abs/2505.17267)
Append: [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
Append: [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
Append: [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)
Append: [PEDANTIC: A Dataset for the Automatic Examination of Definiteness in Patent Claims](https://arxiv.org/abs/2505.21342)
Append: [How much do language models memorize?](https://arxiv.org/abs/2505.24832)
Append: [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
Append: [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
Append: [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
Append: [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
Append: [Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding](https://arxiv.org/abs/2206.13155)
Append: [OM4OV: Leveraging Ontology Matching for Ontology Versioning](https://arxiv.org/abs/2409.20302)
Append: [A Guide to Misinformation Detection Data and Evaluation](https://arxiv.org/abs/2411.05060)
Append: [Entropy-based Exploration Conduction for Multi-step Reasoning](https://arxiv.org/abs/2503.15848)
Append: [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
Append: [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
Append: [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
append_entries: 114
Finish: 2025-06-19 04:30:23.432981
------------------------------------------------------
Started: 2025-06-19 06:25:12.483518
Existing_entries: 1114
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
Token length: 1543
Summarized using GPT-3.5-turbo
Append: [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
append_entries: 2
Finish: 2025-06-19 06:25:15.646540
------------------------------------------------------
Started: 2025-06-19 08:23:00.473611
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 08:23:00.759202
------------------------------------------------------
Started: 2025-06-19 10:18:21.199639
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 10:18:21.476186
------------------------------------------------------
Started: 2025-06-19 12:35:04.321412
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 12:35:04.641154
------------------------------------------------------
Started: 2025-06-19 14:15:59.158276
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 14:15:59.441146
------------------------------------------------------
Started: 2025-06-19 16:20:31.477448
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 16:20:31.768552
------------------------------------------------------
Started: 2025-06-19 18:22:22.826300
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 18:22:23.136957
------------------------------------------------------
Started: 2025-06-19 20:19:40.338760
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 20:19:40.666703
------------------------------------------------------
Started: 2025-06-19 22:15:56.631994
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-19 22:15:56.915795
------------------------------------------------------
Started: 2025-06-20 01:20:32.802720
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 01:20:33.090081
------------------------------------------------------
Started: 2025-06-20 03:14:37.109688
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 03:14:37.473648
------------------------------------------------------
Started: 2025-06-20 04:24:07.088822
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 04:24:07.149362
------------------------------------------------------
Started: 2025-06-20 06:25:14.333314
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 06:25:14.414442
------------------------------------------------------
Started: 2025-06-20 08:22:20.292950
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 08:22:20.356995
------------------------------------------------------
Started: 2025-06-20 10:18:15.119469
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 10:18:15.195183
------------------------------------------------------
Started: 2025-06-20 12:35:01.974185
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 12:35:02.049921
------------------------------------------------------
Started: 2025-06-20 14:16:16.518692
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 14:16:16.579628
------------------------------------------------------
Started: 2025-06-20 16:20:52.062176
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 16:20:52.139675
------------------------------------------------------
Started: 2025-06-20 18:22:54.087380
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 18:22:54.205989
------------------------------------------------------
Started: 2025-06-20 20:18:25.833157
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 20:18:25.894284
------------------------------------------------------
Started: 2025-06-20 22:15:56.021602
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-20 22:15:56.079643
------------------------------------------------------
Started: 2025-06-21 01:19:37.418597
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 01:19:37.477271
------------------------------------------------------
Started: 2025-06-21 03:10:42.750228
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 03:10:42.840465
------------------------------------------------------
Started: 2025-06-21 04:20:31.525742
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 04:20:31.588142
------------------------------------------------------
Started: 2025-06-21 06:22:24.325548
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 06:22:24.418093
------------------------------------------------------
Started: 2025-06-21 08:20:22.672897
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 08:20:22.767589
------------------------------------------------------
Started: 2025-06-21 10:16:07.977687
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 10:16:08.044368
------------------------------------------------------
Started: 2025-06-21 12:31:03.698777
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 12:31:03.807688
------------------------------------------------------
Started: 2025-06-21 14:15:32.980635
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 14:15:33.051506
------------------------------------------------------
Started: 2025-06-21 16:19:22.967227
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 16:19:23.025289
------------------------------------------------------
Started: 2025-06-21 18:20:50.235298
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 18:20:50.312556
------------------------------------------------------
Started: 2025-06-21 20:16:57.679019
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 20:16:57.740157
------------------------------------------------------
Started: 2025-06-21 22:14:58.307841
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-21 22:14:58.384846
------------------------------------------------------
Started: 2025-06-22 01:28:15.958453
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 01:28:16.017776
------------------------------------------------------
Started: 2025-06-22 03:23:13.160309
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 03:23:13.219590
------------------------------------------------------
Started: 2025-06-22 04:28:00.002643
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 04:28:00.128051
------------------------------------------------------
Started: 2025-06-22 06:23:04.042004
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 06:23:04.128212
------------------------------------------------------
Started: 2025-06-22 08:20:20.025841
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 08:20:20.088587
------------------------------------------------------
Started: 2025-06-22 10:16:47.223640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 10:16:47.279912
------------------------------------------------------
Started: 2025-06-22 12:31:10.869150
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 12:31:10.968034
------------------------------------------------------
Started: 2025-06-22 14:14:29.745239
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 14:14:29.816109
------------------------------------------------------
Started: 2025-06-22 16:19:13.016825
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 16:19:13.130723
------------------------------------------------------
Started: 2025-06-22 18:21:35.481086
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 18:21:35.575113
------------------------------------------------------
Started: 2025-06-22 20:17:26.769902
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 20:17:26.867949
------------------------------------------------------
Started: 2025-06-22 22:15:10.386097
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-22 22:15:10.485625
------------------------------------------------------
Started: 2025-06-23 01:25:44.551640
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 01:25:44.614689
------------------------------------------------------
Started: 2025-06-23 03:23:59.220343
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 03:23:59.282722
------------------------------------------------------
Started: 2025-06-23 04:34:43.213992
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 867
Summarized using GPT-3.5-turbo
Append: [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
Token length: 699
Summarized using GPT-3.5-turbo
Append: [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
Token length: 1664
Summarized using GPT-3.5-turbo
Append: [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
Token length: 873
Summarized using GPT-3.5-turbo
Append: [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
Token length: 1199
Summarized using GPT-3.5-turbo
Append: [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
Token length: 1172
Summarized using GPT-3.5-turbo
Append: [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
Token length: 1212
Summarized using GPT-3.5-turbo
Append: [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
Token length: 1134
Summarized using GPT-3.5-turbo
Append: [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
Token length: 998
Summarized using GPT-3.5-turbo
Append: [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
Token length: 1345
Summarized using GPT-3.5-turbo
Append: [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
Token length: 1378
Summarized using GPT-3.5-turbo
Append: [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
Token length: 1264
Summarized using GPT-3.5-turbo
Append: [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
Token length: 803
Summarized using GPT-3.5-turbo
Append: [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
Token length: 1380
Summarized using GPT-3.5-turbo
Append: [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
Token length: 1614
Summarized using GPT-3.5-turbo
Append: [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
Token length: 1103
Summarized using GPT-3.5-turbo
Append: [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
Token length: 1553
Summarized using GPT-3.5-turbo
Append: [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
Token length: 1642
Summarized using GPT-3.5-turbo
Append: [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
Token length: 1480
Summarized using GPT-3.5-turbo
Append: [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
Token length: 1466
Summarized using GPT-3.5-turbo
Append: [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
Token length: 1260
Summarized using GPT-3.5-turbo
Append: [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
Token length: 704
Summarized using GPT-3.5-turbo
Append: [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
Token length: 576
Summarized using GPT-3.5-turbo
Append: [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
Token length: 1106
Summarized using GPT-3.5-turbo
Append: [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
Token length: 1046
Summarized using GPT-3.5-turbo
Append: [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
Token length: 1042
Summarized using GPT-3.5-turbo
Append: [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
Token length: 1110
Summarized using GPT-3.5-turbo
Append: [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
Token length: 1205
Summarized using GPT-3.5-turbo
Append: [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
Token length: 970
Summarized using GPT-3.5-turbo
Append: [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
Token length: 926
Summarized using GPT-3.5-turbo
Append: [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
Token length: 1086
Summarized using GPT-3.5-turbo
Append: [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
Token length: 1670
Summarized using GPT-3.5-turbo
Append: [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
Token length: 1220
Summarized using GPT-3.5-turbo
Append: [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
Token length: 669
Summarized using GPT-3.5-turbo
Append: [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
Token length: 1262
Summarized using GPT-3.5-turbo
Append: [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
Token length: 1711
Summarized using GPT-3.5-turbo
Append: [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
Token length: 1067
Summarized using GPT-3.5-turbo
Append: [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
Token length: 997
Summarized using GPT-3.5-turbo
Append: [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
Token length: 1186
Summarized using GPT-3.5-turbo
Append: [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
Token length: 1947
Summarized using GPT-3.5-turbo
Append: [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
Token length: 1473
Summarized using GPT-3.5-turbo
Append: [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
Token length: 1204
Summarized using GPT-3.5-turbo
Append: [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
Token length: 1390
Summarized using GPT-3.5-turbo
Append: [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
Token length: 1028
Summarized using GPT-3.5-turbo
Append: [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
Token length: 1019
Summarized using GPT-3.5-turbo
Append: [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
Token length: 665
Summarized using GPT-3.5-turbo
Append: [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
Token length: 1682
Summarized using GPT-3.5-turbo
Append: [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
Token length: 1241
Summarized using GPT-3.5-turbo
Append: [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
Token length: 1934
Summarized using GPT-3.5-turbo
Append: [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
Token length: 1230
Summarized using GPT-3.5-turbo
Append: [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
Append: [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
Append: [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
Append: [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
Append: [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
Append: [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
Append: [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
Append: [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
Append: [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
Append: [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
Append: [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
Append: [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
Append: [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
Append: [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
Append: [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
Append: [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
Append: [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
Append: [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
Append: [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
Append: [Instituto de Telecomunica\c{c}\~oes at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
Append: [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
Append: [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
Append: [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
Append: [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
Append: [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
Append: [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
Append: [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
Append: [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
Append: [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
Append: [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
Append: [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
Append: [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
Append: [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
Append: [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
Append: [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
Append: [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
Append: [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
Append: [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
Append: [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
Append: [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
Append: [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
Append: [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
Append: [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
Append: [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
Append: [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
Append: [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
Append: [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
Append: [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
Append: [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
Append: [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
Append: [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
Append: [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
Append: [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
Append: [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
Append: [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
Append: [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
Append: [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
Append: [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
Append: [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
Append: [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
Append: [Voices of Her: Analyzing Gender Differences in the AI Publication World](https://arxiv.org/abs/2305.14597)
Append: [LaRS: Latent Reasoning Skills for Chain-of-Thought Reasoning](https://arxiv.org/abs/2312.04684)
Append: [AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability](https://arxiv.org/abs/2402.09404)
Append: [A Survey of Automatic Hallucination Evaluation on Natural Language Generation](https://arxiv.org/abs/2404.12041)
Append: [BEADs: Bias Evaluation Across Domains](https://arxiv.org/abs/2406.04220)
Append: [Learning to Refine with Fine-Grained Natural Language Feedback](https://arxiv.org/abs/2407.02397)
Append: [sPhinX: Sample Efficient Multilingual Instruction Fine-Tuning Through N-shot Guided Prompting](https://arxiv.org/abs/2407.09879)
Append: [Contextual modulation of language comprehension in a dynamic neural model of lexical meaning](https://arxiv.org/abs/2407.14701)
Append: [Deep Learning based Visually Rich Document Content Understanding: A Survey](https://arxiv.org/abs/2408.01287)
Append: [Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives](https://arxiv.org/abs/2408.06904)
Append: [Can Large Language Models Replace Human Subjects? A Large-Scale Replication of Scenario-Based Experiments in Psychology and Management](https://arxiv.org/abs/2409.00128)
Append: [Core Knowledge Deficits in Multi-Modal Language Models](https://arxiv.org/abs/2410.10855)
Append: [SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments](https://arxiv.org/abs/2410.11331)
Append: [Learning to Route LLMs with Confidence Tokens](https://arxiv.org/abs/2410.13284)
Append: [Principles of semantic and functional efficiency in grammatical patterning](https://arxiv.org/abs/2410.15865)
Append: [Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models](https://arxiv.org/abs/2411.04291)
Append: [Song Form-aware Full-Song Text-to-Lyrics Generation with Multi-Level Granularity Syllable Count Control](https://arxiv.org/abs/2411.13100)
Append: [Incivility and Rigidity: The Risks of Fine-Tuning LLMs for Political Argumentation](https://arxiv.org/abs/2411.16813)
Append: [On Domain-Adaptive Post-Training for Multimodal Large Language Models](https://arxiv.org/abs/2411.19930)
Append: [Think&Cite: Improving Attributed Text Generation with Self-Guided Tree Search and Progress Reward Modeling](https://arxiv.org/abs/2412.14860)
Append: [Theoretical Guarantees for Minimum Bayes Risk Decoding](https://arxiv.org/abs/2502.12685)
Append: [Knapsack Optimization-based Schema Linking for LLM-based Text-to-SQL Generation](https://arxiv.org/abs/2502.12911)
Append: [Group-Level Data Selection for Efficient Pretraining](https://arxiv.org/abs/2502.14709)
Append: [From RAG to Memory: Non-Parametric Continual Learning for Large Language Models](https://arxiv.org/abs/2502.14802)
Append: [Batayan: A Filipino NLP benchmark for evaluating Large Language Models](https://arxiv.org/abs/2502.14911)
Append: [Uncertainty Quantification in Retrieval Augmented Question Answering](https://arxiv.org/abs/2502.18108)
Append: [olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models](https://arxiv.org/abs/2502.18443)
Append: [FRIDA to the Rescue! Analyzing Synthetic Data Effectiveness in Object-Based Common Sense Reasoning for Disaster Response](https://arxiv.org/abs/2502.18452)
Append: [Large-Scale Data Selection for Instruction Tuning](https://arxiv.org/abs/2503.01807)
Append: [AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation](https://arxiv.org/abs/2503.02832)
Append: [Coreference as an indicator of context scope in multimodal narrative](https://arxiv.org/abs/2503.05298)
Append: [Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models](https://arxiv.org/abs/2503.05328)
Append: [QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation](https://arxiv.org/abs/2503.05888)
Append: [LLMs in Disease Diagnosis: A Comparative Study of DeepSeek-R1 and O3 Mini Across Chronic Health Conditions](https://arxiv.org/abs/2503.10486)
Append: [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/abs/2503.11280)
Append: [Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content](https://arxiv.org/abs/2503.16031)
Append: [CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness](https://arxiv.org/abs/2504.05154)
Append: [TALE: A Tool-Augmented Framework for Reference-Free Evaluation of Large Language Models](https://arxiv.org/abs/2504.07385)
Append: [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)
Append: [Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)
Append: [ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization](https://arxiv.org/abs/2505.02819)
Append: [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)
Append: [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)
Append: [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)
Append: [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)
Append: [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
Append: [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/abs/2505.18110)
Append: [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
Append: [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
Append: [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
Append: [CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models](https://arxiv.org/abs/2506.01495)
Append: [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
Append: [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
Append: [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
Append: [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
Append: [Techniques for supercharging academic writing with generative AI](https://arxiv.org/abs/2310.17143)
Append: [Alto: Orchestrating Distributed Compound AI Systems with Nested Ancestry](https://arxiv.org/abs/2403.04311)
Append: [PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental Learning for Document Retrieval](https://arxiv.org/abs/2406.12593)
Append: [Cost-effective Instruction Learning for Pathology Vision and Language Analysis](https://arxiv.org/abs/2407.17734)
Append: [xGen-MM (BLIP-3): A Family of Open Large Multimodal Models](https://arxiv.org/abs/2408.08872)
Append: [MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension](https://arxiv.org/abs/2409.13609)
Append: [COS-DPO: Conditioned One-Shot Multi-Objective Fine-Tuning Framework](https://arxiv.org/abs/2410.08316)
Append: [ALTA: Compiler-Based Analysis of Transformers](https://arxiv.org/abs/2410.18077)
Append: [Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation](https://arxiv.org/abs/2411.00412)
Append: [A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning](https://arxiv.org/abs/2411.04105)
Append: [Watermarking Language Models through Language Models](https://arxiv.org/abs/2411.05091)
Append: [Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation](https://arxiv.org/abs/2411.05261)
Append: [Quantifying artificial intelligence through algorithmic generalization](https://arxiv.org/abs/2411.05943)
Append: [On the Limits of Language Generation: Trade-Offs Between Hallucination and Mode Collapse](https://arxiv.org/abs/2411.09642)
Append: [Multi-Preference Optimization: Generalizing DPO via Set-Level Contrasts](https://arxiv.org/abs/2412.04628)
Append: [AutoPresent: Designing Structured Visuals from Scratch](https://arxiv.org/abs/2501.00912)
Append: [Ladder-residual: parallelism-aware architecture for accelerating large model inference with communication overlapping](https://arxiv.org/abs/2501.06589)
Append: [Rethinking External Slow-Thinking: From Snowball Errors to Probability of Correct Reasoning](https://arxiv.org/abs/2501.15602)
Append: [On Almost Surely Safe Alignment of Large Language Models at Inference-Time](https://arxiv.org/abs/2502.01208)
Append: [Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2502.14321)
Append: [FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation](https://arxiv.org/abs/2503.06680)
Append: [LLM-Guided Indoor Navigation with Multimodal Map Understanding](https://arxiv.org/abs/2503.11702)
Append: [DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents](https://arxiv.org/abs/2503.23804)
Append: [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
Append: [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
Append: [On Path to Multimodal Historical Reasoning: HistBench and HistAgent](https://arxiv.org/abs/2505.20246)
Append: [RefAV: Towards Planning-Centric Scenario Mining](https://arxiv.org/abs/2505.20981)
Append: [UniWorld-V1: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
Append: [CIVET: Systematic Evaluation of Understanding in VLMs](https://arxiv.org/abs/2506.05146)
Append: [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/abs/2506.05333)
append_entries: 194
Finish: 2025-06-23 04:36:13.305513
------------------------------------------------------
Started: 2025-06-23 06:27:09.102080
Existing_entries: 1194
Fetching from https://rss.arxiv.org/rss/cs.CL
Token length: 1310
Summarized using GPT-3.5-turbo
Append: [LogProber: Disentangling confidence from contamination in LLM responses](https://arxiv.org/abs/2408.14352)
Token length: 1402
Summarized using GPT-3.5-turbo
Append: [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
append_entries: 2
Finish: 2025-06-23 06:27:13.239749
------------------------------------------------------
Started: 2025-06-23 08:24:30.708759
Existing_entries: 1002
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 08:24:31.130028
------------------------------------------------------
Started: 2025-06-23 10:19:20.968776
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 10:19:21.415369
------------------------------------------------------
Started: 2025-06-23 12:36:21.435869
Existing_entries: 1000
Fetching from https://rss.arxiv.org/rss/cs.CL
append_entries: 0
Finish: 2025-06-23 12:36:21.920319
